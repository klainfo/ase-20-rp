Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
CLASS,lucene-4.0,LUCENE-4461,2012-10-05T10:21:38.000-05:00,create inconsistent results,"FacetSearchParams facetSearchParams = new FacetSearchParams();
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
Multiple FacetRequest are getting merged into one creating wrong results in this case:",org.apache.lucene.facet.search.StandardFacetsAccumulator
CLASS,tika-1.3,TIKA-1152,2013-07-23T08:45:11.000-05:00,process loops on parsing parse of CHM file,"{code}
 
    
     
    
    
    
    
    
    
    
    
 {code}
By parsing [the attachment CHM file|^eventcombmt.chm] (MS Microsoft Help Files), Java process stuck.",tika-parsers.src.main.java.org.apache.tika.parser.chm.lzx.ChmLzxBlock
FILE,DATAMONGO,DATAMONGO-467,2012-06-24T08:58:49.000-05:00,use QueryDSL,"@Document 
 @Id String id;






 
 QUser.id.eq(""4f43b6a384aea4e77d403709"")
Using this entity (@Document) definition with a String as the declared ID:
and the following query
not translate String 4f43b6a384aea4e77d403709 to ObjectId(""4f43b6a384aea4e77d403709"") look at mongoDb query do in mongo shell",org.springframework.data.mongodb.repository.support.SpringDataMongodbSerializerUnitTests
FILE,DATAMONGO,DATAMONGO-505,2012-08-14T03:07:56.000-05:00,not work for collection values,"class Entity {









  Long id;




  @DBRef




  Property property;




}









 class Property {




  Long id;




}









 interface EntityRepository extends Repository<Entity, Long> {









  Entity findByPropertyIn(Property... property);




}






  findByProperty()
Assume you have the following scenario:
not translate given array into collection
treat value create DBRef","org.springframework.data.mongodb.repository.query.ConvertingParameterAccessor
org.springframework.data.mongodb.repository.query.ConvertingParameterAccessorUnitTests"
FILE,DATAMONGO,DATAMONGO-523,2012-09-01T03:39:51.000-05:00,not use with AbstractMongoConfiguration,"@TypeAlias      @Document  @TypeAlias
When using the AbstractMongoConfiguration without any further modifications regarding the converter (afterMappingMongoConverterCreation) the @TypeAlias annotation is not used when writing the _class property.
use SimpleTypeInformationMapper",org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
FILE,DATAMONGO,DATAMONGO-585,2012-12-01T08:28:43.000-06:00,exception during authentication,"class which implements Runnable.  
Those
Specifically what can be used for a test case is use a ThreadPoolExecutor, set a minpool and maxpool value of 5, add 25 objects to be executed.
Those objects are a class which implements Runnable.
Those objects each then perform a bunch of inserts into mongoDB using a DocumentDao which calls mongoOperations.insert.",org.springframework.data.mongodb.core.MongoDbUtils
FILE,DATAMONGO,DATAMONGO-629,2013-03-22T04:08:25.000-05:00,count with field find with field,"Query q = query 
    
  
 
 
 {




		""id"" : /zzz/




	} 
 
 
 
 
 
  
  
 
 
 {




		""count"" : ""test"",




		""query"" : {




			""_id"" : /zzz/




		}




	 
 
 
 
 
 
     find()     count()
Assume we have following query:
use in count )
have records with field id retrieve from db
give bad results use _ id field
not use QueryMapper not use count()","org.springframework.data.mongodb.core.mapping.MongoMappingContext
org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-571,2012-11-09T08:00:10.000-06:00,not save null values add @Version to domain class,"Scenario 
 CrudRepository.findOne()  
 @Version 
 CrudRepository.save()  
 @Version
1. Domain class is loaded from mongodb using CrudRepository.findOne() method.
2. The loaded instances any non id nor @Version annotated field is set to null.
3. The loaded instance is saved to same mongodb using CrudRepository.save() method.
4. The field that has been set to null doesnt write to database, its unchanged.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.Update"
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,update object not write type information for objects,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
I'm using quite complex domain model, that consist of instantiable domain classes as well as of abstract ones.
use 1.0.0
read from database store m5 version with object store type information with object
break application break RELEASE version save objects without type information read back to java model
call addCustomTypeIfNecessary(...) in turn call MappingMongoConverter.writeInternal(...) method in turn put type information into DBObject put addCustomTypeIfNecessary(...) into DBObject
call removeTypeInfoRecursively(...) during execution save under _ class key","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-702,2013-06-20T00:12:30.000-05:00,not configure spring Data MongoDB projection with respective Java pojo,"@Field 
 query.fields() 
  
 @Field
If Model Java POJO, is annotated with @Field, then during search with query.fields().
include(field)
uning mongoTemplate it does not java property field as field parameter, rather it needs the name declared with name defined with @Field annotation.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-717,2013-07-10T11:13:46.000-05:00,not distribute application context to persistent entities,"@Override




	protected <T> BasicMongoPersistentEntity<T> createPersistentEntity(TypeInformation<T> typeInformation) {









		BasicMongoPersistentEntity<T> entity = new BasicMongoPersistentEntity<T>(typeInformation);









		if (context != null) {




			entity.setApplicationContext(context);




		}









		return entity;




	}









	




	 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




	}






 
  
 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




                // Send the application context to ALL the PersistentEntities, not just ones created after this point




               for (BasicMongoPersistentEntity entity : getPersistentEntities()) {




                   entity.setApplicationContext(applicationContext);




               }




	}






      testMultiTenantSave()  
   initialize()    
 
 @Bean




	public MongoMappingContext mongoMappingContext() throws ClassNotFoundException {









		MongoMappingContext mappingContext = new MongoMappingContext();




		mappingContext.setInitialEntitySet(getInitialEntitySet());




		mappingContext.setSimpleTypeHolder(customConversions().getSimpleTypeHolder());




		mappingContext.initialize(); // <----









		return mappingContext;




	}
not distribute application context set to persistent entities
Current code:",org.springframework.data.mongodb.config.AbstractMongoConfigurationUnitTests
FILE,DATAMONGO,DATAMONGO-721,2013-07-11T11:36:06.000-05:00,persist on update operations,"@Document
public class ParentClass {
   private List<ChildClass> list;
}
    @Document   
        
  
 mongoTemplate.updateFirst(Query.query(criteria), 
  new Update().push(""list"", child));
Here is our situation: we have an entity which have an attribute which is a list of another kind of entity, like the code below.
When using MongoTemplate class with code such as below, the _class attribute is not inserted on the embedded document, so, if one of the items of the list attribute is a subclass of ChildClass, and ChildClass is an abstract class, we begin to face instantiation problems.
Here is one example of usage of MongoTemplate in which we found a problem.
not add _ class attribute to embedded document",org.springframework.data.mongodb.core.convert.QueryMapper
FILE,DATAMONGO,DATAMONGO-602,2013-01-30T02:22:53.000-06:00,Querying with $in operator on the id field of type BigInteger returns zero results,"List<BigInteger> profileIds = findProfileIds();




Predicate predicate = QProfileDocument.profileDocument.id.in(profileIds);




Iterable<ProfileDocument> profiles = profileRepository.findAll(predicate);
There is a problem when trying to query for documents with id field is in a given list of BigInteger values.
For example, these lines will work only if given id list contains only one item.
return results in first case return results in first case work Iterable with item work Iterable in second return Iterable with item return Iterable in second",org.springframework.data.mongodb.core.MongoTemplateTests
FILE,DATAMONGO,DATAMONGO-805,2013-12-02T06:34:36.000-06:00,Excluding DBRef field in a query causes a MappingException,"Query query = new Query(Criteria.where(""parentField"").is(""test""));
        query.fields().exclude(""children"");
        ParentClass parentClass = mongoOperations.findOne(query, ParentClass.class);
Excluding a field in a query where the field is a DBRef as below throws a MappingException.
I've attached a simple test case that throws the MappingException.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.mapping.MappingTests
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-897,2014-04-01T04:38:51.000-05:00,use @DbRef as target use interface as target,"MongoTemplate.findAndModify(...)   @DbRef  @DbRef
NullPointerException is thrown when using MongoTemplate.findAndModify(...) with @DbRef and interface as @DbRef target.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.QueryMapper"
FILE,DATAMONGO,DATAMONGO-892,2014-03-28T09:08:03.000-05:00,not configure as nested bean definition,"parserContext.isNested()
The sample config which worked in the 1.1.1 version, but doesn't work now:
not check if returns null not check if parserContext.isNested() BeanDefinition","org.springframework.data.mongodb.config.MappingMongoConverterParser
org.springframework.data.mongodb.config.MappingMongoConverterParserIntegrationTests"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,ignore @Field annotation for field alias,"@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
I created a method using the ""query by method name"" approach:
Inside my Answer object, I have a field called ""Score"" that is annotated with
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-938,2014-05-21T06:09:48.000-05:00,Exception when creating geo within Criteria using MapReduce,"Criteria.where(""location"")  within(new Box(lowerLeft, upperRight));
I am getting an IllegalArgumentException when I try to query a MongoDB collection using a Criteria.within and a Box.","org.springframework.data.mongodb.core.mapreduce.MapReduceTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-952,2014-06-10T22:45:47.000-05:00,not work with only field restrictions,"@Query 
 @Query(fields = ""{ 'email' : 1 }"")




User findByEmail(String email)






  @Query
If you are using repository based queries and try to use @Query annotation to limit the fetched fields, it has zero effect.
For example repository query:
return fields of User have effect
If you are using @Query with value attribute to define the query, then the fields limitation is applied though.",org.springframework.data.mongodb.repository.query.PartTreeMongoQuery
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,Problem with lazy loading in @DBRef when getting data using MongoTemplate,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
The situation is simple: if we reference on an entity class another entity (both annotated with @Document) called Parent and Child.
Here is the code:
// Than we try to load this same entity from the database
attach project with junit test reproduce project with junit test","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-1068,2014-10-08T19:43:32.000-05:00,build special cirteria,"public class Room {




		private String name;




		private List<Date> occupied;




	}






 
 {




		occupied : {




			$not : {




				$elemMatch : {




					$gte : start,




					$lte : end




				}




			}




		}




	}






 
 Criteria c1 = new Criteria().gte(start).lte(end);




	Criteria c = Criteria.where(""occupied"").not().elemMatch(c1);






 
 {




	occupied : {




		$not : {




			$elemMatch : {




			}




		}




	}




}






  elemMatch(Criteria)
There is an entity like this:
I need to issue a criteria to fetch all documents in which none of elements of occupied falls into a specified date range.
The query JSON is:
Then, I tried on
empty map by invoking invoke elemMatch(Criteria) not assign key","org.springframework.data.mongodb.core.query.CriteriaTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-1078,2014-10-28T02:23:26.000-05:00,map complex id structure,"@Query(""{'_id': {$in: ?0}}"")




List<User> findByUserIds(Collection<MyUserId> userIds) 
 {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}
convert complex object
Therefore annotated queries like:","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,not remove _ class property on collection,"@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
Following method on repository
generate incorrect query
I attached test project demonstrating this bug.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,not return matching elements return max of documents,"public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
I have the following query:
expect matching documents
restrict result",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,keyword query findByInId with pageable,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
I've been trying to use the In-keyword with identifiers and making the query pageable.
Also when you try to get any other page than 0, no results return.
Below is a strip down example I used for testing;
I've created 3 types and 10 items per those types, results in a total of 30 items.
return //
return //","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1250,2015-07-03T21:07:44.000-05:00,not use in updates,"@Document 
 
 
 @Document




public class MyPersistantObject  
 public Allocation allocation;




     public BigDecimal value;









     
 private final String code;









         Allocation(String code) {




            this.code = code;




        }









         public static Converter<Allocation, String> writer() {




            return new Converter<Allocation, String>() {




                public String convert(Allocation allocation) {




                    return allocation.getCode();




                }




            };




        }









         public static Converter<String, Allocation> reader() {




            return new Converter<String, Allocation>() {




                public Allocation convert(String source) {




                    return Allocation.getByCode(source);




                }




            };




        }









         public static Allocation getByCode(String code)  
 return AVAILABLE;




                 
 return ALLOCATED;




             
 throw new IllegalArgumentException(""Unable to get Allocation from: "" + code);




         
 public String getCode() {




            return code;




        }




     
 @Bean




    public CustomConversions customConversions() {




        return new CustomConversions(Arrays.asList(




                MyPersistantObject.Allocation.reader(),




                MyPersistantObject.Allocation.writer()




        ));




    }






 
 @Test




    public void testConversion() {




        Update update;




        Query query;




        MyPersistantObject returned;




        MyPersistantObject myPersistantObject = new MyPersistantObject();




        myPersistantObject.allocation = AVAILABLE;




        myPersistantObject.value = new BigDecimal(1234567);









        mongoTemplate.save(myPersistantObject);









        // Check it was saved correctly - first with invalid allocation to confirm conversion in query




        query = query(where(""allocation"").is(ALLOCATED));




        assertThat(mongoTemplate.findOne(query, MyPersistantObject.class), is(nullValue()));









        // Check it was saved correctly - now with valid allocation to confirm conversion in query




        query = query(where(""allocation"").is(AVAILABLE));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(AVAILABLE));




        assertThat(returned.value.longValue(), is(1234567L));









        try {




            // Update allocation from constant - will fail




            update = update(""allocation"", ALLOCATED);




            mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        } catch (Exception e) {




            System.err.println(""failed to convert allocation: java.lang.IllegalArgumentException: can't serialize class converter_test.MyPersistantObject$Allocation"");




        }









        // Update allocation from string value - succeeds




        update = update(""allocation"", ALLOCATED.getCode());




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check allocation update




        query = query(where(""allocation"").is(ALLOCATED));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(ALLOCATED));









        // Update value only - will fail: Caused by: java.lang.IllegalArgumentException: Unable to get MyPersistantObject.Allocation from: 54321




        // Tries to use MyPersistantObject.Allocation converter to String




        update = update(""value"", new BigDecimal(54321));




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check value update




        returned = mongoTemplate.findAll(MyPersistantObject.class).get(0);




        assertThat(returned.value.longValue(), is(54321L));




    }
I have a custom (de)serialiser for an enumerated type, and it works perfectly when saving and loading a @Document annotated POJO.
build Query execute Query
However when used in an Update, it is either ignored, or called in situations where it shouldn't.
Please clone https://github.com/patrickherrera/converter_test.git for a full test application.
In brief there is a POJO and for the purposes of the test it has a static enum with the desired converters:
There is a unit test that drives a few scenarios:
appear by use use in Query builder
come to update throw exception to effect
convert from Enum not call customer converter in situation not call customer converter for converting
try numeric String into Allocation enum convert numeric String into Allocation enum convert BigDecimal to String fail Allocation enum of course
not use Allocation converter call in MappingMongoConverter
use first converter handle input type handle first converter seem without type information be in case","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.UpdateMapper"
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,involve generic types,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
involve generic types not infer type information at startup time result in missing indexes
Given:
When defining a class Catalog with a list of ""wrapped"" books:
infer type infromation from ProductWrapper class definition inherit from AbstractProduct create index name inside catalog define on Book class not create Book class as Spring data Mongo
define wrapper class as ProductWrapper<T> create indexes on Catalog.books2.content.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1360,2016-01-16T07:47:34.000-06:00,not query with JSR310,"query.addCriteria(where(""createdDate"").lte(LocalDateTime.now()));
I have a MongoDb document I successfully store using Spring Data MongoDb.
It looks like this:
When I create a custom Criteria query that looks like this:
fail with message
use java.util.Date in query persist document with java.time.LocalDateTime object","org.springframework.data.mongodb.core.Venue
org.springframework.data.mongodb.core.geo.AbstractGeoSpatialTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1288,2015-09-16T23:51:16.000-05:00,work with AtomicInteger,"org.springframework.data.mongodb.core.query.Update.inc(String, Number)
take parameter as number not work with things
For example, if you pass in an AtomicInteger, the update fails to execute, as the generated json isn't correct.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters
org.springframework.data.mongodb.core.convert.CustomConversions"
FILE,DATAMONGO,DATAMONGO-1438,2016-05-26T14:01:14.000-05:00,get warning in logs switch in Spring boot 1.3.5 switch to Spring,"@Document
When I start my Spring Boot 1.3.5 application with no custom conversions and with Spring Data MongoDB Release Train Hopper-SR1 I get following warning in my logs:
I have alle my Domain classes they are saved in MongoDB annotated with @Document (see DATAMONGO-1413)","org.springframework.data.mongodb.core.convert.MongoConvertersUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,not use @Field field name nest fields with nested keywords nest fields in combination,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
we have a document class;
In MongoTemplate.java, the call to :
resolve to stat
The query submitted to mongo after getMappedObject is called:
not get data call resolution
Note: The query input to getMappedObject is:
convert to value
operate queries on fields","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
CLASS,derby-10.7.1.1,DERBY-4835,2010-10-06T11:05:13.000-05:00,cause java.lang.NoSuchMethodError not recompile with upgrade not recompile from 10.5.3.0 not recompile to 10.6.1.0,"tidlggls(blt_number,create_date,update_date,propagation_date,glossary_status,
     time_stamp,min_max_size )
    
      
 
  
 tidlrblt(BLT,BLT_SIZE,MIN_MAX_SIZE)  
 
     
  
   GeneratedMe
thod;    
  
  
 if (fromVersion.majorVersionNumber >= DataDictionary.DD_VERSION_DERBY_10_5)
				bootingDictionary.updateMetadataSPSes(tc);
			else
				bootingDictionary.clearSPSPlans();

  clearSPSPlans()
cause following exception not recompile to 10.6.1.0 not recompile on upgrade fire trigger after upgrade fire first time after upgrade
To reproduce, run the attached script 10_5_3_work.sql with the 10.5.3.0  release and then connect with 10.6.1.0 and insert into the table with the trigger:","org.apache.derby.impl.sql.catalog.DD_Version
org.apache.derbyTesting.functionTests.tests.upgradeTests.BasicSetup"
CLASS,derby-10.7.1.1,DERBY-4889,2010-11-05T20:06:56.000-05:00,boolean conversion on embedded boolean conversion on client,"PreparedStatement ps = c.prepareStatement(""values cast(? as boolean)"");
        ps.setByte(1, (byte) 32);
        ResultSet rs = ps.executeQuery();
        rs.next();
        System.out.println(rs.getBoolean(1));

 If setByte()   setInt()
The following code prints ""true"" with the embedded driver and ""false"" with the client driver:
If setByte() is replaced with setInt(), they both print ""true"".","org.apache.derbyTesting.functionTests.tests.jdbcapi.ParameterMappingTest
org.apache.derby.impl.drda.DRDAConnThread"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,create wrong schema after dereferencing dereference nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
The following script fails:","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2828,2012-07-19T05:03:16.000-05:00,null in DataType.compare,"Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);
While using TOP, and if the DataBag contains null value to compare, it will generate the following exception:","src.org.apache.pig.data.DataType
src.org.apache.pig.builtin.TOP
test.org.apache.pig.test.TestNull"
CLASS,pig-0.11.1,PIG-3114,2013-01-03T19:49:42.000-06:00,duplicate macro name error use pigunit,"{code:title=test.pig|borderStyle=solid}
    {
    $C = ORDER $QUERY BY total DESC, $A;
}  
  
     AS total;

queries_ordered = my_macro_1(queries_count, query);

    
   ;
{code}
I'm using PigUnit to test a pig script within which a macro is defined.
get parsing error with pigunit
Pig script which is failing :","src.org.apache.pig.PigServer
test.org.apache.pig.test.pigunit.TestPigTest
test.org.apache.pig.pigunit.PigTest
test.org.apache.pig.pigunit.pig.PigServer"
CLASS,pig-0.11.1,PIG-3267,2013-04-03T16:14:30.000-05:00,fail in limit query,"{code}
 
  
  
     ;
{code}

 
 {code}
  {code}
The following query fail:
launch second job happen before launching","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.11.1,PIG-3292,2013-04-24T03:06:41.000-05:00,get cross product,"{code}
 
  
   {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}

 
 {code}
   
 {code}

 
 {code}
 
  
   {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}
The problem:
We want to do self join to get cross-product
{code}
a = load '/input' as (key, x);","test.org.apache.pig.test.TestEvalPipelineLocal
src.org.apache.pig.newplan.logical.relational.LOCross"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,not generate new uids for nested schema fields lead to miscomputations,"{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
Consider the following example
On input data:
{code}
1       1001    101
1       1002    103
1       1003    102
1       1004    102
2       1005    101
2       1003    101
2       1002    123
3       1042    101
3       1005    101
3       1002    133
{code}
give wrongful output",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,pig-0.11.1,PIG-3316,2013-05-08T14:01:36.000-05:00,interpret DateTime values in special cases,";
dump A;
For the query
with input data
1 1970-01-01
2 1970-01
generate following output
interpret day month part as time zone","test.org.apache.pig.test.TestDefaultDateTimeZone
src.org.apache.pig.builtin.ToDate"
CLASS,pig-0.11.1,PIG-3329,2013-05-16T22:44:41.000-05:00,work with SPLIT,"RANK b;
dump d;
input.txt:
1 2 3
4 5 6
7 8 9
script:
a = load 'input.txt' using PigStorage(' ') as (a:int, b:int, c:int);
SPLIT a into b if a > 0, c if a > 5;
d = RANK b;
dump d;
fail with error message","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.11.1,PIG-3379,2013-07-16T13:37:26.000-05:00,cause PIG script,"{code:title=temp.pig}
       
      
    
    {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);

  DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);

  GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
        
  GENERATE timeStamp;
describe A;
{code}
 
 {code}
   
    
 {code}
The following script fails:","src.org.apache.pig.parser.LogicalPlanBuilder
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.expression.ProjectExpression"
CLASS,pig-0.11.1,PIG-3495,2013-10-02T16:40:25.000-05:00,test failures for streaming test failures on windows,"{code}
 
 PigStorage()  
 myfuncs.square(age);
dump b;
{code}
Register a jython script with an absolute path fail.
For Example:
{code}
register 'D:\scriptingudf.py' using jython as myfuncs;
a = load 'studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = foreach a generate myfuncs.square(age);
dump b;
{code}",src.org.apache.pig.scripting.ScriptEngine
CLASS,pig-0.11.1,PIG-3510,2013-10-09T18:02:45.000-05:00,fail with filter statement,";
{code}
{code:title=one filter}
      ;
{code}
Here is an example that demonstrates the problem:
{code:title=two filters}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001);
c = FILTER b BY (event_id == 419 OR event_id == 418);
{code}
{code:title=one filter}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001) AND (event_id == 419 OR event_id == 418);
{code}
Both dateint and event_id are partition columns.
push down whole expression for filter case push down for filter case",src.org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer
CLASS,zookeeper-3.4.5,ZOOKEEPER-1619,2013-01-11T09:57:16.000-06:00,allow spaces in URL,"{code}
 
 {code}

 
 {code}
 
 {code}
not allow spaces in url
This format will work.
This format will not (notice the spaces around the comma)",src.java.main.org.apache.zookeeper.client.ConnectStringParser
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,ZooKeeper Server fails if snapCount is set to 1,"int randRoll = r.nextInt(snapCount/2);
{code}
If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
CLASS,argouml-0.22,3923,2006-02-07T13:17:48.000-06:00,import Poseidon activity diagrams from XMI,"Collection actionStates = getModel().getAllActionStates();
  Iterator iterActionState = actionStates.iterator();
iterActionState.hasNext(); 
 ActionStateFacade actionState =
(ActionStateFacade) iterActionState.next();
1) Import an XMI from Poseidon, which works well with AndroMDA (the
PiggyBank example).
2) If I add my activity diagram under the use case diagram I always get a new activity graph, so I have 2 activity graphs alltogether.
not add activity diagram under imported activity graph
see screenshot
3) This code, which works with Poseidon, won't work with ArgoUML:
4) Importing the activity diagram from Poseidon works and the result can be processed by AndroMDA but if you are making the activity diagram from the beginning with ArgoUML, it won't work because of the error above",org.argouml.persistence.XMIParser
CLASS,argouml-0.22,4200,2006-05-11T23:30:25.000-05:00,vanish in new package,"Model folder;
vanish in package
That is how you can get the defect:
stand on the main Model folder;
right mouse click and choose from droped meniu ""Add Package"";
rename the package;
there are some diagrams below, so i jus drag them one by one using right
mouse button and releasing over the package;
see diagrams in selected package Properties owned Elements","org.argouml.ui.explorer.PerspectiveManager
org.argouml.ui.explorer.rules.GoModelElementToBehavior"
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,ensure [ resources,"getStat()
find new file from file system
not exist for different reasons
This problem appears to the user when executing refresh operations.
At the first moment, the file is found in the file system and assumed to be a folder, and a corresponding resource is created in the workspace.
At the second refresh, the folder corresponding to that resource is not found in the file system, and then it is removed from the workspace.","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
it happens every time the REDUCE_STREAMING_KMEANS is set to true.",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
CLASS,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}

 {Code}

  StreamingKMeansThread.call()

 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }

    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
The issue is caused by the following code in StreamingKMeansThread.call()",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread
CLASS,jabref-2.6,1631548,2007-01-09T14:20:57.000-06:00,depend on working dir,"{HOME\}
depend on working directory
Example:
open HOME \",net.sf.jabref.JabRefFrame
METHOD,mahout-0.8,MAHOUT-1301,2013-08-01T09:31:21.000-05:00,have excess comma at end,"SequentialAccessSparseVector toString()   toString()  
 {code:java}
 Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
  v.toString()  
 {code:java}
 {1:0.1,3:0.3}
 {code}
 
 {code:java}
 {1:0.1,3:0.3,}
 {code}
introduce new bug add at end
Example: 
Consider following sparse vector
{code:java}
Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
In 0.7 v.toString() returns following string:
{code:java}
{1:0.1,3:0.3}
{code}
but in 0.8 it returns
{code:java}
{1:0.1,3:0.3,}
{code}
As you can see, there is extra comma at the end of the string.","org.apache.mahout.math.SequentialAccessSparseVector:toString()
org.apache.mahout.math.RandomAccessSparseVector:toString()"
METHOD,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,throw NullPointerException set REDUCE_STREAMING_KMEANS to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
it happens every time the REDUCE_STREAMING_KMEANS is set to true.","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:reduce(IntWritable, Iterable<CentroidWritable>, Context)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:getBestCentroids(List<Centroid>, Configuration)"
METHOD,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,throw IllegalArgumentException set REDUCE_STREAMING_KMEANS to true,"{Code}


 {Code}


  StreamingKMeansThread.call()


 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }


    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Path, Configuration)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Iterable<Centroid>, Configuration)"
CLASS,openjpa-2.0.1,OPENJPA-1787,2010-09-10T11:23:51.000-05:00,merge new entity,"EntityManager em = entityManagerFactory.createEntityManager();
        Person person = new Person();
        person.setName(""Oliver"");                               // Employee.name is annotated @NotNull 
        person = em.merge(person);
If you try to merge a new entity.
get ConstraintValidationException","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.kernel.AttachStrategy
org.apache.openjpa.integration.validation.TestValidationGroups"
CLASS,openjpa-2.0.1,OPENJPA-1903,2010-12-06T13:05:34.000-06:00,work first time,"@Entity
@IdClass(MandantAndNameIdentity.class)
public class Website {
    @Id
    private String mandant;
   
    @Id
    private String name;
...
}

 @Entity
@IdClass(WebsiteProduktDatumIdentity.class)
public class Preis {
    @Id
    @ManyToOne(cascade = CascadeType.MERGE)
    private Website website;

    @Id
    @Basic
    private String datum;
...
}

 
 em.getTransaction().begin();

        Website website = em.merge(new Website(""Mandant"", ""Website""));

        em.merge(new Preis(website, DATUM));
       
        em.getTransaction().commit();

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website.name = :website "", Preis.class);
       q.setParameter(""website"", website.getName());

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website = :website "", Preis.class);
        q.setParameter(""website"", website);
return data
Basically I have two Entities which both use multiple Ids to produce the Primary Key, ""Preis"" contains a foreign key on ""Website"":
I use the following to set up a website and a Preis:
Afterwards, if I run the query as follows:
work time
However if I put the query as
work ONCE not return ONCE",org.apache.openjpa.jdbc.kernel.PreparedQueryImpl
CLASS,openjpa-2.0.1,OPENJPA-1912,2011-01-03T13:48:09.000-06:00,generate invalid code,"@Entity
public abstract class AbstractGroup {
   ...
    @Temporal(TemporalType.TIMESTAMP)
    @TrackChanges
    private Date applicationBegin;
 ...
}

 
 @Entity
public class Group extends AbstractGroup {
...
}

 
 public void writeExternal(ObjectOutput objectoutput)
        throws IOException
     
 pcWriteUnmanaged(objectoutput);
        if(pcStateManager != null)
        {
            if(pcStateManager.writeDetached(objectoutput))
                return;
        } else
        {
            objectoutput.writeObject(pcGetDetachedState());
            objectoutput.writeObject(null);
        }
        objectoutput.writeObject(applicationBegin);
        objectoutput.writeObject(applicationEnd);
        objectoutput.writeObject(applicationLocked);
        objectoutput.writeObject(approvalRequired);
If openjpa.DetachState =fetch-groups is used, the enhancer will add a 'implements Externalizable' + writeExternal + readExternal.
externalize private members of given superclass
get runtime Exception access fields
Example:",org.apache.openjpa.enhance.PCEnhancer
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,ignore class loader,"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We pass the appliation class loeader as part of our PersistenceUnitInfo implementation by returning it from PersistenceUnitInfo.getClassLoader().
use context class loader from PersistenceUnitInfo leade PersistenceUnitInfo to ClassNotFoundExpcetions mention at end","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
take different parameters take method annotate with @Factory not override method with method
not take type not take same name
For example:
        @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}",org.apache.openjpa.meta.FieldMetaData
CLASS,openjpa-2.0.1,OPENJPA-1986,2011-04-27T11:44:53.000-05:00,cascade persist,"@Entity
public class CascadePersistEntity implements Serializable {
    private static final long serialVersionUID = -8290604110046006897L;

    @Id
    long id;

    @OneToOne(cascade = CascadeType.ALL)
    CascadePersistEntity other;
...
}

 
 CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);
find scenario cascade persist to new entity generate extra queries while cascading
See the following example:
and the following scenario:
CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.conf.Compatibility
org.apache.openjpa.kernel.SingleFieldManager"
METHOD,lang,LANG-300,2006-12-19T17:47:43.000-06:00,throw NumberFormatException for digit,"isDigits(numeric.substring(1))  numeric.substring(1)
NumberUtils.createNumber throws a NumberFormatException when parsing ""1l"", ""2l"" .
.
etc..
.
It works fine if you try to parse ""01l"" or ""02l"".",org.apache.commons.lang.math.NumberUtils:createNumber(String)
METHOD,lang,LANG-363,2007-10-23T07:12:48.000-05:00,not escape /' into '\/ make IE render page,"document.getElementById(""test"")   document.getElementById(""test"") 
  
 String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);
For example, document.getElementById(""test"").
make IE render page uncorrect
Try  to run below codes, you will find the difference:
  String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);","org.apache.commons.lang.StringEscapeUtils:escapeJavaStyleString(Writer, String, boolean)"
METHOD,lang,LANG-477,2009-01-09T10:05:53.000-06:00,contain single quotes,"{code:title=ExtendedMessageFormatTest.java|borderStyle=solid}

 private static Map<String, Object> formatRegistry = new HashMap<String, Object>();    
     static {
        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());
    }
    
     public static void main(String[] args) {
        ExtendedMessageFormat mf = new ExtendedMessageFormat(""it''s a {dummy} 'test'!"", formatRegistry);
        String formattedPattern = mf.format(new String[] {""great""});
        System.out.println(formattedPattern);
    }
 
 {code}

 
 {code:title=ExtendedMessageFormat.java|borderStyle=solid}
 
 if (escapingOn && c[start] == QUOTE) {
        return appendTo == null ? null : appendTo.append(QUOTE);
}

WORKING:
if (escapingOn && c[start] == QUOTE) {
        next(pos);
        return appendTo == null ? null : appendTo.append(QUOTE);
}
{code}
use ExtendedMessageFormat with custom format registry conatine single quotes
Example that will cause error:","org.apache.commons.lang.text.ExtendedMessageFormat:appendQuotedString(String, ParsePosition, StringBuffer, boolean)"
METHOD,lang,LANG-480,2009-01-20T17:36:44.000-06:00,convert unicode characters into characters convert unicode characters above u ffff,"import org.apache.commons.lang.*;

public class J2 {
    public static void main(String[] args) throws Exception {
        // this is the utf8 representation of the character:
        // COUNTING ROD UNIT DIGIT THREE
        // in unicode
        // codepoint: U+1D362
        byte[] data = new byte[] { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 };

        //output is: &amp;#55348;&amp;#57186;
        // should be: &amp;#119650;
        System.out.println(""'"" + StringEscapeUtils.escapeHtml(new String(data, ""UTF8"")) + ""'"");
    }
}
represent characters as characters convert characters
The following test displays the problem quite nicely:","org.apache.commons.lang.Entities:escape(Writer, String)"
METHOD,lang,LANG-538,2009-10-16T16:47:39.000-05:00,not change Calendar TimeZone in certain situations,"Calenar.getTime()    
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";

    // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)
    // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);


    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}

 
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
    cal.getTime();

    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}
not change Calendars fields construct Calendar object in certain ways
For example, the following unit test fails:
However, this unit test passes:","org.apache.commons.lang3.time.FastDateFormat:format(Calendar, StringBuffer)"
METHOD,lang,LANG-710,2011-07-01T20:57:30.000-05:00,"StringIndexOutOfBoundsException when calling unescapeHtml4(""&#03"")","unescapeHtml4()
When calling unescapeHtml4() on the String ""&#03"" (or any String that contains these characters) an Exception is thrown:","org.apache.commons.lang3.text.translate.NumericEntityUnescaper:translate(CharSequence, int, Writer)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,throw ClassNotFoundException clone primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
contain reference to primitive class throw ClassNotFoundException clone object
use ClassLoaderAwareObjectInputStream override resoleClass method without delegating override ClassLoaderAwareObjectInputStream without delegating delegate to super method
For example:","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-832,2012-09-27T00:27:58.000-05:00,not handle unterminated quotes,"{IsNd}
not handle unterminated quotes as SimpleDateFormat
For example:
parse format",org.apache.commons.lang3.time.FastDateParser:init()
METHOD,lang,LANG-879,2013-03-18T21:46:29.000-05:00,fail with new Locale,"import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;

import java.util.Locale;

import org.testng.annotations.Test;

import com.scispike.foundation.i18n.StringToLocaleConverter;

public class LocaleStringConverterTest {

	StringToLocaleConverter converter = new StringToLocaleConverter();

	public void testStringToLocale(Locale l) {
		String s = l.toString();

		assertThat(converter.convert(s), equalTo(l));
	}

	@Test
	public void testAllLocales() {

		Locale[] locales = Locale.getAvailableLocales();
		for (Locale l : locales) {
			testStringToLocale(l);
		}
	}
}


  
 import java.util.Locale;

import org.apache.commons.lang3.LocaleUtils;
import org.springframework.core.convert.converter.Converter;

public class StringToLocaleConverter implements Converter<String, Locale> {

	@Override
	public Locale convert(String source) {
		if (source == null) {
			return LocaleToStringConverter.DEFAULT;
		}
		return LocaleUtils.toLocale(source);
	}
}
The Test below fails with the following error on JDK7, but succeeds on JDK6:",org.apache.commons.lang3.LocaleUtils:toLocale(String)
FILE,SWARM,SWARM-528,2016-06-22T02:53:46.000-05:00,not work with @ArquillianResource URL baseURL,"@ArquillianResource 
  
 
 
 
 @ArquillianResource 
  
 
 
 
 @ArquillianResource
If you set the swarm port using either swarm.http.port or swarm.port.offset via arquillian.xml e.g.
start arquillian swarm container on specified port/offset
The problem is that if you use:
retrieve url return http
If you set the port property in arquillian.xml
start swarm container on 8080 and
Attempting to combine the port property and the offset does not work either e.g.
start container
use swarm.http.port",org.wildfly.swarm.arquillian.resources.SwarmURLResourceProvider
FILE,SWARM,SWARM-486,2016-05-28T18:25:37.000-05:00,not load project-stages.yml with arq not load project-stages.yml on classpath,"classpath(src/main/resources)  
 
 
 container.withStageConfig(Paths.get(""/tmp"", ""external-project-stages.yml"").toUri().toURL())
not load yml with Arquillian tests not load yml on classpath(src/main/resources)
I attached the error log and the reproducer in 'Steps to Reproduce' section.",org.wildfly.swarm.container.ProjectStagesTest
METHOD,derby-10.9.1.0,DERBY-5951,2012-10-16T10:33:53.000-05:00,use Clobs with territory base collation,"db;create=true; 
   varchar( 32672 )  
  
  
  
 clobTable( a )   makeClob( 'a' )  
   varchar( 32672 )  
  
  
 clobTable( a )   makeClob( 'a' )  
     Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;   
   Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;     
  
 clobTable( a )   makeClob( 'a' )
use territory-based collation with clobs raise error invoke missing method
The following script shows this problem:
fail with java.lang.NoSuchMethodError exception",org.apache.derby.iapi.types.CollatorSQLClob:getNewNull()
FILE,eclipse-3.1,102427,2005-06-30T20:45:00.000-05:00,not import methods,"public class Helper {
    public static int getValue() {...}
}
  
import static Helper.*;

public class Doer {
    public void doit() {
        int i = getValue();
    }
}
 
 getValue() 
 getValue()
Consider:
When debugging, if you select 'getValue()' in the method 'doit' and execute
display (or inspect) you get an error indicating that the method 'getValue()' is
not undefined for type Doer.",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,103379,2005-07-11T15:37:00.000-05:00,leak time leak editor instance,"dispose()
open editor close editor
It creates a new simple project and a new file.
It opens up the new file in the editor that comes with the testcase, then close the editor.
Repeat 500 times.
allocate size
If you run this testcase with -Xmx256M, you will run out of memory.
However, if you explicitly set the String array to null in the dispose() method of the editor, then the same testcase will not run out of memory.",org.eclipse.ui.operations.OperationHistoryActionHandler
FILE,eclipse-3.1,103918,2005-07-14T17:25:00.000-05:00,create dynamic proxy in rich client,"public void start(BundleContext context) throws Exception {
  super.start(context);
  XmlBeanFactory bf = new XmlBeanFactory(
     new ClassPathResource(""/bug/beans.xml""));
  bf.getBean(""hang"");
}

  bf.getBean(""hang"")  
 bf.getBean()
get % load need % load fall into infinit loop
my start method contains the following code:",org.eclipse.core.runtime.internal.adaptor.ContextFinder
FILE,eclipse-3.1,110837,2005-09-27T13:16:00.000-05:00,throw exception in IDE,"KeyAgreement.getInstance(""DiffieHellman"")  
  
 
import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;

public class KeyAgreementProblem
{
    public static void main(String[] args) throws NoSuchAlgorithmException
    {
        KeyAgreement ka = KeyAgreement.getInstance(""DiffieHellman"");
        System.out.println(ka);
    }
}
 
 
  
  
 javax.crypto.KeyAgreement.getInstance(DashoA12275)
throw a.
run from Eclipse run from command line
Here's the code:
produce exception run with same JDK run with environment run in Eclipse","org.eclipse.jdt.launching.AbstractJavaLaunchConfigurationDelegate
org.eclipse.jdt.internal.launching.JRERuntimeClasspathEntryResolver
org.eclipse.jdt.internal.launching.StandardVMType"
FILE,eclipse-3.1,133072,2006-03-23T16:58:00.000-06:00,not launch eclipse application without ws argument,"package Fred;

import javax.swing.JFrame;
import javax.swing.SwingUtilities;

import org.eclipse.core.runtime.IPlatformRunnable;

public class Main implements IPlatformRunnable {

       public Object run(Object args) throws Exception {
               SwingUtilities.invokeLater(new Runnable() {
                       public void run() {
                               new JFrame(""Fred"").setVisible(true);
                       }
               });
               synchronized(this)
               {
                       wait();
               }
               return IPlatformRunnable.EXIT_OK;
       }

}
get dreaded 2006-03-23 run against 1.4.2_09
get messages on startup get 1.5.0_06 on startup
appear under 1.5.0_06
Here's the test class we used:","org.eclipse.pde.internal.ui.IPDEUIConstants
org.eclipse.pde.internal.ui.launcher.LaunchAction"
FILE,eclipse-3.1,300054,2010-01-19T10:12:00.000-06:00,copy changes from right copy changes to left,"public class Bug {
	void bar() {
		System.out.println();
	}
}
  System.out.println();
1 start with new workspace
2 paste this into the Package Explorer:
4 delete ""System.out.println();"" and save
5 compare the current state with the previous one
6 double-click on 'bar' in the compare editor's upper pane
7 click 'Copy Current Change from Right to Left' button",org.eclipse.compare.internal.Utilities
FILE,eclipse-3.1,76534,2004-10-18T22:57:00.000-05:00,not perform evaluations with constructor _ parameters not perform evaluations inside inner class,"createViewer(...)
disallow evaluations in inner classes take parameters in referenced constructor _ take parameters in inner classes take evaluations in referenced constructor _ take evaluations in inner classes
For example, see the CheckBoxTreeViewer that's created in BreakpointsView#createViewer(...).",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,77234,2004-10-28T15:41:00.000-05:00,not see inherited method,"getTypeName() 
  
  
  
 getTypeName()   JavaExceptionBreakpoint

getTypeName()
1 Create a detail formatter for the type JavaExceptionBreakpoint.
2 Set the contents to ""getTypeName()""
3 Debug to a breakpoint with a JavaExceptionBreakpoint in the variables view.
I'm debugging RemoveBreakpointAction and I delete an exception breakpoint to see
this.
4 Select the JavaExceptionBreakpoint.
get following in details pane",org.eclipse.jdt.internal.debug.ui.JavaDetailFormattersManager
FILE,eclipse-3.1,77573,2004-11-03T04:43:00.000-06:00,not propose static fields,"import static java.lang.Math
- Write ""import static java.lang.Math."" in a cu
- Press Ctrl+Space","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.core.CompletionProposal
org.eclipse.jdt.core.CompletionRequestor"
FILE,eclipse-3.1,78740,2004-11-16T10:57:00.000-06:00,represent interface flags,"becomeDetailed()   

package org.example.jdom;

import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;

public class Test implements IPlatformRunnable
{
  public Object run(Object object)
  {
    DOMFactory factory = new DOMFactory();
    IDOMCompilationUnit jCompilationUnit =
factory.createCompilationUnit(""package x; /** @model */ interface X  {}"", ""NAME"");
    IDOMType jType = (IDOMType)jCompilationUnit.getFirstChild().getNextNode(); 
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    jType.getComment();
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    return new Integer(0);
  }
}
This code demonstrates that calling getComment on an IDOMType will change the flags from correctly encoding the type as being interface to incorrectly encoding it (because during becomeDetailed() that information is lost):",org.eclipse.jdt.internal.compiler.DocumentElementParser
FILE,eclipse-3.1,79957,2004-12-02T00:47:00.000-06:00,change input usingTableViewer,"Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
 
 tv.setInput(model1);
i've straight forward code ... 
<code>
Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
.
throw null pointer exception",org.eclipse.jface.viewers.TableViewer
FILE,eclipse-3.1,80672,2004-12-10T04:44:00.000-06:00,not trigger recompilation,"package p;
@q.Ann
public class Use {
}
  
package q;
public @interface Ann {
}


 
 
package q;
import java.lang.annotation.*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
 
 @Ann
Define 2 files:
p/Use.
Build - all is fine
Now incrementally change q/Ann.
java to:
package q;
import java.lang.annotation.
*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
Build - still fine though p/Use.",org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader
FILE,eclipse-3.1,81045,2004-12-14T20:13:00.000-06:00,change value,"public class Test {
	static class Inner {
	}
	public static void main(String[] args) {
		Inner inner= null;
		System.out.println(1);  //  <- breakpoint here
	}
}
Debug to the breakpoint.
Right-click on the 'inner' variable > change value ...","org.eclipse.jdt.internal.debug.ui.actions.JavaVariableValueEditor
org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
org.eclipse.jdt.internal.debug.core.model.JDILocalVariable"
FILE,eclipse-3.1,82712,2005-01-12T15:54:00.000-06:00,not show method parameters from static imports,"import static java.lang.Math.*; 
 public class Test {

    void t() {
        abs(<CTRL+SPACE>);
    }
}
Test:",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,83489,2005-01-22T17:33:00.000-06:00,select ] code select returns itype instead_of ITypeParameter,"class Test<T> {
  void foo(T t) {}
}
Consider following test case:
When I select ""T"" in method declaration, selection engine returns an IType",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,84194,2005-02-01T18:05:00.000-06:00,assist at end assist in import statements insert,"import org.eclipse.core.runtime.*;
Open any Java file that contains > 1 import statements.
Let's say the first import statement reads:
delete the '*;' from the end and try to use code assist to insert
IRunnableWithProgress for example.
You will see that upon pressing Enter to select, the text gets inserted several lines down under all the import statements.
be in random position","org.eclipse.jdt.internal.ui.text.java.JavaTypeCompletionProposal
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector"
FILE,eclipse-3.1,84724,2005-02-08T13:41:00.000-06:00,find call sites for varargs constructor_s,"public class Test {
    public void foo() {
        Cell c= new Cell("""", """"); // calls Cell.Cell(String...)
    }
}
 class Cell {
    public Cell(String... args) { }
}
find call to varargs constructor _
Simply highlight the constructor_'s name and invoke ""References"" -
> ""Workspace"" from the Java editor context menu; no occurrences will be found.",org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
FILE,eclipse-3.1,84770,2005-02-09T06:46:00.000-06:00,fail in specific case,"public class FormatterTest {
  void doTest(
      ) {
     System.out.println(""("" + 
         Object.class + "")"");
  }
}
 
 toString()
1 Make a new class in default package:
2 Try to format it by Ctrl+Shift+F  - nothing happens
3 Now change the 'Object.class' to 'Object.class.toString()'
(You can also delete the last ' + "")""'
4 Try to format it by Ctrl+Shift+F  - everything is ok
have string operatation after keyword class",org.eclipse.jdt.internal.formatter.BinaryExpressionFragmentBuilder
FILE,eclipse-3.1,85344,2005-02-15T17:36:00.000-06:00,evaluate logical structure value in Java evaluate logical structure value for map,"public class Test {
  public static void main(String[] args) {
    Map<String, Integer> map= new HashMap<String, Integer>();
    System.out.println();     // <-- breakpoint here
  }
}

  entrySet()
System.out.println();     // <-- breakpoint here
I get ""Error: The method entrySet() is undefined for the type Map__"" when I expand map in the variables view.",org.eclipse.jdt.internal.debug.eval.ast.engine.BinaryBasedSourceGenerator
FILE,eclipse-3.1,85397,2005-02-16T08:20:00.000-06:00,produce error on constructor _,"strictfp enum Natural {
	ONE, TWO;
}

 
 strictfp enum Natural {
	ONE, TWO;
	
	private Natural() {
	}
}
- have this code:
not allow strictfp on enum actual
- alternatively, have this code:
report wrong modifier with type name show error for constructor _","org.eclipse.jdt.internal.compiler.lookup.SyntheticMethodBinding
org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyViewPart
org.eclipse.jdt.internal.compiler.lookup.MethodScope"
FILE,eclipse-3.1,85402,2005-02-16T08:50:00.000-06:00,assist ] NPE while trying complete on empty annotation,"import e.Team;
   @Author(name={Team.DAVID, Team.JEROME})
    
  public class Test {
	@Author(name=Team.PHILIPPE) void foo() {}
	@Author int t;
  }
  
  import e.Team;
  public @interface Author {
	Team[] name() default Team.FREDERIC;
  }
  
  package e;
  public enum Team {
	PHILIPPE, DAVID, JEROME, FREDERIC;
  }

 
 ResultCollector.accept(CompletionProposal)
Having following test case:
If you try to complete at caret position, then you get an Error Excuting Command
dialog.
name for CompletionProposal","org.eclipse.jdt.internal.codeassist.complete.CompletionOnAnnotationOfType
org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.codeassist.complete.CompletionParser"
FILE,eclipse-3.1,85672,2005-02-17T05:53:00.000-06:00,unfold folded region with line delimiter,"package folding;

class Test {
    
}
Have this code:
Put the caret right after the closing brace and fold the region.
Put the caret on the last line and unfold the type.",org.eclipse.jface.text.source.projection.ProjectionViewer
FILE,eclipse-3.1,85734,2005-02-17T12:28:00.000-06:00,debug view excessively for debug,"Runtime.exec(...)
In particular, I have set a
breakpoint on ""Runtime.exec(...)"" and started a debugging session on Eclipse.","org.eclipse.debug.internal.ui.views.RemoteTreeViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewEventHandler
org.eclipse.debug.internal.ui.views.RemoteTreeContentManager"
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,produce invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
produce bad JPG images
Simple test case below loads
 PNG Files and Saves them as JPEG.
produce proper JPG images expected","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,87171,2005-03-04T14:19:00.000-06:00,declare node use type parameters not work for methods/fields,"public class Inline<T> {
	void foo(T t) {
		System.out.println(t);
	}
}

 class Use {
	public static void main(String[] args) {
		Inline<String> i= null;
		i.foo(""Eclipse"");
	}
}

  i.foo(""Eclipse"");
 
 root.findDeclaringNode(methodBinding);
The code below is all in compilation unit Inline.java
- take the method binding of the invocation foo in i.foo(""Eclipse"");
- take the root node representing the whole CU
- call root.findDeclaringNode(methodBinding);
observe: null is returned although the CU contains the corresponding declaration.
use type parameters happen for fields","org.eclipse.jdt.core.dom.CompilationUnit
org.eclipse.jdt.core.dom.DefaultBindingResolver"
FILE,eclipse-3.1,87569,2005-03-09T16:41:00.000-06:00,obtain image switch to Debug perspective,"class which implements java.io.Serializable
Create a Java Project using Java 5 (not sure it matters)
Create a class which implements java.io.Serializable
Use the ""Add generated serial version ID"" quickfix
Switch to the Debug perspective",org.eclipse.debug.internal.ui.DefaultLabelProvider
FILE,eclipse-3.1,87665,2005-03-10T11:38:00.000-06:00,open details with errors,"testOpenJavaEditor1()
Take a look at:
Scroll down to performance.OpenJavaEditorStressTest#testOpenJavaEditor1()""
Click on the red x ==> details show up all green.","org.eclipse.swt.printing.PrintDialog
org.eclipse.swt.widgets.MessageBox"
FILE,eclipse-3.1,89632,2005-03-30T13:10:00.000-06:00,evaluate in Snippet editor,"Collection<String> c = new ArrayList<String>();
        c.add(""a"");
        c.add(""b"");
        c.add(""c"");

        for (Iterator<String> i = c.iterator(); i.hasNext(); )
            if (i.next().length() == 4)
            {
                String x = i.next();
                System.out.println(x);
            }
        
        return c;

 
   
  run()
Testcase:
I added the testcase to the snippet editor.
I then did a ""Set Imports..."" to include java.util.
* to resolve collection and iterator.
Trying a ""Display"" or ""Inspect"" resulted in the following error in the console:",org.eclipse.jdt.internal.eval.CodeSnippetMessageSend
FILE,eclipse-3.1,90289,2005-04-05T09:17:00.000-05:00,call IStackFrame.getVariables(),"IStackFrame.getVariables()
If I add a Suspsend VM breakpoint on the call to IStackFrame.getVariables() in 
my StackFrame object, I am seeing it hit 2 or more times.
show duplicates as result show duplicates of local variables","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.registers.RegistersView
org.eclipse.debug.internal.ui.views.registers.RegistersViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
FILE,eclipse-3.1,90600,2005-04-07T09:32:00.000-05:00,use project options for rewriter,"rewriter.rewriteAST(document, null)
When executing the rewriter, (rewriter.rewriteAST(document, null))
pass in current projects options","org.eclipse.jface.text.source.SourceViewer
org.eclipse.jdt.internal.core.CreateElementInCUOperation
org.eclipse.jface.text.source.OverviewRuler"
FILE,eclipse-3.1,91098,2005-04-12T06:07:00.000-05:00,not mark occurrences,"String a;
String[] b;
String[][] c;
The precise test case is the following:
Put the cursor on String or String[].
Now put the cursor on String[][].",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,91346,2005-04-13T16:43:00.000-05:00,mark occurrences not find for marking,"{buildDirectory}
Cursor in property=""${buildDirectory}"" does not mark the property declaration 
occurrence",org.eclipse.ant.internal.ui.model.AntPropertyNode
FILE,eclipse-3.1,92451,2005-04-22T16:36:00.000-05:00,assist failure cast + arrays,"public class Test {
	public static void main(String[] args) {
		java.util.List elements = null;
		// code assist works on this line
		new Test(Test.toStrings((Test[])elements.toArray(new Test
[0])));
		//code assist fails on this line
	}
	public Test(Object object) {
	}
	public static Object toStrings(Test[] objects) {
		return null;
	}
}
Code assist fails in the following (self-contained) class (see comments for line of error)",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,93249,2005-04-29T05:49:00.000-05:00,not propose full method stub,"IRunnableWithProgress runnable= new IRunnableWithProgress() {
};

  
  
 public void run(org.eclipse.core.runtime.IProgressMonitor monitor) throws
InvocationTargetException, InterruptedException
- take revision 1.8 of BuildPathAction.
- in run method add the following
- inside the runnable type run<code assist> and select run","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.ui.text.java.OverrideCompletionProposal"
FILE,eclipse-3.1,93727,2005-05-04T17:43:00.000-05:00,fail with Method Parameter annotations,"import org.drools.semantics.annotation.DroolsParameter;

public class Test
{
  public Object passthrough( @DroolsParameter(""parameter"") Object parameter ) {
    return parameter;
  }
}
not see error in <Workspace> /
Example:
import org.drools.semantics.annotation.DroolsParameter;",org.eclipse.jdt.internal.formatter.CodeFormatterVisitor
FILE,eclipse-3.1,94216,2005-05-09T20:04:00.000-05:00,not work for generic types,"interface IGeneric<T> {
}
 public class Generic<T> implements IGeneric<T> {
    public static void main(String[] args) {
        IGeneric<String> gen= new Generic<String>();
        System.out.println();  // <-- breakpoint here
    }
}
Try to do 'open declaring type' or 'open concrete type' for 'gen' at the breakpoint, nothing happens.","org.eclipse.jdt.internal.debug.ui.actions.OpenVariableDeclaredTypeAction
org.eclipse.jdt.internal.debug.ui.actions.OpenVariableConcreteTypeAction"
FILE,eclipse-3.1,94465,2005-05-10T14:33:00.000-05:00,modify value in Variables view,"String [] elms= { ""abc"", ""cde"", ""xyz"" };
Test case:
1. In the variables, expand the array, and then expand the first element, [0]
=""abc"".
2. Select the ""value=char[3]"" field.
RMC->Change Value.
3. In the Change Primitive Value dialog, type in a new string value.
4. Click ok and it will result in a java dump.","org.eclipse.jdt.internal.debug.ui.JDIModelPresentation
org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.ActionMessages"
FILE,eclipse-3.1,95096,2005-05-13T06:16:00.000-05:00,assist popup complete imported method name,"import static java.lang.Math
- Create a new Class ""Foo""
- Type ""import static java.lang.Math.""
- Press Ctrl+Space
- Type ""a""
constrain proposals to members","org.eclipse.jdt.internal.ui.text.java.JavaMethodCompletionProposal
org.eclipse.jdt.internal.ui.text.java.LazyJavaCompletionProposal"
FILE,eclipse-3.1,95152,2005-05-13T12:14:00.000-05:00,not find synthetic constructor _,"InputReadJob readJob = new InputReadJob(streamsProxy);
1) Add org.eclipse.debug.ui to the search path (i.e., by clicking ""Add to Java
Search"" in the plugins view.
2) Open type on ""ProcessConsole"" (class file with source attached)
3) Go to line 483:
4) Highlight the InputReadJob constructor_ and hit F3.
click entry in outline view not jump to constructor _
not handle synthetic addition of enclosing class not handle synthetic addition by compiler
break kind of navigation break kind to corresponding constructor _","org.eclipse.ant.internal.ui.views.AntViewDropAdapter
org.eclipse.ant.internal.ui.launchConfigurations.AntLaunchShortcut
org.eclipse.ant.internal.ui.AntUtil
org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
org.eclipse.jdt.internal.core.search.indexing.BinaryIndexer
org.eclipse.jdt.internal.core.index.DiskIndex
org.eclipse.jdt.internal.core.search.matching.ConstructorPattern"
FILE,eclipse-3.1,95505,2005-05-17T02:56:00.000-05:00,not use code completion,"{cursor}
know type write type press ctrl + space show type
For example:",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,96440,2005-05-24T11:11:00.000-05:00,determine sizes lay out times,"table.getClientArea()
STEPS
1 Put a breakpoint in the JFace TableLayout class
2 Launch a self hosted workspace
3 Open Preferences-> Ant Runtime
4 You will see a client area size of about 81
5 Do the same in M6 - it will be about 320 or so.",org.eclipse.jface.preference.PreferencePage
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,have border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
- change the browser example's BrowserPerspectiveFactory to have the following instead of the regular addView layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
- run the example, and show the history view
have border","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,97674,2005-05-31T15:16:00.000-05:00,not report error for changing,"VectorTest.testCapacity()
 
 fEmpty.toString()  charAt(100000)
* Suspended in VectorTest.testCapacity()
* selected fFull in variables viwe
* selecetd ""change value"" from context menu
* entered expression ""fEmpty.toString().
charAt(100000)""
* pressed OK
> no error, no change","org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.EvaluateAction"
FILE,eclipse-3.1,97722,2005-05-31T16:41:00.000-05:00,Pref Page Ant/Runtime/Tasks/Add Task dialog problems,"@

Dialog
crop error message at bottom",org.eclipse.ant.internal.ui.preferences.AddCustomDialog
FILE,eclipse-3.1,98147,2005-06-02T13:09:00.000-05:00,not show children,"package xy;
public class Try {
	String fName;
	int fID;
	
	public Try(String name, int id) {
		fName= name;
		fID= id;
	}
	
	public static void main(String[] args) {
		Try t= new Try(""Hello"", 5);
		callee(t, t);
	}
	
	static void callee(Try t1, Try t2) {
		boolean same= t1.equals(t2); //breakpoint here
	}
	
}
- Debug the class below with the breakpoint where indicated.
- Expand t1 in the Variables view -> expands fine and shows fID and fName.
- Expand t2 -> only child fID is shown",org.eclipse.debug.internal.ui.views.RemoteTreeViewer
FILE,eclipse-3.1,98686,2005-06-07T10:39:00.000-05:00,pref page mnemonics,"@

Click
context menu of shared project -> properties; cvs page
have mnemonic
have mnemonic
Click on ""Change Sharing...""
have mnemonic","org.eclipse.pde.internal.ui.IHelpContextIds
org.eclipse.team.internal.ccvs.ui.CVSProjectPropertiesPage"
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,refresh children on project,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
Take an existing simple project on disk and import the project into the workspace by performing a simple create with code like:
Do not open the project with the project.open() API.
Now create a project either by API or UI and open it.
Or simply switch to the
Java perspective.
start background refresh job for closed project stick in infinite loop
call members on iproject
Our particular use case is that we are loading existing Java projects on disk by performing a create, but never an open.
On the next UI gesture, we get refresh infinite loops, one for each closed project.","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,not initialize switch method in thread safe way,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
For example, the following program should print ""1999"" 40 times
(once from each thread).
print default on machine","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99355,2005-06-10T09:48:00.000-05:00,extract method trips,"package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      final Container<String> container = c.get();
      final String string = container.get();
      //to here
   }
}
 
 

package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      extractedMethod(c);
      //to here
   }

   private void extractedMethod(final final final Container<Container<String>> c)
   {
      final Container<String> container = c.get();
      final String string = container.get();
   }
}
if you extract method where indicated below.
declare paramater with many final modifiers
notice final modifiers in extractedMethod signature",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,99693,2005-06-13T11:29:00.000-05:00,invalid during display,"private static void doGenerics() {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < 1000; i++) {
			int num = rand.nextInt(10000) + 1;
			list.add(num);
		}
		
		int max = 0;
//start eval
		for (Integer integer : list) { // BREAKPOINT HERE
			max = Math.max(max, integer);
		}
		System.out.println(max);
//end eval
	}
Debug the following method to a breakpoint:
Select everything between start eval and end eval comments and ctrl-shift-d.
watch Variables view see lot of stack frames","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
CLASS,openjpa-2.2.0,OPENJPA-2227,2012-07-09T14:24:05.000-05:00,not find custom SequenceGenerators,"{code}
 @Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable  
 @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
 {code}

 
     JavaTypes.classForName()     Class.forName()
define custom Sequence insert data into database
ExampleConfiguration:
{code}
@Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable {
    @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
{code}
produce stacktrace
instantiate custom sequence class
occur after deploying work with JavaSE deploy into WAS",openjpa-kernel.src.main.java.org.apache.openjpa.meta.SequenceMetaData
CLASS,openjpa-2.2.0,OPENJPA-2247,2012-08-03T10:29:58.000-05:00,map unidirectional own OneToOne be OneToOne in SecondaryTable,"@Entity
@SecondaryTable(name = ""ParentSecondaryTable"", pkJoinColumns = 
    { @PrimaryKeyJoinColumn(name = ""idParent"", referencedColumnName = ""idParent"") })
public class Parent {

    @Id
    @GeneratedValue
    int idParent;

    String child_ref;

    @OneToOne
    @JoinColumn(name = ""CHILD_REF"", table = ""ParentSecondaryTable"", referencedColumnName = ""idChild"")
    PChild child;

}
ignore @JoinColumn
map unidirectional own OneToOne be OneToOne in SecondaryTable
For example:
look for fk",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingRepository
METHOD,atunes-1.10.0,231,2008-10-04T18:31:26.000-05:00,not add image read repository,"public boolean isSupportsInternalImage()
1. Read repository by older version of aTunes
2. Update to latest SVN
3. Try to add an image (assuming supported file format)
return false never set to true",net.sourceforge.atunes.kernel.modules.repository.audio.AudioFile:supportsInternalPicture()
CLASS,solr-4.4.0,SOLR-5295,2013-10-02T00:09:02.000-05:00,create maxShardsPerNode number of replicas,"{quote}
 
  
  
  
 {quote}
I've just created a very simple test collection on 3 machines where I set maxShardsPerNode at collection creation time to 1, and
I made 3 shards.
So I try again -- I create a collection with 3 shards and set maxShardsPerNode to 1000 (just as a silly test).
Now I add shard4 and it immediately tries to add 1000 replicas of shard4...",solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor
CLASS,solr-4.4.0,SOLR-5296,2013-10-02T00:20:01.000-05:00,create collection with implicit router add shard ranges to shard,"{quote}
 {quote}
create collection with implicit router add shard ranges to shard
Using the Example A from SolrCloud wiki:
bq.
do right thing",solr.core.src.java.org.apache.solr.cloud.Overseer
FILE,AMQP,AMQP-190,2011-09-10T20:24:17.000-05:00,synchronize with TransactionManager,"convertAndSend()
It seems that when I use RabbitTemplate, channelTransacted=true, to convertAndSend() a message to an exchange within the context of a synchronized TransactionManager (e.g. an active transaction on the current thread), the channel is never closed, hence new publishes will always get their ""own"", shiny, new channel (that is never closed or released to the channel pool) until Rabbit can't handle any more channels.","org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer
org.springframework.amqp.rabbit.core.RabbitTemplatePerformanceIntegrationTests
org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils
org.springframework.amqp.rabbit.connection.RabbitResourceHolder"
FILE,AMQP,AMQP-340,2013-11-18T09:41:01.000-06:00,not consider wrong RabbitMQ credentials as fatal error,"start()  
 SimpleMessageListenerContainer.run()
If I connect an AmqpInboundChannelAdapter to the RabbitMQ (3.1.0) with wrong credentials (wrong password) the SimpleMessageListenerContainer doesn't detect the real reason and continues to restart the consumer in a seemingly endless loop.
return as AmqpInboundChannelAdapter user return after pause
have notification about wrong credentials","org.springframework.amqp.rabbit.listener.BlockingQueueConsumer
org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests
org.springframework.amqp.rabbit.support.RabbitExceptionTranslator"
FILE,AMQP,AMQP-502,2015-06-19T03:02:33.000-05:00,route key not create fanout binding due_to missing,"@RabbitListener(




      bindings = @QueueBinding(




          value = @Queue(




              autoDelete = ""true""




          ),




          exchange = @Exchange(




              type = ""fanout"",




              value = ""mytest.broadcast"",




              autoDelete = ""true""




          ),




          key = ""#""




      )




  )




  public void processBroadcast(String data) {




    int i = 0;




  }






 
  
  
  
     
 
     
 
  
  
  
  
  
   {}   
     
 
     
 
     
 
  
     
 
     
 
     
 
     
 
     
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   {}   
  
     
 
      
   {}
Currently i am using spring-cloud-starter-bus-amqp which in terms references spring-amqp 1.4.3.
when i declare a rabbitlistener like this:
get error","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-653,2016-10-08T02:53:08.000-05:00,not take advantage of registered converters,"@Bean




Jackson2JsonMessageConverter jackson2JsonMessageConverter() {




	return new Jackson2JsonMessageConverter();




}
When using RabbitTemplate in a Spring Boot application, it's very easy to register a Spring AMQP Message Converter.
Just add this to your code:
However, if you switch to RabbitMessagingTemplate, that bean no longer works, because RabbitMessagingTemplate doesn't offer to look up RabbitTemplate's converters, and instead relies on its own.","org.springframework.amqp.rabbit.core.RabbitMessagingTemplateTests
org.springframework.amqp.rabbit.core.RabbitMessagingTemplate"
FILE,AMQP,AMQP-656,2016-10-15T00:25:46.000-05:00,use @Argument within @RabbitListener refer to default exchange,"@Argument 
 @RabbitListener(bindings =




        @QueueBinding(




            value = @Queue(




                value = ""app.events.myEvent"",




                durable = ""true"",




                exclusive = ""false"",




                autoDelete = ""false"",




                arguments = {




                        @Argument(name=""x-dead-letter-exchange"", value = """"),




                        @Argument(name=""x-dead-letter-routing-key"", value=""app.dlq"")




                }),




            exchange = @Exchange(value=""amq.topic"", durable = ""true"", type = ""topic""),




            key=""event.app.myEvent.v1""




        ))






 
 @Bean




    public Queue appMyEventQueue() {




        return QueueBuilder.durable(""app.events.myEvent"")




            .withArgument(""x-dead-letter-exchange"", """")




            .withArgument(""x-dead-letter-routing-key"", deadLetterQueue().getName())




            .build();




    }
use @Argument annotations refer to default exchange
For example, you should be able to configure a queue to use the default exchange as part of the dead letter config similar to the following:
not send empty string","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
METHOD,commons-math-3-3.0,MATH-718,2011-12-03T18:40:44.000-06:00,return wrong value for large trials,"{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}
return wrong value for large trials
Following code will be reproduce the problem.","org.apache.commons.math3.util.ContinuedFraction:evaluate(double, double, int)"
FILE,DATACMNS,DATACMNS-157,2012-04-20T01:24:38.000-05:00,extend interface,"@Query 
 @NoRepositoryBean




public interface EntityRepository<T> extends JpaRepository<T, Long> {









	T findByDealer(Dealer dealer);




}









 public interface CarRepository extends EntityRepository<PersonalSiteVehicle> {









	@Override




	@Query(""select p from PersonalSiteVehicle p join p.detail d join d.enrichable e where e.dealer = ?1"")




	PersonalSiteVehicle findByDealer(Dealer dealer);




}






 
  @Query
I try to define an interface method in a super repository interface and 'implement' this in an extending interface with @Query.
Tested in the latest nightly build:
not use @Query annotation in sub interface","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,return null for null sources return null for empty strings,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
Imagine the use case where you have an Order domain class which has a ManyToOne reference to Customer.
When posting a new Order where Order.customer == """" then a converter exception is thrown:
This is the code I used:","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-257,2012-11-29T02:29:27.000-06:00,not deal with uppercase fields,"@Id 
 class Foo{




  




  @Id




  private String UID;









  //code omitted




}
not execute MongoOperations.findOne method contain @Id field
Here an example:
1) create an entity like in example code snippet in MongoDb
2) try to perform find by id operation by calling MongoOperations.findOne
get exception get step","org.springframework.data.mapping.PropertyPath
org.springframework.data.mapping.PropertyPathUnitTests"
FILE,DATACMNS,DATACMNS-509,2014-05-08T08:39:02.000-05:00,break JSON conversion,"{




    final Set<Pos> allPos = posService.findAll();




    return ImmutableSortedSet.copyOf(allPos);




}






 
 {""name: ""pos1""}  {""name: ""pos2""} 
  
     {""name: ""pos1""}  {""name: ""pos2""}
address NullableWrapper fail since upgrade return with contents contain with NullableWrapper
I have a MVC method that is returning:
get in JSON format get of pos get on wire
get NullableWrapper with Spring data JPA get NullableWrapper with contents","org.springframework.data.repository.core.support.DummyRepositoryFactory
org.springframework.data.repository.core.support.RepositoryFactorySupport
org.springframework.data.repository.core.support.RepositoryFactorySupportUnitTests"
FILE,DATACMNS,DATACMNS-511,2014-05-22T00:04:43.000-05:00,cause infinite loop,"public class User extends AbstractTenantUser<User, Role, Permission, Tenant> {




    ...




}




 public abstract class AbstractTenantUser<USER extends AbstractTenantUser<USER, ROLE, PERMISSION, TENANT>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>, TENANT extends AbstractTenant<USER>> extends AbstractUser<USER, ROLE, PERMISSION> implements TenantEntity<TENANT> {




    ...




}




 public abstract class AbstractUser<USER extends AbstractUser<USER, ROLE, PERMISSION>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AbstractPermission<USER extends AbstractUser<USER, ?, ?>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AuditingDateBaseEntity<USER extends AbstractUser<USER, ?, ?>> extends AbstractDateBaseEntity implements AuditingEntity<USER> {




    ...




}




 public abstract class AbstractDateBaseEntity extends AbstractBaseEntity implements DateEntity {




    ...




}




 public abstract class AbstractBaseEntity implements BaseEntity {




    ...




}
We use quite a few abstract MappedSuperclasses that have circular references and apparently this does not work.
An example:",org.springframework.data.util.TypeVariableTypeInformation
FILE,DATACMNS,DATACMNS-609,2014-11-30T07:31:51.000-06:00,create multiple bean definitions for RepositoryInterfaceAwareBeanPostProcessor,"predictBeanType()
use tag jpa:repositories in XML files create new instance with bean factory create new instance of RepositoryInterfaceAwareBeanPostProcessor register new instance with bean factory register new instance of RepositoryInterfaceAwareBeanPostProcessor
call n times for relevant bean
This can easily be reproduced with the version 51d1c5d of git@github.com:spring-projects/spring-data-jpa-examples.git by adding an extra stanza like <jpa:repositories base-package=""org.springframework.dummy"" /> to simple-repository-context.xml and running XmlConfigCachingRepositoryTests.","org.springframework.data.repository.config.RepositoryConfigurationExtensionSupportUnitTests
org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport"
FILE,DATACMNS,DATACMNS-616,2014-12-17T02:25:54.000-06:00,not access private fields,"@Entity




@RevisionEntity(ExtendedRevisionListener.class)




@Table(name = ""revinfo"")




public class ExtendedRevision implements Serializable  
 @Id




	@GeneratedValue




	@Column(name = ""REV"")




	@RevisionNumber




	private Integer id;









	 @RevisionTimestamp




	@Temporal(TemporalType.TIMESTAMP)




	@Column(name = ""REVTSTMP"", nullable = false)




	private Date date;









	 @Column(nullable = false, length = 15)




	private String username;









	 public Integer getId() {




		return id;




	}









	 public Date getDate() {




		return date;




	}









	 public String getUsername() {




		return username;




	}









	 public void setUsername(String username) {




		this.username = username;




	}
Trying to use a custom Envers revision class:
trigger error",org.springframework.data.util.AnnotationDetectionFieldCallback
FILE,DATACMNS,DATACMNS-683,2015-04-13T05:31:25.000-05:00,enable Spring break @ModelAttribute binding in Spring MVC,"package be.vdab.web;









import org.springframework.context.annotation.ComponentScan;




import org.springframework.context.annotation.Configuration;




import org.springframework.data.web.config.EnableSpringDataWebSupport;




import org.springframework.web.servlet.config.annotation.EnableWebMvc;




import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;









// enkele imports




@Configuration




@EnableWebMvc




@EnableSpringDataWebSupport




@ComponentScan




public class CreateControllerBeans extends WebMvcConfigurerAdapter {




}






  






package be.vdab.web;









import org.springframework.stereotype.Controller;




import org.springframework.web.bind.annotation.ModelAttribute;




import org.springframework.web.bind.annotation.RequestMapping;




import org.springframework.web.bind.annotation.RequestMethod;




import org.springframework.web.servlet.ModelAndView;









import be.vdab.entities.Person;









@Controller




@RequestMapping(value = ""/"")




public class PersonController {




	private static final String TOEVOEGEN_VIEW = ""/WEB-INF/JSP/index.jsp"";














	@RequestMapping(method=RequestMethod.GET)




	ModelAndView get() {




		return new ModelAndView(TOEVOEGEN_VIEW).addObject(new Person());




	}




	




	@RequestMapping(method = RequestMethod.POST)




	String post(@ModelAttribute Person person) {




	  if (person == null) {




		  throw new IllegalArgumentException(""person IS NULL"");




	  }




	  return ""redirect:/"";




	}



















}






 
    
 
 
 
    
 @EnableSpringDataWebSupport   
 @ModelAttribute
Given following Java config class
, following Controller class
and following JSP
throw InvalidArgumentException","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
CLASS,derby-10.9.1.0,DERBY-5407,2011-09-12T08:50:38.000-05:00,produce unusable DDL for VARCHAR FOR BIT DATA columns run across network,"varchar( 20 )  
 
 
 VARCHAR ()
omit length specification for VARCHAR FOR BIT DATA columns report in private correspondence run across network
I can reproduce this problem as follows:
1 Bring up a server (here I am using port 8246).
2 Create a database with the following ij script:
3 Now run dblook across the network:
produce following DDL for table
use embedded database produce usable DDL include length specification for VARCHAR FOR BIT DATA column include usable DDL for VARCHAR FOR BIT DATA column","java.testing.org.apache.derbyTesting.functionTests.tests.lang.SystemCatalogTest
java.engine.org.apache.derby.catalog.types.BaseTypeIdImpl"
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,use prepared statement for Connection.setTransactionIsolation use regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
The program below shows repeated calls to setTransactionIsolation.",java.client.org.apache.derby.client.am.Connection
CLASS,derby-10.9.1.0,DERBY-6131,2013-03-27T08:17:37.000-05:00,throw ClassCastException,"varchar(1000) 
   varchar(1000)
the issue can be reproduced
1 create table myTbl1 (name varchar(1000));
2 create table myTbl2 (name varchar(1000));
3 create view myView (name) as select t1.name from myTbl1 t1 union all select t2.name from myTbl2 t2;
4 select name from myView where upper(name) in ('AA', 'BB');
#4 failed with ""org.apache.derby.impl.sql.compile.SimpleStringOperatorNode incompatible with org.apache.derby.impl.sql.compile.ColumnReference: java.lang.ClassCastException""
If the view is created as ""create myView (name) as select t1.name from myTbl1 t1"", the query worked fine.","java.testing.org.apache.derbyTesting.functionTests.tests.lang._Suite
java.testing.org.apache.derbyTesting.junit.BaseJDBCTestCase
java.engine.org.apache.derby.impl.sql.compile.PredicateList"
METHOD,time,77,2013-10-16T15:36:22.000-05:00,change value of MutableDateTime,"final MutableDateTime mdt = new MutableDateTime(2011, 10, 30, 3, 0, 0, 0, DateTimeZone.forID(""Europe/Berlin""));
System.out.println(""Start date: "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(-1);
System.out.println(""addHours(-1): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(0);
System.out.println(""addHours(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addDays(0);
System.out.println(""addDays(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
 
 
 addHours(-1)  
 addHours(0)  
 addDays(0)  
 
          
 addDays(0)
add amount of days add amount to mutable date time object change value of object change value upon DST transition
The code
show same problem","org.joda.time.MutableDateTime:add(DurationFieldType, int)
org.joda.time.MutableDateTime:addWeeks(int)
org.joda.time.MutableDateTime:addYears(int)
org.joda.time.MutableDateTime:addMonths(int)
org.joda.time.MutableDateTime:addMinutes(int)
org.joda.time.MutableDateTime:addHours(int)
org.joda.time.MutableDateTime:addWeekyears(int)
org.joda.time.MutableDateTime:addDays(int)
org.joda.time.MutableDateTime:addSeconds(int)
org.joda.time.MutableDateTime:addMillis(int)"
METHOD,time,93,2013-12-01T09:33:58.000-06:00,fail with NPE,"new Partial(yearOfCentury(), 1)  with(weekyear(), 1);
// NullPointerException
// org.joda.time.Partial.with (Partial.java:447)
With the latest master:
fail with yearOfCentury fail with year fail with yearOfEra","org.joda.time.Partial:Partial(DateTimeFieldType[], int[], Chronology)
org.joda.time.field.UnsupportedDurationField:compareTo(DurationField)"
FILE,COMPRESS,COMPRESS-189,2012-06-26T21:30:39.000-05:00,read bytes read from nested zip file,"ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
      ZipArchiveEntry entry = iterator.nextElement();
      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
      ZipArchiveEntry innerEntry;
      while ((innerEntry = zipInput.getNextZipEntry()) != null){
        if (innerEntry.getName().endsWith(""XML""))
{

          //zipInput.read();

          System.out.println(IOUtils.toString(zipInput));

        }
      }
    }
When the following code is run an error ""Underlying input stream returned zero bytes"" is produced.
return bytes
The zip file used to produce this behavious is available at http://wwmm.ch.cam.ac.uk/~dl387/test.ZIP
iterate over entries look at master table","org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream"
FILE,COMPRESS,COMPRESS-309,2015-02-18T17:22:16.000-06:00,read to full buffer,"BZip2CompressorInputStream.read(buffer, offset, length)  
 @Test

	public void testApacheCommonsBZipUncompression () throws Exception {

		// Create a big random piece of data

		byte[] rawData = new byte[1048576];

		for (int i=0; i<rawData.length; ++i) {

			rawData[i] = (byte) Math.floor(Math.random()*256);

		}



		// Compress it

		ByteArrayOutputStream baos = new ByteArrayOutputStream();

		BZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);

		bzipOut.write(rawData);

		bzipOut.flush();

		bzipOut.close();

		baos.flush();

		baos.close();



		// Try to read it back in

		ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());

		BZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);

		byte[] buffer = new byte[1024];

		// Works fine

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		// Fails, returns -1 (indicating the stream is complete rather than that the buffer 

		// was full)

		Assert.assertEquals(0, bzipIn.read(buffer, 1024, 0));

		// But if you change the above expected value to -1, the following line still works

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		bzipIn.close();

	}
It seems like a pretty stupid thing to do - but I'm getting this when trying to use Kryo serialization (which is probably a bug on their part, too), so it does occur and has negative affects.
Here's a JUnit test that shows the problem specifically:
return ( indicate stream",org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
FILE,swt-3.1,104150,2005-07-16T19:58:00.000-05:00,click on empty space click on grid lines,"table.getLinesVisible()  
 table.setLinesVisible(true)
use table cursor have potential of table regions have kinds of table regions separate table cursor from table selection
To reproduce the problem, use snippet 96 with an added table.setLinesVisible(true).",org.eclipse.swt.custom.TableCursor
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,fail to setTopIndex add new items to table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
Here's my testcase to demonstrate the problem:
position to correct table item add new items to table
position table at item
add new table items to table have effect after adding","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,84609,2005-02-07T13:35:00.000-06:00,have NPE while calling call pack() on last column,"lvtTable.getColumn(0).pack();
lvtTable.getColumn(1).pack();
lvtTable.getColumn(2).pack();

   
 parent.getColumns()
Consider followed code, table has only 3 columns:
catch NPE in TableColumn return array with elements get on third call","org.eclipse.swt.widgets.TableColumn
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,produce invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
produce bad JPG images
Simple test case below loads
 PNG Files and Saves them as JPEG.
produce proper JPG images expected",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,87997,2005-03-14T19:21:00.000-06:00,cause NPE,"TableEdtior.dispose( )  
  
   

import org.eclipse.swt.custom.TableEditor;
import org.eclipse.swt.events.*;
import org.eclipse.swt.widgets.*;

public class Test
{
    public static void main( String[ ] args )
    {
        Shell shell = new Shell( );
        Table table = new Table( shell, 0 );
        new TableColumn( table, 0 );
        TableItem item = new TableItem( table, 0 );
        final TableEditor editor = new TableEditor( table );
        final Text text = new Text( table, 0 );
        editor.setEditor( text, item, 0 );
        item.addDisposeListener( new DisposeListener( ) {
            public void widgetDisposed( DisposeEvent e )
            {
                text.dispose( );
                editor.dispose( ); // Triggers a NPE
            }
        } );
        shell.dispose( );
    }
}
be in process result in NPE
Specifically this prevents one from adding a dispose listener on the Table
or a TableItem and trying to dispose of the associated editor, as in the code
below.
set dispose listener on parent throw instead_of NPE","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,88829,2005-03-22T20:41:00.000-06:00,not fire enough Move events,"Table.setColumnOrder(new int[] {4,1,2,3,0});
- start with five columns, all different widths
- do Table.setColumnOrder(new int[] {4,1,2,3,0});
swap positions fire SWT.Move events for columns",org.eclipse.swt.widgets.Table
FILE,swt-3.1,93724,2005-05-04T17:35:00.000-05:00,create signal names,"byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);	
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
Here is an example of some code in DragSource.java for GTK+:
convert names for signals define signal names in OS.java","org.eclipse.swt.dnd.DropTarget
org.eclipse.swt.dnd.DragSource"
FILE,swt-3.1,97651,2005-05-31T14:43:00.000-05:00,mark cheese,"Tree.redraw() 
 public static void main(String[] args) {
	final Display display = new Display();
	final Shell shell = new Shell(display);
	shell.setBounds(10, 10, 300, 300);
	final Tree tree = new Tree(shell, SWT.NONE);
	tree.setBounds(10, 10, 200, 200);
	new TreeItem(tree, SWT.NONE).setText(""pre-root"");
	TreeItem root1 = new TreeItem(tree, SWT.NONE);
	root1.setText(""root"");
	TreeItem child = new TreeItem(root1, SWT.NONE);
	child.setText(""child"");
	Button button = new Button(shell, SWT.PUSH);
	button.setBounds(230,10,30,30);
	button.addSelectionListener(new SelectionAdapter() {
		public void widgetSelected(SelectionEvent e) {
			tree.redraw();
		}
	});
	root1.setExpanded(true);
	tree.setInsertMark(root1, false);
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose();
}
- run the snippet below
- the insert line is set to be under the ""root"" item
- collapse the root item
except for pointy ends
- press the button to the right of the Table: this does a Tree.redraw(), and note that the insert line reappears, so I guess it never really meant to go away
expand root item copy insert mark to child item copy insert mark to initial location","org.eclipse.swt.dnd.TreeDragUnderEffect
org.eclipse.swt.widgets.Tree"
CLASS,pig-0.8.0,PIG-1188,2010-01-14T13:32:46.000-06:00,null to input tuple,"{code}
  as (a0, a1);
dump a;
{code}
 
 {code}
 
 {code}
 
 {code}
 
 {code}

 
 {code}
 
 {code}
determine number of fields
Here is one example:
Pig script:
Input file:","test.org.apache.pig.test.TestMergeForEachOptimization
src.org.apache.pig.newplan.logical.rules.TypeCastInserter
test.org.apache.pig.test.TestNewPlanLogicalOptimizer
test.org.apache.pig.test.TestNewPlanFilterRule
test.org.apache.pig.test.TestNewPlanPushDownForeachFlatten
test.org.apache.pig.test.TestEvalPipeline2
test.org.apache.pig.test.TestMultiQueryCompiler
test.org.apache.pig.test.TestPartitionFilterPushDown
test.org.apache.pig.test.TestNewPlanFilterAboveForeach"
CLASS,pig-0.8.0,PIG-1277,2010-03-05T13:02:03.000-06:00,give cogroup on tuple keys,"UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

 
 {code}
 
  
 by (c0, c1);
dump e;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}

 
 {code}
  {(1,1)}  {(1,1)} 
 {code}

 
 {code}
  {(1,1)}  {} 
 {}  {(1,1)} 
 {code}
treat as different keys cogroup on tuple
Here is one example:
UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}
Pig script: 
{code}
a = load '1.","src.org.apache.pig.impl.io.NullableBytesWritable
test.org.apache.pig.test.TestPackage
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator
src.org.apache.pig.backend.hadoop.HDataType
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestSecondarySort
src.org.apache.pig.newplan.logical.relational.LOUnion"
CLASS,pig-0.8.0,PIG-1771,2010-12-16T14:45:37.000-06:00,return different schema with load AS,"{code}
 
 BinStorage() 
         tuple()  ;
dump auxData;
{code}
The following script fail:","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1776,2010-12-17T16:28:09.000-06:00,"changing statement corresponding to alias after explain , then doing dump gives incorrect result","{code}
 
  
 {code}
/* but dumping c after following steps gives incorrect results */","src.org.apache.pig.PigServer
src.org.apache.pig.newplan.logical.relational.LOLoad
test.org.apache.pig.test.TestUDFContext"
CLASS,pig-0.8.0,PIG-1813,2011-01-20T10:25:01.000-06:00,throw ERROR while refer map in result,"flatten(org.myudf.GETFIRST(value))  
 PigStorage()
The above script fails when run with Pig 0.8 but runs fine with Pig 0.7 or if pig.usenewlogicalplan=false.","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-1856,2011-02-15T17:26:16.000-06:00,not package custom jar with new job,"{code}
 
  
 {code}
The script:
not ship piggybank jar with additional job not ship piggybank jar to backend","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1858,2011-02-17T02:27:48.000-06:00,result frontend exception,"{code}
 
 PigStorage()  
 {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        } 
   ;
{code}
The below is my script :
{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).
When i trun off new logical plan the script executes successfully.",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-1868,2011-02-24T00:42:05.000-06:00,have complex data types from udf,"{code}
 
 {
 Tuples = order B1 by ts;
 generate Tuples;
} 
   { t: ( previous, current, next ) } 
 as id;
dump C3;
{code}

 
 {code}
 
 {code}

  on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The below is my script :
match uid for project code } fail on c3
The below is the describe on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The script works if I turn off new logical plan or use Pig 0.7.","src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,report input size for empty input file,"{code}
 
 by b0;
dump c;
{code}
In the following script:
read records read 1.txt","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1963,2011-04-04T17:18:24.000-05:00,take input from order-by not get results in nested foreach,"{code}
 
 explain d;
dump d;
{code}
This happens only when secondary sort is not being used for the order-by.
For example -
{code}
a1 = load 'fruits.txt' as (f1:int,f2);
a2 = load 'fruits.txt' as (f1:int,f2);","test.org.apache.pig.test.TestAccumulator
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.AccumulatorOptimizer"
CLASS,pig-0.8.0,PIG-1979,2011-04-08T02:24:01.000-05:00,not find matching uid fail with ERROR,"{code}
 
  
    
    
      
     PigStorage() 
  
  
  
   PigStorage() ;
{code}

   
  
    
 {code}

 import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;

public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}

  @Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().newTuple(1);
      tp2.set(0, (inputTuple.get(0).toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}

 {code}
The below is my script 
{code}
register myudf.jar;
c01 = LOAD 'input'  USING org.test.MyTableLoader('');
c02 = FILTER c01  BY result == 'OK'  AND formatted IS NOT NULL  AND formatted !
build plan fail in building fail while applying apply for logical optimization rule
The problem is happening when I try to include doc_005::category in the projection for relation finalresult.
The script goes through fine if I disable AddForEach rule by -t AddForEach","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression"
CLASS,pig-0.8.0,PIG-1993,2011-04-12T19:47:41.000-05:00,throw NPE with ColumnPruning,"{code}
 
  
  
 GENERATE a1;
dump b;
{code}
The following script fail:","contrib.piggybank.java.src.test.java.org.apache.pig.piggybank.test.TestPigStorageSchema
contrib.piggybank.java.src.main.java.org.apache.pig.piggybank.storage.PigStorageSchema"
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.","flatten(outlinks.target);
  flatten(outlinks.target);
give error
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,report from DESCRIBE,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
The following script:
print for DESCRIBE commands
be against section schemas be for Complex data types","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-946,2009-09-04T17:51:37.000-05:00,follow group,"10;
dump d;
The following script is combinable but is not optimized:
a = load '/user/pig/tests/data/singlefile/studenttab10k';
b = group a by $1;
c = foreach b generate group, AVG(a.$2);
d = limit c 10;
dump d;","test.org.apache.pig.test.TestCombiner
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer"
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,use materialized blobs with Postgresql cause error,"@Lob
I have entity with byte[] property annotated as @Lob and lazy fetch type, when table is createad the created column is of type oid, but when the column is read in application, the Hibernate reads the OID value instead of bytes under given oid.
create oid column","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
CLASS,hibernate-3.5.0b2,HHH-5042,2010-03-26T05:06:09.000-05:00,not increment hibernate_sequences.next_hi_value exhaust current lo-range,"class MultipleHiLoPerTableGenerator 
 IntegralDataTypeHolder value;
 
 int lo;

 
  
  
 IntegralDataTypeHolder hiVal = (IntegralDataTypeHolder) doWorkInNewTransaction( session );

   
  
 varchar(255) 
     varchar(255)
deliver numbers without update hibernate_sequences
duplicate keys insert new objects on concerning table
Please see attached testcase.
as the testcase uses 2 sessionfactories hibernate.hbm2ddl.auto=create cannot be used!!
Schema has to be exported separately and the testcase must run without hbm2ddl.auto property!
Here the schema for HSQLDB:","org.hibernate.id.SequenceHiLoGenerator
org.hibernate.id.enhanced.OptimizerFactory
org.hibernate.id.SequenceGenerator
org.hibernate.id.MultipleHiLoPerTableGenerator"
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,use wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
be for columns map in transaction entity NOT
So the Account class has the following fragment....
do on columns map in transaction","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-526,2008-02-27T13:28:05.000-06:00,cause sqlexception,"public class Exam 
 @Lob 
 @Column(name = ""text"", nullable = false)  
 private String text;
 
 With nullable = false
Here's the persistence class:
public class Exam... {
    @Lob
    @Column(name = ""text"", nullable = false) ***** NOTE: set nullable = true will fix the problem but it leads to bug OPENJPA-525 *****
    private String text;
}","org.apache.openjpa.persistence.kernel.common.apps.Lobs:getId()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:getLob()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:Lobs(String, int)
org.apache.openjpa.persistence.kernel.common.apps.Lobs:setLob(String)
org.apache.openjpa.jdbc.sql.OracleDictionary:setNull(PreparedStatement, int, int, Column)"
METHOD,adempiere-3.1.0,1240,2008-05-16T03:03:55.000-05:00,produce produc,"Production Quantity= 2
1. En Gardenworld , Production windows , create a row in the tab Production Header for  ""Production 2 Patio  set"".
2. in the tab Production Plan create a row for the Patio Furniture Set product, Production Quantity= 2
3. Then click on  ""Create/post Production"" button in the Production header tab, this create the production line.
4. the first line is necesary divide in two for give a serial each one.
then in the Patio Furniture Set product set in movement quantity 1 and give a serial  in the attribute set instance field, then create other row similar but the other serial
5. then click on  ""Create/post Production"" button in the Production header tab
6. click on ""Not Postet"" Button, then there the botton label is changed to ""Dont Balanced""",org.compiere.acct.Doc_Production:createFacts(MAcctSchema)
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,add integer cause exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
Try this in a BeanShell:
treat as string","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
FILE,CONFIGURATION,CONFIGURATION-241,2006-12-02T00:03:48.000-06:00,not generate events,"clearProperty() 
 ConfigurationFactory configurationFactory = new ConfigurationFactory();
   
 configurationFactory.setConfigurationURL(configFileURL);
Configuration configuration = ConfigurationFactory.getConfiguration();
configuration.addConfigurationListener(new ConfigurationListener() {
    public void configurationChanged(ConfigurationEvent e) 
{
        System.out.println(e.getPropertyName() + "": "" + e.getPropertyValue());
    }
});
System.out.println(configuration.getProperty(""name.first"")); // prints ""Mike""
 configuration.claerProperty(""name.first"")  ; // no output whatsoever
System.out.println(configuration.getProperty(""name.first"")); // prints ""null""
I am loading configuration information from multiple sources and have registered a listener with the resulting configuration object.
not receive clear property events
receive other events clear property
Below is a watered down version of what I am doing (note, my configuration file simply pulls in a property file containing this property: name.first=Mike):","org.apache.commons.configuration.TestCompositeConfiguration
org.apache.commons.configuration.CompositeConfiguration"
FILE,CONFIGURATION,CONFIGURATION-332,2008-07-04T15:54:10.000-05:00,not persist properties add through DataConfiguration,"public void testSaveWithDataConfiguration() throws ConfigurationException
{
    File file = new File(""target/testsave.properties"");
    if (file.exists()) {
        assertTrue(file.delete());
    }

    PropertiesConfiguration config = new PropertiesConfiguration(file);

    DataConfiguration dataConfig = new DataConfiguration(config);

    dataConfig.setProperty(""foo"", ""bar"");
    assertEquals(""bar"", config.getProperty(""foo""));
    config.save();

    // reload the file
    PropertiesConfiguration config2 = new PropertiesConfiguration(file);
    assertFalse(""empty configuration"", config2.isEmpty());
}
add through DataConfiguration
The following test fails on the last assertion :","org.apache.commons.configuration.TestPropertiesConfiguration
org.apache.commons.configuration.DataConfiguration"
FILE,CONFIGURATION,CONFIGURATION-347,2008-11-05T21:06:22.000-06:00,Iterating over the keys of a file-based configuration can cause a ConcurrentModificationException,"getKeys()
return iterator in getKeys() method
perform iteration access configuration access only instance","org.apache.commons.configuration.TestFileConfiguration
org.apache.commons.configuration.AbstractFileConfiguration"
FILE,CONFIGURATION,CONFIGURATION-408,2010-02-11T01:01:05.000-06:00,"When I save a URL as a property value, the forward slashes are getting escaped","public static void main(String[] args)
  {
    try
    {

      PropertiesConfiguration config = new PropertiesConfiguration();     

      File newProps = new File(""foo.properties"");

      config.setProperty(""foo"", ""http://www.google.com/"");     

      config.save(newProps);

      

    }
    catch (Exception e){}
  }
save URL as property value
Example Code :",org.apache.commons.configuration.TestPropertiesConfiguration
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,suffer from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,go past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-369,2010-05-03T15:48:27.000-05:00,throw NullPointerException,"new BisectionSolver()  solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);
invoke:","org.apache.commons.math.analysis.solvers.BisectionSolver:solve(UnivariateRealFunction, double, double, double)"
METHOD,math,MATH-60,2006-05-14T04:20:21.000-05:00,illogical result,"Fraction parse(String source, 
ParsePostion pos)  class ProperFractionFormat  
 ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);

//Test 1 : fail 
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

 
 parse(String, ParsePosition)
Please see the following code segment for more details:
Note: Similarly, when I passed in the following inputs:
input 2: (source = 1 2 / -3, pos = 0)
input 3: ( source =  -1 -2 / 3, pos = 0)
return Fraction for inputs
parse numberator/ denominator properly incase input string provide invalid numerator/denominator","org.apache.commons.math.fraction.ProperFractionFormat:parse(String, ParsePosition)"
METHOD,math,MATH-691,2011-10-16T17:18:34.000-05:00,produce nan,"new Variance(true/false)    
 {code:java}
 int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use ""population variance""
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);
{code}

 
 {code:java}
   double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
{code}
invoke SummaryStatistics.setVarianceImpl(new Variance(true/false) makes produce nan for invoking
The code to reproduce it:",org.apache.commons.math.stat.descriptive.SummaryStatistics:addValue(double)
METHOD,math,MATH-929,2013-01-15T11:45:28.000-06:00,return wrong value,"{code}
 Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);
{code}
To reproduce:
{code}
Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).",org.apache.commons.math3.distribution.MultivariateNormalDistribution:density(double[])
METHOD,math,MATH-942,2013-03-09T15:05:04.000-06:00,throw exception if first element,"Array.newInstance(singletons.get(0).getClass(), sampleSize)   
 singleons.get(0) 
 {{DiscreteDistribution.sample()}}  
 {code}
 List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).sample(1);
{code}
be of type t1
return object be object
To reproduce:
{code}
List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).",org.apache.commons.math3.distribution.DiscreteDistribution:sample(int)
METHOD,math,MATH-949,2013-03-15T18:11:56.000-05:00,report iterations,"LevenbergMarquardtOptimizer.getIterations()     BaseOptimizer.incrementEvaluationsCount()

 
 {noformat}
     @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }

 {noformat}
not report correct number of iterations
I've put a test case below.","org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer:doOptimize()
org.apache.commons.math3.optim.BaseOptimizer:BaseOptimizer(ConvergenceChecker<PAIR>)
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer:doOptimize()"
FILE,WFCORE,WFCORE-267,2014-11-19T19:47:31.000-06:00,print output use cli client jar,"INFO: {




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}




{




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}
If you are using the CLI client jar, all output is printed twice.
print log messages to standard
look something",org.jboss.as.cli.CommandLineMain
FILE,WFCORE,WFCORE-495,2015-01-12T08:48:29.000-06:00,not startup to wflyctl0212,"file(standalone.xml)  file(standalone.xml)
not startup to wflyctl0212
If you firstly deploy a .
war application by CLI command, then deploy 2nd time same .
war application by copying it to /depolyment directory.
After server restart, it shows:
1. start wildfly and deploy a .
war application by CLI.
2. copy same .
war application to /deployment directory
3. restart wildfly to see the error message.
do full-replace-deployment operation not remove full-replace-deployment operation from configuration file(standalone.xml)
have xxx.war in standalone/data/content configuration file(standalone.xml)
cause duplicate resource error",org.jboss.as.server.deployment.DeploymentFullReplaceHandler
FILE,WFCORE,WFCORE-626,2015-04-06T15:53:19.000-05:00,create list elements,"clear(name=attribute)
  get(name=attribute, index=0)
  add(name=attribute, value=test)
  get(name=attribute, index=0)
Consider the following sequence of operations:
return <undefined> expected
return <undefined>
create missing element at index operate on index","org.jboss.as.controller.operations.global.ListOperations
org.jboss.as.controller.operations.global.MapOperations"
FILE,WFCORE,WFCORE-442,2014-12-02T19:15:13.000-06:00,not propagate failures to top level failure-description,"{read-only=1} 
 {




                ""name"" => ""jboss.server.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""user.home"",




                ""path"" => ""/Users/hbraun"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.base.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
  
 {




                ""name"" => ""user.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.data.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/data"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.home.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.log.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/log"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.controller.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            }
An example is worth a thousand words:
have failure description",org.jboss.as.controller.operations.global.GlobalOperationHandlers
FILE,WFCORE,WFCORE-716,2015-05-27T10:21:36.000-05:00,check at stage,"attribute(name=security-realm)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }




 
 attribute(name=security-domain, value=MgMtDom)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }
check e.g. check reload-required capabilities check requirements check state be in state
The following command is referencing a non-existent capability: -
When I execute :reload it will fail: -",org.jboss.as.controller.CapabilityReferenceRecorder
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
have more ancestors with same submodules
lead to wflyctl0212
get fresh EAP","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-955,2015-08-27T14:34:07.000-05:00,Server is not responding after attempt to set parent of profile to non-existent profile,"add()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""java.lang.NullPointerException:null""




}




 
 add()
Server is not responding after attempt to set parent of profile to non-existent profile.
Server is not responding also after attempt to set parent of socket-binding-group to non-existent socket-binding-group.
Get fresh EAP
Get fresh EAP","org.jboss.as.controller.OperationContextImpl
org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.SocketCapabilityResolutionUnitTestCase
org.jboss.as.controller.capability.registry.IncludingResourceCapabilityScope
org.jboss.as.controller.AbstractCapabilityResolutionTestCase"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,remove extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
When I use migration operation the console log is filled with warning messages of type
If I do the sequence of operation","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,scop roles,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
Setting up host scoped roles as follows https://gist.github.com/heiko-braun/0dc810ed04db8739defd there are inconsistent results in the filtering.
use role select master select role show filtered resources appear in results
use role select slave select role get proper access-control header","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-1214,2015-12-11T23:17:45.000-06:00,header to domain servers,"{blocking-timeout=5;rollback-on-runtime-failure=false}  
 {

[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3)     ""blocking-timeout"" => ""5"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""rollback-on-runtime-failure"" => ""false"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""caller-type"" => ""user"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""access-mechanism"" => ""NATIVE""

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3) }
add request headers to op not propagate to servers not propagate during domain rollout
For example, if I add some stdout printing of what the headers are on the various processes and invoke this:
log on HC
note deploy
involve use among other places involve use of composite","org.jboss.as.domain.controller.operations.coordination.DomainRolloutStepHandler
org.jboss.as.domain.controller.operations.coordination.OperationCoordinatorStepHandler"
FILE,WFCORE,WFCORE-1198,2015-12-09T09:30:00.000-06:00,not resolve multiple properties,"{PROFILE-NAME}  {APP-VERSION}  {VAR}  add(auto-start=true, group=""${PROFILE-NAME}${APP-VERSION}-server-group"")
have value work with EAP 6.4.3 +
For example :
and if I execute "".
have value","org.jboss.as.cli.parsing.test.PropertyReplacementTestCase
org.jboss.as.cli.parsing.ExpressionBaseState"
FILE,WFCORE,WFCORE-701,2015-05-19T15:06:17.000-05:00,report between server-config resource report between server resource,"attribute(name=status)




 {




    ""outcome"" => ""success"",




    ""result"" => ""FAILED""




}




  attribute(name=server-state)




 {




    ""outcome"" => ""success"",




    ""result"" => ""STOPPED""




}
fail in way
To reproduce, run domain.sh, find the pid of a server process, and kill -9 <thepid>.
Then with the CLI:",org.jboss.as.host.controller.ManagedServer
FILE,WFCORE,WFCORE-1572,2016-05-27T16:20:10.000-05:00,contain whitespace,"{rollout id=foo}
When an argument value contains a space (eg.
deploy /path/to/test-application.
war --all-server-groups --headers={rollout id=foo}) and if user hits tab after the whitespace, suggestions are generated based on the command, rather than the current argument's name.","org.jboss.as.cli.parsing.DefaultStateWithEndCharacter
org.jboss.as.cli.parsing.ParserUtil
org.jboss.as.cli.parsing.test.CommandTestCase"
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,save name id attribute discrepancy,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
When using rollout plans for EAP deployment scenarios I can create my own named rollout-plan for ease of use.
I can then apply rollout command later on, referring with name of my own rollout plan that should be used.
When I create rollout-plan, I use command like:
see name attribute name rollout plan
When I then refer to it I use following command:
see id attribute","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,add local | remote-destination-outbound-socket-binding,"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
Lets have some /socket-binding-group=standard-sockets/socket-binding with particular name.
Then when I create some /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding or /socket-binding-group=standard-sockets/local-destination-outbound-socket-binding using same name as of already existing socket-binding resource, add operation is successful but when I perform server reload, it crashes as it is not able to parse configuration.
Start EAP and log to CLI create your own socket-binding resource and {remote|local}-destination-outbound-socket-binding resource with same names and perform reload
crash with following stacktrace","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1635,2016-07-05T07:04:51.000-05:00,fail in batch,"add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)




 
 
 add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)
Creating a new deployment-scanner and altering it's attribute fails if done in single batch.
run commands without batch run batch on CLI embed-server",org.jboss.as.server.deployment.scanner.AbstractWriteAttributeHandler
FILE,WFCORE,WFCORE-1590,2016-06-12T14:18:43.000-05:00,ignore setMinSize(0),"static final SimpleAttributeDefinition REPLACEMENT = new SimpleAttributeDefinitionBuilder(ElytronDescriptionConstants.REPLACEMENT, ModelType.STRING, false)




        .setAllowExpression(true)




        .setMinSize(0)




        .setFlags(AttributeAccess.Flag.RESTART_RESOURCE_SERVICES)




        .build();






 
 add(pattern=""@ELYTRON.ORG"", replacement="""", replace-all=true)
With the following attribute definition: -
The following error is reported if an empty string is used as a parameter: -","org.jboss.as.controller.operations.validation.BytesValidator
org.jboss.as.controller.SimpleAttributeDefinitionUnitTestCase
org.jboss.as.controller.test.WriteAttributeOperationTestCase
org.jboss.as.controller.AbstractAttributeDefinitionBuilder
org.jboss.as.controller.AttributeDefinition"
FILE,WFCORE,WFCORE-1765,2016-09-05T16:22:02.000-05:00,unclear NullPointerException if the deployment-scanner element is removed from the configuration,"{xml}
         {xml}
If the deployment scanner element is removed from the configuration of the standalone server a NullPointerException is logged which is unclear and difficult to find as the stack does not show any hint.",org.jboss.as.controller.ParallelBootOperationStepHandler
FILE,WFCORE,WFCORE-1793,2016-09-14T08:08:21.000-05:00,overwrite existing content with overwrite = true pass content by file path,"{""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




  {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}
Upon overwriting content in managed exploded deployments on wildfly-core, the following errors are produced:","org.jboss.as.server.controller.resources.DeploymentAttributes
org.jboss.as.server.deployment.ExplodedDeploymentAddContentHandler"
FILE,WFCORE,WFCORE-1864,2016-10-13T09:12:31.000-05:00,add command not remove whitespaces from dependencies,"{{
...
    <dependencies>
        <module name=""org.a""/>
        <module name="" org.b ""/>
    </dependencies>
...
}}
Running module add --name=foo.bar --resources=foo.jar --dependencies=[org.a, org.b ] will result in following dependencies in module.xml","org.jboss.as.cli.handlers.module.ASModuleHandler
org.jboss.as.test.integration.management.cli.ModuleTestCase"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,write attribute have access type metric have attribute,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
suggest attributes
Example
On attempt to write metric attribute, for example message-count, non writable error is printed","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,not match reality for socket-binding not match reality for *,"description(recursive=true)
If you tries to change such attributes you are informed that reload is necessary.","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
FILE,WFCORE,WFCORE-1959,2016-11-08T16:28:30.000-06:00,Deploying an empty managed exploded deployment to server group in domain fails,"{empty=true} 
 add()
Deploying an empty exploded deployment created on domain controller fails with the following:",org.jboss.as.domain.controller.operations.coordination.ServerOperationResolver
CLASS,jedit-4.3,1193683,2005-05-02T09:22:25.000-05:00,be in black hole,"{\{\{ test
aaaa
bbbb
cccc
\}
Hi, when you have some folded text \(folding closed\)
Close it you'll get
Delete one \{
type \ { \
hide text",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1571752,2006-10-05T21:26:12.000-05:00,fold wrong comments in PHP mode,"{

\} 
 {\{\{  --&gt;
function foo\(\) \{

\} //\}\}\}
Before 'Add Explicit fold' the content of buffer looks like this \('X' means selection boundaries\):",org.gjt.sp.jedit.textarea.TextArea
CLASS,jedit-4.3,1658252,2007-02-12T17:48:03.000-06:00,match in multi-line defines,"{ \
code;                         \
more code;                    \
even more code;               \
\}
Try this define:","org.gjt.sp.jedit.syntax.ParserRule
org.gjt.sp.jedit.syntax.XModeHandler
org.gjt.sp.jedit.syntax.XModeHandler.TagDecl
org.gjt.sp.jedit.syntax.TokenMarker"
CLASS,jedit-4.3,1724940,2007-05-24T15:02:18.000-05:00,type in multiple,"lt;body&gt;
  lt;p&gt;
 
 the &lt;p&gt;  
 lt;body&gt;
  lt;d&gt;
If I highlight multiple selections of text in the text area and then begin typing, only the first character of what I type is inserted in the selected areas \(except for where the cursor ended up after making the selection\).
For example if I have the text:
and I highlight both p's in the &lt;p&gt; tags and then type ""div"" I end up with:",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,fold expantion,"{\{\{ hello

something

\}
\(Quoted from Matthieu's comment for patch \#1999448\)
if I use explicit fold, with this buffer
I remove one ""l"" from hello.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
CLASS,jedit-4.3,2129419,2008-09-25T23:53:11.000-05:00,quit jEdit,"lt;init&gt;
When trying to quit jEdit, I get the following Null-Pointer-Exception, which is probably related to some files being changed/deleted \(due to a ""cvs up"" in the background\).",org.gjt.sp.jedit.gui.CloseDialog.ListHandler
