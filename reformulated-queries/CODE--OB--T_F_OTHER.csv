Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
CLASS,tika-1.3,TIKA-1070,2013-01-31T05:43:33.000-06:00,StackOverflow error in org.apache.tika.sax.ToXMLContentHandler$ElementInfo.getPrefix(ToXMLContentHandler.java:58),"new ElementInfo(currentElement, namespaces);
The error occurs when parsing big ""XLS"" files and is caused by the ElementInfo stored in ""currentElement"".
Each time a new element is started (method startElement) the current elment is newly overwritten with
currentElement = new ElementInfo(currentElement, namespaces);
where the existing element is used as the parent element.
Since the currentElement is not reset to the parent element after finishing the element (method: endElement) the method getPrefix recursively traverses the parents and finally causes the StackOverFlowError
solve issue in endElement",tika-core.src.main.java.org.apache.tika.sax.ToXMLContentHandler
CLASS,tika-1.3,TIKA-1192,2013-11-06T15:31:41.000-06:00,ArrayIndexOutOfBoundsException: 9 parsing RTF,"{noformat}
  
    
    
    
    
    
    
  
    
    
  
  
  
    
  
  
  
    
  
  
  
    
    
 {noformat}
When trying to parse an RTF file I'm getting the following exception.
attach file for privacy reasons","tika-parsers.src.test.java.org.apache.tika.parser.rtf.RTFParserTest
tika-parsers.src.main.java.org.apache.tika.parser.rtf.TextExtractor"
FILE,DATAMONGO,DATAMONGO-260,2011-09-05T01:06:36.000-05:00,MapReduce fails when using with Long as key-type.,"class Foo {




  String id;




  Long number;




  Long version;




} 






 
 function()  
 emit(this.number, this.version)




 
 function(key, values)  {




	return Math.max.apply(Math, values);




}
MapReduce will fail calling emit with Long as key-type.
then it tries to convert _id to objectid which fails:
find to java.lang.String convert from java.lang.Long","org.springframework.data.mongodb.core.convert.AbstractMongoConverter
org.springframework.data.mongodb.core.mapreduce.MapReduceTests"
FILE,DATAMONGO,DATAMONGO-413,2012-03-09T17:21:02.000-06:00,"Using ""Or"" in repository query yields a ClassCastException","List<User> findByEmailOrAlias(String email, String alias) 
        
 
 List<User> findByEmailAndAlias(String email, String alias)
Gives:
create query { email
thread main java.lang.ClassCastException com.mongodb.BasicDBObject not cast exception in thread not cast exception to java.lang.Iterable","org.springframework.data.mongodb.core.query.OrQuery
org.springframework.data.mongodb.repository.query.MongoQueryCreator
org.springframework.data.mongodb.repository.query.MongoQueryCreatorUnitTests"
FILE,DATAMONGO,DATAMONGO-423,2012-03-29T06:03:26.000-05:00,Criteria.regex should use java.util.Pattern instead of $regex,"c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", new BasicDBObject(""$regex"", ""b"") ) ) );






  
  
 c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", Pattern.compile( ""b"" , Pattern.CASE_INSENSITIVE ) ) ) );
complain about $regex
throws this exception:
not use $not with $regex use BSON regex type","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.QueryTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-462,2012-06-19T08:57:02.000-05:00,findAll() fails with NPE - discovering the root cause,"@PersistenceConstructor
base bug report on forum thread(see Reference URL)
duplicate original posts
provide informative error provide source build from source
not use @PersistenceConstructor annotation
find on github
not instantiate bean class [ java.net.URL]
not instantiate bean class","org.springframework.data.mongodb.core.convert.MongoConverters
org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
org.springframework.data.mongodb.core.convert.CustomConversionsUnitTests
org.springframework.data.mongodb.core.convert.CustomConversions"
FILE,DATAMONGO,DATAMONGO-523,2012-09-01T03:39:51.000-05:00,@TypeAlias annotation not used with AbstractMongoConfiguration,"@TypeAlias      @Document  @TypeAlias
When using the AbstractMongoConfiguration without any further modifications regarding the converter (afterMappingMongoConverterCreation) the @TypeAlias annotation is not used when writing the _class property.
Seems like it always uses the SimpleTypeInformationMapper.
annotate @Document classes with @TypeAlias annotations",org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
FILE,DATAMONGO,DATAMONGO-629,2013-03-22T04:08:25.000-05:00,Different results when using count and find with the same criteria with 'id' field,"Query q = query 
    
  
 
 
 {




		""id"" : /zzz/




	} 
 
 
 
 
 
  
  
 
 
 {




		""count"" : ""test"",




		""query"" : {




			""_id"" : /zzz/




		}




	 
 
 
 
 
 
     find()     count()
mongoTemplate.find(q,java.util.HashMap.class,'test') gives following query (peeked in mongo console):
The same query, when used in count (mongoTemplate.count(q,'test')) gives:
This is inconsistent since we could have records with field id and they will be retrieved properly from the db.
Count on the other hand will give bad results since it uses _id field.
treat id _ id as same field treat id _ id in org.springframework.data.mongodb.core.convert.QueryMapper
This is probably the cause of the strange behaviour because find() method does not use QueryMapper and count() does.","org.springframework.data.mongodb.core.mapping.MongoMappingContext
org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-571,2012-11-09T08:00:10.000-06:00,Spring Data for MongoDb doesn't save null values when @Version is added to domain class,"Scenario 
 CrudRepository.findOne()  
 @Version 
 CrudRepository.save()  
 @Version
not use @Version annotation in domain class definition","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.Update"
FILE,DATAMONGO,DATAMONGO-663,2013-04-24T03:21:19.000-05:00,,"class Field   equals()  
  
 boolean fieldsEqual = this.fieldSpec == null ? that.fieldSpec == null : this.fieldSpec.equals(that.fieldSpec);
The above class Field does not has an equals() method.
But org.springframework.data.mongodb.core.query.Query has an equals method which is using the equals method of the Field class.
implement equals on Field method
need equals method for unit testing",org.springframework.data.mongodb.core.query.Field
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,Updating an object does not write type information for objects to be updated,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
I used 1.0.0.
M5 version, and the type information (under _class key) was stored with object when it was necessary to be able to read it from database later.
work till upgrade work to 1.0.0
RELEASE version that broke my application as it saves the objects without type information and later it is impossible to read it back to java model.
What I found is that MappingMongoConverter.writeInternal(...) method that in turn calls addCustomTypeIfNecessary(...) (line 330) which puts type information into DBObject.
During execution of convertToMongoType(...) (at line 851) removeTypeInfoRecursively(...) is called which clears type data saved earlier under _class key.
comment out call
save type information to DBObject","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-717,2013-07-10T11:13:46.000-05:00,Application context is not properly distributed to persistent entities,"@Override




	protected <T> BasicMongoPersistentEntity<T> createPersistentEntity(TypeInformation<T> typeInformation) {









		BasicMongoPersistentEntity<T> entity = new BasicMongoPersistentEntity<T>(typeInformation);









		if (context != null) {




			entity.setApplicationContext(context);




		}









		return entity;




	}









	




	 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




	}






 
  
 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




                // Send the application context to ALL the PersistentEntities, not just ones created after this point




               for (BasicMongoPersistentEntity entity : getPersistentEntities()) {




                   entity.setApplicationContext(applicationContext);




               }




	}






      testMultiTenantSave()  
   initialize()    
 
 @Bean




	public MongoMappingContext mongoMappingContext() throws ClassNotFoundException {









		MongoMappingContext mappingContext = new MongoMappingContext();




		mappingContext.setInitialEntitySet(getInitialEntitySet());




		mappingContext.setSimpleTypeHolder(customConversions().getSimpleTypeHolder());




		mappingContext.initialize(); // <----









		return mappingContext;




	}
The MongoMappingContext does not properly distribute the application context when set to persistent entities that have already been added.
protect <T> BasicMongoPersistentEntity<T>
throw BeansException {
throw BeansException {
send application context to ALL create after point
see referenced URL poc
call initialize() on MongoMappingContext return object
throw ClassNotFoundException {",org.springframework.data.mongodb.config.AbstractMongoConfigurationUnitTests
FILE,DATAMONGO,DATAMONGO-721,2013-07-11T11:36:06.000-05:00,Polymorphic attribute type not persisted on update operations,"@Document
public class ParentClass {
   private List<ChildClass> list;
}
    @Document   
        
  
 mongoTemplate.updateFirst(Query.query(criteria), 
  new Update().push(""list"", child));
find with Spring data find for Mongo DB
annotate with @Document
When using MongoTemplate class with code such as below, the _class attribute is not inserted on the embedded document, so, if one of the items of the list attribute is a subclass of ChildClass, and ChildClass is an abstract class, we begin to face instantiation problems.
If child is a subclass of ChildClass, the _class attribute is not added to the embedded document.",org.springframework.data.mongodb.core.convert.QueryMapper
FILE,DATAMONGO,DATAMONGO-602,2013-01-30T02:22:53.000-06:00,Querying with $in operator on the id field of type BigInteger returns zero results,"List<BigInteger> profileIds = findProfileIds();




Predicate predicate = QProfileDocument.profileDocument.id.in(profileIds);




Iterable<ProfileDocument> profiles = profileRepository.findAll(predicate);
map id field as BigInteger
The query looks like this for a profileIds with multiple elements
and it looks like this when there is only one item in the list.
In the first case, query will return no results and in the second it will work and return an Iterable with one item.
be with representation
use decimal format not work with MongoDB",org.springframework.data.mongodb.core.MongoTemplateTests
FILE,DATAMONGO,DATAMONGO-805,2013-12-02T06:34:36.000-06:00,Excluding DBRef field in a query causes a MappingException,"Query query = new Query(Criteria.where(""parentField"").is(""test""));
        query.fields().exclude(""children"");
        ParentClass parentClass = mongoOperations.findOne(query, ParentClass.class);
Excluding a field in a query where the field is a DBRef as below throws a MappingException.
Exception trace:
find for class java.lang.Integer
include for other fields","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.mapping.MappingTests
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-887,2014-03-20T21:01:31.000-05:00,Repository not instantiated when entity contains field of type TreeMap,"public interface SourceRepository extends PagingAndSortingRepository<MyEntity, ObjectId> {




}









// Entity




 @Document(""test_collection"")




public class MyEntity {




private TreeMap<String,MyClass> treeMap;




}









// TreeMap value class




 public class MyClass {




private String foo = ""bar"";




}
Repository fails to instantiate when the entity contains a TreeMap with value class that does not have a convertor.
affect TreeMap test with HashMap TreeSet
build convertor for MyClass
work in 1.4.0
fail in version 1.4.1
Throws: java.lang.ArrayIndexOutOfBoundsException: 0
In: org.springframework.data.util.ParameterizedTypeInformation.getComponentType(ParameterizedTypeInformation.java:148)",org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
FILE,DATAMONGO,DATAMONGO-897,2014-04-01T04:38:51.000-05:00,,"MongoTemplate.findAndModify(...)   @DbRef  @DbRef
NullPointerException is thrown when using MongoTemplate.findAndModify(...) with @DbRef and interface as @DbRef target.
attach project for more details
pass with Spring Commons 1.6.3","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.QueryMapper"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,"Using ""OrderBy"" in ""query by method name"" ignores the @Field annotation for field alias.","@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
int score
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-938,2014-05-21T06:09:48.000-05:00,Exception when creating geo within Criteria using MapReduce,"Criteria.where(""location"")  within(new Box(lowerLeft, upperRight));
I am getting an IllegalArgumentException when I try to query a MongoDB collection using a Criteria.within and a Box.
The exception reads:
not serialize class org.springframework.data.mongodb.core.query.GeoCommand","org.springframework.data.mongodb.core.mapreduce.MapReduceTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
and Child class
assertNull(persisted.getChild()); // it fails
bring lot of problems persist same entity by accident
I attached a project with the JUnit test which reproduces the problem for you.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-1053,2014-09-12T12:14:49.000-05:00,"In 1.6, any field in a mapped object named ""language"" will fail to map if it is a type other than String","package com.instantly.pipeline.engine.model;









import javax.persistence.Id;









public class Foobar {









    @Id




    String id;




    




    private Number language;









    public String getId() {




        return id;




    }









    public void setId(String id) {




        this.id = id;




    }









    public Number getLanguage() {




        return language;




    }









    public void setLanguage(Number language) {




        this.language = language;




    }




}
In 1.6, there is now a restriction that a field named ""language"" must be a String.
So, in 1.5, the following will map but will not map in 1.6:
package com.instantly.pipeline.engine.model;
import javax.persistence.Id;","org.springframework.data.mongodb.core.mapping.BasicMongoPersistentEntity
org.springframework.data.mongodb.core.mapping.BasicMongoPersistentEntityUnitTests"
FILE,DATAMONGO,DATAMONGO-1050,2014-09-09T03:41:12.000-05:00,"SimpleMongoRepository.findById(id, class)  don't return ids for nested documents.","class A {




 private String id; //stored in mongo as ""id""




 private String name;




}









 class B {




  private String id; // stored in mongo as ""_id""




  private List<A> innerDocs;




}






     
  
 @Data  @NoArgsConstructor
SimpleMongoRepository.findById(id, class)  don't return ids for nested documents.
store as _ id store in mongo
When it return B documents, innerDocs A objects have id == null.
refactor A.id to A.myId
save fields
create ids
have characters have ids relate to _ id relate to ids
have different field name","org.springframework.data.mongodb.core.mapping.BasicMongoPersistentProperty
org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
org.springframework.data.mongodb.core.mapping.BasicMongoPersistentPropertyUnitTests
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,"@Query $in does not remove ""_class"" property on collection of embedded objects","@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
generates incorrect query.
relate bug to https://jira.spring.io/browse/DATAMONGO-893","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,"geoNear, does not return all matching elements, it returns only a max of 100 documents","public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
document geoNear method
return { @link georesults } for matching match given { @link nearquery }
I expect 1000 ""matching"" documents But i only get 100.
There is some default being set, that restricts the result to 100.
state in method
have pageable",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
The query returns results but getTotalElements() and getTotalPages() always returns 0.
Also when you try to get any other page than 0, no results return.
use with member
Assert.assertEquals(30, results.getTotalElements()); // this is returning 0
Assert.assertEquals(6, results.getTotalPages());     // this is returning 0","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1202,2015-04-14T02:36:40.000-05:00,,"@Indexed
Under simple scenarios with reflexive DBRef relations works as expected, but if used in conjunction with generics it doesn't create the index expected.
run without Mongo server
If you run Application it will just create indexes and put some data in customer collection assuming a MongoD is running locally at 27017","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreator
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreatorIntegrationTests
org.springframework.data.mongodb.core.index.IndexResolver"
FILE,DATAMONGO,DATAMONGO-1250,2015-07-03T21:07:44.000-05:00,Custom converter implementation not used in updates,"@Document 
 
 
 @Document




public class MyPersistantObject  
 public Allocation allocation;




     public BigDecimal value;









     
 private final String code;









         Allocation(String code) {




            this.code = code;




        }









         public static Converter<Allocation, String> writer() {




            return new Converter<Allocation, String>() {




                public String convert(Allocation allocation) {




                    return allocation.getCode();




                }




            };




        }









         public static Converter<String, Allocation> reader() {




            return new Converter<String, Allocation>() {




                public Allocation convert(String source) {




                    return Allocation.getByCode(source);




                }




            };




        }









         public static Allocation getByCode(String code)  
 return AVAILABLE;




                 
 return ALLOCATED;




             
 throw new IllegalArgumentException(""Unable to get Allocation from: "" + code);




         
 public String getCode() {




            return code;




        }




     
 @Bean




    public CustomConversions customConversions() {




        return new CustomConversions(Arrays.asList(




                MyPersistantObject.Allocation.reader(),




                MyPersistantObject.Allocation.writer()




        ));




    }






 
 @Test




    public void testConversion() {




        Update update;




        Query query;




        MyPersistantObject returned;




        MyPersistantObject myPersistantObject = new MyPersistantObject();




        myPersistantObject.allocation = AVAILABLE;




        myPersistantObject.value = new BigDecimal(1234567);









        mongoTemplate.save(myPersistantObject);









        // Check it was saved correctly - first with invalid allocation to confirm conversion in query




        query = query(where(""allocation"").is(ALLOCATED));




        assertThat(mongoTemplate.findOne(query, MyPersistantObject.class), is(nullValue()));









        // Check it was saved correctly - now with valid allocation to confirm conversion in query




        query = query(where(""allocation"").is(AVAILABLE));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(AVAILABLE));




        assertThat(returned.value.longValue(), is(1234567L));









        try {




            // Update allocation from constant - will fail




            update = update(""allocation"", ALLOCATED);




            mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        } catch (Exception e) {




            System.err.println(""failed to convert allocation: java.lang.IllegalArgumentException: can't serialize class converter_test.MyPersistantObject$Allocation"");




        }









        // Update allocation from string value - succeeds




        update = update(""allocation"", ALLOCATED.getCode());




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check allocation update




        query = query(where(""allocation"").is(ALLOCATED));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(ALLOCATED));









        // Update value only - will fail: Caused by: java.lang.IllegalArgumentException: Unable to get MyPersistantObject.Allocation from: 54321




        // Tries to use MyPersistantObject.Allocation converter to String




        update = update(""value"", new BigDecimal(54321));




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check value update




        returned = mongoTemplate.findAll(MyPersistantObject.class).get(0);




        assertThat(returned.value.longValue(), is(54321L));




    }
use in mongoTemplate.update*
It also works when building and executing a Query object.
However when used in an Update, it is either ignored, or called in situations where it shouldn't.
return new converter <allocation, string> {
return new converter <string, allocation> {
throw new illegalargumentexception
use full Enum name use short code
register in Spring boot application entry point
return new customconversions
save first allocation confirm conversion in query
confirm conversion in query save with valid allocation
try {
convert allocation not serialize class converter_test
check allocation update
update value get MyPersistantObject.Allocation
use MyPersistantObject.Allocation converter to string
check value update
make sense
save for object query for object call converters on document
store name store Enum code
By use of a positive and negative case, it appears that the converter is being called correctly when used in the Query builder.
When it comes to an Update, the Enum is unable to be serialised correctly, and an exception is thrown to that effect.
use code back by Querying back from DB
So it appears that the customer converter for converting from my Enum is not called in this situation.
update other value in document
The BigDecimal is converted to a String by an existing converter I assume, but then my customer converter is called to try and convert the numeric String into an Allocation Enum which of course fails.
debug code be in customconversions
That second variant never seems to be called in MappingMongoConverter, but perhaps if that type information was passed then it would not use my Allocation converter.
Without type information it seems the default is just to use the first converter that can handle the input type, in this case a String.
not handle custom one
not find workaround do update resort to Mongo","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.UpdateMapper"
FILE,DATAMONGO,DATAMONGO-1307,2015-10-20T12:33:45.000-05:00,,"throw exceptionTranslator.translateExceptionIfPossible(ex);
  
 
 
 
 return null;
MongoTemplate has code like this in many places:
throw exceptionTranslator.translateExceptionIfPossible(ex)
MongoExceptionTranslator, however, often does NOT return an exception.
If it encounters an unknown exception it does this:
// If we get here, we have an exception that resulted from user code,
// rather than the persistence provider, so we return null to indicate
// that translation should not occur.
return null;
MongoTemplate then ""eats"" the original exception and throws a null-pointer exception instead.",org.springframework.data.mongodb.core.MongoTemplate
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
When an association between documents involves generic types, the type information is not correctly inferred at startup time resulting in missing indexes.
see https://github.com/agustisanchez/SpringDataMongoDBBug for code samples
attribute super class AbstractProduct with index
The index ""name"" inherited from AbstractProduct is created (book2.content.name) inside ""catalog"" , but the index defined on the Book class itself (isbn) is not created as Spring Data Mongo is only inferring type infromation from the ProductWrapper class definition (ProductWrapper <T extends AbstractProduct>).
If the wrapper class is defined as ProductWrapper<T>, then no indexes are created at all on Catalog.books2.content.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1360,2016-01-16T07:47:34.000-06:00,,"query.addCriteria(where(""createdDate"").lte(LocalDateTime.now()));
The resulting MongoDb query looks like this:
It consequently fails with this message:
not serialize class java.time.LocalDateTime at org.bson.BasicBSONEncoder.
It does not fail when I use a java.util.Date in my query even though I have stilled persisted my document with a java.time.LocalDateTime object.
The query then looks slightly different like this:
convert LocalDateTime objects to Date objects","org.springframework.data.mongodb.core.Venue
org.springframework.data.mongodb.core.geo.AbstractGeoSpatialTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1438,2016-05-26T14:01:14.000-05:00,I get a warning in my logs since switched to Spring Data MongoDB Hopper-SR1 Release Train in Spring Boot 1.3.5,"@Document
When I start my Spring Boot 1.3.5 application with no custom conversions and with Spring Data MongoDB Release Train Hopper-SR1 I get following warning in my logs:
register converter from class java.lang.Number register converter to class write converter support type for registering not convert to Mongo
check annotation setup at converter implementation","org.springframework.data.mongodb.core.convert.MongoConvertersUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,Query mapper does not use @Field field name when querying nested fields in combination with nested keywords,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
protect <s, t> List<T>
resolves the fields to the input query to the ones in the @Field annotations, except for these in embedded arrays.
So, in the example above, resolution fields in DBObject remains resolution.
While, the status field resolves to stat.
note queries in inner list
Which doesn't get any data, because there is no field called resolution (the field in mongo is res).
Notice the status and displays fields correctly get converted to the value in the @Field annotation.
This basically means that any queries that operate on fields (with a name different from the peristed name) in the inner list will fail.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-1486,2016-09-07T16:46:53.000-05:00,Changes to MappingMongoConverter Result in Class Cast Exception,"Map<Integer, Map<Platform, String>> descriptions = new HashMap<>();






 
 public void setAlternateDescriptionMap(int compositeId, Map<Integer, Map<Platform, String>> alternateDescriptionsMap) {




		Query query = new Query();




		query.addCriteria(Criteria.where(""_id"").is(compositeId));




		Update update = new Update();




		update.set(""alternateDescriptionMap"", alternateDescriptionsMap);









		coreMongoTemplate.updateFirst(query, update, ""product"");




	}






   
    
 
 MappingMongoConverter.convertMongoType()
upgrade software to use Spring boot + spring
use Spring data Mongo 1.9.2 as part
We end up getting the following exception:
not cast java.lang.Integer to java.lang.String
not get exception
appear after lot relate to code
introduce issue
work on overriding default method
break in other cases map key
need further input","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-1498,2016-09-25T22:21:26.000-05:00,,"@JsonProperty(""myDate"")




    private DateTime myDate;






 
 
 
  
 
  @JsonUnwrapped 
 @JsonProperty(""myDate"")




    @JsonUnwrapped




    private DateTime myDate;






  @JsonUnwrapped
As described in this SO thread, in Spring Boot 1.3.6, Spring Data 1.8.4, Spring Data REST 2.4.4 the Spring Data MongoDB entities that have a property of Joda DateTime type:
gets rendered into in the REST representation of my Spring Data entity as
With Spring Boot 1.4.1, Spring Data MongoDB 1.9.3, Spring Data REST 2.5.3 same properties suddenly get represented as
It looks like Joda's DateTime started to get treated as data entity again:
get representation
not be in_case_of auto-generated properties not be from JSON schema
generate annotations set @JsonUnwrapped on child/empbedded objects
These types will be considered simple ones (which means they neither need deeper inspection nor nested conversion.
Thus the
CustomConversions also act as factory for SimpleTypeHolder
Which is in fact not true for Spring Data REST [any more], from what I can tell debugging my code.
The types that are added via default CustomConversions are not treated as simple ones when rendering JSON and properties of those types get serialised as embedded entity objects.
Here is the place in the Spring Data REST PersistentEntityJackson2Module that calls to Spring Data MongoDB PersistentEntity implementation to check whether the property type is simple or not.
And since the simpleTypeHolder in the Spring Data Commons AbstractMappingContext does not contain the types for which Spring Boot auto-configuration adds Joda DateTime converters, the DateTime field is treated as complex object.
present major problem upgrade with generic solution upgrade from Spring boot break REST apis by default know for Joda DateTime properties","org.springframework.data.mongodb.repository.config.MongoRepositoryConfigurationExtension
org.springframework.data.mongodb.config.AuditingViaJavaConfigRepositoriesTests
org.springframework.data.mongodb.config.MongoAuditingRegistrar"
CLASS,derby-10.7.1.1,DERBY-4624,2010-04-21T05:30:55.000-05:00,Broken logic for avoiding testing across midnight in TimestampArithTest,"while (calendar.get(Calendar.HOUR) == 23
						&& calendar.get(Calendar.MINUTE) >= 58) {
					try {
						Thread.sleep((60 - calendar.get(Calendar.SECOND)) * 1000);
					} catch (InterruptedException ie) {
						// ignore it
					}
				}

 
 calendar.get(Calendar.HOUR)   calendar.get(Calendar.HOUR)
have code avoid failures in case
midnight TODAY
try {
never evaluate to true value in range
2 If the current time is after 23:58 and before 23:59, the code sleeps until 23:59, the test will wait until 23:59 before it starts, making it even more likely that it will cross midnight while running.
3 The code is executed after the Calendar object has been initialized, so if this code is ever triggered and waits until after midnight, the TODAY field is guaranteed to be yesterday when the test starts executing.",org.apache.derbyTesting.functionTests.tests.lang.TimestampArithTest
CLASS,derby-10.7.1.1,DERBY-4654,2010-05-12T05:27:40.000-05:00,Restriction.toSQL() doesn't escape special characters,"org.apache.derby.vti.Restriction.toSQL()  
 Restriction.doubleQuote()   IdUtil.normalToDelimited()
org.apache.derby.vti.Restriction.toSQL() adds double quotes around column names, but it does not escape the special characters (like double quotes) in the column names, so the returned string may not be valid SQL.
cause problems use restriction generate query against external database","org.apache.derbyTesting.functionTests.tests.lang.RestrictedVTITest
org.apache.derby.vti.Restriction"
CLASS,derby-10.7.1.1,DERBY-4664,2010-05-18T18:56:12.000-05:00,,"DriverManager.getConnection(""jdbc:default:connection"");    
  
   
    SystemProcedures.getDefaultConn()   InternalDriver.activeDriver()
Some Derby internal Stored procedures and functions call DriverManager.getConnection(""jdbc:default:connection""); and this url can be recognized by another Driver in the same classpath that is used for server side JDBC for another product.
For example the below occurred in NetworkServer when JCC was also loaded because the JCC Type 2 driver is used for server side JDBC:
avoid specific error by changing change LobStoredProcedure use code in SystemProcedures.getDefaultConn() connect code in SystemProcedures.getDefaultConn() connect code to Derby
not solve general problem for user create function create procedure perform SQL perform procedure perform function
ask JCC driver team run inside db2 process
think regardless_of product workarounds
not differentiate URL use same one for server side JDBC",org.apache.derby.impl.jdbc.LOBStoredProcedure
CLASS,derby-10.7.1.1,DERBY-4665,2010-05-19T03:12:33.000-05:00,Unidiomatic error handling in TimestampArithTest,"printStackTrace(sqle);
					fail(""Unexpected exception from statement '"" + sql + ""'"");

 
 {
			System.out.println(s + "" is not a proper timestamp string."");
			System.out.println(e.getClass().getName() + "": "" + e.getMessage());
			e.printStackTrace();
			System.exit(1);
			return null;
		}
TimestampArithTest contains some error handling code that prevents the underlying error from being reported to the JUnit framework, and it may even terminate the JVM running the tests on some errors.
print stack trace of underlying error not include in report
terminate JVM on error prevent subsequent tests prevent junit framework report results from tests do run do tests
catch { System.out.println(s + "" is not a proper timestamp string."")","org.apache.derbyTesting.functionTests.tests.lang.TimestampArithTest.OneTest
org.apache.derbyTesting.functionTests.tests.lang.TimestampArithTest"
CLASS,derby-10.7.1.1,DERBY-4835,2010-10-06T11:05:13.000-05:00,Trigger plan does not recompile with upgrade from 10.5.3.0 to 10.6.1.0 causing  java.lang.NoSuchMethodError,"tidlggls(blt_number,create_date,update_date,propagation_date,glossary_status,
     time_stamp,min_max_size )
    
      
 
  
 tidlrblt(BLT,BLT_SIZE,MIN_MAX_SIZE)  
 
     
  
   GeneratedMe
thod;    
  
  
 if (fromVersion.majorVersionNumber >= DataDictionary.DD_VERSION_DERBY_10_5)
				bootingDictionary.updateMetadataSPSes(tc);
			else
				bootingDictionary.clearSPSPlans();

  clearSPSPlans()
Trigger plan does not recompile on upgrade from 10.5.3.0 to 10.6.1.0  causing the following exception  the first time the trigger is fired after upgrade.
have code in handleMinorRevisionChange have DERBY-1107 change in handleMinorRevisionChange relate to DERBY-1107 change relate if bootingDictionary.updateMetadataSPSes(tc)
not be in else clause
recreate trigger work after connecting work around issue drop after connecting drop around issue connect with 10.6.1","org.apache.derby.impl.sql.catalog.DD_Version
org.apache.derbyTesting.functionTests.tests.upgradeTests.BasicSetup"
CLASS,derby-10.7.1.1,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:
be after latest check
attach derby.log.",org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.7.1.1,DERBY-4889,2010-11-05T20:06:56.000-05:00,Different byte to boolean conversion on embedded and client,"PreparedStatement ps = c.prepareStatement(""values cast(? as boolean)"");
        ps.setByte(1, (byte) 32);
        ResultSet rs = ps.executeQuery();
        rs.next();
        System.out.println(rs.getBoolean(1));

 If setByte()   setInt()
The following code prints ""true"" with the embedded driver and ""false"" with the client driver:
cast (
If setByte() is replaced with setInt(), they both print ""true"".","org.apache.derbyTesting.functionTests.tests.jdbcapi.ParameterMappingTest
org.apache.derby.impl.drda.DRDAConnThread"
CLASS,derby-10.7.1.1,DERBY-4892,2010-11-06T04:14:51.000-05:00,Unsafe use of BigDecimal constructors,"test_06_casts(org.apache.derbyTesting.functionTests.tests.lang.UDTTest)
have code use BigDecimal constructors work on Java not add BigDecimal constructors until Java
take single int
use constructors in following classes
compile against } point to proper Java
pick constructor not find for int
work on Java
The problem appears when the build does not use the Java 1.4 libraries.
end up building java14compile.classpath build without customized ant.properties
find int in case find int of constructor find long variants in case find long variants of constructor
The compiled byte-code will therefore use those Java 5 constructors, and the code will fail at run-time if ever executed on a Java 1.4 JVM.
You'll see two errors of this kind:","org.apache.derbyTesting.functionTests.tests.lang.Price
org.apache.derby.client.am.Cursor
org.apache.derbyTesting.system.oe.client.Submitter"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,Pig creates wrong schema after dereferencing nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
DESCRIBE thinks it is {f1: int, f2: int} instead.
When dump is
used, the data is actually in form of the correct schema however, ex.
Because the schema is incorrect,
the reference to ""nested_tuple"" in the ""uses_dereferenced"" statement is
considered to be invalid, and the script fails to run.
The error is:
invalid field projection
not exist in schema","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2970,2012-10-12T15:58:03.000-05:00,Nested foreach getting incorrect schema when having unrelated inner query,"{noformat}
 
 {
     4          sym = daily.symbol;
     5          uniq_sym = distinct sym;
     6          --ignoring uniq_sym result
     7          generate group, daily;
     8  } 
  
  
 {noformat}
While looking at PIG-2968, hit a weird error message.
explain zzz
column > invalid field projection
not exist in schema","src.org.apache.pig.PigServer
src.org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
test.org.apache.pig.test.TestEvalPipelineLocal"
CLASS,pig-0.11.1,PIG-3060,2012-11-20T02:29:04.000-06:00,FLATTEN in nested foreach fails when the input contains an empty bag,"{code}
  {(t:chararray)} 
 {
  c1 = foreach A generate FLATTEN(a1);
  generate COUNT(c1);
} ;
{code}
FLATTEN inside a foreach statement produces wrong results, if the input contains an empty bag.
generate COUNT filter out empty bags","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach
test.org.apache.pig.test.TestEvalPipelineLocal"
CLASS,pig-0.11.1,PIG-3114,2013-01-03T19:49:42.000-06:00,Duplicated macro name error when using pigunit,"{code:title=test.pig|borderStyle=solid}
    {
    $C = ORDER $QUERY BY total DESC, $A;
}  
  
     AS total;

queries_ordered = my_macro_1(queries_count, query);

    
   ;
{code}
Pig runs fine on cluster but getting parsing error with pigunit.
try basic pig script with macro try basic pig script with getting macro similar error get similar error
parse <line 9> null
remove macro pigunit
define macro parse error result in parsing","src.org.apache.pig.PigServer
test.org.apache.pig.test.pigunit.TestPigTest
test.org.apache.pig.pigunit.PigTest
test.org.apache.pig.pigunit.pig.PigServer"
CLASS,pig-0.11.1,PIG-3267,2013-04-03T16:14:30.000-05:00,,"{code}
 
  
  
     ;
{code}

 
 {code}
  {code}
generate age into using org.apache.hcatalog.pig.HCatStorer generate age as number
Error happens before launching the second job. Error message:","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.11.1,PIG-3292,2013-04-24T03:06:41.000-05:00,Logical plan invalid state: duplicate uid in schema during self-join to get cross product,"{code}
 
  
   {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}

 
 {code}
   
 {code}

 
 {code}
 
  
   {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}
work in different way
release note include into CDH dist http://archive.cloudera.com/cdh4/cdh/4/pig-0.10.0-cdh4.2.0.CHANGES.txt
generate flatten(pair)
And an error:
{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2270: Logical plan invalid state: duplicate uid in schema : 1-7::x#16:bytearray,y::x#16:bytearray
{code}
generate flatten(pair)","test.org.apache.pig.test.TestEvalPipelineLocal
src.org.apache.pig.newplan.logical.relational.LOCross"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,"ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations","{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
inp AS
j by shopId
provide minimal reproduction case
This will give a wrongful output like .
.
{code}
(1 1001,1001)
(1 1002,1002)
(1 1002,1002)
(1 1002,1002)
{code}
move LOFilter operation before join be in initial case fail at place fail because_of PushUpFilter optimization work on tuple
create LOSplitOutputs reset schema regenerate uids for fields
get new uid
have same uid have join looks
get separate uids
recurse on nested schema fields
have light understanding
reproduce issue
run unit tests with fix
like wrong way fix issue",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,pig-0.11.1,PIG-3329,2013-05-16T22:44:41.000-05:00,,"RANK b;
dump d;
job will fail with error message:
read counter pig.counters.counter_4929375455335572575_-1","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.11.1,PIG-3379,2013-07-16T13:37:26.000-05:00,Alias reuse in nested foreach causes PIG script to fail,"{code:title=temp.pig}
       
      
    
    {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);

  DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);

  GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
        
  GENERATE timeStamp;
describe A;
{code}
 
 {code}
   
    
 {code}
not exist in schema
use distinct alias name for 2nd distinctdevices
remove last filter statement fix as observation","src.org.apache.pig.parser.LogicalPlanBuilder
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.expression.ProjectExpression"
CLASS,mahout-0.8,MAHOUT-1261,2013-06-13T10:09:15.000-05:00,TasteHadoopUtils.idToIndex can return an int that has size Integer.MAX_VALUE,"Longs.hashCode(id)
run ItemSimilarityJob on large matrix
The job fails because of an IndexException in ToUserVectorsReducer.
For some id (I don't know what value), the result returned is Integer.MAX_VALUE.
This cannot be set in the userVector because the cardinality of that is also Integer.MAX_VALUE and it throws an exception.
So, the issue is that values from 0 to INT_MAX are returned by idToIndex but the vector only has 0 to INT_MAX - 1 possible entries.
think of %
ssc ] ssc everyone",core.src.main.java.org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
reduce method on line
full stack trace:",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
CLASS,mahout-0.8,MAHOUT-1317,2013-08-23T13:05:58.000-05:00,,"Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"");
get errors in experimenting get errors from RowSimilarityJob experiment with things realize in looking look at source
be in case be of form
Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's ""incorrect"", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.
show few more cases across code base save time get relevant error","core.src.main.java.org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluator
math.src.main.java.org.apache.mahout.math.CholeskyDecomposition
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategy
core.src.main.java.org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveImplicitFeedbackMapper
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity
core.src.main.java.org.apache.mahout.math.neighborhood.ProjectionSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.TopItems
math.src.main.java.org.apache.mahout.math.random.Empirical
core.src.main.java.org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob
core.src.test.java.org.apache.mahout.math.neighborhood.SearchQualityTest
integration.src.main.java.org.apache.mahout.utils.SplitInput
core.src.test.java.org.apache.mahout.math.neighborhood.SearchQualityTest.StripWeight
core.src.main.java.org.apache.mahout.clustering.kmeans.RandomSeedGenerator
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.LuceneIterator
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver
examples.src.main.java.org.apache.mahout.cf.taste.example.kddcup.KDDCupDataModel
math.src.main.java.org.apache.mahout.math.als.AlternatingLeastSquaresSolver
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveExplicitFeedbackMapper
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader
math.src.main.java.org.apache.mahout.math.random.ChineseRestaurant
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.TreeID
core.src.main.java.org.apache.mahout.classifier.df.data.DataConverter
core.src.main.java.org.apache.mahout.math.Varint
core.src.main.java.org.apache.mahout.math.neighborhood.BruteSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.AbstractDifferenceRecommenderEvaluator
core.src.main.java.org.apache.mahout.classifier.naivebayes.training.WeightsMapper
core.src.main.java.org.apache.mahout.math.neighborhood.FastProjectionSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper
core.src.main.java.org.apache.mahout.classifier.df.data.Dataset
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel
core.src.main.java.org.apache.mahout.common.iterator.SamplingIterator
core.src.main.java.org.apache.mahout.cf.taste.impl.common.WeightedRunningAverage"
CLASS,mahout-0.8,MAHOUT-1320,2013-08-29T04:07:09.000-05:00,,"{noformat}
  
 testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)   
 {noformat}


 
 {noformat}
 
      
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)   
 {noformat}
From time to time this test fails with following in build log:
Here is a bit more of build log output, which also shows other tests were running in parallel with this one:
run org.apache.mahout.common.distance.TestChebyshevMeasure
run org.apache.mahout.common.distance.TestMinkowskiMeasure
run org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure
run org.apache.mahout.common.distance.TestManhattanDistanceMeasure
run org.apache.mahout.common.distance.CosineDistanceMeasureTest
run org.apache.mahout.common.distance.TestTanimotoDistanceMeasure
run org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure
run org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest
run org.apache.mahout.common.distance.TestEuclideanDistanceMeasure
run org.apache.mahout.common.iterator.TestFixedSizeSampler
run org.apache.mahout.common.iterator.CountingIteratorTest
run org.apache.mahout.common.iterator.TestSamplingIterator
run org.apache.mahout.common.iterator.TestStableFixedSizeSampler
run org.apache.mahout.common.DummyRecordWriterTest
run org.apache.mahout.common.StringUtilsTest
run org.apache.mahout.common.AbstractJobTest
run org.apache.mahout.common.IntPairWritableTest
run org.apache.mahout.common.lucene.AnalyzerUtilsTest
run org.apache.mahout.clustering.topdown.PathDirectoryTest
run org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest
run org.apache.mahout.clustering.classify.ClusterClassificationDriverTest
run org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest
run org.apache.mahout.clustering.spectral.TestVectorCache
run org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob
run org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob
run org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer
run org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob
run org.apache.mahout.clustering.spectral.TestUnitVectorizerJob
run org.apache.mahout.clustering.canopy.TestCanopyCreation
run org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator
run org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator
run org.apache.mahout.clustering.kmeans.TestKmeansClustering
run org.apache.mahout.clustering.TestGaussianAccumulators
run org.apache.mahout.clustering.iterator.TestClusterClassifier
run org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering
run org.apache.mahout.clustering.TestClusterInterface
run org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest
run org.apache.mahout.clustering.streaming.cluster.BallKMeansTest
run org.apache.mahout.math.stats.OnlineAucTest
run org.apache.mahout.math.stats.SamplerTest
run org.apache.mahout.math.VarintTest
run org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest
run org.apache.mahout.math.hadoop.stats.BasicStatsTest
run org.apache.mahout.math.hadoop.TestDistributedRowMatrix
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest
run org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI
run org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver
run org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob
run org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest
run org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest
run org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI
run org.apache.mahout.math.VectorWritableTest
run org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest
run org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest
run org.apache.mahout.math.neighborhood.SearchSanityTest
run org.apache.mahout.math.MatrixWritableTest
run org.apache.mahout.vectorizer.DocumentProcessorTest
run org.apache.mahout.vectorizer.HighDFWordsPrunerTest
run org.apache.mahout.math.neighborhood.SearchQualityTest
run org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest
run org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest
run org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest
run org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest
run org.apache.mahout.vectorizer.encoders.TextValueEncoderTest
run org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest
run org.apache.mahout.vectorizer.collocations.llr.GramTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest
run org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest
run org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest
run org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyTest
run org.apache.mahout.vectorizer.DictionaryVectorizerTest
run org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest
be on ubuntu-1 node","core.src.test.java.org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest
core.src.test.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansTestMR
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.BallKMeansTest
core.src.test.java.org.apache.mahout.clustering.streaming.cluster.DataUtils"
CLASS,mahout-0.8,MAHOUT-1336,2013-09-16T23:35:54.000-05:00,HighDFWordsPrunerTest is failing silently,"{noformat}
 
        
        
      
         
      
 {noformat}
Apparently ToolRunner does not allow the --mapred option.
The validation is not very foolproof, so there is a resulting silent failure in HighDFWordsPrunerTest.
Error message:
mapr while processing options",core.src.test.java.org.apache.mahout.vectorizer.HighDFWordsPrunerTest
CLASS,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
not support use case
have repository of 500K documents generate dictionary generate vectors generate 500K documents use custom code
The kmeans ran fine and generate sensible looking results, but when I tried
to run ClusterDumper I got the following error:
use /usr/bin/hadoop HADOOP_CONF_DIR = run on hadoop
endphase = [ 2147483647 ]
access dictionary for feature
look at dictionary loading code (
size dictionary array for number
run custom dictionary/feature generation code have unique features die on index
not create dictionary array not create reason have size have dictionary array read size from dictionary sequence file
misunderstand something
It worked fine when I reduced the hash size to be <= than the total number
of features, but this is not desirable in general (for me) since I don't
know the number of features before I run the job (and if I guess too high
then ClusterDumper crashes)",integration.src.main.java.org.apache.mahout.utils.vectors.VectorHelper
CLASS,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}

 {Code}

  StreamingKMeansThread.call()

 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }

    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
have nonzero number of training test vectors
ask for %
use same iterator fail on second use",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread
CLASS,mahout-0.8,MAHOUT-1396,2014-01-15T02:02:41.000-06:00,Accidental use of commons-math won't work with next Hadoop 2 release,"import org.apache.commons.math.special.Gamma;
pull commons-math
be in HEAD
So this will no longer compile against the latest Hadoop.
I believe it will also not actually run again the latest Hadoop, even if one were to use a version compiled versus older Hadoop 2, since the class that uses it is used in the context of Writables -- that is, outside the client environment that might happen to have packaged commons-math -- and so would fail on the cluster.
import commons-math3 class
be for pending release
get in IMHO work with next Hadoop release",core.src.main.java.org.apache.mahout.classifier.sgd.TPrior
CLASS,zookeeper-3.4.5,ZOOKEEPER-1642,2013-02-08T04:30:06.000-06:00,Leader loading database twice,"getLastLoggedZxid()    
 loadData()
The leader server currently loads the database before running leader election when trying to figure out the zxid it needs to use for the election and again when it starts leading.
remove redundant load",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
CLASS,zookeeper-3.4.5,ZOOKEEPER-1700,2013-05-07T19:43:31.000-05:00,FLETest consistently failing - setLastSeenQuorumVerifier seems to be hanging,"{noformat}
   
  
    
    
          
      
    
    
  
  
  
  
      
  
  
     
      
      
    
    
 {noformat}
I'm consistently seeing a failure on my laptop when running the FLETest ""testJoin"" test.
What seems to be happening is that the call to setLastSeenQuorumVerifier is hanging.
See the following log from the test, notice 17:35:57 for the period in question.
Note that I turned on debug logging and added a few log messages around the call to setLastSeenQuorumVerifier (you can see the code enter but never leave)
run test
be in sync
get diff from leader 0x0
receive NEWLEADER message
set quorum verifier
call with stale config 0
shut down acceptor
set to LOOKING
initialize leader election protocol",src.java.test.org.apache.zookeeper.test.FLETest
CLASS,zookeeper-3.4.5,ZOOKEEPER-1753,2013-09-05T09:37:15.000-05:00,"ClientCnxn is not properly releasing the resources, which are used to ping RwServer","pingRwServer()
{code}
             try {
                Socket sock = new Socket(addr.getHostName(), addr.getPort());
                BufferedReader br = new BufferedReader(
                        new InputStreamReader(sock.getInputStream()));
                ......
                sock.close();
                br.close();
            } catch (ConnectException e) {
                // ignore, this just means server is not up
            } catch (IOException e) {
                // some unexpected error, warn about it
                LOG.warn(""Exception while seeking for r/w server "" +
                        e.getMessage(), e);
            }
 {code}
open socket use BufferedReader
These are not properly closed in finally block and could cause leaks on exceptional cases.
catch { // ignore unexpected error catch { //",src.java.main.org.apache.zookeeper.ClientCnxn
CLASS,zookeeper-3.4.5,ZOOKEEPER-1774,2013-10-01T18:14:39.000-05:00,"QuorumPeerMainTest fails consistently with ""complains about host"" assertion failure","{noformat}
     
     
 {noformat}
QuorumPeerMainTest fails consistently with ""complains about host"" assertion failure.
complain about host
complain about host",src.java.test.org.apache.zookeeper.server.quorum.QuorumPeerMainTest
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,ZooKeeper Server fails if snapCount is set to 1,"int randRoll = r.nextInt(snapCount/2);
{code}
If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:
take snapshot at same time
mention restriction in documentation add validation in source code",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
METHOD,bookkeeper-4.1.0,BOOKKEEPER-294,2012-06-12T23:56:56.000-05:00,Not able to start the bookkeeper before the ZK session timeout.,"{noformat}
         
 {noformat}
Not able to start the bookkeeper before the ZK session timeout.
kill bookie start bookie
register ephemeral Znode for bookie","org.apache.bookkeeper.bookie.Bookie:readJournal()
org.apache.bookkeeper.bookie.Bookie:registerBookie(int)
org.apache.bookkeeper.bookie.Bookie:start()
org.apache.bookkeeper.proto.BookieServer:start()"
METHOD,bookkeeper-4.1.0,BOOKKEEPER-355,2012-08-08T05:02:04.000-05:00,"Ledger recovery will mark ledger as closed with -1, in case of slow bookie is added to ensemble during  recovery add","doRecoveryRead()
create ledger with ensemble quorum size write with entry
be first bookie in ensemble
recover same ledger
happen during time add during time
5 This recovery will fail.
recover same ledger
9 Since new bookie is first in the ensemble, doRecoveryRead() is reading from that bookie and getting NoSuchLedgerException and closing the ledger with -1
i.e. Marking the ledger as empty, even though first client had successfully written one entry.","org.apache.bookkeeper.client.LedgerRecoveryOp:doRecoveryRead()
org.apache.bookkeeper.client.LedgerRecoveryOp:readComplete(int, LedgerHandle, Enumeration<LedgerEntry>, Object)
org.apache.bookkeeper.client.LedgerRecoveryOp:LedgerRecoveryOp(LedgerHandle, GenericCallback<Void>)
org.apache.bookkeeper.client.LedgerRecoveryOp:initiate()
org.apache.bookkeeper.client.PendingReadOp:readEntryComplete(int, long, long, ChannelBuffer, Object)"
METHOD,bookkeeper-4.1.0,BOOKKEEPER-371,2012-08-17T05:42:02.000-05:00,NPE in hedwig hub client causes hedwig hub to shut down.,"Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .messageConsumed(messageConsumeData.msg);


  getPipeline()  getLast()   channel.close()   messageConsumed()
connect hedwig client to remote region hub result in channel
disconnect channel
retrieve channel without checking
return null value result in NPE
retrieve channel call messageConsumed()
guess same applies for other instances","org.apache.hedwig.client.netty.HedwigClientImpl:getResponseHandlerFromChannel(Channel)
org.apache.hedwig.client.handlers.MessageConsumeCallback:operationFailed(Object, PubSubException)
org.apache.hedwig.client.handlers.MessageConsumeCallback.MessageConsumeRetryTask:run()"
METHOD,bookkeeper-4.1.0,BOOKKEEPER-387,2012-09-04T04:27:35.000-05:00,,"{code}
     
 {code}
I am trying to upgrade BK from 4.1.0 to 4.2.0, but it will log as ""Directory is current, no need to upgrade�? even then it will continue and fail.
and throwing following exception.
move upgraded directories into place /home/bk4",org.apache.bookkeeper.bookie.UpgradeTest:testCommandLine()
METHOD,bookkeeper-4.1.0,BOOKKEEPER-55,2011-08-25T05:52:23.000-05:00,SubscribeReconnectRetryTask might retry subscription endlessly when another subscription is already successfully created previously,"HedwigSubscriber.subscribe()
recover connection recover subscription try for channelDisconnected envent
call HedwigSubscriber.subscribe() at same time succeed before auto recovery
report topic busy failure
Then the SubscribeReconnectRetryTask will retry again and again endlessly.",org.apache.hedwig.client.handlers.SubscribeReconnectCallback.SubscribeReconnectRetryTask:run()
CLASS,argouml-0.22,3923,2006-02-07T13:17:48.000-06:00,,"Collection actionStates = getModel().getAllActionStates();
  Iterator iterActionState = actionStates.iterator();
iterActionState.hasNext(); 
 ActionStateFacade actionState =
(ActionStateFacade) iterActionState.next();
use activity diagram for AndroMDA
2) If I add my activity diagram under the use case diagram I always get a new activity graph, so I have 2 activity graphs alltogether.
I cannot add an activity diagram under the imported activity graph.
Please see the screenshot I attached.
Screenshot:
actionState is always ""null"".
4) Importing the activity diagram from Poseidon works and the result can be processed by AndroMDA but if you are making the activity diagram from the beginning with ArgoUML, it won't work because of the error above",org.argouml.persistence.XMIParser
METHOD,apache-nutch-1.8,NUTCH-1262,2012-01-31T03:15:33.000-06:00,,"{code}
   
 {code}
Similar or duplicating content-types can end-up differently in an index.
With, for example, both application/xhtml+xml and text/html it is impossible to use a single filter to select `web pages`.
Content-Type mapping is disabled by default and is enabled via moreIndexingFilter.mapMimeTypes.
provide example mapping file in conf /","org.apache.nutch.indexer.more.MoreIndexingFilter:getConf()
org.apache.nutch.indexer.more.MoreIndexingFilter:setConf(Configuration)
org.apache.nutch.indexer.more.MoreIndexingFilter:filter(NutchDocument, Parse, Text, CrawlDatum, Inlinks)"
CLASS,lucene-4.0,LUCENE-4182,2012-06-29T06:45:34.000-05:00,DocumentsWriterFlushControl.assertMemory tripped by NGramTokenizerTest.testRandomStrings,"{noformat}

  
 
  
  
    
 
 
   
  
    
                                       
                                
    
          
     {0,5}     {0,5}      {0,5}         {1,5}  {0, & qgbiwn  < vyge acmvidw xbwgrppk \uf612p\u05f5^\u048f\u056d \ud6436\u9cde0\u274e\u0592 tamcca \ufd7b\ufbf9\ufb88\ufbae tusifiwj \u000f\u0600\uef93}                   {0,5}       
      
 @AfterClass 
         {dummy=DFR I(n)3(800.0)}   
                                                                                            
 {noformat}
execute line
execute line
execute line
execute line
execute line
execute line
execute line
build as failure Archiving artifacts trigger Recording test results Email for failure Sending email trigger failure Archiving artifacts for failure Sending email",org.apache.lucene.index.DocumentsWriterFlushControl
CLASS,lucene-4.0,LUCENE-4461,2012-10-05T10:21:38.000-05:00,Multiple FacetRequest with the same path creates inconsistent results,"FacetSearchParams facetSearchParams = new FacetSearchParams();
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
Multiple FacetRequest are getting merged into one creating wrong results in this case:
define hashcode equal in certain way talk about different requests
test case",org.apache.lucene.facet.search.StandardFacetsAccumulator
CLASS,lucene-4.0,LUCENE-4532,2012-11-04T02:51:37.000-06:00,,"{noformat}
  
 
  
 
     
                                          
 
         
  
 
     
                                                 {}    
 
                                                                              
  
  
  
  
  
  
  
 {noformat}
The following failure on Jenkins:
execute line
execute line
lucene \ build.xml:519: execute line
lucene \ common-build.xml:1691: execute line
lucene \ module-build.xml:61: execute line
lucene \ common-build.xml:1163: execute line
lucene \ common-build.xml:827:
build as failure
test results
trigger failure trigger > email","org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader.ChildrenArraysImpl
org.apache.lucene.facet.taxonomy.directory.TestDirectoryTaxonomyWriter
org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader
org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter"
METHOD,eclipse-2.0,13926,2002-04-16T13:41:00.000-05:00,JFace Text Editor Leaves a Black Rectangle on Content Assist text insertion,"class ContextInformationPopup
	 public void showContextInformation(final IContextInformation info, 
final int position) {
		Control control= fViewer.getTextWidget();
		BusyIndicator.showWhile(control.getDisplay(), new Runnable() {
			public void run() {
				internalShowContextInfo(info, position);
				hideContextSelector();
			}
		});
	}
base on jface Text
We found that after inserting a selected completion 
proposal, from the context information popup, that causes a black rectangle to 
appear on top of the display.
see attached screen shot
trace appearance of rectangle see rectangle after going go in BusyIndicator.showWhile.
appear in execution","org.eclipse.jface.text.contentassist.ContextInformationPopup:internalShowContextFrame(ContextFrame, boolean)"
METHOD,eclipse-2.0,15277,2002-05-05T00:01:00.000-05:00,Recreate test suite allows you to add the suite to itself,"suite() 
 suite()
If you have a test suite that is also a test case (done to allow cascading tests), and recreate the 
test suite, you're allowed to add the suite to itself.
This results in a recursive call to 
.
suite(), which ends up causing a stack overflow when run.
prevent test case from happening believe in situtation add suite add situtation
prefer new behaviour","org.eclipse.jdt.internal.junit.wizards.NewTestSuiteCreationWizardPage:handleFieldChanged(String)
org.eclipse.jdt.internal.junit.wizards.NewTestSuiteCreationWizardPage:setVisible(boolean)
org.eclipse.jdt.internal.junit.wizards.NewTestSuiteCreationWizardPage:testSuiteChanged()"
METHOD,eclipse-2.0,16787,2002-05-22T08:55:00.000-05:00,[Wizards] Wizards are recreated when returning to New page,"showPage()   showStartingPage() 
 page.getControl()  
 page.getControl()    
  
 public void dispose() {
	   super.dispose();
	   this.setControl(null);
	}
ask in showPage() ask in showStartingPage() support lazy page control creation
dispose page page.getControl() not call returns after disposing
add method to WizardPage",org.eclipse.ui.internal.dialogs.NewWizardNewPage:handleWizardSelection(SelectionChangedEvent)
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,,"getStat()
use natives
When the UnifiedTree finds a new file from the file system, it assumes that if the file is not an existing file, then it is a folder.
This is not always true, because for different reasons a file returned by java.io.File.list/listFiles may not actually exist (our CoreFileSystemLibrary#getStat() returns 0).
At the first moment, the file is found in the file system and assumed to be a folder, and a corresponding resource is created in the workspace.
At the second refresh, the folder corresponding to that resource is not found in the file system, and then it is removed from the workspace.
And so on.
reveal bugs","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
CLASS,jabref-2.6,1631548,2007-01-09T14:20:57.000-06:00,"""Open last edited DB at startup"" depends on the working dir","{HOME\}
another little bug/feature: The JabRef option ""Open last edited database at startup"" depends on the working directory at which JabRef is started.
results in an empty JabRef not opening $\{HOME\}/at-work/Bibliography/my\_documents.bib.
store absolute path for open last edited
store dependent configuration file \ ( ~ / in machine store dependent configuration file \ ( ~ / since setting
store relative path store absolute path store home directory try relative path try absolute path try home directory open JabRef
regard bernd
find bug in JabRef",net.sf.jabref.JabRefFrame
METHOD,mahout-0.8,MAHOUT-1261,2013-06-13T10:09:15.000-05:00,TasteHadoopUtils.idToIndex can return an int that has size Integer.MAX_VALUE,"Longs.hashCode(id)
run ItemSimilarityJob on large matrix
The job fails because of an IndexException in ToUserVectorsReducer.
For some id (I don't know what value), the result returned is Integer.MAX_VALUE.
This cannot be set in the userVector because the cardinality of that is also Integer.MAX_VALUE and it throws an exception.
So, the issue is that values from 0 to INT_MAX are returned by idToIndex but the vector only has 0 to INT_MAX - 1 possible entries.
think of %
ssc ] ssc everyone","org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils:idToIndex(long)
org.apache.mahout.cf.taste.hadoop.TasteHadoopUtils:readID(String, boolean)"
METHOD,mahout-0.8,MAHOUT-1301,2013-08-01T09:31:21.000-05:00,toString() method of SequentialAccessSparseVector has excess comma at the end,"SequentialAccessSparseVector toString()   toString()  
 {code:java}
 Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
  v.toString()  
 {code:java}
 {1:0.1,3:0.3}
 {code}
 
 {code:java}
 {1:0.1,3:0.3,}
 {code}
change in MAHOUT-1259 patch
Unfortunately, that patch introduced new bug: output of the toString() method had been changed - extra comma added at the end of the string
Example: 
Consider following sparse vector
{code:java}
Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
In 0.7 v.toString() returns following string:
{code:java}
{1:0.1,3:0.3}
{code}
but in 0.8 it returns
{code:java}
{1:0.1,3:0.3,}
{code}
As you can see, there is extra comma at the end of the string.","org.apache.mahout.math.SequentialAccessSparseVector:toString()
org.apache.mahout.math.RandomAccessSparseVector:toString()"
METHOD,mahout-0.8,MAHOUT-1317,2013-08-23T13:05:58.000-05:00,,"Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"");
get errors in experimenting get errors from RowSimilarityJob experiment with things realize in looking look at source
be in case be of form
Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's ""incorrect"", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.
show few more cases across code base save time get relevant error","org.apache.mahout.math.als.AlternatingLeastSquaresSolver:addLambdaTimesNuiTimesE(Matrix, double, int)
org.apache.mahout.utils.SplitInput:validate()
org.apache.mahout.math.neighborhood.FastProjectionSearch:FastProjectionSearch(DistanceMeasure, int, int)
org.apache.mahout.classifier.naivebayes.training.WeightsMapper:setup(Context)
org.apache.mahout.cf.taste.example.kddcup.KDDCupDataModel:KDDCupDataModel(File, boolean, double)
org.apache.mahout.math.neighborhood.ProjectionSearch:ProjectionSearch(DistanceMeasure, int, int)
org.apache.mahout.classifier.df.mapreduce.partial.TreeID:TreeID(int, int)
org.apache.mahout.math.neighborhood.SearchQualityTest.StripWeight:apply(WeightedThing<Vector>)
org.apache.mahout.classifier.df.data.Dataset:valueOf(int, String)
org.apache.mahout.classifier.df.mapreduce.partial.TreeID:treeId()
org.apache.mahout.classifier.df.data.DataLoader:parseString(Attribute[], Set<String>[], CharSequence, boolean)
org.apache.mahout.cf.taste.impl.common.WeightedRunningAverage:changeDatum(double, double)
org.apache.mahout.math.als.AlternatingLeastSquaresSolver:solve(Iterable<Vector>, Vector, double, int)
org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel:CassandraDataModel(String, int, String)
org.apache.mahout.math.als.AlternatingLeastSquaresSolver:createRiIiMaybeTransposed(Vector)
org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator:SamplingLongPrimitiveIterator(RandomWrapper, LongPrimitiveIterator, double)
org.apache.mahout.math.neighborhood.BruteSearch:search(Vector, int)
org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity.ItemItemSimilarity:ItemItemSimilarity(long, long, double)
org.apache.mahout.math.random.ChineseRestaurant:ChineseRestaurant(double, double)
org.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity.UserUserSimilarity:UserUserSimilarity(long, long, double)"
METHOD,mahout-0.8,MAHOUT-1320,2013-08-29T04:07:09.000-05:00,,"{noformat}
  
 testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)   
 {noformat}



 
 {noformat}
 
      
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 testClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)   
 {noformat}
From time to time this test fails with following in build log:
run in parallel
run org.apache.mahout.common.distance.TestChebyshevMeasure
run org.apache.mahout.common.distance.TestMinkowskiMeasure
run org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure
run org.apache.mahout.common.distance.TestManhattanDistanceMeasure
run org.apache.mahout.common.distance.CosineDistanceMeasureTest
run org.apache.mahout.common.distance.TestTanimotoDistanceMeasure
run org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure
run org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest
run org.apache.mahout.common.distance.TestEuclideanDistanceMeasure
run org.apache.mahout.common.iterator.TestFixedSizeSampler
run org.apache.mahout.common.iterator.CountingIteratorTest
run org.apache.mahout.common.iterator.TestSamplingIterator
run org.apache.mahout.common.iterator.TestStableFixedSizeSampler
run org.apache.mahout.common.DummyRecordWriterTest
run org.apache.mahout.common.StringUtilsTest
run org.apache.mahout.common.AbstractJobTest
run org.apache.mahout.common.IntPairWritableTest
run org.apache.mahout.common.lucene.AnalyzerUtilsTest
run org.apache.mahout.clustering.topdown.PathDirectoryTest
run org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest
run org.apache.mahout.clustering.classify.ClusterClassificationDriverTest
run org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest
run org.apache.mahout.clustering.spectral.TestVectorCache
run org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob
run org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob
run org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer
run org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob
run org.apache.mahout.clustering.spectral.TestUnitVectorizerJob
run org.apache.mahout.clustering.canopy.TestCanopyCreation
run org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator
run org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator
run org.apache.mahout.clustering.kmeans.TestKmeansClustering
run org.apache.mahout.clustering.TestGaussianAccumulators
run org.apache.mahout.clustering.iterator.TestClusterClassifier
run org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering
run org.apache.mahout.clustering.TestClusterInterface
run org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest
run org.apache.mahout.clustering.streaming.cluster.BallKMeansTest
run org.apache.mahout.math.stats.OnlineAucTest
run org.apache.mahout.math.stats.SamplerTest
run org.apache.mahout.math.VarintTest
run org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest
run org.apache.mahout.math.hadoop.stats.BasicStatsTest
run org.apache.mahout.math.hadoop.TestDistributedRowMatrix
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest
run org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest
run org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI
run org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver
run org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob
run org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest
run org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest
run org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI
run org.apache.mahout.math.VectorWritableTest
run org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest
run org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest
run org.apache.mahout.math.neighborhood.SearchSanityTest
run org.apache.mahout.math.MatrixWritableTest
run org.apache.mahout.vectorizer.DocumentProcessorTest
run org.apache.mahout.vectorizer.HighDFWordsPrunerTest
run org.apache.mahout.math.neighborhood.SearchQualityTest
run org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest
run org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest
run org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest
run org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest
run org.apache.mahout.vectorizer.encoders.TextValueEncoderTest
run org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest
run org.apache.mahout.vectorizer.collocations.llr.GramTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest
run org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest
run org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest
run org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest
run org.apache.mahout.vectorizer.collocations.llr.GramKeyTest
run org.apache.mahout.vectorizer.DictionaryVectorizerTest
run org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest
be on ubuntu-1 node",org.apache.mahout.clustering.streaming.cluster.BallKMeansTest:testClusteringMultipleRuns()
METHOD,mahout-0.8,MAHOUT-1336,2013-09-16T23:35:54.000-05:00,HighDFWordsPrunerTest is failing silently,"{noformat}
 
        
        
      
         
      
 {noformat}
Apparently ToolRunner does not allow the --mapred option.
The validation is not very foolproof, so there is a resulting silent failure in HighDFWordsPrunerTest.
Error message:
mapr while processing options","org.apache.mahout.vectorizer.HighDFWordsPrunerTest:validateVectors(Path, int[], boolean)"
METHOD,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
not support use case
The kmeans ran fine and generate sensible looking results, but when I tried to run ClusterDumper I got the following error:
use /usr/bin/hadoop HADOOP_CONF_DIR = run on hadoop
endphase = [ 2147483647 ]
exception in thread
The error is when it tries to access the dictionary for the feature with index 698948
look at dictionary loading code
It looks like the dictionary array is sized for the number of unique keywords, not the highest index:
run custom dictionary/feature generation code have unique features die on index
not create dictionary array not create reason have size have dictionary array read size from dictionary sequence file
misunderstand something
reduce hash size not know number of features run job","org.apache.mahout.utils.vectors.VectorHelper:loadTermDictionary(Configuration, String)
org.apache.mahout.utils.vectors.VectorHelperTest:testJsonFormatting()"
METHOD,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}


 {Code}


  StreamingKMeansThread.call()


 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }


    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
have nonzero number of training test vectors ask for %
cause issue
use same iterator fail on second use","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Path, Configuration)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Iterable<Centroid>, Configuration)"
CLASS,openjpa-2.0.1,OPENJPA-1752,2010-07-29T23:33:45.000-05:00,TestPessimisticLocks JUNIT test produced inconsistent behavior with various backends,"testFindAfterQueryWithPessimisticLocks()
  testFindAfterQueryOrderByWithPessimisticLocks()
  testQueryAfterFindWithPessimisticLocks()
  testQueryOrderByAfterFindWithPessimisticLocks()


 
 testFindAfterQueryWithPessimisticLocks() 
  No exception;
TestPessimisticLocks JUNIT tests pass all assertions for Derby backend, but failures are seen on DB2, MySQL, Oracle.
occur on other backends
handle pessimistic lock requests
There is also inconsistency in reporting exceptions - lock timout or query timeout should be non-fatal; but with Derby the PessimisticLockException is reported  which is considered fatal.
have test cases work for backend
list problem test cases
The failure symptoms are summarized below -   Each test contains 2 variations.
expect exception in testFindAfterQueryWithPessimisticLocks() expect exception in testFindAfterQueryWithPessimisticLocks() expect first scenario in testFindAfterQueryWithPessimisticLocks() get results from database
hang QueryTimeoutException
NOTE: for Oracle, many test scenarios caused process to hang (test 3.1, 3.2, and 4.2) - ie.
test never run to completion
      for MySQL, Server shutdown (test 3.1 and 3.2)
      here is the  stack trace:
prepstmnt SELECT t1.id",org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks
CLASS,openjpa-2.0.1,OPENJPA-1903,2010-12-06T13:05:34.000-06:00,Some queries only work the first time they are executed,"@Entity
@IdClass(MandantAndNameIdentity.class)
public class Website {
    @Id
    private String mandant;
   
    @Id
    private String name;
...
}

 @Entity
@IdClass(WebsiteProduktDatumIdentity.class)
public class Preis {
    @Id
    @ManyToOne(cascade = CascadeType.MERGE)
    private Website website;

    @Id
    @Basic
    private String datum;
...
}

 
 em.getTransaction().begin();

        Website website = em.merge(new Website(""Mandant"", ""Website""));

        em.merge(new Preis(website, DATUM));
       
        em.getTransaction().commit();

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website.name = :website "", Preis.class);
       q.setParameter(""website"", website.getName());

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website = :website "", Preis.class);
        q.setParameter(""website"", website);
I have a problem in my application where a query that sometimes returns data and sometimes not.
reduce to code project into Eclipse project at http://ubuntuone.com/p/S9n/
happen with OpenJPA 2.0.1 happen with daily snapshot happen from out-of-process Derby database
datum }
this query works all the time, note that it uses website.name for matching, not the full Website-object.
it only works ONCE and then does not return any results any more!!
see testcase DataAccessVerifyTest for details",org.apache.openjpa.jdbc.kernel.PreparedQueryImpl
CLASS,openjpa-2.0.1,OPENJPA-1912,2011-01-03T13:48:09.000-06:00,enhancer generates invalid code if fetch-groups is activated,"@Entity
public abstract class AbstractGroup {
   ...
    @Temporal(TemporalType.TIMESTAMP)
    @TrackChanges
    private Date applicationBegin;
 ...
}

 
 @Entity
public class Group extends AbstractGroup {
...
}

 
 public void writeExternal(ObjectOutput objectoutput)
        throws IOException
     
 pcWriteUnmanaged(objectoutput);
        if(pcStateManager != null)
        {
            if(pcStateManager.writeDetached(objectoutput))
                return;
        } else
        {
            objectoutput.writeObject(pcGetDetachedState());
            objectoutput.writeObject(null);
        }
        objectoutput.writeObject(applicationBegin);
        objectoutput.writeObject(applicationEnd);
        objectoutput.writeObject(applicationLocked);
        objectoutput.writeObject(approvalRequired);
If openjpa.DetachState =fetch-groups is used, the enhancer will add a 'implements Externalizable' + writeExternal + readExternal.
The problem is, that writeExternal and readExternal will also try to externalize the private members of any given superclass.
Thus we get a runtime Exception that we are not allowed to access those fields.
will result in the following code (decompiled with jad):
throw IOException",org.apache.openjpa.enhance.PCEnhancer
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
use openjpa inside osgi container
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
append return value of PersistenceUnitInfo.getClassLoader() append return value to list establihe fix participate in MultiClassLoader set up in MetaDataRepository.java:310ff
set classloader as context loader set classloader in meanwhile set classloader by PersistenceProvider.createContainerEntityManagerFactory() set classloader during creation
instantiate bean entityManagerFactory of class null
see nested stacktrace for details
cause by org.clazzes.fancymail.server.entities.EMail","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,Resolving factory method does not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
If a get method is annotated with @Factory then the method cannot be overridden with a method which take different parameters.
The system randomly selects one of the several methods with the same name which may or may not take the type which will be provided.
Actual results:
valueOf(String) may or may not be selected.
fix defect by applying apply method invocation conversion rules from Java",org.apache.openjpa.meta.FieldMetaData
CLASS,openjpa-2.0.1,OPENJPA-2132,2012-02-14T14:48:29.000-06:00,Traversal of a OneToMany relationship returns an empty list when InheritanceType.JOINED or SINGLE_TABLE is used.,"@Inheritance(strategy=InheritanceType.JOINED)
include test name OneManyJoinableTest.test recreate issue of JIRA recreate test of JIRA
consist of parent class define with @Inheritance
contain OneToMany relationship
When traversing the ManyToOne side of the relations, all works well.
But when traversing the OneToMany side an empty list is returned.
When running the test, it can be seen that OpenJPA generate incorrect SQL, as follows:
Note that the 't2.id IS NULL AND t3.id IS NULL' seems suspect.
pass on OpenJPA 1.2.x fail on OpenJPA 2.0.x",org.apache.openjpa.jdbc.kernel.JDBCStoreManager
CLASS,openjpa-2.0.1,OPENJPA-2289,2012-10-31T13:40:55.000-05:00,Additional SQL alias generated for query with subquery causes incorrect # of rows returned - Oracle only,"createQuery(""SELECT e FROM MaxQueryEntity e, MaxQueryMapEntity map ""
                    + ""WHERE map.selectCriteria = 'B3' AND map.refEntity = e ""
                    + ""  AND e.revision = ( SELECT MAX(e_.revision)""
                    + ""                     FROM MaxQueryEntity e_""
                    + ""                     WHERE e_.domainId = e.domainId )""
                    + ""  AND map.revision = ( SELECT MAX(map_.revision)""
                    + ""                       FROM MaxQueryMapEntity map_""
                    + ""                       WHERE map_.refEntity = map.refEntity )"");
e FROM MaxQueryEntity
On Oracle we generate SQL like this on 2.0.
The additional alias ""OPENJPA_MAXQUERY_MAPENTITY t4"" caused more unexpected rows to return.","org.apache.openjpa.jdbc.sql.SelectImpl
org.apache.openjpa.jdbc.sql.OracleDictionary
org.apache.openjpa.jdbc.sql.DBDictionary"
METHOD,lang,LANG-346,2007-07-06T20:06:55.000-05:00,,"public void testRound()
{
    Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
    testCalendar.set(2007, 6, 2, 8, 9, 50);
    Date date = testCalendar.getTime();
    System.out.println(""Before round() "" + date);
    System.out.println(""After round()  "" + DateUtils.round(date, Calendar.MINUTE));
}

 
 Before round()  
 After round()   
 Before round()  
 After round()
get unexpected output for rounding round by minutes round by seconds
--2.1 produces
Before round() Mon Jul 02 03:09:50 CDT 2007
--2.2 and 2.3 produces
Before round() Mon Jul 02 03:09:50 CDT 2007
After round()  Mon Jul 02 03:01:00 CDT 2007 -- this appears to be wrong","org.apache.commons.lang.time.DateUtils:modify(Calendar, int, boolean)"
METHOD,lang,LANG-477,2009-01-09T10:05:53.000-06:00,ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes,"{code:title=ExtendedMessageFormatTest.java|borderStyle=solid}

 private static Map<String, Object> formatRegistry = new HashMap<String, Object>();    
     static {
        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());
    }
    
     public static void main(String[] args) {
        ExtendedMessageFormat mf = new ExtendedMessageFormat(""it''s a {dummy} 'test'!"", formatRegistry);
        String formattedPattern = mf.format(new String[] {""great""});
        System.out.println(formattedPattern);
    }
 
 {code}

 
 {code:title=ExtendedMessageFormat.java|borderStyle=solid}
 
 if (escapingOn && c[start] == QUOTE) {
        return appendTo == null ? null : appendTo.append(QUOTE);
}

WORKING:
if (escapingOn && c[start] == QUOTE) {
        next(pos);
        return appendTo == null ? null : appendTo.append(QUOTE);
}
{code}
When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur.
start at line start on release","org.apache.commons.lang.text.ExtendedMessageFormat:appendQuotedString(String, ParsePosition, StringBuffer, boolean)"
METHOD,lang,LANG-480,2009-01-20T17:36:44.000-06:00,StringEscapeUtils.escapeHtml incorrectly converts unicode characters above U+00FFFF into 2 characters,"import org.apache.commons.lang.*;

public class J2 {
    public static void main(String[] args) throws Exception {
        // this is the utf8 representation of the character:
        // COUNTING ROD UNIT DIGIT THREE
        // in unicode
        // codepoint: U+1D362
        byte[] data = new byte[] { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 };

        //output is: &amp;#55348;&amp;#57186;
        // should be: &amp;#119650;
        System.out.println(""'"" + StringEscapeUtils.escapeHtml(new String(data, ""UTF8"")) + ""'"");
    }
}
Characters that are represented as a 2 characters internaly by java are incorrectly converted by the function.
import org.apache.commons.lang.
throw Exception { //
want patch","org.apache.commons.lang.Entities:escape(Writer, String)"
METHOD,lang,LANG-538,2009-10-16T16:47:39.000-05:00,DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations,"Calenar.getTime()    
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";

    // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)
    // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);


    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}

 
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
    cal.getTime();

    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}
If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.
For example, the following unit test fails:
construct with cal = construct with GMT-8 default locale time zone GregorianCalendar cal work in time zone
However, this unit test passes:","org.apache.commons.lang3.time.FastDateFormat:format(Calendar, StringBuffer)"
METHOD,lang,LANG-552,2009-11-09T12:40:57.000-06:00,,"{code}
 import static org.junit.Assert.assertEquals;

import org.apache.commons.lang.StringUtils;
import org.junit.Test;


public class StringUtilsTest {

	@Test
	public void replaceEach(){
		String original = ""Hello World!"";
		String[] searchList = {""Hello"", ""World""};
		String[] replacementList = {""Greetings"", null};
		String result = StringUtils.replaceEach(original, searchList, replacementList);
		assertEquals(""Greetings !"", result);
		//perhaps this is ok as well
                //assertEquals(""Greetings World!"", result);
                //or even
		//assertEquals(""Greetings null!"", result);
	}

	
}
 {code}
The following Test Case for replaceEach fails with a null pointer exception.
stuff Values into replacementList
happen on replace
outline expectations in test case outline expectations of course
update documentation pass null as replacement string
import static org.junit.Assert.assertEquals;
import org.apache.commons.lang.StringUtils;
import org.junit.Test;
hello world
greeting null","org.apache.commons.lang3.StringUtils:replaceEach(String, String[], String[], boolean, int)"
METHOD,lang,LANG-624,2010-05-27T21:09:29.000-05:00,SystemUtils.getJavaVersionAsFloat throws StringIndexOutOfBoundsException on Android runtime/Dalvik VM,"{noformat}

   
   
         
   
          {noformat}
replicate in Android emulator",org.apache.commons.lang3.SystemUtils:toJavaVersionInt(String)
METHOD,lang,LANG-645,2010-08-20T14:11:08.000-05:00,FastDateFormat.format() outputs incorrect week of year because locale isn't respected,"format()     
  
 {code}
 import java.util.Calendar;
import java.util.Date;
import java.util.Locale;
import java.text.SimpleDateFormat;

import org.apache.commons.lang.time.FastDateFormat;

public class FastDateFormatWeekBugDemo {
    public static void main(String[] args) {
        Locale.setDefault(new Locale(""en"", ""US""));
        Locale locale = new Locale(""sv"", ""SE"");

        Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome
        cal.set(2010, 0, 1, 12, 0, 0);
        Date d = cal.getTime();
        System.out.println(""Target date: "" + d);

        FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
 {code}
  Locale.setDefault()
FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. ""ww"") in format().
It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output.
Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:
import java.util.Locale; import java.text.SimpleDateFormat;
import org.apache.commons.lang.time.FastDateFormat;
not change outcome cal.set
FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
{code}",org.apache.commons.lang3.time.FastDateFormat:format(Date)
METHOD,lang,LANG-662,2010-12-06T22:40:30.000-06:00,"org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)","class Fraction    
    
 
  public void testReducedFactory_int_int()  
 
  f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2);
		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator());
		assertEquals(1, f.getDenominator());

	 public void testReduce()  
 
  f = Fraction.getFraction(Integer.MIN_VALUE, 2);
		result = f.reduce();
		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator());
		assertEquals(1, result.getDenominator());
{code}
The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator.
take Integer.MIN_VALUE as denominator handle case in getReducedFraction factory method handle case of taking","org.apache.commons.lang3.math.Fraction:greatestCommonDivisor(int, int)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,SerializationUtils throws ClassNotFoundException when cloning primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.
noformat } import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;
fix java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 since java version fix ObjectInputStream since java version
The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException.
throw IOException {
try { return Class.forName(name, false, classLoader)
try { return Class.forName ( name
catch { return super.resolveClass(desc)
fix java bug in ObjectInputStream fix code in ObjectInputStream
protect class <?> resolveClass(ObjectStreamClass desc)
try { return Class.forName","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-857,2012-11-20T12:36:14.000-06:00,StringIndexOutOfBoundsException in CharSequenceTranslator,"{code:java}
 @Test
public void testEscapeSurrogatePairs() throws Exception {
    assertEquals(""\uD83D\uDE30"", StringEscapeUtils.escapeCsv(""\uD83D\uDE30""));
}
 {code}

 
 {code}
 {code}

 
 public final void translate(CharSequence input, Writer out) throws IOException
handle in CharSequenceTranslator
You'll get the exception as shown below.
throw IOException","org.apache.commons.lang3.text.translate.CharSequenceTranslator:translate(CharSequence, Writer)"
METHOD,lang,LANG-879,2013-03-18T21:46:29.000-05:00,"LocaleUtils test fails with new Locale ""ja_JP_JP_#u-ca-japanese"" of JDK7","import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;

import java.util.Locale;

import org.testng.annotations.Test;

import com.scispike.foundation.i18n.StringToLocaleConverter;

public class LocaleStringConverterTest {

	StringToLocaleConverter converter = new StringToLocaleConverter();

	public void testStringToLocale(Locale l) {
		String s = l.toString();

		assertThat(converter.convert(s), equalTo(l));
	}

	@Test
	public void testAllLocales() {

		Locale[] locales = Locale.getAvailableLocales();
		for (Locale l : locales) {
			testStringToLocale(l);
		}
	}
}


  
 import java.util.Locale;

import org.apache.commons.lang3.LocaleUtils;
import org.springframework.core.convert.converter.Converter;

public class StringToLocaleConverter implements Converter<String, Locale> {

	@Override
	public Locale convert(String source) {
		if (source == null) {
			return LocaleToStringConverter.DEFAULT;
		}
		return LocaleUtils.toLocale(source);
	}
}
The Test below fails with the following error on JDK7, but succeeds on JDK6:
remove stack frames
import static org.hamcrest.Matchers.equalTo;
import java.util.Locale;
import org.testng.annotations.Test;
import com.scispike.foundation.i18n.StringToLocaleConverter;
import java.util.Locale;
import org.springframework.core.convert.converter.Converter;",org.apache.commons.lang3.LocaleUtils:toLocale(String)
FILE,SWARM,SWARM-486,2016-05-28T18:25:37.000-05:00,Can't load project-stages.yml on classpath with Arq,"classpath(src/main/resources)  
 
 
 container.withStageConfig(Paths.get(""/tmp"", ""external-project-stages.yml"").toUri().toURL())
problem project-stages.
yml on classpath(src/main/resources) is not loaded with Arquillian tests.
Though -swarm try to load it, apparently can't see it when Arq tests.
load yml",org.wildfly.swarm.container.ProjectStagesTest
FILE,SWARM,SWARM-863,2016-11-30T14:54:40.000-06:00,,"container = new Swarm(); // fractions being added here also




    container.start();




    container.deploy(...);






 
 container.stop();
We now have the problem that stopping such a Swarm service in version 2016.11.0 does not properly shutdown the Swarm container (or better the underlying `Server`).
I did a debug session and found out that there remains one non-daemon thread blocking the JVM shutdown.
work with version
The shutdown is clean and fast.
find example project at https://github.com/seelenvirtuose/de.mwa.testing.wfs.
attach as zip
download procrun at http://mirror.serversupportforum.de/apache//commons/daemon/binaries/windows/commons-daemon-1.0.15-bin-windows.zip
After a succesful start you can ""GET http://localhost:8080/hello"", which should result in a ""Hello World"" response.
6) The service has many threads running.
See first attached screenshot.
Windows will hang in that stopping attempt and spit out a failure message after some time.
The process is still running afterwards.
The log file shows some output that indeed a shutdown is initalized.
The GET does not work anymore.
8) The service still has many threads (especially non-daemon threads).
See second attached screenshot.
have other services show non-deamon threads after shutdown attempt show other services after shutdown attempt
kill task testing-wfs.exe stop process
switch Wildfly swarm version to 2016.10.0",org.wildfly.swarm.container.runtime.ServerBootstrapImpl
METHOD,derby-10.9.1.0,DERBY-5424,2011-09-20T14:24:29.000-05:00,,"testConnectWrongSubprotocolWithSystemProperty(org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest) 
    
  
  testConnectWrongSubprotoctestolWithoutSystemProperty(org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest)   
  
 
 String ijResult = runIjScript(ijScript, useSystemProperties);       
                assertTrue(ijResult.indexOf(""08001"") > -1);
With the release candidate  10.8.2.1 - (1170221) I saw the following two failures on z/OS in testConnectWrongSubprotoctestolWithoutSystemProperty
There were 2 failures:
convert encoding issue convert test relate to test","org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest:runIjScript(String, boolean)"
METHOD,derby-10.9.1.0,DERBY-5663,2012-03-17T23:45:11.000-05:00,Getting NPE when trying to set derby.language.logStatementText property to true inside a junit suite.,"patch(DERBY5663_patch1.txt)
have large data suite run LobLimitsTest with embedded network server configurations run LobLimitsTest with small data size run large data suite with embedded network server configurations run large data suite with small data size
run large data suite as follows time java -Dderby.tests.trace=true -Dderby.infolog.append=true junit.textui.TestRunner org.apache.derbyTesting.functionTests.tests.largedata.
make simple change to suite log statement text show in attached patch ( derby5663_patch1
This causes the large data suite to run into NPE (NPE can be seen in runall.out) as shown below.
Not sure what I am doing wrong while trying to set the property, which results in NPE.
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms EF
not register org.apache.derby.jdbc.EmbeddedDriver with JDBC driver manager
not register caused with JDBC driver manager not register caused by org.apache.derby.client.am.SqlException
expect <XJ015>","org.apache.derbyTesting.functionTests.tests.largedata.LobLimitsTest:baseSuite(int, int)
org.apache.derbyTesting.junit.SystemPropertyTestSetup:setProperties(Properties)"
METHOD,derby-10.9.1.0,DERBY-5816,2012-06-13T15:12:35.000-05:00,store.ServicePropertiesFileTest fails on z/OS,"testSevicePropertiesFileWithBackup(org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest) 
 
   
  
  
  
  testSevicePropertiesFileCorruptedWithBackup(org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTe
st)   {```
@    @    k@   }
store.ServicePropertiesFileTest fails on z/OS with two failures below.
encode issue
testsevicepropertiesfilecorruptedwithbackup ( org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTe
not put after line","org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest:grepForToken(String, File)
org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest:assertEOFToken(File)
org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest:removeEOFToken(File)"
METHOD,derby-10.9.1.0,DERBY-5951,2012-10-16T10:33:53.000-05:00,Missing method exception raised when using Clobs with territory based collation,"db;create=true; 
   varchar( 32672 )  
  
  
  
 clobTable( a )   makeClob( 'a' )  
   varchar( 32672 )  
  
  
 clobTable( a )   makeClob( 'a' )  
     Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;   
   Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;     
  
 clobTable( a )   makeClob( 'a' )
When using territory-based collation with Clobs, Derby raises an error trying to invoke a missing method.
connect jdbc:derby:memory:db
create function makeClob( contents varchar( 32672 ) ) returns
create table clobTable( a clob )
-- fails with a java.lang.NoSuchMethodError exception
insert into clobTable( a ) values
connect jdbc:derby:memory:db1
create function makeClob( contents varchar( 32672 ) ) returns
create table clobTable( a clob )
insert into clobTable( a ) values
Here is the error:
evaluate expression throw org.apache.derby.iapi.types.DataValueFactory.getClobDataValue while evaluating
fail with java.lang.NoSuchMethodError exception
insert into clobTable( a ) values
evaluate expression throw org.apache.derby.iapi.types.DataValueFactory.getClobDataValue while evaluating
cause java.lang.NoSuchMethodError",org.apache.derby.iapi.types.CollatorSQLClob:getNewNull()
METHOD,derby-10.9.1.0,DERBY-6073,2013-02-15T10:11:27.000-06:00,,"StatementPoolingTest.testPoolingEnabledByCheckingImplementationDetails()   testCacheOverflow()  If testPoolingEnabledByCheckingImplementationDetails()    testPoolingEnabledByCheckingImplementationDetails()
StatementPoolingTest.testPoolingEnabledByCheckingImplementationDetails() assumes that the client-side statement cache will have been primed by a previous test case, testCacheOverflow().
If testPoolingEnabledByCheckingImplementationDetails() is the first test case to run, then it fails with this error:
attach patch force testPoolingEnabledByCheckingImplementationDetails() force patch
fail with patch run on branch run on trunk","org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testIsolationLevelIsResetExplicitCloseQuery()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testPoolingEnabledByCheckingImplementationDetails()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testDeleteReferringTableWhenInCache()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testGetStatementCallable()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testGetStatementPrepared()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testIsolationLevelIsResetExplicitCloseNoQuery()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testPrepareCallPath()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testCachingLogicalConnectionCloseLeavesPhysicalStatementsOpen()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testHoldabilityIsResetNoExplicitClose()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testCacheOverflow()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testPrepareCallWithNoCallPath()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:suite()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testIsolationLevelIsResetNoExplicitCloseNoQuery()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testTemporaryTablesAreDeletedInNewLogicalConnection()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testClosingPSClosesRS()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testIsolationLevelIsResetNoExplicitCloseQuery()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testPrepareStatementPath()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testDeleteReferringTableWhenOpen()
org.apache.derbyTesting.functionTests.tests.jdbcapi.StatementPoolingTest:testHoldabilityIsResetExplicitClose()"
METHOD,derby-10.9.1.0,DERBY-6089,2013-02-21T22:39:19.000-06:00,CallableStatement#registerOutParameter on client lacks check of legal types.,"registerOutParameter(int parameterIndex,
int sqlType, String typeName)
cf negative test in patch cf negative test to PreparedStatement42
read in JDBC
not support data type
For the new overloads, for embedded this is checked inside Util42#getTypeAsInt.
The similar client method, Utils42#getTypeAsInt does not do this checking.","org.apache.derby.impl.jdbc.EmbedPreparedStatement:setNull(int, int)
org.apache.derby.client.am.Agent:accumulateDeferredException(SqlException)
org.apache.derby.client.am.PreparedStatement:checkScaleForINOUTDecimal(int, int)
org.apache.derby.client.am.CallableStatement:registerOutParameterX(int, int, int)
org.apache.derby.impl.jdbc.ConnectionChild:newSQLException(String)
org.apache.derby.client.am.PreparedStatement:checkForSupportedDataType(int)
org.apache.derby.impl.sql.GenericParameterValueSet:registerOutParameter(int, int, int)"
FILE,IO,IO-180,2008-09-08T18:03:20.000-05:00,,"LineIterator it = FileUtils.lineIterator(file, ""UTF-8"");
   try {
     while (it.hasNext()) 
{

       String line = it.nextLine();

       /// do something with line

     }
   } finally 
{

     LineIterator.closeQuietly(iterator);

   }
In the Javadoc for rg.apache.commons.io.LineIterator (in Commons IO 1.4), this code snippet is incorrect:  the last instance of ""iterator"" should be
try { while
do something with line",org.apache.commons.io.LineIterator
FILE,eclipse-3.1,100137,2005-06-15T04:29:00.000-05:00,,"public class A {
	String dog1 = ""Max"", dog2 = ""Bailey"", dog3 = ""Harriet"";
	public static void main(String[] args) {
		new A().foo();
	}
	
	void foo() {
		String p= """";
	}
}
show correct values
- source is not found
not work in detail pane",org.eclipse.jdt.launching.StandardClasspathProvider
FILE,eclipse-3.1,100807,2005-06-20T09:30:00.000-05:00,Source not found,"JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
  
 JavaModelManager.getZipFile(IPath) 
 
   
 JavaModelManager.closeZipFile(ZipFile) 
    
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile)
do result in new failure
The source files in the project are not being found.
turn on debug flags
options file and only the following archives were searched for the source file when a breakpoint was reached.",org.eclipse.debug.internal.core.sourcelookup.containers.ContainerSourceContainer
FILE,eclipse-3.1,103379,2005-07-11T15:37:00.000-05:00,[MPE] [EditorMgmt] An editor instance is being leaked each time an editor is open and closed,"dispose()
Every we open and close an editor.
That editor instance is being leaked.
have testcase demostrate testcase
What's interesting is that the editor, upon open, will allocate a 200000 size
String array as a private field.
If you run this testcase with -Xmx256M, you will run out of memory.
However, if you explicitly set the String array to null in the dispose() method of the editor, then the same testcase will not run out of memory.
This leads us to believe that the editor instance is being leaked.",org.eclipse.ui.operations.OperationHistoryActionHandler
FILE,eclipse-3.1,103918,2005-07-14T17:25:00.000-05:00,100% CPU load while creating dynamic proxy in rich client app,"public void start(BundleContext context) throws Exception {
  super.start(context);
  XmlBeanFactory bf = new XmlBeanFactory(
     new ClassPathResource(""/bug/beans.xml""));
  bf.getBean(""hang"");
}

  bf.getBean(""hang"")  
 bf.getBean()
integrate ecplipse-rcp application with springframework
I've
noticed that when spring tries to instantiate any dynamic proxy RCP falls into
infinit loop, CPU gets 100% load and the application needs to be killed.
throw Exception { super.start(context)
xml ) )
When bf.getBean(""hang"") is executed the application hangs.
execute outside eclipse-rcp
create proxy class for given interface
attach sample project",org.eclipse.core.runtime.internal.adaptor.ContextFinder
FILE,eclipse-3.1,106492,2005-08-09T11:01:00.000-05:00,NPE on console during debug session,"name.equals(""IResourceTest.testDelete"")  
  
  
          
       
  
       
  
       
   testDelete()  
  
   
    
 
  
   
  
   
    
 
   runTest()  
   runBare()  
   protect()
While debugging, I noticed the attached stack trace on my Java console (not in the log file).
There was nothing in the log file.
I see from the stack that it occurred during evaluation of a conditional breakpoint.
After this error occurred, the debug process hung, and ""Terminate"" and
""Terminate All"" had no effect.
I was still able to ""Suspend"" the process, and it resulted in a debug view showing:
invoke ( method
invoke ( method
remove from debug view have to shutdown Eclipse",org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
FILE,eclipse-3.1,107031,2005-08-15T11:21:00.000-05:00,[content assist] SWTException in code assist,"SWTError (Widget disposed)
The fix for bug 31427 can cause an SWTError (Widget disposed) under gtk.
The
error is not noticable in the UI, but I found the following stack trace below in
the log (no steps).
backport trivial fix go into 3.1.1 stream
attach patch",org.eclipse.jface.text.contentassist.CompletionProposalPopup
FILE,eclipse-3.1,110837,2005-09-27T13:16:00.000-05:00,javax.crypto.KeyAgreement.getInstance(String) throws exception in IDE,"KeyAgreement.getInstance(""DiffieHellman"")  
  
 
import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;

public class KeyAgreementProblem
{
    public static void main(String[] args) throws NoSuchAlgorithmException
    {
        KeyAgreement ka = KeyAgreement.getInstance(""DiffieHellman"");
        System.out.println(ka);
    }
}
 
 
  
  
 javax.crypto.KeyAgreement.getInstance(DashoA12275)
A call to KeyAgreement.getInstance(""DiffieHellman"") will throw a
NoSuchAlgorithmException if run from Eclipse but not if it's run directly from the command line.
import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;
throw NoSuchAlgorithmException
run from command line go command line
Running in Eclipse with the same JDK and environment produces this exception:","org.eclipse.jdt.launching.AbstractJavaLaunchConfigurationDelegate
org.eclipse.jdt.internal.launching.JRERuntimeClasspathEntryResolver
org.eclipse.jdt.internal.launching.StandardVMType"
FILE,eclipse-3.1,113455,2005-10-22T11:32:00.000-05:00,[Markers] Some error markers do not appear,"problemView.getCurrentMarkers()
linux 2.6.13
Even with all the code from HEAD, no errors showed up in my Problems view.
(ResourceMappingMarkersTest), and it had several errors in it.
Now one error appears in the
Problems view: ""ModelProvider cannot be resolved"" in CompositeResourceMapping.
The one error that was there previously then disappeared.
I had a hard time deciding whether to make this ""blocker"" or ""major"".
lead to broken builds
mark as blocker","org.eclipse.ui.views.markers.internal.Util
org.eclipse.ui.views.markers.internal.MarkerView"
FILE,eclipse-3.1,115363,2005-11-07T13:28:00.000-06:00,"java.lang.VerifyError in org.eclipse.ui.workbench from HEAD, using N20051107","Ljava/lang/String;Ljava/lang/String;Lorg/eclipse/jface/action/IContributionManager;
get stack trace",org.eclipse.jdt.internal.compiler.codegen.Label
FILE,eclipse-3.1,117890,2005-11-24T06:38:00.000-06:00,JavaElement.getURLContents(...) leaves file open,"JavaElement.getURLContents(...)   tearDownSuite()  
 URLConnection.getContentEncoding()   URLConnection.getContentEncoding()
The AttachedJavadocTests sometimes fail because JavaElement.getURLContents(...) leaves a file open and the test cannot delete the project in the tearDownSuite() method because of that.
More specifcally URLConnection.getContentEncoding() opens a file stream on the doc.zip and never closes it.
look like bug","org.eclipse.jdt.internal.compiler.util.Util
org.eclipse.jdt.core.IJavaElement
org.eclipse.jdt.internal.core.JavaElement"
FILE,eclipse-3.1,133072,2006-03-23T16:58:00.000-06:00,"Cannot launch an ""Eclipse Application"" without the -ws argument","package Fred;

import javax.swing.JFrame;
import javax.swing.SwingUtilities;

import org.eclipse.core.runtime.IPlatformRunnable;

public class Main implements IPlatformRunnable {

       public Object run(Object args) throws Exception {
               SwingUtilities.invokeLater(new Runnable() {
                       public void run() {
                               new JFrame(""Fred"").setVisible(true);
                       }
               });
               synchronized(this)
               {
                       wait();
               }
               return IPlatformRunnable.EXIT_OK;
       }

}
launch Eclipse application do NOT use SWT from 3.2M5 do NOT use SWT under OSX do Eclipse application from 3.2M5 do Eclipse application under OSX get following problems
When running against 1.4.2_09, you get the dreaded ""2006-03-23
load java [ 1053 ] apple AWT java VM on first thread
The application then stops.
When running against 1.5.0_06 you get these messages on startup:
Under 1.5.0_06, the app appears to run, but there is no menu-bar,
dock-icon, or window that shows up.
Basically nothing happens, but
the event thread is running and you have to kill it.
tell Eclipse not force first-thread flags by passing not pass ws carbon flag to application
have ability suppress flag
remove check for ws flag rebuild plugin hack by rebuilding hack by removing
override ws flag want per Launch configuration
import javax.swing.SwingUtilities;
import org.eclipse.core.runtime.IPlatformRunnable;
implement IPlatformRunnable {
throw Exception { SwingUtilities.invokeLater(new Runnable() {                        public void run() {                                new JFrame(""Fred"")","org.eclipse.pde.internal.ui.IPDEUIConstants
org.eclipse.pde.internal.ui.launcher.LaunchAction"
FILE,eclipse-3.1,133292,2006-03-26T06:15:00.000-06:00,,"{
    /* On the next line, the first semicolon is a syntax error. 
     * The error is not reported, and the program will run and print ""Fail"".
     * Without the semicolon, it prints ""foo"" as expected. */
    public static Baz.C a[] =  { new Baz.C(""foo"") ; } 
 public static void main(String args[]) {
	if (a == null)
	    System.out.println(""Fail"");
	else
	    System.out.println(a[0]);
    }
 
 class Baz {
    public static class C {
	String name;
	public C(String name) { this.name = name; }
	public String toString() { return name; }
    }
}
* The error is not reported, and the program will run and print ""Fail"".
* Without the semicolon, it prints ""foo"" as expected.
void main(String args[]) {","org.eclipse.jdt.internal.compiler.parser.RecoveredField
org.eclipse.jdt.internal.compiler.parser.RecoveredLocalVariable"
FILE,eclipse-3.1,300054,2010-01-19T10:12:00.000-06:00,Unexpected 'Save Resource' dialog appears when copying changes from right to left,"public class Bug {
	void bar() {
		System.out.println();
	}
}
  System.out.println();
void bar() {
==> 'Save Resource' dialog appears which is a major interruption of my workflow.
focus compare editor on method",org.eclipse.compare.internal.Utilities
FILE,eclipse-3.1,76534,2004-10-18T22:57:00.000-05:00,Can't perform evaluations inside inner class with constructor_ parameters,"createViewer(...)
We currently disallow evaluations in inner classes that take parameters in the referenced constructor_.
allow evaluations in kinds",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,76677,2004-10-20T13:18:00.000-05:00,,"public class ConsoleTest {
    public static void main(String[] args) {
        try {
	        byte[] b = new byte[100];
	        for(;;) {
	            int read = System.in.read(b);
	            System.out.write(b, 0, read);
	        }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
catch { e.printStackTrace()
console output is 13x instead of 1x3.","org.eclipse.ui.internal.console.IOConsolePartition
org.eclipse.ui.internal.console.IOConsolePartitioner"
FILE,eclipse-3.1,76860,2004-10-22T15:30:00.000-05:00,watch expression tests fail,"testDeferredExpression()
  testNonDeferredExpression()
fail with new support
These exceptions appear in the log:",org.eclipse.jdt.internal.debug.eval.ast.engine.ASTInstructionCompiler
FILE,eclipse-3.1,77234,2004-10-28T15:41:00.000-05:00,,"getTypeName() 
  
  
  
 getTypeName()   JavaExceptionBreakpoint

getTypeName()
I get the following in the details pane:
extend JavaBreakpoint declare getTypeName() on JavaBreakpoint",org.eclipse.jdt.internal.debug.ui.JavaDetailFormattersManager
FILE,eclipse-3.1,78245,2004-11-09T18:34:00.000-06:00,,"public enum TestEnum {
  a;
  public static void main(String[] args) {
    System.out.println();   // <- add a breakpoint here
  }
}
We are now able to add breakpoint in enum classes, but they're not correctly created, the associated type is wrong.
It's working OK if the enum is an inner type, but not if it's a top level type.
void main(String[] args) {
The breakpoint is created, but displayed in the breakpoint view as 'null [line
XX] - main(String[])', and the program doesn't stop on the breakpoint.",org.eclipse.jdt.internal.debug.ui.actions.ValidBreakpointLocationLocator
FILE,eclipse-3.1,78315,2004-11-10T12:53:00.000-06:00,org.eclipes.team.ui plugin's startup code forces compare to be loaded,"Platform.getAdapterManager()  registerAdapters(factory, DiffNode.class);

   
 startup()
write tests ensure plug-ins like Search ensure plug-ins like Compare open Java editor
The one for compare fails because org.eclipes.team.ui forces compare to be loaded in its start(BundleContext) method:
The direct reference to DiffNode causes the compare plug-in to be loaded even if it is not needed yet.",org.eclipse.team.internal.ui.TeamUIPlugin
FILE,eclipse-3.1,78647,2004-11-15T13:52:00.000-06:00,Breakpoint can be added on an invalid location,"public class Test {
  public static void main(String[] args) {
    int i= 
      2
      +
      (short)3;  // <-- breakpoint here
  }
}
manage constant expressions not handle number",org.eclipse.jdt.internal.debug.ui.actions.ValidBreakpointLocationLocator
FILE,eclipse-3.1,78740,2004-11-16T10:57:00.000-06:00,IDOMType.getFlags() fails to represent interface flags correctly.,"becomeDetailed()   

package org.example.jdom;

import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;

public class Test implements IPlatformRunnable
{
  public Object run(Object object)
  {
    DOMFactory factory = new DOMFactory();
    IDOMCompilationUnit jCompilationUnit =
factory.createCompilationUnit(""package x; /** @model */ interface X  {}"", ""NAME"");
    IDOMType jType = (IDOMType)jCompilationUnit.getFirstChild().getNextNode(); 
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    jType.getComment();
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    return new Integer(0);
  }
}
package org.example.jdom;
import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;
implement IPlatformRunnable
return new integer
break JavaEcoreBuilder",org.eclipse.jdt.internal.compiler.DocumentElementParser
FILE,eclipse-3.1,78746,2004-11-16T11:50:00.000-06:00,[Contributions] [JFace] Compiler error message containing '& ' is rendered with '_' in status line,"interface I {
    static void m(/*caret_here*/);
}
In the workbench window status line, '& ' is rendered as '_' (a mnemonic Alt+Space ?
-).
not mention behavior","org.eclipse.ui.texteditor.AbstractDecoratedTextEditor
org.eclipse.ui.internal.util.Util
org.eclipse.ui.internal.texteditor.quickdiff.QuickDiffRestoreAction
org.eclipse.ui.internal.editors.quickdiff.QuickDiffRestoreAction
org.eclipse.jface.action.StatusLine
org.eclipse.ui.internal.editors.quickdiff.CompositeRevertAction
org.eclipse.jface.util.Util"
FILE,eclipse-3.1,79309,2004-11-23T12:04:00.000-06:00,,"import test.Testable;
import test.Testable.Types; // F3 leads to java.sql.Types

public class Test { 

	public static void main( String[] args ) {
		System.out.println(Testable.Types.TEST);
	}
}
 
 
package test;

public interface Testable {
	public interface Types {
		String TEST = ""test"";
	}
}
Using F3 on ""Types"" from the import statement ""Test.java"" leads to java.sql.Types.
import test.Testable;
lead to java.sql.Types
void main( String[] args ) {",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,79545,2004-11-26T05:58:00.000-06:00,,"public class CharIntTest
{
    /**
     * Eclipse value: "" ""
     * JDK value:     ""32""
     */
    public static String C = """" + +' ';
    /**
     * Eclipse value: ""32""
     * JDK value:     ""32""
     */
    public static String I = """" + +32;

    public static void main(String[] args)
    {
        System.out.println(C);
        System.out.println(I);
    }
}
The problem is connected with +' ': Eclipse treats it as ' ' but Sun JDK converts that space into 32 (+' ' => + (int) ' ' => +32 => 32) (which IMHO is correct).
pick proper product
void main(String[] args)","org.eclipse.jdt.internal.compiler.impl.Constant
org.eclipse.jdt.internal.compiler.ast.EqualExpression"
FILE,eclipse-3.1,79690,2004-11-29T13:21:00.000-06:00,,"public class A_test1105 {
	public <E> void foo(E param) {
		/*[*/E local= param;
		foo(local);/*]*/
	}
}
void foo(E param) {
- calling CompllationUnit#findDeclaringNode with the type binding representing
E doesn't return the node <E> (e.g. the one in the method declaration)","org.eclipse.jdt.core.dom.ASTConverter
org.eclipse.jdt.core.dom.CompilationUnit
org.eclipse.jdt.internal.corext.refactoring.code.ExtractMethodAnalyzer"
FILE,eclipse-3.1,79957,2004-12-02T00:47:00.000-06:00,[Viewers] NPE changing input usingTableViewer and virtual,"Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
 
 tv.setInput(model1);
use latest code with private virtual manager class use latest code for Table viewer
reset model input
Same code works fine without the SWT.VIRTUAL style bit,but when VIRTUAL is set
it throws a null pointer exception...",org.eclipse.jface.viewers.TableViewer
FILE,eclipse-3.1,81045,2004-12-14T20:13:00.000-06:00,ClassNotLoadedException when trying to change a value,"public class Test {
	static class Inner {
	}
	public static void main(String[] args) {
		Inner inner= null;
		System.out.println(1);  //  <- breakpoint here
	}
}
void main(String[] args) {
A ClassNotLoadedException dialog appears.","org.eclipse.jdt.internal.debug.ui.actions.JavaVariableValueEditor
org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
org.eclipse.jdt.internal.debug.core.model.JDILocalVariable"
FILE,eclipse-3.1,82402,2005-01-07T16:13:00.000-06:00,,"DebugUIMessages.getString(ERROR)  
 e.getMessage()
If getBytesFromAddress fails, the following message appears in the Memory view:
create tab
generate output
replace e","org.eclipse.debug.internal.ui.views.memory.MemoryViewTab
org.eclipse.jface.resource.FontRegistry"
FILE,eclipse-3.1,82712,2005-01-12T15:54:00.000-06:00,[1.5] Code assist does not show method parameters from static imports,"import static java.lang.Math.*; 
 public class Test {

    void t() {
        abs(<CTRL+SPACE>);
    }
}
import static java.lang.Math.
void t() { abs",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,83205,2005-01-19T11:25:00.000-06:00,[osgi] shutdown did not complete,"System.exit()  
  
    
  
  
    
   
  
 
  
   Object.wait()  
   
  
  
  
  
  
  
  
 
 it()  
    
 
 
 
 
  
 Object.wait()  
   
  Object.wait()
The console window stayed open, and responsive (could use console).
No further activity seemed to be happening.
close for business
take snapshot of VM state
It seems System.exit() was not called.
The console and an event dispatching threads were left behind.
cause VM
dump Java HotSpot(TM) client VM
tid on condition [ 0x00000000
tid in Object.wait() [ 0x19fcf000
wait on <0x042db588>
tid in Object.wa it() [ 0x19f8f000
lock <0x042db620>
tid on condition [
tid on condition
tid in Object.wait() [ 0x1984f000
tid in Object.wait() [ 0x
tid on condition",org.eclipse.core.launcher.Main
FILE,eclipse-3.1,83321,2005-01-20T12:13:00.000-06:00,[1.5][assist][enum] no override completion proposals in type when followed by a package visible enum,"package test1;

public class Completion {
	| // does not work here
}

enum Natural {
	ONE;
     // works here
}

class After {
    // works here
}
I don't get any completion proposals in the following cu, with the caret at |.
report completion proposals after semicolon-terminated list report completion proposals after enum declaration within",org.eclipse.jdt.internal.compiler.parser.Parser
FILE,eclipse-3.1,83383,2005-01-21T06:39:00.000-06:00,IllegalArgumentException in Signature.getParameterCount,"String signature= ""foo(+Ljava.lang.Comparable;)"";
Signature.getParameterCount(signature);
unrelease code
copy code from CompletionRequestorWrapper
When completing a METHOD_REF proposal for a method that has a parameter with an open type bound, getParameterPackages I get an IAE:
not handle bounded types not handle Util.scanTypeSignature",org.eclipse.jdt.internal.core.util.Util
FILE,eclipse-3.1,83489,2005-01-22T17:33:00.000-06:00,[select] Code select returns IType instead of ITypeParameter on method parameters types,"class Test<T> {
  void foo(T t) {}
}
use HEAD
When I select ""T"" in method declaration, selection engine returns an IType
""Test"" instead of expected ITypeParameter ""T"".",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,83536,2005-01-24T10:02:00.000-06:00,,"package t1;
public class Test {
    public static void main (String[] args) {
        new Test ().test (new byte[5]);
    }
    private void test (Object... params) {
    }
}

 
  
 new Test ()  new Object[] {new byte[5]}
The code generates the following error at runtime when started from Eclipse (and
only when started from Eclipse): 
java.lang.VerifyError: (class: t1/Test, method: main signature:
([Ljava/lang/String;)V) Incompatible argument to function
eliminate new Test eliminate error express call to method test","org.eclipse.jdt.internal.compiler.ast.Statement
org.eclipse.ui.internal.WorkbenchWindow
org.eclipse.ui.internal.Workbench"
FILE,eclipse-3.1,83699,2005-01-26T06:03:00.000-06:00,Font reset to default after screen saver,"StyledText.setFont(Font)  
   updateFont(Font, Font)  
   updateFont(Font, Font)  
 updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 updateFont(Font, Font)  
 Display.updateFont()  
  
  
 Display.readAndDispatch()  
 Workbench.runEventLoop(Window$IExceptionHandler, Display)  
 Workbench.runUI()  
 Workbench.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 PlatformUI.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 IDEApplication.run(Object)  
 EclipseStarter.run(Object)
All editors and views using a StyledText widget have the font reset to default after coming back from my screen saver.
build i20050125-0800 unusable
replace org.eclipse.swt.win32_3.1.0
This breakpoint gets hit when I return from the screen saver",org.eclipse.swt.widgets.Control
FILE,eclipse-3.1,84194,2005-02-01T18:05:00.000-06:00,[content assist] Code assist in import statements insert at the end,"import org.eclipse.core.runtime.*;
import org.eclipse.core.runtime.
You will see that upon pressing Enter to select, the text gets inserted several lines down under all the import statements.
the cursor is now in a random position also.","org.eclipse.jdt.internal.ui.text.java.JavaTypeCompletionProposal
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector"
FILE,eclipse-3.1,84724,2005-02-08T13:41:00.000-06:00,[1.5][search] fails to find call sites for varargs constructor_s,"public class Test {
    public void foo() {
        Cell c= new Cell("""", """"); // calls Cell.Cell(String...)
    }
}
 class Cell {
    public Cell(String... args) { }
}
The search engine fails to find the call to the varargs constructor_ in the example below.
> ""Workspace"" from the Java editor context menu; no occurrences will be found.
call Cell.Cell(String...)",org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
FILE,eclipse-3.1,84944,2005-02-10T16:49:00.000-06:00,[1.5][builder] Parameterized return type is sometimes not visible.,"package parser;

public interface ValueParser<T> {
	T parse(final String string);
}
However the return type seems to be not visible for some of the implementations.
The strange thing about this behavior is that a ""clean project"" may clean the
error until next compile, sometimes the error did not occur in the different
implementation.
create minimal persion project show bug attach minimal persion project in bugzilla
show error in line","org.eclipse.jdt.internal.compiler.lookup.BinaryTypeBinding
org.eclipse.jdt.internal.compiler.lookup.ParameterizedTypeBinding
org.eclipse.jdt.internal.compiler.lookup.WildcardBinding"
FILE,eclipse-3.1,85734,2005-02-17T12:28:00.000-06:00,Debug view flickers excessively,"Runtime.exec(...)
linux 2.6.10
The debug view flickers excessively when debugging.","org.eclipse.debug.internal.ui.views.RemoteTreeViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewEventHandler
org.eclipse.debug.internal.ui.views.RemoteTreeContentManager"
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
contain files not save files to JPEG
package com.ibm.test.image;
import org.eclipse.swt.
try { for { string filein = dir + files [ i ] +","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,86610,2005-02-25T06:25:00.000-06:00,Lots of Worker threads (around 100),"class AllZRZHTests
When running all our tests together (see attached class AllZRZHTests.java) on our buildmachine (Linux Fedora Core using Sun VM 1.4.2_06) we see that more and more Worker threads get created but not terminated.
When the last test cases are reached, tons of ""java.lang.StackOverflowError"" are written to the console but no stack trace.
The .log is empty.
work errors with i.e. write to console
use VM
When running in the Debugger the VM often ""dies"" (or freezes) and the tests never finish.
I put a breakpoint into the Worker constructor_ and it looked as if the
DecorationScheduler is causing the creation of the Worker in the UI/main thread.",org.eclipse.ui.internal.decorators.DecorationScheduler
FILE,eclipse-3.1,86614,2005-02-25T07:10:00.000-06:00,Deadlock on startup,"Target Workspace=c 
    
 
 
  
  
 Object.wait()
 
 
  
 
  
  Object.wait()
 
   
   
  Object.wait()
 
   
   
 
   
 
  
 
 Object.wait()  
 
  
   
 
   
   
 
 Object.wait()
 
   
   
  Object.wait()
start Eclipse 3.1_M5a
eclipse \ workspaces
showlocation data
dump Java HotSpot(TM) client VM
tid in Object.wait()
wait on <0x12daaf80>
tid in Object.wait()
wait on <0x12f58438>
tid in Object.wait()
wait on <0x11c4a670>
tid for monitor entry
lock <0x11ea79c0>
wait on <0x11af0060>
tid for monitor entry [ 2fef000
lock <0x11ea79c0>
tid on condition
tid in Object.wait()
wait on <0x11af0310>
tid in Object.wait()
wait on <0x11af00b0>
tid for monitor entry [ 7e000
lock <0x13010d00>
tid on condition
hold 0x009bf394
hold 0x009bf334
lock <0x13010d00>
lock <0x11ea79c0>","org.eclipse.ui.internal.AbstractWorkingSetManager
org.eclipse.ui.IWorkingSetUpdater"
FILE,eclipse-3.1,87211,2005-03-05T13:36:00.000-06:00,,"The dragOver()  
   
   
 PartStack.getDropTarget()
Thus, you cannot prevent a ViewStack 
from being dropped onto another ViewStack.
be in PartStack.getDropTarget()
code presentation not combine views with other views","org.eclipse.ui.internal.ide.dialogs.ResourceTreeAndListGroup
org.eclipse.ui.internal.PartSashContainer"
FILE,eclipse-3.1,87665,2005-03-10T11:38:00.000-06:00,Clicking on x on performance page opens details with no errors,"testOpenJavaEditor1()
like bug regarding handling","org.eclipse.swt.printing.PrintDialog
org.eclipse.swt.widgets.MessageBox"
FILE,eclipse-3.1,87796,2005-03-11T12:18:00.000-06:00,[WorkbenchParts] IWorkbenchSite.getShell() returns null / every text-based editor is leaked,"AbstractTextEditor.dispose() 
 Shell shell= getSite().getShell();
	if (shell != null && !shell.isDisposed())
		shell.removeShellListener(fActivationListener);

  getSite()  getShell()
do following
return null
This now causes every single editor to be kept in
memory until the workbench window gets closed.","org.eclipse.jface.bindings.BindingManager
org.eclipse.ui.keys.IBindingService
org.eclipse.ui.internal.PartSite
org.eclipse.ui.internal.keys.BindingService"
FILE,eclipse-3.1,88339,2005-03-17T10:49:00.000-06:00,configuration area tests failing if bundles are JAR'd,"Platform.asLocalURL(IPluginDescriptor.getInstallURL())
The configuration area tests failed because org.eclipse.osgi and org.eclipse.core.runtime were in JAR'd form.
have following effects
returns a URL in the form jar:file:/path/file.jar!
/ but such URLs are not supported by the framework/Main.","org.eclipse.jdt.internal.debug.ui.JDIModelPresentation
org.eclipse.debug.internal.ui.DefaultLabelProvider
org.eclipse.debug.internal.ui.DebugUIMessages
org.eclipse.jdt.internal.debug.ui.snippeteditor.JavaSnippetEditor"
FILE,eclipse-3.1,89621,2005-03-30T12:41:00.000-06:00,,"import java.awt.Frame;
import java.awt.event.WindowAdapter;

public class Foo extends Frame {

    public void bar() {
        addWindow<CODE ASSIST HERE>Listener(new WindowAdapter());
    }
}
get weird behavior
import java.awt.Frame;
import java.awt.event.WindowAdapter;
extend Frame {
The result is:
occur for method name proposal","org.eclipse.jdt.ui.text.java.CompletionProposalCollector
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector
org.eclipse.jdt.internal.ui.text.java.GenericJavaTypeProposal"
FILE,eclipse-3.1,90283,2005-04-05T08:56:00.000-05:00,[WorkbenchParts]IPartListener2#partInputChanged is not being sent,"partActivated(IWorkbenchPartReference ref)  
 ref.getId()  
 ref.getPart(true)  getSite()  
 ASTProvider.ActivationListener.isJavaEditor()
receive partActivated(IWorkbenchPartReference ref)
==> occurrence marking not working
To see the null value you can put a breakpoint in
ASTProvider.ActivationListener.isJavaEditor().
relate might to bug",org.eclipse.ui.texteditor.AbstractTextEditor
FILE,eclipse-3.1,91098,2005-04-12T06:07:00.000-05:00,The Mark Occurrences feature does not mark all occurrences,"String a;
String[] b;
String[][] c;
All occurrences of String get highlighted.
No occurrence of String is highlighted.
remove square brackets
use 3.1M6",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,92291,2005-04-21T16:48:00.000-05:00,,"Util.compare(Object, Object)   toString() 
 org.eclipse.ui.commands.HandlerSubmission.compareTo(Object)  
 org.eclipse.ui.contexts.EnabledSubmission.compareTo(Object)
Noticed that Util.compare(Object, Object) compares the toString()s of the args.
This could be very inefficient (not to mention inaccurate).
compare site objects",org.eclipse.ui.internal.util.Util
FILE,eclipse-3.1,92451,2005-04-22T16:36:00.000-05:00,,"public class Test {
	public static void main(String[] args) {
		java.util.List elements = null;
		// code assist works on this line
		new Test(Test.toStrings((Test[])elements.toArray(new Test
[0])));
		//code assist fails on this line
	}
	public Test(Object object) {
	}
	public static Object toStrings(Test[] objects) {
		return null;
	}
}
Code assist fails in the following (self-contained) class (see comments for line of error)
work new testelements.toArray on line
fail on line",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,92602,2005-04-25T12:07:00.000-05:00,Deadlock in reentrant lock acquire during waitEnd,"ThreadJob.joinRun(IProgressMonitor)  
      
 JobManager.beginRule(ISchedulingRule, IProgressMonitor)  
 WorkManager.checkIn(ISchedulingRule, IProgressMonitor)  
 Workspace.prepareOperation(ISchedulingRule, IProgressMonitor)  
    
    
 SynchronizationManager$ActivationListener.handleActivated(ResourceInfo)  
 SynchronizationManager$ActivationListener.handleActivated()  
 SynchronizationManager$ActivationListener.windowActivated(IWorkbenchWindow)
 
 Workbench$6.run()  
 InternalPlatform.run(ISafeRunnable)  
 Platform.run(ISafeRunnable)  
 Workbench.fireWindowActivated(IWorkbenchWindow)  
 WorkbenchWindow$5.shellActivated(ShellEvent)  
 TypedListener.handleEvent(Event)  
 EventTable.sendEvent(Event)  
 sendEvent(Event)  
  
  
  
  
  
  
 destroyWidget()  
 dispose()  
 dispose()  
 Shell.dispose()  
   close()  
   close()  
 BlockedJobsDialog.close()  
 BlockedJobsDialog.close(IProgressMonitor)  
 BlockedJobsDialog.clear(IProgressMonitor)  
 WorkbenchDialogBlockedHandler.clearBlocked()  
 EventLoopProgressMonitor.clearBlocked()  
 JobManager.reportUnblocked(IProgressMonitor)  
 ThreadJob.waitEnd(IProgressMonitor)  
 ThreadJob.joinRun(IProgressMonitor)  
      
 JobManager.beginRule(ISchedulingRule, IProgressMonitor)  
 WorkManager.checkIn(ISchedulingRule, IProgressMonitor)  
 Workspace.prepareOperation(ISchedulingRule, IProgressMonitor)  
    
    
 URIConverterImpl$WorkbenchHelper.createPlatformResourceInputStream(String)
 
 URIConverterImpl.createPlatformResourceInputStream(String)  
 URIConverterImpl.createInputStream(URI)  
    
 MonUtils.loadResource(ResourceSet, String)  
 MonUtils.getMonitorResource(ResourceSet, String)  
 MonUtils.getMonitorResource(Resource)  
     initializeMonAdapters()  
     
   
 Class.newInstanceImpl()  
 Class.newInstance()  
 ConfigurationElement.createExecutableExtension(Bundle, String, Object,
IConfigurationElement, String)  
 ConfigurationElement.createExecutableExtension(Bundle, String, String, Object,
IConfigurationElement, String)  
 ConfigurationElement.createExecutableExtension(String)  
 SectionDescriptor.getSectionClass()  
 TabDescriptor.createTab()  
        
     setInput(IWorkbenchPart,
ISelection)  
     selectionChanged(IWorkbenchPart,
ISelection)  
 BPELTabbedPropertySheetPage.selectionChanged(IWorkbenchPart, ISelection)  
 PropertySheet.selectionChanged(IWorkbenchPart, ISelection)  
 AbstractSelectionService$3.run()  
 InternalPlatform.run(ISafeRunnable)  
 Platform.run(ISafeRunnable)  
     fireSelection(IWorkbenchPart,
ISelection)  
     partActivated(IWorkbenchPart)
 
 WorkbenchPage.firePartActivated(IWorkbenchPart)  
 WorkbenchPage.setActivePart(IWorkbenchPart)  
 WorkbenchPage.activate(IWorkbenchPart)  
    
    
      
 WorkbenchPage$9.run()  
 BusyIndicator.showWhile(Display, Runnable)  
    
      
 SCDLUIUtils.openEditor(IWorkbenchPart, IFileEditorInput)  
 SCDLUIUtils.openEditor(IWorkbenchPart, IFile)  
 ComponentEditPart.performOpen()  
     performRequest(Request)  
 CreateImplementationAction.run()  
   runWithEvent(Event)  
  
    
 ActionContributionItem$7.handleEvent(Event)  
 EventTable.sendEvent(Event)  
   sendEvent(Event)  
 Display.runDeferredEvents()  
 Display.readAndDispatch()  
 Workbench.runEventLoop(Window$IExceptionHandler, Display)  
 Workbench.runUI()  
 Workbench.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 PlatformUI.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 IDEApplication.run(Object)  
 PlatformActivator$1.run(Object)  
 EclipseStarter.run(Object)
wait for scheduling rule
get rule
call waitEnd call UI call waitEnd
4 If this call to the UI results in a recursive attempt to acquire the same
lock, the thread deadlocks with itself.
add thread job to list run jobs before calling call waitEnd handle case","org.eclipse.core.internal.jobs.ImplicitJobs
org.eclipse.core.internal.jobs.ThreadJob
org.eclipse.core.internal.jobs.JobManager"
FILE,eclipse-3.1,93069,2005-04-28T03:38:00.000-05:00,Eclipse can't start,"org.eclipse.core.internal.compatibility.PluginActivator.start()
activate bundle org.eclipse.core.resources","org.eclipse.core.internal.resources.Synchronizer
org.eclipse.core.internal.resources.MarkerManager"
FILE,eclipse-3.1,93727,2005-05-04T17:43:00.000-05:00,Code Formatter fails with Method Parameter Annotations,"import org.drools.semantics.annotation.DroolsParameter;

public class Test
{
  public Object passthrough( @DroolsParameter(""parameter"") Object parameter ) {
    return parameter;
  }
}
have methods with parameter annotations
It fails silently, and I don't see an error in
<Workspace>/.",org.eclipse.jdt.internal.formatter.CodeFormatterVisitor
FILE,eclipse-3.1,93854,2005-05-05T17:23:00.000-05:00,IAE in  Util.scanTypeSignature when scanning a signature retrieved from a binding key,"Signature.getTypeParameters(String)  
    
      
    
  
     getText(Object)  
 HierarchyLabelProvider.getText(Object)  
     getText(Object)  
     updateLabel(ViewerLabel,
Object)  
     buildLabel(ViewerLabel, Object,
IViewerLabelProvider)  
     doUpdateItem(Item, Object)  
 AbstractTreeViewer$UpdateItemSafeRunnable.run()  
 InternalPlatform.run(ISafeRunnable)  
 Platform.run(ISafeRunnable)  
 JFaceUtil$1.run(ISafeRunnable)  
 SafeRunnable.run(ISafeRunnable)  
      
 StructuredViewer$UpdateItemSafeRunnable.run()  
 InternalPlatform.run(ISafeRunnable)  
 Platform.run(ISafeRunnable)  
 JFaceUtil$1.run(ISafeRunnable)  
 SafeRunnable.run(ISafeRunnable)  
     updateItem(Widget, Object)  
      
 AbstractTreeViewer$1.run()  
 BusyIndicator.showWhile(Display, Runnable)  
     createChildren(Widget)  
      
      
      
      
  
 TypeHierarchyViewPart$11.run()  
 BusyIndicator.showWhile(Display, Runnable)  
  
 TypeHierarchyViewPart.updateInput(IJavaElement)  
 TypeHierarchyViewPart.setInputElement(IJavaElement)  
 OpenTypeHierarchyUtil.openInViewPart(IWorkbenchWindow, IJavaElement)  
      
    
 OpenTypeHierarchyAction.run(ITextSelection)  
     dispatchRun(ISelection)  
     run()  
   runWithEvent(Event)  
 ActionHandler.execute(Map)  
 LegacyHandlerWrapper.execute(ExecutionEvent)  
 Command.execute(ExecutionEvent)  
 ParameterizedCommand.execute(Object, Object)  
 WorkbenchKeyboard.executeCommand(Binding, Object)  
 WorkbenchKeyboard.press(List, Event)  
 WorkbenchKeyboard.processKeyEvent(List, Event)  
 WorkbenchKeyboard.filterKeySequenceBindings(Event)  
 WorkbenchKeyboard.access$3(WorkbenchKeyboard, Event)  
 WorkbenchKeyboard$KeyDownFilter.handleEvent(Event)  
 EventTable.sendEvent(Event)  
 Display.filterEvent(Event)  
   sendEvent(Event)  
    
    
    
    
    
  
    
  
  
 Display.readAndDispatch()  
 Workbench.runEventLoop(Window$IExceptionHandler, Display)  
 Workbench.runUI()  
 Workbench.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 PlatformUI.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 IDEApplication.run(Object)  
 PlatformActivator$1.run(Object)  
 EclipseStarter.run(Object)
retrieve from Java element
ask type parameters retrieve from binding key
hierarchy on generic types","org.eclipse.jdt.internal.core.hierarchy.HierarchyBinaryType
org.eclipse.jdt.internal.ui.typehierarchy.HierarchyLabelProvider"
FILE,eclipse-3.1,94216,2005-05-09T20:04:00.000-05:00,,"interface IGeneric<T> {
}
 public class Generic<T> implements IGeneric<T> {
    public static void main(String[] args) {
        IGeneric<String> gen= new Generic<String>();
        System.out.println();  // <-- breakpoint here
    }
}
implement IGeneric<T>
Try to do 'open declaring type' or 'open concrete type' for 'gen' at the breakpoint, nothing happens.","org.eclipse.jdt.internal.debug.ui.actions.OpenVariableDeclaredTypeAction
org.eclipse.jdt.internal.debug.ui.actions.OpenVariableConcreteTypeAction"
FILE,eclipse-3.1,94465,2005-05-10T14:33:00.000-05:00,Java Core Dump where modifying value in the Variables View.,"String [] elms= { ""abc"", ""cde"", ""xyz"" };
have string array
4. Click ok and it will result in a java dump.
get following error in console
write to d
have Processed exception Signal","org.eclipse.jdt.internal.debug.ui.JDIModelPresentation
org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.ActionMessages"
FILE,eclipse-3.1,94540,2005-05-10T17:28:00.000-05:00,[Undo] ClassNotFoundException disposing undoable operations on shutdown,"Workbench.shutdown()  
  
  
  
  
                           
  
  
 org.eclipse.ui.internal.WorkbenchPlugin.stop()
build n20050509
notice following in console output
dispose operations dispose undo support shutdown undo support during WorkbenchPlugin.stop come operations from higher level plug-ins
stop higher level plug-ins
shut down undo support during Workbench.shutdown()
not load org.eclipse.ltk.core.refactoring.CompositeChange$2
shut down shut plug-in org.eclipse.ltk.core.refactoring
not load org.eclipse.ltk.core.refactoring.CompositeChange$2
shut down shut plug-in org.eclipse.ltk.core.refactoring
stop org.eclipse.ui.workbench_3.1.0
cause java.lang.NoClassDefFoundError",org.eclipse.ui.internal.WorkbenchPlugin
FILE,eclipse-3.1,95152,2005-05-13T12:14:00.000-05:00,[search] F3 can't find synthetic constructor_,"InputReadJob readJob = new InputReadJob(streamsProxy);
open new class file editor position at top
have constructor _
Clicking this entry in the outline view does not jump to the constructor_ in the editor.
The mapping of class file to source is not handling the synthetic addition of the enclosing class by the compiler.
This breaks any kind of navigation to the corresponding constructor_ in the source attachment.","org.eclipse.ant.internal.ui.views.AntViewDropAdapter
org.eclipse.ant.internal.ui.launchConfigurations.AntLaunchShortcut
org.eclipse.ant.internal.ui.AntUtil
org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
org.eclipse.jdt.internal.core.search.indexing.BinaryIndexer
org.eclipse.jdt.internal.core.index.DiskIndex
org.eclipse.jdt.internal.core.search.matching.ConstructorPattern"
FILE,eclipse-3.1,95505,2005-05-17T02:56:00.000-05:00,,"{cursor}
It was very convinient in Eclipse that when it already knows type and I write
""new "" and press Ctrl+Space, it shows this type.
have look in m7 write several first letters write look
return old behaviour",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,95731,2005-05-18T07:29:00.000-05:00,[Progress] NPE in JobErrorDialog.refresh(...) line 92,"JobErrorDialog.refresh(...)
JDT Text Performance tests which used to run now fail with an NPE in
JobErrorDialog.refresh(...) line 92, for details see:
happen on test machine
execute runnable
execute runnable",org.eclipse.ui.internal.progress.JobErrorDialog
FILE,eclipse-3.1,96440,2005-05-24T11:11:00.000-05:00,Tables laying out 3 times when trying to determine sizes,"table.getClientArea()
layout table
return width in m6 return width of table return smaller value make in Table layout
4 You will see a client area size of about 81
5 Do the same in M6 - it will be about 320 or so.",org.eclipse.jface.preference.PreferencePage
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,[Presentations] (regression) Standalone view without title has no border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
build n20050523
- the history view (a regular view) has a border, but the standalone view does not","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,96604,2005-05-25T09:31:00.000-05:00,[1.5][codeassist] missing proposals for wildcard capture,"import java.util.List;
 public class X<U, V extends List<U>>  
 void foo(X<String, ?> x1, X<Object, ?> x2)  
 get(...)
import java.util.List;
void foo(X<String, ?> x1, X<Object, ?> x2) {
offer list #ce",org.eclipse.jdt.internal.compiler.lookup.BlockScope
FILE,eclipse-3.1,96766,2005-05-26T07:47:00.000-05:00,,"public class Tst {
    
    public static void main(String[] args) throws Exception {
        
        System.out.println(""Log: Tst.main(Tst.java:5) Some message"");
        System.out.println(""Log: Tst.main(Tst.java:6)"");
    }
}
print log messages in releases act log messages as hyperlinks act log messages in console
require new line following final ) change parsing in 3.1M7 change parsing of line number
In the console the 'Tst.java:6' hyperlink will work,
but the 'Tst.java:5' hyperlink will give 'Hyperlink Error' with reason 'Unable
to parse line number from hyperlink'.
return parsing need ) \ n need terminating )",org.eclipse.jdt.internal.debug.ui.console.JavaStackTraceHyperlink
FILE,eclipse-3.1,96820,2005-05-26T12:27:00.000-05:00,JME during Source lookup,"enable()
After opening the Find/Replace dialog and enabling ""Regular Expressions"", I got a ""Source not found."" editor and the JME below.
import projects as source have project org.eclipse.core.boot
log from Debug core
not exist ] not exist java Model status [",org.eclipse.jdt.internal.launching.JavaSourceLookupUtil
FILE,eclipse-3.1,97190,2005-05-30T05:16:00.000-05:00,,"System.out.println(Long.MAX_VALUE);
System.out.println(23092395825689123986L);
not handle long values
When a long value is longer than Long.MAX_VALUE it doesn't complain about it and compiles it.
When you run the code you get invalid results of course.
is compiled and produces this output",org.eclipse.jdt.internal.compiler.ast.LongLiteral
FILE,eclipse-3.1,97655,2005-05-31T14:51:00.000-05:00,,"Bundle.getState()  
  
 Bundle.getState()
not define custom
Upon updating my Eclipse platform to 3.0 M7, I find that this plug-in's underlying bundle no longer auto-starts when its extensions are loaded and executed.
In consequence, the evaluation of XML enablement expressions that depend on PropertyTesters defined by my plug-in do not work because the property test always results in EvaluationResult.NOT_LOADED.
provide plug-in lifecycle class
convert to bundle
appear from bug
build for Eclipse not be under control
check bundles for active-ness check expression property testers for active-ness use Bundle.getState() cause unexpected results for people
do lot of checking
make bundles auto-start
not change to Bundle.ACTIVE
require activator hooks
run code
do stuff in platform engage bundle
document in on-line help
find work-around","org.eclipse.core.runtime.internal.adaptor.IPluginInfo
org.eclipse.core.runtime.internal.adaptor.PluginParser
org.eclipse.core.runtime.internal.adaptor.PluginConverterImpl"
FILE,eclipse-3.1,98147,2005-06-02T13:09:00.000-05:00,Variables View does not show all children if same instance is expanded twice,"package xy;
public class Try {
	String fName;
	int fID;
	
	public Try(String name, int id) {
		fName= name;
		fID= id;
	}
	
	public static void main(String[] args) {
		Try t= new Try(""Hello"", 5);
		callee(t, t);
	}
	
	static void callee(Try t1, Try t2) {
		boolean same= t1.equals(t2); //breakpoint here
	}
	
}
- Expand t2 -> only child fID is shown
void main(String[] args) {",org.eclipse.debug.internal.ui.views.RemoteTreeViewer
FILE,eclipse-3.1,98202,2005-06-02T18:28:00.000-05:00,NPE placing breakpoint on task outside of target,"lStack(java.lang.StringBuffer)  
 
 r.getStackFrames()  
  
 org.eclipse.ant.internal.ui.debug.model.AntThread.getStackFrames0() 
 
 org.eclipse.ant.internal.ui.debug.model.AntThread.getTopStackFrame() 
 
  
    
 
     
  
 org.eclipse.debug.core.DebugPlugin$EventNotifier.run()  
  
  
  
  
  
  
  
 org.eclipse.core.internal.jobs.Worker.run()
own org.eclipse.ant.internal.ui.debug.model.AntThread
own java.lang.Object",org.eclipse.ant.internal.ui.antsupport.logger.util.AntDebugState
FILE,eclipse-3.1,98621,2005-06-06T22:04:00.000-05:00,[refactoring] [rename] Rename Type hangs,"class I18L  
 public class I18N {

	protected static void loadMessages(Class clazz, String name) {
		...
	}
}

 
 public class Messages extends I18L {
  public static String unexpectedException;
  ...
  static {
    loadMessages(Messages.class, ""messages.properties"");
  }
}
protect static void loadMessages(Class clazz, String name) {
extend i18l { unexpectedexception static { loadmessages unexpectedexception i18l {
The only
noticeable effect was the Cancel button was disabled.
Clicking the window exit
box had no effect.
not get thread dump
kill Eclipse with Task manager
When I restarted Eclipse, 7 of the references had been changed to I18N and 3 had
not.
Open Type (after re-indexing its database) still shows the non-existant
I18L type, though if you try to open it, the path cannot be found.","org.eclipse.core.internal.jobs.ImplicitJobs
org.eclipse.core.internal.jobs.ThreadJob"
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,Container attempts to refresh children on project that is not open,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
A background refresh job has now been started for the closed project, but it never finishes and is stuck in an infinite loop.
be in class org.eclipse.core.internal.resources.Container.
The members() method is excuting if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
workspace.refreshManager.refresh(this);
not know members because projects
Both the AliasManager and the Java
Perspective are calling members on the IProject.
override method in Project not refresh for closed projects
On the next UI gesture, we get refresh infinite loops, one for each closed project.
want projects in workspace
work in Eclipse","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
initialize enum/switch table initialize synthetic method place initialization in static initializer
But on my machine, it prints ""default"" 22 times & 1999
18 times.
package com.bea;","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99355,2005-06-10T09:48:00.000-05:00,,"package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      final Container<String> container = c.get();
      final String string = container.get();
      //to here
   }
}
 
 

package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      extractedMethod(c);
      //to here
   }

   private void extractedMethod(final final final Container<Container<String>> c)
   {
      final Container<String> container = c.get();
      final String string = container.get();
   }
}
you will see that the extracted method declares its paramater with too many final modifiers:
t get()
return new GenericContainer(outerContainer)
void method()
t get()
return new GenericContainer(outerContainer)
void method()
notice the 3 final modifiers in the extractedMethod signature.",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,99693,2005-06-13T11:29:00.000-05:00,Invalid stack frames during display,"private static void doGenerics() {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < 1000; i++) {
			int num = rand.nextInt(10000) + 1;
			list.add(num);
		}
		
		int max = 0;
//start eval
		for (Integer integer : list) { // BREAKPOINT HERE
			max = Math.max(max, integer);
		}
		System.out.println(max);
//end eval
	}
void doGenerics() {
Watching the Variables View I see a lot of invalid stack frames.
They are not destructive (and nothing is logged).
not request and/or cancelling requests","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
CLASS,openjpa-2.2.0,OPENJPA-2149,2012-03-05T01:50:18.000-06:00,Criteria.function adds wrong casts to parameters making it unsuable,"Expression<String> stPointFunc = cb.function(
				""db2gse.st_point"", 
				String.class,
				cb.literal(0.0),
				cb.literal(0.0),
				cb.literal(1003));
		
		Expression<Double> distanceFunc = cb.function(
				""db2gse.st_distance"", 
				Double.class, 
				stPointFunc, 
				usersLocations.get(""location""));
		
		criteriaQuery.select(usersLocations).where(cb.lessThan(distanceFunc, cb.literal(50.0)));

 
    
    
 args.appendTo(sel, ctx, state, sql, 0);
     
 for (int i = 1; i < vals.length; i++) {
                sql.addCastForParam(getOperator(), vals[i]);
            }
     
  
  
 Expression<String> stPointFunc = cb.function(
				""db2gse.st_point"", 
				String.class,
				cb.coalesce(cb.literal(0.0), cb.literal(0.0)),
				cb.coalesce(cb.literal(1.0), cb.literal(1.0)),
				cb.coalesce(cb.literal(1003), cb.literal(1003)));
Criteria.function will generate an SQL with only the last parameter casted and to the wrong type.
Will generate the following SQL:
(db2gse.st_distance(db2gse.st_point(?
Notice the 3rd parameter is an Integer and its being cast as Double.
be in org.apache.openjpa.jdbc.kernel.exps.DatastoreFunction#appendTo
mean cast for param add in sql buffer
extend DBDictionary remove cast as work
coalesce uses raw value instead of parameters and makes it work (the same value twice becuase if I put cb.nullLiteral I get a NullPointerException, might be another bug)","openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.kernel.exps.DatastoreFunction
openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.kernel.exps.Args
openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.kernel.exps.UnaryOp"
CLASS,openjpa-2.2.0,OPENJPA-2163,2012-03-27T15:56:55.000-05:00,Lifecycle event callback occurs more often than expect,"final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
 
 MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
 
 ((OpenJPAEntityManagerSPI)em).addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).addLifecycleListener(l2, null);
em = factory.createEntityManager()
When life cycle event occurs for a specific entity manager, all the listeners created under the emf are being invoked.","openjpa-kernel.src.main.java.org.apache.openjpa.conf.OpenJPAConfigurationImpl
openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.validation.TestValidationMode"
CLASS,openjpa-2.2.0,OPENJPA-2197,2012-05-16T17:10:22.000-05:00,,"@PreUpdate
    public void updateChangeLog(Object entity)  
 private void updateChangeLog(BaseEntity he, ChangeLogEntry cle)

 
   @PreUpdate
AnnotationPersistenceMetaDataParser contains a MethodComparator which only compares the class + the method name.
have methods with same name
void updateChangeLog(Object entity) {
Due to the bug in MethodComparator, my @PreUpdate sometimes didn't get detected.",openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.callbacks.ListenerImpl
CLASS,openjpa-2.2.0,OPENJPA-2227,2012-07-09T14:24:05.000-05:00,OpenJPA doesn't find custom SequenceGenerators,"{code}
 @Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable  
 @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
 {code}

 
     JavaTypes.classForName()     Class.forName()
use custom SequenceGenerator within enterprise application use openJPA
When defining a custom Sequence a ClassNotFoundException (for the Sequence class) will be thrown when trying to insert data into the database.
The example will produce the stacktrace attached.
It seems that the wrong class loader is used to instantiate the custom sequence class.
With JavaSE (JUnit) all is working fine, but after deploying into WAS the Exception will occur.
think within method SequenceMetaData.instantiate(Classloader envLoader) use method with parameter mustexist use method instead_of pure Class.forName() call
need for method call",openjpa-kernel.src.main.java.org.apache.openjpa.meta.SequenceMetaData
CLASS,openjpa-2.2.0,OPENJPA-2247,2012-08-03T10:29:58.000-05:00,JoinColumn annotation is ignored when mapping a unidirectional owned OneToOne that is in a SecondaryTable,"@Entity
@SecondaryTable(name = ""ParentSecondaryTable"", pkJoinColumns = 
    { @PrimaryKeyJoinColumn(name = ""idParent"", referencedColumnName = ""idParent"") })
public class Parent {

    @Id
    @GeneratedValue
    int idParent;

    String child_ref;

    @OneToOne
    @JoinColumn(name = ""CHILD_REF"", table = ""ParentSecondaryTable"", referencedColumnName = ""idChild"")
    PChild child;

}
The runtime incorrectly ignores @JoinColumn.
name when mapping a unidirectional owned OneToOne that is in a SecondaryTable.
run with persistence.xml set persistence.xml to 2.0
pchild child
The column ""CHILD_REF"" will be ignored and the runtime will look for the fk in non-existent column ParentSecondaryTable.CHILD_IDCHILD.",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingRepository
CLASS,openjpa-2.2.0,OPENJPA-2255,2012-08-30T20:15:52.000-05:00,Couldn't load the referencedColumn definition when create the JoinTable,"@Entity 
public class Student  
 @Id @Column(name=""id"", length=128, nullable=false) private String id; 
   @Column(name=""sName"", length=255) private String sName; 
   @ManyToMany 
  @JoinTable( 
    name=""student_course_map"", 
    joinColumns={@JoinColumn(name=""student_id"", referencedColumnName=""id"", nullable=false)}, 
    inverseJoinColumns={@JoinColumn(name=""course_id"", referencedColumnName=""id"", nullable=false)} 
  ) 
  public Collection getCourses() 

   
 @Entity 
public class Courses{ 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""cName"", length=255) private String cName; 

  ... 
}
The JoinColumn couldn't have the referencedColumn's definition which includes the length definition.
define student id length
The warning message will occur like this",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingInfo
CLASS,openjpa-2.2.0,OPENJPA-428,2007-11-01T12:39:40.000-05:00,"Bad error message regarding ""openjpa.Id""","@Id 
 @Id  @Id 
 @Id 
 @Entity
@Table(name=""TAX"", schema=""JPA_SC"")
public class Tax  {
	
	// Class variables  
	protected double taxamount;
 
	public Tax(){
		
	}
	
	public Tax(double taxamount){
		this.taxamount = taxamount;
	}
//plus getter and setter for taxamount

}
report confusing misplaced error message
request more info
When running my project with OpenJPA, I get the following error message:
use dictionary class org.apache.openjpa.jdbc.sql.DB2Dictionary
call openjpa.Id
use dictionary class org.apache.openjpa.jdbc.sql.DB2Dictionary
As you can see, the two property names printed are the same, not different or similar.
retype @Id annotations come from copy come from paste
Furthermore, I was able to identify that the error message was being printed only when I removed the @Id annotation from one of my classes (all the other classes still have @Id).","openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.Configurations
openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.ConfigurationImpl"
METHOD,atunes-1.10.0,281,2008-12-25T02:58:34.000-06:00,,"Debug mode = false
  Execution path = C   
  
  
 {MPlayer}
I tried to use the tray icon mute but it doesn't have effect unless I use the main UI's mute.
I mute the OS but the GUI doesn't display oin the main UI Wondow or the tray icon.
run in Java virtual Machine 1.6.0_11
serialize favorites cache
serialize repository cache
check in path check for Windows check for DLL
load jintellitype DLL
update index for class net.sourceforge.atunes.kernel.modules.search.searchableobjects.FavoritesSearchableObject
update search index
update index for class net.sourceforge.atunes.kernel.modules.search.searchableobjects.RepositorySearchableObject
update search index
feed entries done
play list
start play of file
feed entries done
feed entries done
start play of file
feed entries done",net.sourceforge.atunes.kernel.handlers.SystemTrayHandler:fillMenu(JTrayIconPopupMenu)
CLASS,solr-4.4.0,SOLR-5295,2013-10-02T00:09:02.000-05:00,The createshard collection API creates maxShardsPerNode number of replicas if replicationFactor is not specified,"{quote}
 
  
  
  
 {quote}
report by Brett hoerner report on solr-user
set maxShardsPerNode
want 4th shard create because cluster
have shard per node
not require more hardware
Now I add shard4 and it immediately tries to add 1000 replicas of shard4...",solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor
FILE,AMQP,AMQP-164,2011-04-27T02:00:51.000-05:00,Exchange parsers do not handle anonymous queues correctly,"testContext(org.springframework.amqp.rabbit.stocks.web.ServletConfigurationTests)
The problem manifests itself as a test failure in the stocks sample because there is no integration test for the binding parsers in spring-rabbit.
create org.springframework.amqp.rabbit.core.RabbitAdmin#0","org.springframework.amqp.rabbit.config.HeadersExchangeParser
org.springframework.amqp.rabbit.config.TopicExchangeParser
org.springframework.amqp.core.Binding
org.springframework.amqp.rabbit.config.FanoutExchangeParser
org.springframework.amqp.rabbit.config.DirectExchangeParser"
FILE,AMQP,AMQP-190,2011-09-10T20:24:17.000-05:00,CachingConnectionFactory leaks channels when synchronized with a TransactionManager,"convertAndSend()
It seems that when I use RabbitTemplate, channelTransacted=true, to convertAndSend() a message to an exchange within the context of a synchronized TransactionManager (e.g. an active transaction on the current thread), the channel is never closed, hence new publishes will always get their ""own"", shiny, new channel (that is never closed or released to the channel pool) until Rabbit can't handle any more channels.
see Forum reference for more info
not observe on consumer side
observe on publishing side
use RabbitTemplate use spring-integration use <int-amqp:outbound-channel-adapter...> tag
supply simple recreate scrounge time","org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer
org.springframework.amqp.rabbit.core.RabbitTemplatePerformanceIntegrationTests
org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils
org.springframework.amqp.rabbit.connection.RabbitResourceHolder"
FILE,AMQP,AMQP-481,2015-03-02T05:28:36.000-06:00,,"@Scope(proxyMode=TARGET_CLASS)  @Lazy    RabbitListenerEndpointRegistrar.afterPropertiesSet()   RabbitListenerAnnotationBeanPostProcessor.afterSingletonsInstantiated()
Because of the way the endpoints are detected in a BeanPostProcessor and the message listener container is started in a SmartInitializingBean, the listener is never registered if it is a scoped proxy (e.g. @Scope(proxyMode=TARGET_CLASS) @Lazy).
The BeanPostProcessor is not presented with the actual listener until it is instantiated, and the instantiation happens lazily, the container is never created or started (I believe it's because the call to RabbitListenerEndpointRegistrar.afterPropertiesSet() in RabbitListenerAnnotationBeanPostProcessor.afterSingletonsInstantiated() is never made).
see https://github.com/spring-cloud/spring-cloud-config/issues/96 for issue","org.springframework.amqp.rabbit.listener.RabbitListenerEndpointRegistry
org.springframework.amqp.rabbit.annotation.EnableRabbitTests
org.springframework.amqp.rabbit.listener.RabbitListenerEndpointRegistrar"
FILE,AMQP,AMQP-502,2015-06-19T03:02:33.000-05:00,Fanout binding is not created due to missing routing key,"@RabbitListener(




      bindings = @QueueBinding(




          value = @Queue(




              autoDelete = ""true""




          ),




          exchange = @Exchange(




              type = ""fanout"",




              value = ""mytest.broadcast"",




              autoDelete = ""true""




          ),




          key = ""#""




      )




  )




  public void processBroadcast(String data) {




    int i = 0;




  }






 
  
  
  
     
 
     
 
  
  
  
  
  
   {}   
     
 
     
 
     
 
  
     
 
     
 
     
 
     
 
     
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   {}   
  
     
 
      
   {}
I will get an error, that the binding can not be created.
try exchange type
omit key
The following is the debug logout and the stacktrace:
close connection reopen connection
declare mytest.broadcast
declare 5df237ec-13d4-4aab-a0ff-772707bd7d03
exchange [ mytest.broadcast] with routing route key [ null ]
start on 5df237ec-13d4-4aab-a0ff-772707bd7d03 amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA
raise exception
cause by java.lang.IllegalStateException
close channel on exception
close channel on exception","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-633,2016-08-18T15:48:45.000-05:00,,"RabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager




		.getResource(connectionFactory);




if (resourceHolder != null) {




	Channel channel = resourceFactory.getChannel(resourceHolder);




	if (channel != null) {




		return resourceHolder;




	}




}






    resourceFactory.isSynchedLocalTransactionAllowed()
When running a RabbitTemplate on a transactional container thread, the container channel is used, even if the RabbitTemplate is not marked transactional.
consider case publish message reject inbound message
If you publish with a template that is not transactional, the publish should occur on a new channel.","org.springframework.amqp.rabbit.listener.LocallyTransactedTests
org.springframework.amqp.rabbit.core.RabbitTemplate"
FILE,AMQP,AMQP-653,2016-10-08T02:53:08.000-05:00,,"@Bean




Jackson2JsonMessageConverter jackson2JsonMessageConverter() {




	return new Jackson2JsonMessageConverter();




}
return new Jackson2JsonMessageConverter()
However, if you switch to RabbitMessagingTemplate, that bean no longer works, because RabbitMessagingTemplate doesn't offer to look up RabbitTemplate's converters, and instead relies on its own.
hook up message converters look inside Spring boot","org.springframework.amqp.rabbit.core.RabbitMessagingTemplateTests
org.springframework.amqp.rabbit.core.RabbitMessagingTemplate"
FILE,AMQP,AMQP-196,2011-09-23T11:23:51.000-05:00,RabbitTemplate/RabbitGatewaySupport incompatible with JDK proxy,"class RabbitTemplate
RabbitGatewaySupport uses class RabbitTemplate rather than interface RabbitOperations for its rabbitTemplate property.
If AOP advice creates a proxy for a RabbitTemplate a JDK proxy will be created since RabbitTemplate implements the RabbitOperations interface.
If the advised bean is injected into RabbitGatewaySupport, an error will occur because the bean is not an instance of RabbitTemplate.
make proxies in application CGLIB proxies have negative consequences in application CGLIB proxies have proxies in application CGLIB proxies
change RabbitGatewaySupport make type of rabbitTemplate RabbitOperations
invoke getConnection method on rabbitTemplate add method to RabbitOperations interface
avoid use of RabbitGatewaySupport create local copy",org.springframework.amqp.rabbit.core.RabbitOperations
FILE,AMQP,AMQP-656,2016-10-15T00:25:46.000-05:00,Unable to refer to the default exchange using @Argument within a @RabbitListener,"@Argument 
 @RabbitListener(bindings =




        @QueueBinding(




            value = @Queue(




                value = ""app.events.myEvent"",




                durable = ""true"",




                exclusive = ""false"",




                autoDelete = ""false"",




                arguments = {




                        @Argument(name=""x-dead-letter-exchange"", value = """"),




                        @Argument(name=""x-dead-letter-routing-key"", value=""app.dlq"")




                }),




            exchange = @Exchange(value=""amq.topic"", durable = ""true"", type = ""topic""),




            key=""event.app.myEvent.v1""




        ))






 
 @Bean




    public Queue appMyEventQueue() {




        return QueueBuilder.durable(""app.events.myEvent"")




            .withArgument(""x-dead-letter-exchange"", """")




            .withArgument(""x-dead-letter-routing-key"", deadLetterQueue().getName())




            .build();




    }
It seems you are unable to use @Argument annotations that use empty strings to refer to the default exchange.
exchange @Exchange
This fails though as spring seems to not send the empty string.
use things like SPEL evaluate things like SPEL evaluate things to empty string evaluate things to same result
use bean configs get configuration use something like following be with annotation
return QueueBuilder.durable(""app.events.myEvent"")","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
METHOD,commons-math-3-3.0,MATH-718,2011-12-03T18:40:44.000-06:00,inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.,"{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}
The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.
This returns 499525, though it should be 499999.
return infinity
not work as result","org.apache.commons.math3.util.ContinuedFraction:evaluate(double, double, int)"
METHOD,commons-math-3-3.0,MATH-841,2012-08-05T04:27:07.000-05:00,,"public void testApache(){
        Random rng=new Random(0);
        long checksum=0;
        long start=System.nanoTime();
        checksum+=gcd(0,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,0);
        checksum+=gcd(Integer.MAX_VALUE,rng.nextInt());
        for(int i=0;i<10000;i++) checksum+=gcd(rng.nextInt(),Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MIN_VALUE,1<<30);
        checksum+=gcd(1<<30,1<<30);
        checksum+=gcd(3 * (1<<20),9 * (1<<15));
        for(int i=0;i<30000000;i++) checksum+=gcd(rng.nextInt(),rng.nextInt());
        long end=System.nanoTime();
        long tns=end-start;
        long tms=(tns+500000)/1000000;
        long ts=(tms+500)/1000;
        System.out.println(""exec time=""+ts+""s, (""+tms+""ms), checksum=""+checksum);
        assertEquals(9023314441L,checksum);
    }
The gcd(int,int) method of ArithmeticUtils seems 2 times slower than the naive approach using modulo operator.
The following test code runs in 11s with current version and in 6s with the patch.
gcd * )","org.apache.commons.math3.util.ArithmeticUtils:gcd(int, int)"
FILE,DATACMNS,DATACMNS-57,2011-07-26T08:39:54.000-05:00,,"Constructor.getParameterTypes()     getGenericParameterTypes()
When an object contains a List of another object (no simple types), the MongoMappingConverter finds correct generic type for the field, but when this field is used in the PersistenceConstructor, the type is considered to be the raw type and then no information is available to convert batck the DBObject instances for this list.
In the TypeDiscover class, the method getParameterTypes(Constructor<?>)
use the reflection method Class<?>
[] Constructor.getParameterTypes() that give no hint on the generic parameter types.
replace call with type [ ] constructor solve getGenericParameterTypes()","org.springframework.data.util.TypeDiscovererUnitTests
org.springframework.data.util.TypeDiscoverer"
FILE,DATACMNS,DATACMNS-114,2011-12-19T03:21:41.000-06:00,,"AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...)  getImplementationClassName()
When automatically scanning the repositories, and their custom implementation, the wrong custom implementation is wired to our repository bean.
Resulting in the following exception:
find for type class com.myproject.Contract
When starting the application context, the contractRepository bean is linked to our anotherContractRepositoryImpl rather than the contractRepositoryImpl.
occur on Linux CI server
find cause at AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...)",org.springframework.data.repository.config.AbstractRepositoryConfigDefinitionParser
FILE,DATACMNS,DATACMNS-154,2012-04-17T06:02:43.000-05:00,,"public interface UserRepository extends JpaRepository<User, Long>,




		PagingAndSortingRepository<User, Long> {









	public User saveAndFlush(User entity);









	public void delete(User entity);




}
During initialization of the following repository an exception occurs for these two methods.
extend jparepository <user, long>
create bean with userrepository throw exception on object creation not create for method public abstract entities.User repositories.UserRepository.saveAndFlush(entities.User)
not create for method public abstract entities.User repositories.UserRepository.saveAndFlush(entities.User)
find for type class entities.User
work with snapshot work before weekend","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-157,2012-04-20T01:24:38.000-05:00,@Query in extending interface is not picked up correctly,"@Query 
 @NoRepositoryBean




public interface EntityRepository<T> extends JpaRepository<T, Long> {









	T findByDealer(Dealer dealer);




}









 public interface CarRepository extends EntityRepository<PersonalSiteVehicle> {









	@Override




	@Query(""select p from PersonalSiteVehicle p join p.detail d join d.enrichable e where e.dealer = ?1"")




	PersonalSiteVehicle findByDealer(Dealer dealer);




}






 
  @Query
extend EntityRepository<PersonalSiteVehicle> {
join p.detail join d.enrichable
Results in
create bean with carrepository throw exception on object creation not create for method public abstract java.lang.Object nl.inmotiv.indi.repository.EntityRepository.findByDealer(nl.inmotiv.indi.domain.Dealer)
not create for method public abstract java.lang.Object nl.inmotiv.indi.repository.EntityRepository.findByDealer(nl.inmotiv.indi.domain.Dealer)
find for type class nl.inmotiv.indi.domain.PersonalSiteVehicle
It looks like Spring Data does not use the @Query annotation in the sub interface.","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-160,2012-04-21T08:47:35.000-05:00,,"public interface DeleteOnlyRepository<T, ID extends Serializable> extends Repository<T, ID>{









    public void delete(ID paramID);









    public void delete(T paramT);









    public void delete(Iterable<? extends T> paramIterable);









    public void deleteAll();









}
A repository which only defines delete methods is not created by the Spring Data code with the exception:
create bean with treeentitydeleterepository throw exception on object creation not create for method public abstract void example.data.DeleteOnlyRepository.delete(java.lang.Object)
not create for method public abstract void example.data.DeleteOnlyRepository.delete(java.lang.Object)
find for type class example.data.TreeEntity
cause by repository
extend repository <t, ID> {
use Spring
build snapshots
use JPA component report for mo
want repository prevent clients from performing perform CRU operations without use perform CRU operations on child object offer ability","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-176,2012-05-21T11:47:05.000-05:00,StackOverflowError when inserted object is a CGLIB proxy,"@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)
M1)] that is in ""session"" scope and using a CGLIB proxy (ie: ""@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)"") I receive a StackOverflowError.
remove session scoping work correctly.java.lang.StackOverflowError",org.springframework.data.util.ClassTypeInformation
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
notice important issue relate to automatic web binding
When posting a new Order where Order.customer == """" then a converter exception is thrown:
convert property value of type java.lang.String convert property value to required type org.mycomp.domain.Customer convert from type java.lang.String convert to type @javax
cause complete blocker for optional references","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-257,2012-11-29T02:29:27.000-06:00,,"@Id 
 class Foo{




  




  @Id




  private String UID;









  //code omitted




}
Cannot execute MongoOperations.findOne method if my model entity contains @Id field which name is uppercase, like UID.
3) at this step you will get an exception
find on com.xxxxxxxxxxxxx.TemplateDefinitionObject!","org.springframework.data.mapping.PropertyPath
org.springframework.data.mapping.PropertyPathUnitTests"
FILE,DATACMNS,DATACMNS-390,2013-10-28T09:07:46.000-05:00,ConcurrentModificationException in MappingContextTypeInformationMapper.resolveTypeFrom,"getPersistentEntities()
	 public Collection<E> getPersistentEntities() {
		try 
{
			read.lock();
			return persistentEntities.values();
		}
 finally 
{
			read.unlock();
		}
	}
       
     values()
I am using Spring Data and found the following Stacktrace in the logs:
I think that there might be a data race in the implementation of  org.springframework.data.mapping.context.AbstractMappingContext#getPersistentEntities()
Although the code acquires a read lock when returning the values of the HashMap ""persistentEntities"" it immediately releases the lock.
This opens up the possibility that a thread modifies the values of ""persistentEntities"" while another thread is iterating over the values.
The JavaDoc of java.util.HashMap#values() says ""If the map is modified while an iteration over the collection is in progress
(except through the iterator's own <tt>remove</tt> operation), the results of the iteration are undefined.""
In my case I get a ConcurrentModificationException.
take look at issue","org.springframework.data.mapping.context.AbstractMappingContext
org.springframework.data.mapping.context.AbstractMappingContextUnitTests"
FILE,DATACMNS,DATACMNS-511,2014-05-22T00:04:43.000-05:00,AbstractMappingContext.addPersistentEntity causes infinite loop,"public class User extends AbstractTenantUser<User, Role, Permission, Tenant> {




    ...




}




 public abstract class AbstractTenantUser<USER extends AbstractTenantUser<USER, ROLE, PERMISSION, TENANT>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>, TENANT extends AbstractTenant<USER>> extends AbstractUser<USER, ROLE, PERMISSION> implements TenantEntity<TENANT> {




    ...




}




 public abstract class AbstractUser<USER extends AbstractUser<USER, ROLE, PERMISSION>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AbstractPermission<USER extends AbstractUser<USER, ?, ?>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AuditingDateBaseEntity<USER extends AbstractUser<USER, ?, ?>> extends AbstractDateBaseEntity implements AuditingEntity<USER> {




    ...




}




 public abstract class AbstractDateBaseEntity extends AbstractBaseEntity implements DateEntity {




    ...




}




 public abstract class AbstractBaseEntity implements BaseEntity {




    ...




}
not run tests after updating update from Codd sr2 update to Dijkstra
debug issue find after debugging lie in AbstractMappingContext.addPersistentEntity.
never call method
extend AbstractPermission<USER> implement TenantEntity<TENANT> {
extend AuditingDateBaseEntity<USER> {
extend AuditingDateBaseEntity<USER> {
implement AuditingEntity<USER> {
implement DateEntity {
implement BaseEntity {",org.springframework.data.util.TypeVariableTypeInformation
FILE,DATACMNS,DATACMNS-562,2014-08-19T01:25:20.000-05:00,,"public class ClassC extends ClassA {




	private ClassB b;









	public ClassB getB() {




		return b;




	}









	public void setB(ClassB b) {




		this.b = b;




	}




}









 class ClassA {









	private String name;









	private ClassD dObject;









	public String getName() {




		return name;




	}









	public void setName(String name) {




		this.name = name;




	}









	public ClassD getdObject() {




		return dObject;




	}









	public void setdObject(ClassD dObject) {




		this.dObject = dObject;




	}




}









 class ClassB extends ClassA {




}









 class ClassD {









	private TreeMap<String, TreeMap<String, String>> map = new TreeMap<>();









	public TreeMap<String, TreeMap<String, String>> getMap() {




		return map;




	}









	public void setMap(TreeMap<String, TreeMap<String, String>> map) {




		this.map = map;




	}









}






 
 
 
 
 
 
 
 ClassC cObject = new ClassC();




cObject.setName(""Jon"");




try {




	mongoTemplate.save(cObject, ""c"");




} catch (Exception e) {




	e.printStackTrace();




}






 
 
     private transient EntrySet entrySet = null;
extend classa {
try {
Exception caught as below:
get same issues
have something different return empty Type array to emit ArrayIndexOutOfBoundsException
traverse field
use as container","org.springframework.data.mapping.model.AbstractPersistentPropertyUnitTests
org.springframework.data.mapping.model.AbstractPersistentProperty"
FILE,DATACMNS,DATACMNS-616,2014-12-17T02:25:54.000-06:00,AnnotationRevisionMetadata can't access private fields,"@Entity




@RevisionEntity(ExtendedRevisionListener.class)




@Table(name = ""revinfo"")




public class ExtendedRevision implements Serializable  
 @Id




	@GeneratedValue




	@Column(name = ""REV"")




	@RevisionNumber




	private Integer id;









	 @RevisionTimestamp




	@Temporal(TemporalType.TIMESTAMP)




	@Column(name = ""REVTSTMP"", nullable = false)




	private Date date;









	 @Column(nullable = false, length = 15)




	private String username;









	 public Integer getId() {




		return id;




	}









	 public Date getDate() {




		return date;




	}









	 public String getUsername() {




		return username;




	}









	 public void setUsername(String username) {




		this.username = username;




	}
implement Serializable {
triggers this error:
not access member of class ExtendedRevision
make from field callback",org.springframework.data.util.AnnotationDetectionFieldCallback
FILE,DATACMNS,DATACMNS-683,2015-04-13T05:31:25.000-05:00,,"package be.vdab.web;









import org.springframework.context.annotation.ComponentScan;




import org.springframework.context.annotation.Configuration;




import org.springframework.data.web.config.EnableSpringDataWebSupport;




import org.springframework.web.servlet.config.annotation.EnableWebMvc;




import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;









// enkele imports




@Configuration




@EnableWebMvc




@EnableSpringDataWebSupport




@ComponentScan




public class CreateControllerBeans extends WebMvcConfigurerAdapter {




}






  






package be.vdab.web;









import org.springframework.stereotype.Controller;




import org.springframework.web.bind.annotation.ModelAttribute;




import org.springframework.web.bind.annotation.RequestMapping;




import org.springframework.web.bind.annotation.RequestMethod;




import org.springframework.web.servlet.ModelAndView;









import be.vdab.entities.Person;









@Controller




@RequestMapping(value = ""/"")




public class PersonController {




	private static final String TOEVOEGEN_VIEW = ""/WEB-INF/JSP/index.jsp"";














	@RequestMapping(method=RequestMethod.GET)




	ModelAndView get() {




		return new ModelAndView(TOEVOEGEN_VIEW).addObject(new Person());




	}




	




	@RequestMapping(method = RequestMethod.POST)




	String post(@ModelAttribute Person person) {




	  if (person == null) {




		  throw new IllegalArgumentException(""person IS NULL"");




	  }




	  return ""redirect:/"";




	}



















}






 
    
 
 
 
    
 @EnableSpringDataWebSupport   
 @ModelAttribute
package be.vdab.web;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.web.config.EnableSpringDataWebSupport;
import org.springframework.web.servlet.config.annotation.EnableWebMvc;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;
enkele imports
extend WebMvcConfigurerAdapter {
package be.vdab.web;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.ModelAttribute;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.servlet.ModelAndView;
import be.vdab.entities.Person;
return new ModelAndView(TOEVOEGEN_VIEW)
throw new illegalargumentexception
return redirect
the method post in PersonController throws the InvalidArgumentException because the person parameter is null.
put @EnableSpringDataWebSupport in comment
put @ModelAttribute in comment
clone project show bug from https://github.com/desmethans/springDataJpaError.git show project from https://github.com/desmethans/springDataJpaError.git","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-695,2015-05-13T09:08:15.000-05:00,Potential NullPointerException in AbstractMappingContext,"public class External{




 ..




 private Optional<Internal> field = new Optional<Internal>();




 ..




}
find reported issue upgrade Spring data MongoDB library from 1.3.5
The call to mongoOperations throws a NullPointerException originating from AbstractMappingContext.
start from version 1.7.2","org.springframework.data.mapping.context.AbstractMappingContext
org.springframework.data.mapping.context.AbstractMappingContextUnitTests"
FILE,DATACMNS,DATACMNS-748,2015-08-08T00:23:11.000-05:00,,"org.springframework.core.SpringVersion.getVersion()  
 
 
  private static final Version SPRING_VERSION = Version.parse(SpringVersion.getVersion());   Version.parse()   Assert.hasText()  
 SpringVersion.getVersion()
With the latest Gosling RC I'm intermittently seeing the following Exceptions on deployment.
create entrydao
have text cause by java.lang.IllegalArgumentException
commit in question
state in comment section
expose Spring version
attribute from jar file
not expose package metadata
determine Spring version
use reflection-based check
The code in QueryExecutionConverters private static final Version SPRING_VERSION = Version.parse(SpringVersion.getVersion()); fails to handle null, because Version.parse() has an Assert.hasText() and can't handle null.
handle null
say SpringVersion.getVersion() to work
have timing issues in case",org.springframework.data.repository.util.QueryExecutionConverters
FILE,DATACMNS,DATACMNS-943,2016-10-04T21:54:22.000-05:00,Redeclared save(Iterable) results in wrong method overload to be invoked eventually,"myRepository.save(Arrays.asList(new MyEntity(1, ""foo""), new MyEntity(2, ""bar"")));
upgrade project from spring boot 1.3.1
seem after update behave with java 1.8.0_45 64bit
But in jenkins server (on linux) with the same java version the code breaks with this exception
not find field for property not find field during fallback access
In linux and with the upgrade to spring boot 1.4.1 it seems the save(Iterable) method is not invoked, but the save(entity) method is invoked and causes this exception.","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
CLASS,derby-10.9.1.0,DERBY-3024,2007-08-23T05:24:31.000-05:00,Validation of shared plans hurts scalability,"GenericPreparedStatement.upToDate()   BaseActivation.checkStatementValidity()
To investigate whether there was anything in the SQL execution layer that prevented scaling on a multi-CPU machine, I wrote a multi-threaded test which continuously executed ""VALUES 1"" using a PreparedStatement. I ran the test on a machine with 8 CPUs and expected the throughput to be proportional to the number of concurrent clients up to 8 clients (the same as the number of CPUs). However, the throughput only had a small increase from 1 to 2 clients, and adding more clients did not increase the throughput. Looking at the test in a profiler, it seems like the threads are spending a lot of time waiting to enter synchronization blocks in GenericPreparedStatement.upToDate() and BaseActivation.checkStatementValidity() (both of which are synchronized on the a GenericPreparedStatement object).
do same work get own plan
When I made that change, the test scaled more or less perfectly up to 8 concurrent threads.","java.engine.org.apache.derby.impl.store.access.heap.HeapConglomerateFactory
java.engine.org.apache.derby.impl.store.raw.data.FileContainer
java.engine.org.apache.derby.impl.store.raw.data.RAFContainer
java.testing.org.apache.derbyTesting.functionTests.tests.lang.DBInJarTest
java.engine.org.apache.derby.impl.store.raw.data.TempRAFContainer
java.engine.org.apache.derby.impl.store.raw.data.InputStreamContainer
java.engine.org.apache.derby.impl.store.access.btree.index.B2IFactory"
CLASS,derby-10.9.1.0,DERBY-4269,2009-06-12T06:40:38.000-05:00,Failover did not succeed in 2 min.: testReplication_Local_3_p6_autocommit_OK,"testReplication_Local_3_p6_autocommit_OK(org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_3_p6)
Failover did not succeed.
see http://dbtg.thresher.com/derby/test/Daily/jvm1.4/testing/testlog/vista-64/782274-suitesAll_diff.txt","java.engine.org.apache.derby.iapi.util.DoubleProperties
java.testing.org.apache.derbyTesting.functionTests.tests.replicationTests.ReplicationRun_Local_3_p6"
CLASS,derby-10.9.1.0,DERBY-4647,2010-05-07T13:34:26.000-05:00,BaseTestCase.execJavaCmd() does not work with weme 6.2,"BaseTestCase.execJavaCmd()
Spawning a java process with BaseTestCase.execJavaCmd() does not work with weme 6.2, I think because the boot classpath does not get passed.
enable BootLockTest for weme
The error is actually
.
create Java VM JVMEXEX014I run c help for usage
execJavaProcess does pick up the j9 executable but does not pass on the other settings.
have lot of legacy system properties needed pass system properties get bootclasspath
use method in replication network server","java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockMinion
java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockTest"
CLASS,derby-10.9.1.0,DERBY-4669,2010-05-20T05:08:54.000-05:00,ClassLoaderBootTest fails if derbyclient.jar comes before derby.jar on the classpath,"testBootingAnAlreadyBootedDatabase(org.apache.derbyTesting.functionTests.tests.store.ClassLoaderBootTest)  testBootingDatabaseShutdownByAnotherCLR(org.apache.derbyTesting.functionTests.tests.store.ClassLoaderBootTest)
If derbyclient.jar comes before derby.jar on the classpath, and the build is sane, the test fails.
seal violation cause by java.lang.SecurityException
seal violation cause by java.lang.SecurityException","java.client.org.apache.derby.client.net.EncodedInputStream
java.engine.org.apache.derby.iapi.services.info.JVMInfo
java.client.org.apache.derby.client.am.ClobLocatorReader
java.client.org.apache.derby.client.am.ClobLocatorInputStream
java.client.org.apache.derby.client.am.PreparedStatement
java.client.org.apache.derby.client.net.Utf8CcsidManager"
CLASS,derby-10.9.1.0,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:
be after latest check
attach derby.log.",java.engine.org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.9.1.0,DERBY-5251,2011-05-29T04:27:15.000-05:00,,"test_errorcode(org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
)
lang.ErrorCodeTest will fail in Chinese Locale:
delegatingmethodacce at sun.reflect.DelegatingMethodAccessorImpl.invoke",java.testing.org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
CLASS,derby-10.9.1.0,DERBY-5407,2011-09-12T08:50:38.000-05:00,"When run across the network, dblook produces unusable DDL for VARCHAR FOR BIT DATA columns.","varchar( 20 )  
 
 
 VARCHAR ()
In private correspondence, Mani Afschar Yazdi reports that dblook omits the length specification for VARCHAR FOR BIT DATA columns when run across the network.
Embedded dblook runs fine.
create table t( a varchar( 20 ) for bit data )
This produces the following DDL for the table:
A similar experiment using an embedded database produces usable DDL which includes a length specification for the VARCHAR FOR BIT DATA column.","java.testing.org.apache.derbyTesting.functionTests.tests.lang.SystemCatalogTest
java.engine.org.apache.derby.catalog.types.BaseTypeIdImpl"
CLASS,derby-10.9.1.0,DERBY-5424,2011-09-20T14:24:29.000-05:00,,"testConnectWrongSubprotocolWithSystemProperty(org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest) 
    
  
  testConnectWrongSubprotoctestolWithoutSystemProperty(org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest)   
  
 
 String ijResult = runIjScript(ijScript, useSystemProperties);       
                assertTrue(ijResult.indexOf(""08001"") > -1);
With the release candidate  10.8.2.1 - (1170221) I saw the following two failures on z/OS in testConnectWrongSubprotoctestolWithoutSystemProperty
There were 2 failures:
relate to test",java.testing.org.apache.derbyTesting.functionTests.tests.tools.ConnectWrongSubprotocolTest
CLASS,derby-10.9.1.0,DERBY-5531,2011-12-12T06:34:21.000-06:00,Assert failure when inserting NULL into indexed column with territory-based collation,"colldb;territory=en;collation=TERRITORY_BASED; 
 varchar(10)
create table t(x varchar(10) )
ij > insert into t values null","java.engine.org.apache.derby.impl.sql.compile.ResultColumnList
java.engine.org.apache.derby.impl.store.access.btree.OpenBTree
java.testing.org.apache.derbyTesting.functionTests.tests.lang.CollationTest"
CLASS,derby-10.9.1.0,DERBY-5567,2012-01-05T07:35:04.000-06:00,AlterTableTest#testDropColumn fails: drop view cannot be performed due to dependency,"testDropColumn(org.apache.derbyTesting.functionTests.tests.lang.AlterTableTest)
Saw this when running suitesAll on 10.8.2.2:
not perform DROP VIEW because VIEW not perform DROP VIEW on atdc_vw_5a_1
not perform caused by org.apache.derby.client.am.SqlException not perform caused because VIEW not perform caused on atdc_vw_5a_1
Prior to this, though, I saw this on the console, but no error/failure.",java.engine.org.apache.derby.iapi.sql.dictionary.ViewDescriptor
CLASS,derby-10.9.1.0,DERBY-5663,2012-03-17T23:45:11.000-05:00,Getting NPE when trying to set derby.language.logStatementText property to true inside a junit suite.,"patch(DERBY5663_patch1.txt)
have large data suite run LobLimitsTest with embedded network server configurations run LobLimitsTest with small data size run large data suite with embedded network server configurations run large data suite with small data size
run large data suite as follows time java -Dderby.tests.trace=true -Dderby.infolog.append=true junit.textui.TestRunner org.apache.derbyTesting.functionTests.tests.largedata.
make simple change to suite log statement text show in attached patch ( derby5663_patch1
This causes the large data suite to run into NPE (NPE can be seen in runall.out) as shown below.
Not sure what I am doing wrong while trying to set the property, which results in NPE.
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms
use ms EF
not register org.apache.derby.jdbc.EmbeddedDriver with JDBC driver manager
not register caused with JDBC driver manager not register caused by org.apache.derby.client.am.SqlException
expect <XJ015>","java.testing.org.apache.derbyTesting.functionTests.tests.largedata.LobLimitsTest
java.testing.org.apache.derbyTesting.junit.SystemPropertyTestSetup"
CLASS,derby-10.9.1.0,DERBY-5740,2012-05-03T07:31:43.000-05:00,,"CollectNodesVisitor visitor = new CollectNodesVisitor(ColumnReference.class);
			stmtnode.accept(visitor);
			Vector refs = visitor.getList();
The following code is executed, but the results are not used:
			CollectNodesVisitor visitor = new CollectNodesVisitor(ColumnReference.class);
			stmtnode.accept(visitor);
			Vector refs = visitor.getList();  <--- never used
remove code record in case use visitor",java.engine.org.apache.derby.impl.sql.execute.AlterTableConstantAction
CLASS,derby-10.9.1.0,DERBY-5816,2012-06-13T15:12:35.000-05:00,store.ServicePropertiesFileTest fails on z/OS,"testSevicePropertiesFileWithBackup(org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest) 
 
   
  
  
  
  testSevicePropertiesFileCorruptedWithBackup(org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTe
st)   {```
@    @    k@   }
store.ServicePropertiesFileTest fails on z/OS with two failures below.
encode issue
testsevicepropertiesfilecorruptedwithbackup ( org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTe
not put after line","java.engine.org.apache.derby.impl.services.monitor.StorageFactoryService
java.testing.org.apache.derbyTesting.functionTests.tests.store.ServicePropertiesFileTest"
CLASS,derby-10.9.1.0,DERBY-5912,2012-08-28T19:43:59.000-05:00,testIsValidImplemented fails for NetworkServer in some slow running machines/configurations,"isValid()  
 testIsValidImplemented(org.apache.derbyTesting.functionTests.tests.jdbc4.ConnectionTest) 
 
  assertTrue(getConnection().isValid(1));
The following test has been seen to fail as below  in some runs where the machine is under heavy load  and slow running options are specified and the isValid() call takes more than a second to return.
asserttrue ( getConnection()
return in second
not implement int parameter for embedded
do timeout for Network implementation perform query return false","java.testing.org.apache.derbyTesting.functionTests.tests.jdbc4.ConnectionMethodsTest
java.drda.org.apache.derby.impl.drda.DRDAConnThread
java.testing.org.apache.derbyTesting.functionTests.tests.jdbc4.ConnectionTest"
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,Client should use a prepared statement rather than regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
build up time for setTransactionIsolation() build up Statement for setTransactionIsolation()
import java.sql.
start own NetworkServer run SQL
reproduce issues
throw Exception {
try {
start Network server
connect to customer database
clean up from previous run
try {
start Network server
throw SQLException {
try {
start retries
try {
sleep second ping network server
not throw exception
start = true
get reply on ping
throw failexception
throw Exception {
throw Exception {",java.client.org.apache.derby.client.am.Connection
CLASS,derby-10.9.1.0,DERBY-6079,2013-02-19T18:51:03.000-06:00,100's of errors in nightly test run on weme after jacoco property/priviledges checkin,"testAll(org.apache.derbyTesting.functionTests.tests.lang.NativeAuthenticationServiceTest)
100's of errors in nightly test, all seem to be a null pointer while processing policy files during setup.
be in jvm code
The only change being tested in this run was
For instance:","java.testing.org.apache.derbyTesting.junit.SecurityManagerSetup
java.testing.org.apache.derbyTesting.junit.BaseTestCase"
CLASS,derby-10.9.1.0,DERBY-6108,2013-03-13T12:14:01.000-05:00,suites.All no longer runs with weme 6.2,"suite()
Since revision 1454647 the suites.All no longer kicks off with weme 6.2.
The stack trace (which does not get copied to my apache location because the test doesn't finish) looks like this:
invoke class org.apache.derbyTesting.functionTests.tests.derbynet._Suite
invoke suite():java.lang.reflect.InvocationTargetException
rhillega | 2013-03-08 | line
rhillega | 2013-03-08 | line
add optional tool for turning trace trace to console dump trace to console turn on optimizer
add generated source folder to NetBeans project
replace Hashtable fields with HashSets
not run derbynet suite with weme get datasource",java.testing.org.apache.derbyTesting.junit.JDBCDataSource
CLASS,derby-10.9.1.0,DERBY-6138,2013-04-02T13:14:31.000-05:00,org.apache.derbyTesting.functionTests.tests.store.ClassLoaderBootTest fails with  sealing violation: package org.apache.derby.iapi.services.sanity is sealed depending on classpath order,"testBootingDatabaseShutdownByAnotherCLR(org.apache.derbyTesting.functionTests.tests.store.ClassLoaderBootTest)
Depending on classpath order, I believe if derbyclient.jar is before derby.jar in the classpath ClassLoaderBootTest fails with a sealing violation.,
There were 2 errors:
seal violation cause by java.lang.SecurityException
seal violation cause by java.lang.SecurityException
resolve issue",java.client.org.apache.derby.client.am.LogWriter
METHOD,time,18,2013-04-19T08:28:47.000-05:00,NPE in DateTimeZoneBuilder,"PrecalculatedZone.create()  ZoneInfoCompiler.verbose() 
    
 static {
 cVerbose.set(Boolean.FALSE);
 }
 
 public static boolean verbose() {
 return cVerbose.get();
 }
 
 public static boolean verbose(){
 Boolean verbose = cVerbose.get();
 return (verbose != null) ? verbose : false;
}
 
 @Test
 public void testDateTimeZoneBuilder() throws Exception {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ1"", true);
 Thread t = new Thread(new Runnable() {
 @Override
 public void run() {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ2"", true);
 }
 });
 t.start();
 t.join();
 }

  private DateTimeZoneBuilder getTestDataTimeZoneBuilder() {
 return new DateTimeZoneBuilder()
 .addCutover(1601, 'w', 1, 1, 1, false, 7200000)
 .setStandardOffset(3600000)
 .addRecurringSavings("""", 3600000, 1601, Integer.MAX_VALUE, 'w', 3, -1, 1, false, 7200000)
 .addRecurringSavings("""", 0, 1601, Integer.MAX_VALUE, 'w', 10, -1, 1, false, 10800000);
 }
When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create().
When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().
initialize cVerbose ThreadLocal in zoneinfocompiler
initialize cVerbose
cause NPE
remove initialization for null remove test for null
throw Exception { getTestDataTimeZoneBuilder()
return new DateTimeZoneBuilder()",org.joda.time.tz.ZoneInfoCompiler:<clinit0>
METHOD,time,21,2013-05-03T21:03:46.000-05:00,DateTimeFormat.parseInto sometimes miscalculates year (2.2),"public void testParseInto_monthDay_feb29_startOfYear() {
 DateTimeFormatter f = DateTimeFormat.forPattern(""M d"").withLocale(Locale.UK);
 MutableDateTime result = new MutableDateTime(2000, 1, 1, 0, 0, 0, 0, NEWYORK);
 assertEquals(4, f.parseInto(result, ""2 29"", 0));
 assertEquals(new MutableDateTime(2000, 2, 29, 0, 0, 0, 0, NEWYORK), result);
 }
The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999:
assertequal )","org.joda.time.format.DateTimeFormatter:parseInto(ReadWritableInstant, String, int)"
METHOD,time,22,2013-05-07T14:19:36.000-05:00,Days#daysBetween throw exception for MonthDay with 29 February,"final LocalDate january12012 = new LocalDate(2012, 1,1);
final LocalDate february292012 = new LocalDate(2012, 2, 29);
// OK
assertEquals(59, Days.daysBetween(january12012, february292012).getDays());

final MonthDay january1 = new MonthDay(1,1);
final MonthDay february29 = new MonthDay(2, 29);
// FAIL
assertEquals(59, Days.daysBetween(january1, february29).getDays());
be in range
avoid happening
get issues fiddle around with leap year","org.joda.time.base.BaseSingleFieldPeriod:between(ReadablePartial, ReadablePartial, ReadablePeriod)"
METHOD,time,77,2013-10-16T15:36:22.000-05:00,addDays(0) changes value of MutableDateTime,"final MutableDateTime mdt = new MutableDateTime(2011, 10, 30, 3, 0, 0, 0, DateTimeZone.forID(""Europe/Berlin""));
System.out.println(""Start date: "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(-1);
System.out.println(""addHours(-1): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(0);
System.out.println(""addHours(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addDays(0);
System.out.println(""addDays(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
 
 
 addHours(-1)  
 addHours(0)  
 addDays(0)  
 
          
 addDays(0)
Upon DST transition from summer to winter time zone, adding the amount of zero days to a mutable date time object changes the value of the object.
date + mdt + ( + mdt.toInstant()
prints
change by hour change start date
The methods addMonths and addYears show the same problem; addSeconds, addMinutes and addHours are ok.
test with version
repeat test with Joda 1.5.2 not change value","org.joda.time.MutableDateTime:add(DurationFieldType, int)
org.joda.time.MutableDateTime:addWeeks(int)
org.joda.time.MutableDateTime:addYears(int)
org.joda.time.MutableDateTime:addMonths(int)
org.joda.time.MutableDateTime:addMinutes(int)
org.joda.time.MutableDateTime:addHours(int)
org.joda.time.MutableDateTime:addWeekyears(int)
org.joda.time.MutableDateTime:addDays(int)
org.joda.time.MutableDateTime:addSeconds(int)
org.joda.time.MutableDateTime:addMillis(int)"
METHOD,time,79,2013-10-17T20:36:31.000-05:00,none standard PeriodType without year throws exception,"Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()})).normalizedStandard(PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()}));
return p.getMonths();
 
    
      
   
 withYearsRemoved() throws the  
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard()).normalizedStandard(PeriodType.standard());
return p.getMonths();
 
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard().withYearsRemoved()).normalizedStandard(PeriodType.standard().withYearsRemoved());
return p.getMonths();
This throws following exception:
Even removing the year component with .
withYearsRemoved() throws the same exception:
normalizedstandard ( PeriodType.standard()",org.joda.time.Period:normalizedStandard(PeriodType)
METHOD,time,88,2013-11-25T19:15:46.000-06:00,,"Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).with(hourOfDay(), 1);
assert(a == b);
 
 new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
 
 new Partial(clockhourOfDay(), 1)  with(hourOfDay(), 1)  isEqual(new Partial(hourOfDay() ,1).with(clockhourOfDay(), 1)) // throws objects must have matching field types
invoke constructor by merging merge together set of partials call Partial(DateTimeFieldType, int) construct partials construct by calling
``` java
new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).
java new Partial(clockhourOfDay(), 1)
with(clockhourOfDay(), 1)) // throws objects must have matching field types
```","org.joda.time.Partial:with(DateTimeFieldType, int)"
METHOD,time,93,2013-12-01T09:33:58.000-06:00,Partial.with fails with NPE,"new Partial(yearOfCentury(), 1)  with(weekyear(), 1);
// NullPointerException
// org.joda.time.Partial.with (Partial.java:447)
java new Partial(yearOfCentury(), 1)
Fails with yearOfCentury, year and yearOfEra.
have null range duration type","org.joda.time.Partial:Partial(DateTimeFieldType[], int[], Chronology)
org.joda.time.field.UnsupportedDurationField:compareTo(DurationField)"
FILE,COMPRESS,COMPRESS-131,2011-06-03T16:25:45.000-05:00,ArrayOutOfBounds while decompressing bz2,"recvDecodingTables()
Decompressing a bz2 file generated by bzip2 utility throws an ArrayIndexOutOfBounds at method recvDecodingTables() line 469.
generate original text file relate to encodings",org.apache.commons.compress.compressors.BZip2TestCase
FILE,COMPRESS,COMPRESS-189,2012-06-26T21:30:39.000-05:00,ZipArchiveInputStream may read 0 bytes when reading from a nested Zip file,"ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
      ZipArchiveEntry entry = iterator.nextElement();
      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
      ZipArchiveEntry innerEntry;
      while ((innerEntry = zipInput.getNextZipEntry()) != null){
        if (innerEntry.getName().endsWith(""XML""))
{

          //zipInput.read();

          System.out.println(IOUtils.toString(zipInput));

        }
      }
    }
When the following code is run an error ""Underlying input stream returned zero bytes"" is produced.
If the commented line is uncommented it can be seen that the ZipArchiveInputStream returned 0 bytes.
work with line
process zip file of zip files
Also I believe whilst ZipFile can iterate over entries fast due to being able to look at the master table whilst ZipArchiveInputStream cannot.
instantiate ZipFile from zip file instantiate ZipFile without extracting extract nested zip file","org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream"
FILE,COMPRESS,COMPRESS-245,2013-12-05T11:01:39.000-06:00,TarArchiveInputStream#getNextTarEntry returns null prematurely,"FileInputStream fin = new FileInputStream(""exampletar.tar.gz"");

GZIPInputStream gin = new GZIPInputStream(fin);

TarArchiveInputStream tin = new TarArchiveInputStream(gin);            TarArchiveEntry entry;

              tin.getNextTarEntry()
The attached archive decompressed with 1.6 only extracts part of the archive.
not happen with version
with commons-compress-1.6 it looks like this:","org.apache.commons.compress.archivers.tar.TarArchiveInputStream
org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest"
FILE,COMPRESS,COMPRESS-276,2014-04-11T05:03:17.000-05:00,NullPointerException in ZipArchiveOutputStream with invalid entries,"java.io.ByteArrayOutputStream var0 = new java.io.ByteArrayOutputStream();
    org.apache.commons.compress.archivers.jar.JarArchiveOutputStream var1 = new org.apache.commons.compress.archivers.jar.JarArchiveOutputStream((java.io.OutputStream)var0);
    var1.write(25843);
Writing raw data seems to cause problems in multiple ways, because an internal field is not set.
cause same problem","org.apache.commons.compress.archivers.sevenz.SevenZFile
org.apache.commons.compress.archivers.tar.TarArchiveOutputStream
org.apache.commons.compress.archivers.sevenz.SevenZOutputFile
org.apache.commons.compress.archivers.tar.TarArchiveInputStream
org.apache.commons.compress.archivers.arj.ArjArchiveInputStream
org.apache.commons.compress.archivers.dump.DumpArchiveInputStream
org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream"
FILE,COMPRESS,COMPRESS-273,2014-04-11T04:13:32.000-05:00,NullPointerException when creation fields/entries from scratch,"org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
    org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();
have public default constructors for many data types
However, when these 0-argument constructors are used, certain internal references are null, resulting in a NullPointerException soon after.
apply to 1-argument constructors","org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField
org.apache.commons.compress.archivers.cpio.CpioArchiveEntry
org.apache.commons.compress.archivers.zip.ExtraFieldUtils
org.apache.commons.compress.archivers.zip.UnrecognizedExtraField"
FILE,COMPRESS,COMPRESS-278,2014-04-18T16:09:05.000-05:00,,"package TestBed;



import java.io.File;

import java.io.FileInputStream;

import java.io.FileNotFoundException;

import java.io.IOException;



import org.apache.commons.compress.archivers.tar.TarArchiveEntry;

import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;

import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;

import org.junit.Test;



/**

 * Unit test for simple App.

 */

public class AppTest

{



    @Test

    public void extractNoFileOwner()

    {

        TarArchiveInputStream tarInputStream = null;



        try

        {

            tarInputStream =

                new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(

                    ""/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz"" ) ) ) );

            TarArchiveEntry entry;

            while ( ( entry = tarInputStream.getNextTarEntry() ) != null )

            {

                System.out.println( entry.getName() );

                System.out.println(entry.getUserName()+""/""+entry.getGroupName());

            }



        }

        catch ( FileNotFoundException e )

        {

            e.printStackTrace();

        }

        catch ( IOException e )

        {

            e.printStackTrace();

        }

    }



}
With version 1.8 of commons-compress it's no longer possible to decompress  files from an archive if the archive contains entries having null (or being empty?)
set as username and/or usergroup.
With version 1.7 this still worked now I get this exception:
parse header
have identical exception have ticket COMPRESS-262 lead to suspision introduce regression with fix
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
import org.junit.Test;
test for simple app
With 1.7 the TestCase outputed this:
With 1.8 it's failing once it reaches the null valued entry, which is the first.
use maven assembly plugin try with maven ant task
generate archive with set username generate archive with groups
download archive from http
run tar see report see file
show up as root/root","org.apache.commons.compress.archivers.tar.TarUtilsTest
org.apache.commons.compress.archivers.tar.TarUtils"
FILE,COMPRESS,COMPRESS-309,2015-02-18T17:22:16.000-06:00,,"BZip2CompressorInputStream.read(buffer, offset, length)  
 @Test

	public void testApacheCommonsBZipUncompression () throws Exception {

		// Create a big random piece of data

		byte[] rawData = new byte[1048576];

		for (int i=0; i<rawData.length; ++i) {

			rawData[i] = (byte) Math.floor(Math.random()*256);

		}



		// Compress it

		ByteArrayOutputStream baos = new ByteArrayOutputStream();

		BZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);

		bzipOut.write(rawData);

		bzipOut.flush();

		bzipOut.close();

		baos.flush();

		baos.close();



		// Try to read it back in

		ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());

		BZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);

		byte[] buffer = new byte[1024];

		// Works fine

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		// Fails, returns -1 (indicating the stream is complete rather than that the buffer 

		// was full)

		Assert.assertEquals(0, bzipIn.read(buffer, 1024, 0));

		// But if you change the above expected value to -1, the following line still works

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		bzipIn.close();

	}
BZip2CompressorInputStream.read(buffer, offset, length) returns -1 when given an offset equal to the length of the buffer.
This indicates, not that the buffer was full, but that the stream was finished.
throw Exception {
create big random piece of data
// Fails, returns -1 (indicating the stream is complete rather than that the buffer
// was full)
change above expected value",org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
FILE,COMPRESS,COMPRESS-357,2016-05-25T17:50:50.000-05:00,,"finished()  
  
 s.close();
s = null;
  finalize()  finish()  finish()  
 finish()  
 finalize() 
 finalize()
BZip2CompressorOutputStream has an unsynchronized finished() method, and an unsynchronized finalize method.
Finish checks to see if the output stream is null, and if it is not it calls various methods, some of which write to the output stream.
consider something like sequence
At some point the garbage collector call finalize(), which calls finish().
be on different thread result GC thread in bad data
happen % in part happen % of time
derive class override finalize() method",org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream
CLASS,bookkeeper-4.1.0,BOOKKEEPER-294,2012-06-12T23:56:56.000-05:00,Not able to start the bookkeeper before the ZK session timeout.,"{noformat}
         
 {noformat}
Not able to start the bookkeeper before the ZK session timeout.
kill bookie start bookie
register ephemeral Znode for bookie","bookkeeper-server.src.test.java.org.apache.bookkeeper.bookie.BookieJournalTest
bookkeeper-server.src.main.java.org.apache.bookkeeper.bookie.Bookie
bookkeeper-server.src.main.java.org.apache.bookkeeper.proto.BookieServer"
CLASS,bookkeeper-4.1.0,BOOKKEEPER-371,2012-08-17T05:42:02.000-05:00,NPE in hedwig hub client causes hedwig hub to shut down.,"Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .messageConsumed(messageConsumeData.msg);

  getPipeline()  getLast()   channel.close()   messageConsumed()
The hedwig client was connected to a remote region hub that restarted resulting in the channel getting disconnected.
disconnect channel
The channel was retrieved without checking if it was closed and then getPipeline().
getLast() was called which returned a null value resulting in a NPE.
guess same applies for other instances","hedwig-client.src.main.java.org.apache.hedwig.client.netty.WriteCallback
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.SubscribeResponseHandler
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigPublisher
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigSubscriber
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.MessageConsumeCallback
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigClientImpl"
CLASS,bookkeeper-4.1.0,BOOKKEEPER-376,2012-08-22T13:32:57.000-05:00,,"{noformat}
     
 {noformat}
Saw this while running the RW tests:
exception for underreplication exception during garbage collecting ledgers","bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.LedgerLayout
bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.AbstractZkLedgerManager"
FILE,swt-3.1,102794,2005-07-05T17:56:00.000-05:00,,"public static void main(String[] args) {
        Display display = new Display();

        Shell shell = new Shell(display);

        shell.setLayout(new FillLayout());

        ScrolledComposite sc1 = new ScrolledComposite(shell, SWT.H_SCROLL
                | SWT.V_SCROLL);
        Composite editor = new Composite(sc1, SWT.SHADOW_NONE);
        sc1.setContent(editor);
        sc1.setLayout(new FillLayout());

        GridLayout layout = new GridLayout();

        layout.numColumns = 6;
        layout.makeColumnsEqualWidth = true;
        editor.setLayout(layout);

        Label boxLabel = new Label(editor, SWT.NONE);
        boxLabel.setText(""My label"");

        Text textBox = new Text(editor, SWT.H_SCROLL | SWT.V_SCROLL | SWT.MULTI
                | SWT.BORDER);
        textBox.setText(""Some text for the text box\nAlso with a new line"");

        // do layout bits
        GridData labelData = new GridData(SWT.RIGHT, SWT.TOP, false, false);
        boxLabel.setLayoutData(labelData);

        GridData textBoxData = new GridData(SWT.FILL, SWT.CENTER, true, false,
                5, 1);
        textBoxData.widthHint = 400;
        textBox.setLayoutData(textBoxData);

        sc1.setExpandHorizontal(true);
        sc1.setExpandVertical(true);
        sc1.setMinSize(editor.computeSize(SWT.DEFAULT, SWT.DEFAULT));

        shell.pack();
        shell.open();

        while (!shell.isDisposed()) {
            if (!display.readAndDispatch())
                display.sleep();
        }
        display.dispose();
    }
run following problem on 3.0.2 run following problem on shows
void main(String[] args) {
do layout bits
Basically on 3.0.2 the window that appears has a label of about 80 pixels wide and a textbox of 400 pixels wide.
With 3.1 the label is about 400 pixels wide, with the text box being about 2000 pixels wide.
span use of layout.makeColumnsEqualWidth span columns of layout.makeColumnsEqualWidth
turn off makeColumnsEqualWidth
comment out minimumWidth line",org.eclipse.swt.layout.GridLayout
FILE,swt-3.1,104545,2005-07-20T14:21:00.000-05:00,,"static final int DEFAULT_WIDTH	= 64;
 static final int DEFAULT_HEIGHT	= 64;
run tests run tests
Background: When you write an RCP app and enable the cool bar, the cool bar will initially be empty, but 64x64 pixels in size.
On Windows, you cannot see the border of the empty coolbar so the user gets a big empty space at the top of their window and might be confused.
see bug cool bar items occur bug in RCP application start off RCP application with open perspective",org.eclipse.swt.widgets.CoolBar
FILE,swt-3.1,117574,2005-11-22T15:22:00.000-06:00,,"public static void main(String[] args) {
	Display display = new Display();
	Shell shell = new Shell(display, SWT.SHELL_TRIM | SWT.RIGHT_TO_LEFT | SWT.DOUBLE_BUFFERED);
	shell.addListener(SWT.Paint, new Listener() {
		public void handleEvent(Event event) {
			System.out.println(event.gc.getClipping());
			event.gc.drawString(""This is broken "" + event.gc.getClipping(), 10, 10);
		}
	});
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose ();
}
display = new Display() while (
Doesn't draw anything.","org.eclipse.swt.widgets.Composite
org.eclipse.swt.graphics.GC"
FILE,swt-3.1,78634,2004-11-15T12:29:00.000-06:00,,"public ImageData getTransparencyMask()
The implementation of getTransparencyMask appears to return a fully opaque mask when the image has no transparency.
return <code>imagedata</code> specify <code>imagedata</code>
transparency mask information for receiver transparency mask information for null
have transparency
return transparent mask on different note",org.eclipse.swt.graphics.ImageData
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,Table fails to setTopIndex after new items are added to the table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
keep track into table dynamically keep track of loads content keep track of scroll bar keep table viewer into table dynamically keep table viewer of loads content keep table viewer of scroll bar work on table viewer scroll to end
maintain position of table call setTopIndex at end
create small testcase simulate process
Table.setTopIndex fails to position to the correct table item if new items are added to the table after the shell is opened.
The first call to setTopIndex succeeds.
The table is correctly positioned at item 20.
After adding new table items to the table, calling setTopTable(40) has no effect.
Calling getTopIndex continues to return 20.
expect on windows","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,83699,2005-01-26T06:03:00.000-06:00,Font reset to default after screen saver,"StyledText.setFont(Font)  
   updateFont(Font, Font)  
   updateFont(Font, Font)  
 updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 Composite.updateFont(Font, Font)  
 updateFont(Font, Font)  
 Display.updateFont()  
  
  
 Display.readAndDispatch()  
 Workbench.runEventLoop(Window$IExceptionHandler, Display)  
 Workbench.runUI()  
 Workbench.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 PlatformUI.createAndRunWorkbench(Display, WorkbenchAdvisor)  
 IDEApplication.run(Object)  
 EclipseStarter.run(Object)
All editors and views using a StyledText widget have the font reset to default
after coming back from my screen saver.
build i20050125-0800 unusable
replace org.eclipse.swt.win32_3.1.0
return from screen saver",org.eclipse.swt.widgets.Control
FILE,swt-3.1,84609,2005-02-07T13:35:00.000-06:00,TableColumn has NPE while calling pack()  on last column,"lvtTable.getColumn(0).pack();
lvtTable.getColumn(1).pack();
lvtTable.getColumn(2).pack();

   
 parent.getColumns()
refresh table on new data lvtTable.getColumn(0)
On third call I get caught NPE (in debugger) in TableColumn (line 356), because parent.getColumns() (in TableColumn:354) returns array with 4 elements (always one more as existing in the table), and the last element is always null.","org.eclipse.swt.widgets.TableColumn
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
contain files not save files to JPEG
package com.ibm.test.image;
import org.eclipse.swt.
try { for { string filein = dir + files [ i ] +",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,87460,2005-03-08T21:22:00.000-06:00,StyledText: Caret location not updated when line style is used,"import org.eclipse.swt.*;
import org.eclipse.swt.custom.*;
import org.eclipse.swt.graphics.*;
import org.eclipse.swt.layout.*;
import org.eclipse.swt.widgets.*;

public class LineStyleCaretTest {
  public static void main(String[] args) {
    Display display = new Display();
    
    Shell shell = new Shell(display);
    shell.setLayout(new FillLayout());
    
    Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
      
    final StyledText text = new StyledText(shell, SWT.MULTI);
    text.setFont(font);
    text.setText(""Standard Widget Toolkit"");
    text.setCaretOffset(text.getText().length());
    
    text.addLineStyleListener(new LineStyleListener() {
      public void lineGetStyle(LineStyleEvent event) {
        StyleRange[] styles = new StyleRange[1];
        
        styles[0] = new StyleRange();
        styles[0].start  = 0;
        styles[0].length = text.getText().length();
        styles[0].fontStyle = SWT.BOLD;
        
        event.styles = styles;
      }
    });
    
    shell.setSize(300, 100);
    shell.open();
    
    while (!shell.isDisposed()) {
      if (!display.readAndDispatch()) {
        display.sleep();
      }
    }
    
    font.dispose();
    display.dispose();
  }
}
In the line style listener, a bold font style is set, changing the width of the rendered text.
However, this does not happen.
not look right for italic style
import org.eclipse.swt.
import org.eclipse.swt.custom.
import org.eclipse.swt.graphics.
import org.eclipse.swt.layout.
import org.eclipse.swt.widgets.",org.eclipse.swt.custom.StyledText
FILE,swt-3.1,87997,2005-03-14T19:21:00.000-06:00,TableEditor.dispose( ) causes NPE if linked Table is being disposed,"TableEdtior.dispose( )  
  
   

import org.eclipse.swt.custom.TableEditor;
import org.eclipse.swt.events.*;
import org.eclipse.swt.widgets.*;

public class Test
{
    public static void main( String[ ] args )
    {
        Shell shell = new Shell( );
        Table table = new Table( shell, 0 );
        new TableColumn( table, 0 );
        TableItem item = new TableItem( table, 0 );
        final TableEditor editor = new TableEditor( table );
        final Text text = new Text( table, 0 );
        editor.setEditor( text, item, 0 );
        item.addDisposeListener( new DisposeListener( ) {
            public void widgetDisposed( DisposeEvent e )
            {
                text.dispose( );
                editor.dispose( ); // Triggers a NPE
            }
        } );
        shell.dispose( );
    }
}
find in i20050308-0835
call methods on owning own Table remove listeners from columns
If the table is in the process of being
disposed, the columns will have already been disposed and this will result in a
NPE.
Further if the dispose listener is set on the parent of the Table, a
""Widget is disposed"" exception will be thrown instead of the NPE.
leave place to hook trigger disposal of TableEditor
import org.eclipse.swt.events.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,90258,2005-04-05T04:56:00.000-05:00,Table item not updated when item count == 1,"clearAll() 
 table.setItemCount(1);
table.clearAll();

 
 private void handleSetData(Event event) {

	TableItem item= (TableItem) event.item;
	int index= fProposalTable.indexOf(item);
	
	ICompletionProposal current= fFilteredProposals[index];
	
	item.setText(current.getDisplayString());
	item.setImage(current.getImage());
	item.setData(current);
}
use Table with SWT.VIRTUAL.
Everything works fine, except for the case
I set the item count to 1, in which case I do not receive an SWT.SetData notification.
use modified version of snippet151
have idea
One funny thing is that in the variable view, the debugger displays the updated contents of table.items[0] after calling clearAll(), but I have verified that the data is never ever set.
The display fails to update.
ffilteredproposal [ index ]",org.eclipse.swt.widgets.Table
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,Adding an integer and getting it as a long causes an exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
not map to Long object org.apache.commons.configuration.ConversionException not map to Long object not map target exception
The problem is that when an object in a property is not a Long, the only attempt of PropertyConverter.toLong() is that of treating it as a string.","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
FILE,CONFIGURATION,CONFIGURATION-241,2006-12-02T00:03:48.000-06:00,clearProperty() does not generate events,"clearProperty() 
 ConfigurationFactory configurationFactory = new ConfigurationFactory();
   
 configurationFactory.setConfigurationURL(configFileURL);
Configuration configuration = ConfigurationFactory.getConfiguration();
configuration.addConfigurationListener(new ConfigurationListener() {
    public void configurationChanged(ConfigurationEvent e) 
{
        System.out.println(e.getPropertyName() + "": "" + e.getPropertyValue());
    }
});
System.out.println(configuration.getProperty(""name.first"")); // prints ""Mike""
 configuration.claerProperty(""name.first"")  ; // no output whatsoever
System.out.println(configuration.getProperty(""name.first"")); // prints ""null""
Unfortunately the listener does not receive ""clear property"" events.
I've confirmed that it can properly receive other events (like ""set property""), and that calls to ""clearProperty()"" do actually clear the property, so I believe this may be a bug in commons-configuration.
set details to true have effect have true
get config file","org.apache.commons.configuration.TestCompositeConfiguration
org.apache.commons.configuration.CompositeConfiguration"
FILE,CONFIGURATION,CONFIGURATION-259,2007-03-28T08:47:56.000-05:00,,"URL configURL = getClass().getResource(configFile);
ConfigurationFactory factory = new ConfigurationFactory();
factory.setConfigurationURL(configURL);
myConfig = factory.getConfiguration();
 
 
 DefaultConfigurationBuilder builder = new DefaultConfigurationBuilder();
            builder.setURL(configURL);
            myConfig = builder.getConfiguration();
provide wrong results
In particular, after creating a particular subset from a loaded configuration, the subset is empty.
use DefaultConfigurationBuilder load same configurations
So when initializing the configuration as follows, I get the following error:
not map to existing object
exception in thread main
attach full source code attach xml files",org.apache.commons.configuration.ConfigurationFactory
FILE,CONFIGURATION,CONFIGURATION-283,2007-07-02T12:38:23.000-05:00,CombinedConfiguration doesn't take escaped characters into account.,"import org.apache.commons.configuration.CombinedConfiguration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import junit.framework.TestCase;
public class TestProp extends TestCase {
	public void testprop() throws ConfigurationException 
{
		// test.properties contains :
		//    without_escape=aa,bb
		//    with_escape=aa\,bb
		//    with_2escapes=aa\\,bb
		
		String prop_filename = ""c:\\tmp\\test.properties"";
		PropertiesConfiguration properties_config = new PropertiesConfiguration(prop_filename);
		CombinedConfiguration   combined_config   = new CombinedConfiguration();
		combined_config.addConfiguration(properties_config);
		
		System.out.println(""Properties config"");
		System.out.println(properties_config.getString(""without_escape""));
		System.out.println(properties_config.getString(""with_escape""));
		System.out.println(properties_config.getString(""with_2escapes""));

		System.out.println(""\nCombined config"");
		System.out.println(combined_config.getString(""without_escape""));
		System.out.println(combined_config.getString(""with_escape""));
		System.out.println(combined_config.getString(""with_2escapes""));
		
	}
}
I've tried to used CombinedConfiguration but it seems escaped characters are not taken into account :
import org.apache.commons.configuration.CombinedConfiguration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import junit.framework.TestCase;
throw ConfigurationException
aa \
getstr )
getstr )
getstr )
getstr )
getstr )
getstr )
Result :
aa aa","org.apache.commons.configuration.TestCombinedConfiguration
org.apache.commons.configuration.ConfigurationUtils
org.apache.commons.configuration.TestConfigurationUtils"
FILE,CONFIGURATION,CONFIGURATION-332,2008-07-04T15:54:10.000-05:00,PropertiesConfiguration.save() doesn't persist properties added through a DataConfiguration,"public void testSaveWithDataConfiguration() throws ConfigurationException
{
    File file = new File(""target/testsave.properties"");
    if (file.exists()) {
        assertTrue(file.delete());
    }

    PropertiesConfiguration config = new PropertiesConfiguration(file);

    DataConfiguration dataConfig = new DataConfiguration(config);

    dataConfig.setProperty(""foo"", ""bar"");
    assertEquals(""bar"", config.getProperty(""foo""));
    config.save();

    // reload the file
    PropertiesConfiguration config2 = new PropertiesConfiguration(file);
    assertFalse(""empty configuration"", config2.isEmpty());
}
wrap into DataConfiguration
The properties added through a DataConfiguration aren't persisted when the configuration is saved, but they can be queried normally.
not affect commons configuration
throw ConfigurationException
reload file","org.apache.commons.configuration.TestPropertiesConfiguration
org.apache.commons.configuration.DataConfiguration"
FILE,CONFIGURATION,CONFIGURATION-347,2008-11-05T21:06:22.000-06:00,Iterating over the keys of a file-based configuration can cause a ConcurrentModificationException,"getKeys()
Some implementations of FileConfiguration return an iterator in their getKeys() method that is directly connected to the underlying data store.
When now a reload is performed (which can happen at any time) the data store is modified, and the iterator becomes invalid.
relate ConcurrentModificationExceptions to multi-threading access
But even if the code performing the iteration is the only instance that accesses the configuration, the exception can be thrown.","org.apache.commons.configuration.TestFileConfiguration
org.apache.commons.configuration.AbstractFileConfiguration"
FILE,CONFIGURATION,CONFIGURATION-408,2010-02-11T01:01:05.000-06:00,"When I save a URL as a property value, the forward slashes are getting escaped","public static void main(String[] args)
  {
    try
    {

      PropertiesConfiguration config = new PropertiesConfiguration();     

      File newProps = new File(""foo.properties"");

      config.setProperty(""foo"", ""http://www.google.com/"");     

      config.save(newProps);

      

    }
    catch (Exception e){}
  }
When I save a URL as a property value, the forward slashes are getting escaped.
void main(String[] args)",org.apache.commons.configuration.TestPropertiesConfiguration
FILE,CONFIGURATION,CONFIGURATION-481,2012-02-26T20:27:46.000-06:00,,"{myvar}  
 
 
 
 combinedConfig.getConfiguration(""test"")  configurationAt(""products/product[@name='abc']"", true)  getString(""desc"")

  {myvar}
I get ""${myvar}-product"" instead of ""abc-product"".
work in Commons configuration","org.apache.commons.configuration.DefaultConfigurationBuilder
org.apache.commons.configuration.interpol.ConfigurationInterpolator
org.apache.commons.configuration.TestDefaultConfigurationBuilder"
FILE,CONFIGURATION,CONFIGURATION-627,2016-05-04T22:54:12.000-05:00,BeanHelper exception on XMLConfiguration builder.getConfiguration(),"builder =

        new FileBasedConfigurationBuilder<XMLConfiguration>(

                XMLConfiguration.class)

                        .configure(params.xml()

                                .setFileName(

                                        propsFile.getCanonicalPath())

                                .setValidating(false));



config = builder.getConfiguration();



   
 private static boolean isPropertyWriteable(Object bean, String propName)    
  
   org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)
configure ( params.xml()
Causes a non-halting exception originating in org.apache.commons.configuration2.beanutils.BeanHelper, method private static boolean isPropertyWriteable(Object bean, String propName) with parameters XMLConfiguration, ""validating"".
The exception:
create PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)
ignore property",org.apache.commons.configuration2.builder.TestPropertiesBuilderParametersImpl
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,,"@Lob
I have entity with byte[] property annotated as @Lob and lazy fetch type, when table is createad the created column is of type oid, but when the column is read in application, the Hibernate reads the OID value instead of bytes under given oid.
write bytea
If i remember well, auto-creating table with Hibernate creates oid column.","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
CLASS,hibernate-3.5.0b2,HHH-4647,2009-12-07T19:52:36.000-06:00,,"testQuotedReferencedColumnName() 
     getPhysicalColumnName()      getLogicalColumnName()
#1: A referencedColumnName that references a quoted target column cannot be found.
#2: An invalid referencedColumnName (including the unquoted version of a valid column) results in an infinite loop in o.h.c.Configuration$MappingsImpl#getPhysicalColumnName().
exist with getLogicalColumnName()","org.hibernate.test.annotations.backquotes.BackquoteTest
org.hibernate.cfg.Configuration.MappingsImpl"
CLASS,hibernate-3.5.0b2,HHH-5042,2010-03-26T05:06:09.000-05:00,TableGenerator does not increment hibernate_sequences.next_hi_value anymore after having exhausted the current lo-range,"class MultipleHiLoPerTableGenerator 
 IntegralDataTypeHolder value;
 
 int lo;

 
  
  
 IntegralDataTypeHolder hiVal = (IntegralDataTypeHolder) doWorkInNewTransaction( session );

   
  
 varchar(255) 
     varchar(255)
introduce new increment variable modify class MultipleHiLoPerTableGenerator.java in version
The problem in the new code is that only value get's incremented whilst variable lo is still used to check when a new hiVal must be obtained.
as lo is never incremented, MultipleHiLoPerTableGenerator continues to deliver numbers without ever update hibernate_sequences.
next_hi_value on the database (only one unique update is propagates at the first insert)
This lead to duplicate keys as soon another session from another sessionfactory tries to insert new objects on the concerning table.
create table
create table hibernate_sequences","org.hibernate.id.SequenceHiLoGenerator
org.hibernate.id.enhanced.OptimizerFactory
org.hibernate.id.SequenceGenerator
org.hibernate.id.MultipleHiLoPerTableGenerator"
METHOD,openjpa-2.0.1,OPENJPA-1613,2010-04-06T18:28:33.000-05:00,Exception thrown when enhancing a (property access) class that has an abstract @MappedSuperclass with no annotated properties,"@MappedSuperclass 
      
 @Access(AccessType.PROPERTY)
If you have a class (using property access) that has an abstract @MappedSuperclass that happens to have no annotated methods, you get the following exception when enhancing:
use single implicit field use property base access style declare access style
use field access
annotate superclass with not make assumption",org.apache.openjpa.persistence.PersistenceMetaDataDefaults:toFieldNames(List<Field>)
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,ORderBy with @ElementJoinColumn and EmbeddedId uses wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
have compound key
The problem is that the order by in the generated SQL is for columns mapped in the transaction entity NOT the TransacionId as expected.
define _ processDate in TransactionId class define _ tranSequenceNumber in TransactionId class
have following fragment ....
define primary key columns ....
However the generated SQL is doing order by on columns mapped in Transaction:
execute prepstmnt SELECT t0.maccno","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-1784,2010-09-08T08:31:29.000-05:00,Map value updates not flushed,"@Embeddable
public class LocalizedString {


    private String language;
    private String string;


    // getters and setters omitted
}


 


 @Entity
public class MultilingualString {


    @Id
    private long id;


    @ElementCollection(fetch=FetchType.EAGER)
    private Map<String, LocalizedString> map = new HashMap<String, LocalizedString>();
}



 
   ;
    em.getTransaction().begin();
    m.getMap().get(""en"").setString(""foo"");
     em.merge(m)
     em.getTransaction()  commit();
   
 
   ;
    em.getTransaction().begin();
     m.getMap()  put(""en"")  new LocalizedString(""en"", ""foo"") 
 em.merge(m)
     em.getTransaction()  commit();


 
 hashCode()   equals()   equal()
commit();
   
The problem is, the state change of the map does not get saved to the database.
With DEBUG logging on, I can see that the flush on commit does not trigger any SQL UPDATE.
force update put new value into map change existing one
After this change, I do see the expected UPDATE.
have hashCode() have equals()
look like bug","org.apache.openjpa.util.ProxyMaps:beforePut(ProxyMap, Object, Object)"
METHOD,openjpa-2.0.1,OPENJPA-1793,2010-09-14T01:16:17.000-05:00,@EmbeddedId class having only one field java.sql.Data,"@EmbeddedId  
 
  
  
 field.getHandler()  getResultArgument(field) 
 
  
  
 public Object getPrimaryKeyValue(Result res, Column[] cols, ForeignKey fk,
        JDBCStore store, Joins joins)
        throws SQLException {
        Column col;
        Object val = null;
        if (cols.length == 1) {
            col = cols[0];
            if (fk != null)
                col = fk.getColumn(col);
            val = res.getObject(col, field.getHandler().
                getResultArgument(field), joins);
        } else if (cols.length > 1) {
            Object[] vals = new Object[cols.length];
            Object[] args = (Object[]) field.getHandler().
                getResultArgument(field);
            for (int i = 0; i < vals.length; i++) {
                col = cols[i];
                if (fk != null)
                    col = fk.getColumn(col);
                vals[i] = res.getObject(col, (args == null) ? null : args[i],
                    joins);
            }
            val = vals;
        }
        return field.getHandler().toObjectValue(field, val);
    }


     
  
   DATE:
                return getDateInternal(obj, (Calendar) arg, joins);
@EmbeddedId class having only one field java.sql.Data
I become the error such as follows.
thread main <openjpa-2.0.1-r422266:989424 nonfatal user error> org.apache.openjpa.persistence.ArgumentException failed execute query SELECT m FROM mzeiritsu
check query syntax for correctness
see nested exception for details
cause by java.lang.ClassCastException incompatible with java.util.Calendar
occur at 431th line
col [ i ]
return field.getHandler()","org.apache.openjpa.persistence.embed.TestEmbeddable:testEntityA_Coll_String()
org.apache.openjpa.persistence.embed.TestEmbeddable:testEntityA_Embed_Coll_Map()"
METHOD,openjpa-2.0.1,OPENJPA-1830,2010-10-11T13:05:46.000-05:00,Deserialization of EMF causes connectionPassword to be overwritten with Value.INVISIBLE,"ConfigurationImpl.writeExternal()   toProperties()       When readExternal()
contain connectionPassword contain toProperties() contain map _ props serialize out toProperties() serialize out map _ props
When readExternal() deserializes, the StringValue of connectionPassword gets its value set twice, which causes Value.INVISIBLE to get set as the value, which never happens if the EMF is never serialized.","org.apache.openjpa.persistence.simple.TestSerializedFactory:setUp()
org.apache.openjpa.lib.conf.ConfigurationImpl:toProperties(boolean)"
METHOD,openjpa-2.0.1,OPENJPA-1896,2010-11-23T10:32:43.000-06:00,OpenJPA cannot store POJOs if a corresponding record already exists,"merge()  
 merge()   persist()  
      
 merge()
If a POJO is created using a java constructor, merge() cannot store the newly constructed object's data if this means updating a pre-existing record with a matching identity.
have natural key have applications not use OpenJPA
generate own data objects with file path
store into database
Instead, any attempt to execute either merge() or persist() on an independently constructed object with a matching record identity in the database triggers the same error in the database layer, since OpenJPA attempts to execute an insert for a pre-existing primary key, throwing...
org.apache.openjpa.lib.jdbc.ReportingSQLException: ERROR: duplicate key value violates unique constraint ""file_pkey"" {prepstmnt 32879825 INSERT INTO file (locationString, location, version, folder) VALUES (?
be from discussion
exist in database not load from database
attempt merge() set version field before attempting
contain Ricks comments contain links","org.apache.openjpa.kernel.VersionAttachStrategy:compareVersion(StateManagerImpl, PersistenceCapable)
org.apache.openjpa.persistence.relations.BasicEntity:getId()"
METHOD,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
use openjpa inside osgi container
pass appliation class loeader as part pass appliation class loeader by returning return from PersistenceUnitInfo.getClassLoader()
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
append return value of PersistenceUnitInfo.getClassLoader() append return value to list establihe fix participate in MultiClassLoader set up in MetaDataRepository.java:310ff
set classloader as context loader set classloader in meanwhile set classloader by PersistenceProvider.createContainerEntityManagerFactory() set classloader during creation
instantiate bean entityManagerFactory of class null
see nested stacktrace for details
cause by org.clazzes.fancymail.server.entities.EMail","org.apache.openjpa.meta.FieldMetaData:hashCode()
org.apache.openjpa.meta.FieldMetaData:compareTo(Object)"
METHOD,openjpa-2.0.1,OPENJPA-526,2008-02-27T13:28:05.000-06:00,Insert text more than 4K bytes to Clob column causes SQLException: Exhausted Resultset,"public class Exam 
 @Lob 
 @Column(name = ""text"", nullable = false)  
 private String text;
 
 With nullable = false
param with nullable = false
end transaction
see nested exceptions for details
not insert NULL
not insert NULL
not insert NULL","org.apache.openjpa.persistence.kernel.common.apps.Lobs:getId()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:getLob()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:Lobs(String, int)
org.apache.openjpa.persistence.kernel.common.apps.Lobs:setLob(String)
org.apache.openjpa.jdbc.sql.OracleDictionary:setNull(PreparedStatement, int, int, Column)"
METHOD,adempiere-3.1.0,1240,2008-05-16T03:03:55.000-05:00,Posting not balanced when is producing more than 1 produc,"Production Quantity= 2
The accounting is not balanced  when more that 1 product BOM is produced.
put line
apply in case
3. Then click on  ""Create/post Production"" button in the Production header tab, this create the production line.
verify in production line tab
6. click on ""Not Postet"" Button, then there the botton label is changed to ""Dont Balanced""",org.compiere.acct.Doc_Production:createFacts(MAcctSchema)
CLASS,pig-0.8.0,PIG-1771,2010-12-16T14:45:37.000-06:00,"New logical plan: Merge schema fail if LoadFunc.getSchema return different schema with ""Load...AS""","{code}
 
 BinStorage() 
         tuple()  ;
dump auxData;
{code}
The following script fail:
use BinStorage()
dump auxdata
Error message:","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1776,2010-12-17T16:28:09.000-06:00,"changing statement corresponding to alias after explain , then doing dump gives incorrect result","{code}
 
  
 {code}
generate group.str
give correct results
/* but dumping c after following steps gives incorrect results */
generate group
generate group.str","src.org.apache.pig.PigServer
src.org.apache.pig.newplan.logical.relational.LOLoad
test.org.apache.pig.test.TestUDFContext"
CLASS,pig-0.8.0,PIG-1785,2011-01-04T17:20:28.000-06:00,New logical plan: uid conflict in flattened fields,"{code}
 
 b0>b2;
dump c;
{code}

 
 {(1,2),(2,3)}
foreach generate flatten(a0)
We get nothing.","src.org.apache.pig.newplan.logical.rules.ImplicitSplitInserter
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
src.org.apache.pig.newplan.optimizer.PlanOptimizer
src.org.apache.pig.newplan.optimizer.Rule"
CLASS,pig-0.8.0,PIG-1808,2011-01-17T08:50:48.000-06:00,,"null;
DUMP D;
leave outer b
The below script fails both in 0.7 and 0.8 since A requires a valid schema to be defined.
But the error message in 0.8 is not helpful.
Error message in 0.8
open iterator
cause by org.apache.pig.impl.logicalLayer.FrontendException try -Dpig.usenewlogicalplan=false.
process rule PushUpFilter
Error message in 0.7
open iterator for alias
have valid schema","test.org.apache.pig.test.TestPushUpFilter
src.org.apache.pig.newplan.logical.rules.PushUpFilter"
CLASS,pig-0.8.0,PIG-1812,2011-01-19T20:06:36.000-06:00,,"{t:(id:chararray, wht:float)} 
    
 flatten(cat_bag.id)    
    
 {
        I = order M by ts;
        J = order B by ts;
        generate flatten(group) as (pkg:chararray, cat_id:chararray), J.ts as tsorig, I.ts as tsmap;
}
My script is listed below:
use PigStorage('\t') as } )
use PigStorage('\t')
flatten ( cat_bag
cogroup m
j by ts
When running this script, I got a warning about ""Encountered Warning DID_NOT_FIND_LOAD_ONLY_MAP_PLAN 1 time(s)"" and pig error log as below:
pig Stack trace
store alias cause by org.apache.pig.PigException
unexpect error during execution
not cast org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad to org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
But, when I removed the DISTINCT statement before COGROUP, i.e. ""B = distinct B;""  this script can run smoothly.
I have also tried other reducer side operations like ORDER, it seems that they will also trigger above error.","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LimitAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.KeyTypeDiscoveryVisitor
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.RearrangeAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1813,2011-01-20T10:25:01.000-06:00,Pig 0.8 throws ERROR 1075 while trying to refer a map in the result of  eval udf.Works with 0.7,"flatten(org.myudf.GETFIRST(value))  
 PigStorage()
generate id generate register myudf.jar; generate id # rmli as rmli:bytearray
The above script fails when run with Pig 0.8 but runs fine with Pig 0.7 or if pig.usenewlogicalplan=false.
The below is the exception thrown in 0.8 :
receive bytearray from UDF convert bytearray to map","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-1831,2011-01-28T04:02:31.000-06:00,Indeterministic behavior in local mode due to static variable PigMapReduce.sJobConf,"PigStorage()
The below script when run in local mode gives me a different output.
store relation have in local mode obtain through streaming
b into output.B
d into output.D
g GENERATE count_STAR(F) as totalcount
Here if I store relation B and D then everytime i get the result  :
acbd            3
abcd            3
adbc            3
But if i dont store relations B and D then I get an empty output.
Here again I have observed that this behaviour is random ie sometimes like 1out of 5 runs there will be output.","src.org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange
src.org.apache.pig.builtin.Distinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage
src.org.apache.pig.data.InternalSortedBag
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin
src.org.apache.pig.impl.builtin.DefaultIndexableLoader
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce.Map
src.org.apache.pig.impl.io.FileLocalizer
test.org.apache.pig.test.TestFinish
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner
src.org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager
src.org.apache.pig.data.InternalCachedBag
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestPruneColumn
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase
test.org.apache.pig.test.TestFRJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup
test.org.apache.pig.test.utils.FILTERFROMFILE
src.org.apache.pig.data.InternalDistinctBag"
CLASS,pig-0.8.0,PIG-1843,2011-02-04T20:42:39.000-06:00,NPE in schema generation,"{code}
   
   ;
{code}
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(getSchemaName(""parselong"", input), DataType.MAP));
    }
}
{code}
Hit NPE in following script:
{code}
a = load 'table_testBagDereferenceInMiddle2' as (a0:chararray);
b = foreach a generate MapGenerate(STRSPLIT(a0).
extend EvalFunc<Map> { @Override extend code } return m return new Schema(new Schema.FieldSchema(getSchemaName(""parselong"", input) ) )
Error message:","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.EvalFunc"
CLASS,pig-0.8.0,PIG-1856,2011-02-15T17:26:16.000-06:00,Custom jar is not packaged with the new job created by LimitAdjuster,"{code}
 
  
 {code}
limit b
be in classpath
The script, however,  fails since the piggybank jar isn't shipped to the backend with the additional job created by the LimitAdjuster.
register piggybank jar in script","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1858,2011-02-17T02:27:48.000-06:00,UDF in nested plan results frontend exception,"{code}
 
 PigStorage()  
 {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        } 
   ;
{code}
generate Const.sum store d into out_d
fail during compilation
The below is the exception that I get :
try -Dpig.usenewlogicalplan=false.
try -Dpig.usenewlogicalplan=false.
When i trun off new logical plan the script executes successfully.
observe issue",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-1861,2011-02-18T12:39:53.000-06:00,The pig script stored in the Hadoop History logs is stored as a concatenated string without whitespace this causes problems when attempting to extract and execute the script,"{}  {}  {} 
 statement1;statement2;
use com.yahoo.grid.sath.JobHistoryLoader()
The pig script stored in: conf#'pig.script' has the whitespace removed, this makes it difficult to extract and run the
script.
In particular, statements that terminate in "";"" work correctly as
""statement1;statement2;statement99""  but statements that do not end in "";"" result in
""statement1statement2statement3"" and it's difficult to parse the pig script and fix the concatenated string.
There's also a problem with comments as in:
On a side note, I also noticed that in many of the scripts the last statement is missing "";""","test.org.apache.pig.test.TestPigStats
src.org.apache.pig.tools.pigstats.ScriptState"
CLASS,pig-0.8.0,PIG-1866,2011-02-23T14:01:13.000-06:00,,"{code}
     
 t.b1;
dump b;
{code}
foreach generate t.b1
Error from old logical plan:
not cast org.apache.pig.data.BinSedesTuple to org.apache.pig.data.DataBag
Error from new logical plan:
If we change ""b = foreach a generate t.b1;"" to ""b = foreach a generate t.i;"", it works fine, only refer to a bag does not work.","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.8.0,PIG-1868,2011-02-24T00:42:05.000-06:00,,"{code}
 
 {
 Tuples = order B1 by ts;
 generate Tuples;
} 
   { t: ( previous, current, next ) } 
 as id;
dump C3;
{code}

 
 {code}
 
 {code}

  on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
have complex data types return from eval function
register myudf.jar; b1 generate tuples generate TransformToMyDataType(Tuples,-1,0,1) as seq generate FLATTEN(seq) generate current.id as id dump c3 foreach by ts
On C3 it fails with below message :
{code}
Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 45 Input: 0 Column: 1)
{code}
The script works if I turn off new logical plan or use Pig 0.7.","src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-1892,2011-03-10T02:44:12.000-06:00,Bug in new logical plan : No output generated even though there are valid records,"Maploader()
I have the below script which provides me no output even though there are valid records in relation B which is used for the left out join.
store output use Maploader() as = filter a0
see from plan not mark # 's as RequiredKeys not mark map1 type as RequiredKeys","test.org.apache.pig.test.TestPruneColumn.PigStorageWithTrace
src.org.apache.pig.newplan.logical.rules.MapKeysPruneHelper
test.org.apache.pig.test.TestPruneColumn"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,Pig report input size -1 for empty input file,"{code}
 
 by b0;
dump c;
{code}
If 1.txt is empty, Pig will report
Successfully read -1 records from: ""1.txt""
have multiinputcounters see in WebUI record from _0_2.txt","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1912,2011-03-16T16:11:46.000-05:00,non-deterministic output when a file is loaded multiple times,"while (( i < 10 ));  
  
 {results[*]}

 
  
  
  
 
 
  
  
 @operasolutions.com
(360)
I have a small demonstration script (actually, a directory with one main script and several other scripts that it calls) where the output (STOREd to a file) is not consistent between runs.
paste files below message email tarball to anybody like anybody upload tarball not see way
The problem appears to be that when a dataset X gets LOADed twice, with things other than LOADs occurring between the loads (like a FOREACH GENERATE), a FOREACH GENERATE that is later performed on X doesn't always choose the correct columns.
The correctness of the output was highly variable on my computer, for one of my co-workers it *almost* always failed, and for two other of my co-workers they didn't see any failures, so it's likely to be a race condition or something like that.
paste name with content paste name as comment paste name of file
put contents of following files
run pig run shell script file output compare output
-- correct_output.
e | run main.pig
diff correct_output
echo $ results
show non-deterministic bug in pig
Non-deterministic in the sense that the output of the script is not
the same between different times it is run on the same input; usually
the input is right, but sometimes it's wrong for no apparent reason.
demonstrate issue include in directory
The scripts load the file data.csv and write to the output
directory, but the file output/Y/part-m-00000 is sometimes different
between consecutive runs.
make error stop
comment out STORE x_w INTO pigstorage in main.pig
make copy of data.csv call data2.csv call file load_daw_data2
load data2.csv have data2.csv have calc_x_W
appreciate hearing
discuss http://mail-archives.apache.org/mod_mbox/pig-user/201102.mbox/%3CAANLkTi=2ZtkVGJevKLYSSzSH--KCcX38+Xaw2d2STNiS@mail.gmail.com%3E discuss issue
have shell script testmany.sh run script multiple times run reports run shell script testmany.sh run output agrreed with file correct_output
run code on different laptops run pig 0.8.0
give times during runs give times on laptop
get up wrong output until 28th run give right output
never observe wrong output
pigbug $ testmany.sh $","src.org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOLoad"
CLASS,pig-0.8.0,PIG-1927,2011-03-21T19:23:54.000-05:00,Dereference partial name failed,"{code}
 
 generate e0;
describe f;
{code}
The following script fail:
generate flatten(a)
generate c.a0
generate logical plan not find field a0","test.org.apache.pig.test.TestUnionOnSchema
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1955,2011-03-31T13:26:15.000-05:00,"PhysicalOperator has a member variable (non-static) Log object that is non-transient, this causes serialization errors","private final transient Log log = LogFactory.getLog(getClass());
 
 private transient Log log = LogFactory.getLog(getClass());
find while write unit tests
Creating a local PigServer to test my LoadFunc caused a serialization of the PhysicalOperator class, which failed due to:
add transient keyword to definition","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserComparisonFunc
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODemux
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSkewedJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPreCombinerLocalRearrange
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFRJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POFilter
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.ExpressionOperator
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLimit
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit"
CLASS,pig-0.8.0,PIG-1963,2011-04-04T17:18:24.000-05:00,"in nested foreach, accumutive udf taking input from order-by does not get results in order","{code}
 
 explain d;
dump d;
{code}
order secondary sort by f2 generate group
explain d","test.org.apache.pig.test.TestAccumulator
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.AccumulatorOptimizer"
CLASS,pig-0.8.0,PIG-1977,2011-04-07T17:27:11.000-05:00,"""Stream closed"" error while reading Pig temp files (results of intermediate jobs)","{code}
   {code}
In certain cases when compression of temporary files is on Pig scripts fail with following exception:
close at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:145)
turn off compression","test.org.apache.pig.test.TestTmpFileCompression
src.org.apache.pig.impl.io.TFileRecordReader"
CLASS,pig-0.8.0,PIG-1979,2011-04-08T02:24:01.000-05:00,New logical plan failing with ERROR 2229: Couldn't find matching uid -1,"{code}
 
  
    
    
      
     PigStorage() 
  
  
  
   PigStorage() ;
{code}

   
  
    
 {code}

 import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;

public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}

  @Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().newTuple(1);
      tp2.set(0, (inputTuple.get(0).toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}

 {code}
use PigStorage()
The script is failing in building the plan, while applying for logical optimization rule for AddForEach.
match uid for project
orginate field from udf org.vivek.udfs.MyExtractor
import org.apache.pig.EvalFunc; import org.apache.pig.data.
try { return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY) catch { System.err.println ( error while generating schema
throw new ioexception
The script goes through fine if I disable AddForEach rule by -t AddForEach","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression"
CLASS,pig-0.8.0,PIG-1993,2011-04-12T19:47:41.000-05:00,PigStorageSchema throw NPE with ColumnPruning,"{code}
 
  
  
 GENERATE a1;
dump b;
{code}
store into temp use org.apache.pig.piggybank.storage.PigStorageSchema()
exec temp use org.apache.pig.piggybank.storage.PigStorageSchema()
Error message:","contrib.piggybank.java.src.test.java.org.apache.pig.piggybank.test.TestPigStorageSchema
contrib.piggybank.java.src.main.java.org.apache.pig.piggybank.storage.PigStorageSchema"
CLASS,pig-0.8.0,PIG-313,2008-07-14T19:20:04.000-05:00,Error handling aggregate of a computation,"{code}
 
 {code}

 
 {quote}
   
   
   
   
   
    
    
    
    
 {quote}
Error output:
hadoop file system
use hdfs
use hdfs
evaluate output type of mul/div operator
resolve LOForEach schema
cause validation find during validation org.apache.pig.impl.plan.PlanValidationException",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.","flatten(outlinks.target);
  flatten(outlinks.target);
use BinStorage as } ) generate flatten(outlinks.target)
---> Would expect both C and D to work, but only C works.
D gives the error shown below.
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,Schema reported from DESCRIBE and actual schema of inner bags are different.,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
Prints for the DESCRIBE commands:
The reported schemas for urlContentsG and urlContentsF are wrong.
They are also against the section ""Schemas for Complex Data Types"" in http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_Schemas.
sound like technicality
assume inner bag of { chararray } assume UDF of { chararray } not work with { } not work for instance","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,HypergeometricDistribution.sample suffers from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
have application port from commons math
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.
import org.apache.commons.math3.distribution.HypergeometricDistribution;
trace in debugger do { code } return / (double) getPopulationSize(); {code} it could do: {code} return getSampleSize() * ((double) getNumberOfSuccesses() / getPopulationSize() )",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-209,2008-06-17T03:02:48.000-05:00,RealMatrixImpl#operate gets result vector dimensions wrong,"{{double[] out = new double[nRows];}}
 
 {{double[] out = new double[v.length];}}
create result vector have same length as input vector have result vector as input vector
This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square.
create vector with same length be of course
read {{double[] out = new double[nRows];}} instead_of {{double[] out = new double[v.length];}}","org.apache.commons.math.linear.RealMatrixImpl:operate(double[])
org.apache.commons.math.linear.BigMatrixImpl:operate(BigDecimal[])"
METHOD,math,MATH-221,2008-08-29T13:31:56.000-05:00,,"class Complex  
 {code}
 import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));

                System.out.println(""res=comp: ""+res.equals(comp));
        }
}
 {code}
relate on complex numbers
import org.apache.commons.math.complex.
void main(String[] args) {
res:  -0 - 1i
comp: 0 - 1i
res=comp: false
think thats give multiply method",org.apache.commons.math.complex.Complex:equals(Object)
METHOD,math,MATH-280,2009-07-06T21:26:57.000-05:00,,"public class NormalDistributionImpl extends AbstractContinuousDistribution 


  
 public abstract class AbstractContinuousDistribution


 
 DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);

 
 normal.inverseCumulativeProbability(0.977249868051820);
extend AbstractContinuousDistribution
gives the exception below.
give errors","org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtils:bracket(UnivariateRealFunction, double, double, double, int)"
METHOD,math,MATH-305,2009-10-22T06:35:08.000-05:00,NPE in  KMeansPlusPlusClusterer unittest,"package org.fao.fisheries.chronicles.calcuation.cluster;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.Arrays;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;

public class ClusterAnalysisTest {


	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());

	}

}
When running this unittest, I am facing this NPE:
package org.fao.fisheries.chronicles.calcuation.cluster;
import static org.junit.Assert.assertTrue;
import java.util.List; import java.util.Random;","org.apache.commons.math.util.MathUtils:distance(int[], int[])"
METHOD,math,MATH-318,2009-11-06T15:09:36.000-06:00,wrong result in eigen decomposition,"{code}
     public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }
 {code}
Some results computed by EigenDecompositionImpl are wrong.
The following case computed by Fortran Lapack fails with version 2.0
{code}
    public void testMathpbx02() {
use routine DSTEMR
refeigenvector [ i ]
refeigenvector [ i ]","org.apache.commons.math.linear.EigenDecompositionImpl:flipIfWarranted(int, int)"
METHOD,math,MATH-326,2009-12-29T00:09:20.000-06:00,getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways),"{code}
     public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }
 {code}

 
  
 {code}   
     public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }
 {code}

    sparseIterator() 
 {code}
   public double getLInfNorm() {
    double norm = 0;
    Iterator<Entry> it = sparseIterator();
    Entry e;
    while(it.hasNext() && (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }
 {code}
The current implementation in ArrayRealVector has a typo:
Worse, the implementation in OpenMapRealVector is not even positive semi-definite:
check in future check for kind","org.apache.commons.math.linear.ArrayRealVector:getLInfNorm()
org.apache.commons.math.linear.OpenMapRealVector:getLInfNorm()"
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,ODE integrator goes past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
handle end as event handle end of integration range
lead in cases lead to error
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
throw IntegratorException {","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-369,2010-05-03T15:48:27.000-05:00,"BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException","new BisectionSolver()  solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);
throw NullPointerException as member variable
NullPointerException will be thrown.","org.apache.commons.math.analysis.solvers.BisectionSolver:solve(UnivariateRealFunction, double, double, double)"
METHOD,math,MATH-393,2010-07-25T00:14:20.000-05:00,,"{code} 
 public double getResult() {
    return optima[0];
}
 {code}
 
 {code}
 public double getFunctionValue() {
    return optimaValues[0];
}
 {code}
In ""MultiStartUnivariateRealOptimizer"" (package ""optimization""), the method ""getResult"" returns the result of the last run of the ""underlying"" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the ""optimize"" method.
define getresult as { code } public double getResult() { return optima [ 0 ]","org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizer:getFunctionValue()
org.apache.commons.math.optimization.MultiStartUnivariateRealOptimizer:getResult()"
METHOD,math,MATH-546,2011-03-12T04:52:54.000-06:00,,"int sum = 0;
have type have int
Using an int causes the method to truncate the distances between points to (square roots of) integers.
manifest as aside manifest in version manifest by making return empty clusters","org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer:chooseInitialCenters(Collection<T>, int, Random)"
METHOD,math,MATH-567,2011-05-05T17:49:00.000-05:00,"class Dfp toDouble method return -inf whan Dfp value is 0 ""zero""","toDouble()  
 toDouble()  
 import org.apache.commons.math.dfp.DfpField;


 
 
 
 public static void main(String[] args)  
 DfpField field = new DfpField(100);
		    getZero()   field.getZero()  toDouble() 
   newDfp(0.0)  
 field.newDfp(0.0)  toDouble() 
 toDouble()
find bug in toDouble() method
If the Dfp's value is 0 ""zero"", the toDouble() method returns a  negative infini.
have exposant equal
import org.apache.commons.math.dfp.DfpField;
test equality return signed zero begin of toDouble() method","org.apache.commons.math.dfp.Dfp:Dfp(DfpField, double)
org.apache.commons.math.dfp.Dfp:toDouble()"
METHOD,math,MATH-60,2006-05-14T04:20:21.000-05:00,,"Fraction parse(String source, 
ParsePostion pos)  class ProperFractionFormat  
 ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);

//Test 1 : fail 
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

 
 parse(String, ParsePosition)
return result from function
po ) of Commons math library
Function ""Fraction parse(String, ParsePosition)"" returned Fraction 1/3 (means the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs above.
I think the function does not handle parsing the numberator/ denominator properly incase input string provide invalid numerator/denominator.","org.apache.commons.math.fraction.ProperFractionFormat:parse(String, ParsePosition)"
METHOD,math,MATH-618,2011-07-13T20:23:43.000-05:00,"Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same","{code}
       
 {@link #NaN}  
 {@link java.lang.Double}  
 {code}
For both Complex add and subtract, the javadoc states that
have nan value
return in parts
Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.
add test to add implementation",org.apache.commons.math.complex.Complex:add(Complex)
METHOD,math,MATH-645,2011-08-13T16:18:48.000-05:00,MathRuntimeException with simple ebeMultiply on OpenMapRealVector,"{code:java}
 import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}
 {code}
 
 {noformat}
  
 {noformat}
import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;
{code} raises an exception
noformat } org.apache.commons.math.linear.OpenMapRealVector@7170a9b6
thread main org.apache.commons.math.MathRuntimeException$6 map modify exception in thread modify exception while iterating","org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(RealVector)
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(RealVector)"
METHOD,math,MATH-679,2011-10-03T05:36:20.000-05:00,Integer overflow in OpenMapRealMatrix,"computeKey()
computeKey() has an integer overflow.
never create OpenMapRealMatrix with more cells","org.apache.commons.math.linear.OpenMapRealMatrix:OpenMapRealMatrix(int, int)"
METHOD,math,MATH-713,2011-11-25T16:35:02.000-06:00,,"SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);

 
 x = 1; y = -1;
A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:
SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);
Result:
x = 1; y = -1;
not affect values omit variables with coefficients omit variables at point",org.apache.commons.math.optimization.linear.SimplexTableau:getSolution()
METHOD,math,MATH-836,2012-07-31T17:04:25.000-05:00,,"public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }
The Fraction constructor_ Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction.
1: the constructor_ returns a positive Fraction.
Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value
2: the constructor_ does not manage to reduce the Fraction properly.
Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.
find solution
throw FractionConversionException
increase value
change to something small break tests in FractionTest
approximate fraction
throw FractionConversionException throw solution
explore idea of axiom-based testing
show bugs through simplified approach show java test class FractionTestByAxiom through simplified approach show text file through simplified approach cause value/maxDenominator combinations
never specify in documentation be in lowest terms numerator in lowest terms compare fractions for equality","org.apache.commons.math3.fraction.Fraction:Fraction(double, double, int, int)"
METHOD,math,MATH-942,2013-03-09T15:05:04.000-06:00,DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type,"Array.newInstance(singletons.get(0).getClass(), sampleSize)   
 singleons.get(0) 
 {{DiscreteDistribution.sample()}}  
 {code}
 List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).sample(1);
{code}
create array with { { Array.newInstance
An exception will be thrown if:
* {{singleons.get(0)}} is of type T1, an sub-class of T, and
* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.
attach patch",org.apache.commons.math3.distribution.DiscreteDistribution:sample(int)
METHOD,math,MATH-949,2013-03-15T18:11:56.000-05:00,LevenbergMarquardtOptimizer reports 0 iterations,"LevenbergMarquardtOptimizer.getIterations()     BaseOptimizer.incrementEvaluationsCount()

 
 {noformat}
     @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }

 {noformat}
The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0.
call BaseOptimizer.incrementEvaluationsCount()
Notice how the evaluations count is correctly incremented, but the iterations count is not.
return new double [ ] { FastMath.pow(point[0], 4) }
return new double [ ] [ ] { {","org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer:doOptimize()
org.apache.commons.math3.optim.BaseOptimizer:BaseOptimizer(ConvergenceChecker<PAIR>)
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer:doOptimize()"
FILE,WFCORE,WFCORE-267,2014-11-19T19:47:31.000-06:00,CLI prints output twice if using cli client jar,"INFO: {




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}




{




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}
If you are using the CLI client jar, all output is printed twice.
This is because JBoss logging is not set up and by default CommandContextImpl is printing log messages to standard out.
The output will look something like this:
result =
result =",org.jboss.as.cli.CommandLineMain
FILE,WFCORE,WFCORE-495,2015-01-12T08:48:29.000-06:00,"WFLY won't startup due to ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""","file(standalone.xml)  file(standalone.xml)
WFLY won't startup due to ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""
After server restart, it shows:
fail in unrecoverable manner exiting
see previous messages for details
3. restart wildfly to see the error message.
This happens because step 2 does a ""full-replace-deployment"" operation which does not remove content from standalone/data/content/aa/2d0425dd53572294d591b56efdee2680539eaf/content and deployment info from configuration file(standalone.xml).
Therefore, you will have xxx.war in standalone/data/content and configuration file(standalone.xml), also a xxx.war and xxx.war.deployed file inside /standalone/deployments.
A second time server restart will cause a duplicate resource error.",org.jboss.as.server.deployment.DeploymentFullReplaceHandler
FILE,WFCORE,WFCORE-604,2015-03-18T09:19:35.000-05:00,"After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory","{standalone|domaine} 
 {standalone|domaine}  
 {standalone|domaine} 
 {standalone|domaine} 
 {standalone|domaine}
- After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory
see following reproduce steps
4. Find ""new"" deployment info in JBOSS_HOME/{standalone|domaine}/data/content, and the old deployment info will be still there.
change application
remain old info
- The deployment information which created when deploy was failed remains in JBOSS_HOME/{standalone|domaine}/data/content.","org.jboss.as.host.controller.mgmt.MasterDomainControllerOperationHandlerImpl
org.jboss.as.server.controller.resources.ServerRootResourceDefinition
org.jboss.as.host.controller.ManagedServerOperationsFactory
org.jboss.as.host.controller.DomainModelControllerService
org.jboss.as.host.controller.RemoteDomainConnectionService
org.jboss.as.test.shared.ModelParserUtils
org.jboss.as.server.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentAddHandler
org.jboss.as.server.logging.ServerLogger
org.jboss.as.server.deployment.DeploymentRemoveHandler
org.jboss.as.domain.controller.resources.ServerGroupResourceDefinition
org.jboss.as.repository.LocalDeploymentFileRepository
org.jboss.as.domain.controller.operations.ApplyRemoteMasterDomainModelHandler
org.jboss.as.server.deployment.DeploymentReplaceHandler
org.jboss.as.domain.controller.resources.DomainRootDefinition
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentDefinition
org.jboss.as.repository.logging.DeploymentRepositoryLogger
org.jboss.as.domain.controller.operations.deployment.DeploymentFullReplaceHandler
org.jboss.as.core.model.test.LegacyKernelServicesImpl
org.jboss.as.subsystem.test.TestModelControllerService
org.jboss.as.host.controller.HostControllerService
org.jboss.as.domain.controller.resources.DomainDeploymentResourceDefinition
org.jboss.as.core.model.test.TestModelControllerService
org.jboss.as.server.mgmt.domain.RemoteFileRepositoryService
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentAdd
org.jboss.as.server.ApplicationServerService
org.jboss.as.repository.LocalFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeAddHandler
org.jboss.as.server.test.InterfaceManagementUnitTestCase
org.jboss.as.host.controller.model.host.HostResourceDefinition
org.jboss.as.server.deployment.DeploymentAddHandlerTestCase
org.jboss.as.repository.DeploymentFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentReplaceHandler
org.jboss.as.repository.ContentRepository
org.jboss.as.domain.controller.operations.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.DeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeResource
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler
org.jboss.as.server.deployment.DeploymentFullReplaceHandler"
FILE,WFCORE,WFCORE-687,2015-05-11T12:39:25.000-05:00,,"IdentityPatchContext.recordContentLoader(patchID, contentLoader)
Patches that contain duplicate patch-id attribute values in 'element' elements in patch.xml can be applied but can't be rolled back.
An attempt to rollback such a patch will result in an error ""Content loader already registered for patch "" + patchID, thrown from IdentityPatchContext.recordContentLoader(patchID, contentLoader).
add support
implement under different issue","org.jboss.as.patching.metadata.PatchXml
org.jboss.as.patching.metadata.PatchBuilder
org.jboss.as.patching.metadata.PatchXmlUnitTestCase
org.jboss.as.patching.installation.LayerTestCase
org.jboss.as.patching.logging.PatchLogger
org.jboss.as.patching.metadata.PatchXmlUtils"
FILE,WFCORE,WFCORE-442,2014-12-02T19:15:13.000-06:00,AbstractMultiTargetHandler-based handlers do not propagate failures to the top level failure-description,"{read-only=1} 
 {




                ""name"" => ""jboss.server.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""user.home"",




                ""path"" => ""/Users/hbraun"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.base.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
  
 {




                ""name"" => ""user.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.data.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/data"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.home.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.log.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/log"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.controller.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            }
result =
address = > [ ]
result =
name = > jboss.server.temp.dir
address = > [ ]
result =
name = > user.home
address = > [ ]
result =
name = > jboss.server.base.dir
address = > [ ]
result =
name = > java.home
address = > [ ]
result =
name = > user.dir
address = > [ ]
result =
name = > jboss.server.data.dir
address = > [ ]
result =
name = > jboss.home.dir
address = > [ ]
result =
name = > jboss.server.log.dir
address = > [ ]
result =
name = > jboss.server.config.dir
address = > [ ]
result =
name = > jboss.controller.temp.dir
One item in the set has a failure description but the overall response does not.
handle similar things have logic for creating create overall failure-description",org.jboss.as.controller.operations.global.GlobalOperationHandlers
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,One profile can have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
One profile can have more ancestors with same submodules.
It leads to WFLYCTL0212: Duplicate resource [(""subsystem"" => ""subsystem_name"")] .
add hierarchical composition of profiles add hierarchical composition to AS
define wflyctl0401 in mail-02
not support } not support overriding subsystems
add subsystem to default-new profile","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-955,2015-08-27T14:34:07.000-05:00,Server is not responding after attempt to set parent of profile to non-existent profile,"add()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""java.lang.NullPointerException:null""




}




 
 add()
Server is not responding after attempt to set parent of profile to non-existent profile.
Server is not responding also after attempt to set parent of socket-binding-group to non-existent socket-binding-group.
work on wildfly-core ( 2.0.0
occur on wildfly occur on EAP 7.0.0
Actual results:","org.jboss.as.controller.OperationContextImpl
org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.SocketCapabilityResolutionUnitTestCase
org.jboss.as.controller.capability.registry.IncludingResourceCapabilityScope
org.jboss.as.controller.AbstractCapabilityResolutionTestCase"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,Warnings about missing notification descriptions when an operation removes an extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
When I use migration operation the console log is filled with warning messages of type
not describe notification of type resource-removed not describe notification for resource
then I the log looks like
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource
not describe notification of type resource-removed not describe notification for resource","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
When using a role which only selects the master there is no access-control response header showing the filtered resources, and the slave wrongly appears in the results:
result =
address = > [ ]
result =
name = > master
interface =
address = > [ ]
When using a role that only selects the slave we get a proper access-control header
result = >
address = > [ ]
result = >
address = > [ ]
result =
name = > slave
interface =","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-989,2015-09-19T03:11:07.000-05:00,,"{""server-group"" => {




        ""main-server-group"" => {""host"" => {""slave"" => {""main-three"" => ""WFLYHC0153: Channel closed""}
ServerInventoryImpl.reconnectServer is registering the ProxyController for the reconnecting server with the DomainModelControllerService.
The problem is that ProxyController is not yet in a state where it can forward requests to the server.
That won't happen until the server calls back with a DomainServerProtocol.SERVER_RECONNECT_REQUEST and the ServerToHostProtocolHandler.ServerReconnectRequestHandler handles it.
The effect is if a request for the server comes in during this window, it will fail.
happen in testsuite failure
Most significant logging is as follows:
use connection
use connection
send prepared response for ---
send pre-prepare failed response for ---
send pre-prepare failed response for ---
The HC completes boot.
Then servers main-three and other-two register.
The HC sends a prepared response to a request.
This is the host rollout request to the slave from the DC with the response being the ops to invoke on the servers.
The HC reports sending a ""pre-prepare failed response"" to two requests.
These are the requests the DC has asked it to proxy to the servers.
The result of all this for the client is the following failure of a management op:
Failed operation:
address = >
Response:
roll back operation on servers
close } } }
result = > [ ]
The ""WFLYHC0153: Channel closed"" failure is what is produced when an attempt is made to invoke on a disconnected proxy controller.","org.jboss.as.host.controller.ServerInventoryImpl
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler"
FILE,WFCORE,WFCORE-1067,2015-10-21T12:22:10.000-05:00,CVE-2015-5304 Missing authorization check for Monitor/Deployer/Auditor role when shutting down server or canceling op,"context.getServiceRegistry(true)
It was found that the server or host controller did not properly authorize a user performing a shut down.
use flaw shut down EAP server restrict to users
introduce issue
The context.getServiceRegistry(true) call, which throws an exception when write authorization fails, was replaced with a call to context.authorize, which only returns an authorization result.
Nothing was then done with the authorization result.
cancel in-progress operation exist in handling
cancel own operation
lose benefit have consistent RBAC scheme
cancel by doing do soft kill of CLI process
cancel own ops by cancelling use ModelControllerClient executeAsync API cancel Future","org.jboss.as.domain.management.controller.CancelActiveOperationHandler
org.jboss.as.server.operations.ServerShutdownHandler
org.jboss.as.test.integration.mgmt.access.StandardRolesBasicTestCase
org.jboss.as.test.integration.domain.rbac.AbstractStandardRolesTestCase
org.jboss.as.host.controller.operations.HostShutdownHandler"
FILE,WFCORE,WFCORE-1214,2015-12-11T23:17:45.000-06:00,Operation headers not propagated to domain servers when 'composite' op is used,"{blocking-timeout=5;rollback-on-runtime-failure=false}  
 {

[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3)     ""blocking-timeout"" => ""5"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""rollback-on-runtime-failure"" => ""false"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""caller-type"" => ""user"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""access-mechanism"" => ""NATIVE""

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3) }
When the user adds request headers to an op, they are not propagated to the servers during domain rollout if the 'composite' op is involved.
Then on a HC with two servers, this is logged:
add at location /Users/bstansberry/dev/wildfly/wildfly-core/dist/target/wildfly-core-2.0.5
add at location /Users/bstansberry/dev/wildfly/wildfly-core/dist/target/wildfly-core-2.0.5
Note the CLI 'deploy' is far from the only time the 'composite' op is used.
Among other places, the high level CLI 'batch' command in a domain involves use of 'composite'.","org.jboss.as.domain.controller.operations.coordination.DomainRolloutStepHandler
org.jboss.as.domain.controller.operations.coordination.OperationCoordinatorStepHandler"
FILE,WFCORE,WFCORE-1212,2015-12-11T18:04:50.000-06:00,,"TestModule.create()   mkdirs()   remove()  
  
 Once remove()   getModulesDir()
TestModule.create() calls mkdirs() to create its filesystem structure, but remove() only removes the dir above 'main' and below, leaving behind intermediate dirs.
The result of this is if you run the full testsuite with -Dts.basic, the dist/target/wildflyxxx/modules dir ends up with child dir 'test' in addition to the proper 'system'.
not end up in final dists
not run testsuite build with deploy target
know process for releasing release WildFly",org.jboss.as.test.module.util.TestModule
FILE,WFCORE,WFCORE-1198,2015-12-09T09:30:00.000-06:00,CLI does not resolve multiple properties if one property is undefined,"{PROFILE-NAME}  {APP-VERSION}  {VAR}  add(auto-start=true, group=""${PROFILE-NAME}${APP-VERSION}-server-group"")
Multiple property substitution is working with EAP 6.4.3+, however, if a variable amongst the multiple variables is empty or has no value, then the subsequent property in the CLI command is not substituted.
have following in host.xml
Note APP-VERSION had no value, and so the subsequent SERVER-INSTANCE-NUMBER was not properly resolved","org.jboss.as.cli.parsing.test.PropertyReplacementTestCase
org.jboss.as.cli.parsing.ExpressionBaseState"
FILE,WFCORE,WFCORE-1354,2016-02-03T00:19:08.000-06:00,Cannot clone a profile with a remoting subsystem but no io subsystem,"clone(to-profile=test)
The remoting subsystem added a requirement for the new io subsystem's worker capability, but it has special logic such that the requirement is only added if an endpoint resource is configured.
not have resource
This breaks down in the case of the profile 'clone' op, as a placeholder resource we add for the endpoint (to allow reads of the default endpoint config data) ends up getting 'described' and added by the cloning process.
So that added resource triggers an unmet requirement for the io worker:
provide capability provide known registration points be in context","org.jboss.as.remoting.RemotingExtension
org.jboss.as.subsystem.test.AbstractSubsystemBaseTest"
FILE,WFCORE,WFCORE-1114,2015-11-06T02:08:35.000-06:00,NPE in DeploymentStatusHandler,"AbstractDeploymentUnitService.getStatus()  
   
                            
 TransformListener()
Saw an NPE in a test run.
This is because AbstractDeploymentUnitService.getStatus() references the possibly null 'monitor' field.
start container
accept requests on localhost:9091
implementation Version 3.3.2
jboss Remoting version 4.0.14
jboss Modules version 1.4.4
jboss MSC version 1.2.6
use socket-binding
implementation Version 3.3.2
jboss Remoting version 4.0.14
load console module for slot main
add at location /opt/buildagentd74ae55e497982e/testsuite/manualmode/target/wildfly-core/standalone/data/5cf647697bfbd1ed32b458a4568c01fb6e8dd6/content
fail ] )",org.jboss.as.server.deployment.AbstractDeploymentUnitService
FILE,WFCORE,WFCORE-701,2015-05-19T15:06:17.000-05:00,,"attribute(name=status)




 {




    ""outcome"" => ""success"",




    ""result"" => ""FAILED""




}




  attribute(name=server-state)




 {




    ""outcome"" => ""success"",




    ""result"" => ""STOPPED""




}
When a managed server fails in some way, the server status reporting is inconsistent between the /host=<host>/server-config=<server> resources and the /host=<host>/server=<server> resource.
result = > FAILED
result = > STOPPED",org.jboss.as.host.controller.ManagedServer
FILE,WFCORE,WFCORE-1028,2015-10-01T19:12:08.000-05:00,,"{roles=slave-monitor}
A CLI request with an invalid value in the ""roles"" header results in improper behavior:
result = > [ ]
The op should fail because the role doesn't exist, but there is no failure-description.
The following is dumped in the HC log:
address java.lang.IllegalArgumentException unknown slave-monitor","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.controller.access.rbac.RoleMapper
org.jboss.as.test.integration.domain.ServerManagementTestCase"
FILE,WFCORE,WFCORE-1546,2016-05-11T07:07:27.000-05:00,Whitespaces in the middle of the value are siletly ignored,"{




    ""outcome"" => ""failed"",




    ""failure-description"" => ""JBAS011539: Log level I   N   F   O is invalid."",




    ""rolled-back"" => true




}




 
 attribute(name=value)




 {




    ""outcome"" => ""success"",




    ""result"" => ""ha ha ha""




}






 
 attribute(name=level)




 {




    ""outcome"" => ""success"",




    ""result"" => ""INFO""




}




 
 attribute(name=value)




 {




    ""outcome"" => ""success"",




    ""result"" => ""hahaha""




}
Whitespace in the middle of value (e.g. adding a system property with a value like ""my property"") is silently ignored.
go to history introduce in EAP 6.0.1
result = > ha ha ha
result = > INFO
result = > hahaha
ignore whitespace in middle","org.jboss.as.cli.parsing.test.OperationParsingTestCase
org.jboss.as.cli.parsing.operation.PropertyValueState"
FILE,WFCORE,WFCORE-1493,2016-04-20T20:05:27.000-05:00,org.jboss.as.host.controller.Main.getHostSystemProperties() doesn't propagate -Djdk.launcher.addexports.%d=%s value properly,"org.jboss.as.host.controller.Main.getHostSystemProperties()
workaround potential migration problems to modularized JDK
fork new process translate XaddExports values
But method org.jboss.as.host.controller.Main.getHostSystemProperties()
has problems with its values.
The format of -XaddExports (and thus for -Djdk.launcher.addexports.
%d=%s too) is:
see http://openjdk.java.net/jeps/261 for more information",org.jboss.as.host.controller.Main
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
When using rollout plans for EAP deployment scenarios I can create my own named rollout-plan for ease of use.
create such rollout plan create way use such rollout plan use way
see --name attribute given to name my rollout plan
see id attribute given to rollout header operation
use examples from documentation
miss something retrieve more info use rollout header operation in CLI use rollout header operation in deploy command","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,,"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
Then when I create some /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding or /socket-binding-group=standard-sockets/local-destination-outbound-socket-binding using same name as of already existing socket-binding resource, add operation is successful but when I perform server reload, it crashes as it is not able to parse configuration.
server crashes with following stacktrace in console log:
bind INFO [ org.wildfly.extension.undertow] wflyut0007 to 127.0.0.1:8080
declare message in socket-binding-group standard-sockets
fail in unrecoverable manner exiting
see previous messages for details
remove duplicate resources manually by removing
please change","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1607,2016-06-17T12:23:38.000-05:00,"Removing children of security-realm always finishes with {""outcome"" => ""success""}","{""outcome"" => ""success""}
Removing children of security-realm (e.g. authentication) always finishes with
{""outcome"" => ""success""}
not exist in server configuration",org.jboss.as.domain.management.security.SecurityRealmChildRemoveHandler
FILE,WFCORE,WFCORE-1635,2016-07-05T07:04:51.000-05:00,Write attribute on a new deployment scanner fails in batch,"add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)




 
 
 add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)
Creating a new deployment-scanner and altering it's attribute fails if done in single batch.
Running the commands without batch or running batch on CLI embed-server works fine.
use embed server",org.jboss.as.server.deployment.scanner.AbstractWriteAttributeHandler
FILE,WFCORE,WFCORE-1590,2016-06-12T14:18:43.000-05:00,Default parameter length validating ignores setMinSize(0),"static final SimpleAttributeDefinition REPLACEMENT = new SimpleAttributeDefinitionBuilder(ElytronDescriptionConstants.REPLACEMENT, ModelType.STRING, false)




        .setAllowExpression(true)




        .setMinSize(0)




        .setFlags(AttributeAccess.Flag.RESTART_RESOURCE_SERVICES)




        .build();






 
 add(pattern=""@ELYTRON.ORG"", replacement="""", replace-all=true)
The following error is reported if an empty string is used as a parameter: -
have minimum length of characters","org.jboss.as.controller.operations.validation.BytesValidator
org.jboss.as.controller.SimpleAttributeDefinitionUnitTestCase
org.jboss.as.controller.test.WriteAttributeOperationTestCase
org.jboss.as.controller.AbstractAttributeDefinitionBuilder
org.jboss.as.controller.AttributeDefinition"
FILE,WFCORE,WFCORE-1718,2016-08-16T09:49:11.000-05:00,,"remove()
  remove()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0158: Operation handler failed: java.lang.NullPointerException"",




    ""rolled-back"" => true




}






   
 attribute(name=level,value=DEBUG)
If Audit Logger is removed, destination handlers (i.e. its child nodes) are not removed properly.
They are not present in the config file.
They seem to be not removed ""internally"" though.
lead to couple
1. It is not possible to remove referenced File/Syslog handlers.
If user tries to remove them the NullPointerException is given as a result.
Their output is:
2. AuditLog continues to send auditable events to previously referenced File/Syslog handlers.
See log in the file (WILDFLY_HOME/standalone/data/audit-log.log)
See log in the syslog (/var/log/messages)","org.jboss.as.domain.management.audit.AuditLogLoggerResourceDefinition
org.jboss.as.domain.management.audit.AccessAuditResourceDefinition
org.jboss.as.domain.management.audit.AuditLogHandlerReferenceResourceDefinition"
FILE,WFCORE,WFCORE-1715,2016-08-15T19:04:54.000-05:00,HostProcessReloadHandler does not reset the HostRunningModeControl's restartMode,"The doReload()     ServerInventoryService.stop()
The ReloadContext created by HostProcessReloadHandler sets the HostRunningModeControl's restartMode but then it never gets restored to the default value.
A concern here is that ServerInventoryService only goes into its ""shutdownServers"" logic if the restartMode == RestartMode.SERVERS.
follow HC reload due_to bug
change default from null
happen by default",org.jboss.as.host.controller.operations.StartServersHandler
FILE,WFCORE,WFCORE-1793,2016-09-14T08:08:21.000-05:00,add-content operation fails to overwrite existing content with overwrite=true set when passing content by file path,"{""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




  {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}
Upon overwriting content in managed exploded deployments on wildfly-core, the following errors are produced:
update content of exploded deployment
update content of exploded deployment
update content of exploded deployment
pass content to server","org.jboss.as.server.controller.resources.DeploymentAttributes
org.jboss.as.server.deployment.ExplodedDeploymentAddContentHandler"
FILE,WFCORE,WFCORE-1851,2016-10-04T12:28:40.000-05:00,Inconsistent behaviour with browse-content(depth=1) operation between archived and exploded deployments,"content(depth=1)  
 {




    ""outcome"" => ""success"",




    ""result"" => [




        {




            ""path"" => ""jboss-kitchensink-ear-web.war"",




            ""directory"" => false,




            ""file-size"" => 63190L




        } 
 {




            ""path"" => ""jboss-kitchensink-ear-ejb.jar"",




            ""directory"" => false,




            ""file-size"" => 12256L




        } 
 {




            ""path"" => ""META-INF/"",




            ""directory"" => true




        }




     
 {




    ""outcome"" => ""success"",




    ""result"" => [




        {




            ""path"" => ""META-INF/"",




            ""directory"" => true




        } 
 {




            ""path"" => ""META-INF/MANIFEST.MF"",




            ""directory"" => false,




            ""file-size"" => 130L




        } 
 {




            ""path"" => ""jboss-kitchensink-ear-web.war"",




            ""directory"" => false,




            ""file-size"" => 63190L




        } 
 {




            ""path"" => ""jboss-kitchensink-ear-ejb.jar"",




            ""directory"" => false,




            ""file-size"" => 12256L




        } 
 {




            ""path"" => ""META-INF/application.xml"",




            ""directory"" => false,




            ""file-size"" => 802L




        } 
 {




            ""path"" => ""META-INF/kitchensink-ear-quickstart-ds.xml"",




            ""directory"" => false,




            ""file-size"" => 1955L




        }
/deployment=jboss-kitchensink-ear.
ear:browse-content(depth=1) operation returns inconsistent result depending on whether the deployment is exploded or not.
Archived:
result =
Exploded:
result =","org.jboss.as.repository.PathUtil
org.jboss.as.repository.ContentRepositoryTest"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,Tab completion suggest writing attribute which has access type metric and is not writable,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
CLI tab completion suggests attributes that are not writable and their access-type is metric
execute read-resource-description be of type metric be from executing
On attempt to write metric attribute, for example message-count, non writable error is printed","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,,"description(recursive=true)
If you tries to change such attributes you are informed that reload is necessary.
define attributes as restart-required = > no-services","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
FILE,WFCORE,WFCORE-1896,2016-10-26T09:44:51.000-05:00,Deployment operation browse-content(archive=true) does not return archives in archived deployments,"content(archive=true)  
 content()




 {




    ""outcome"" => ""success"",




    ""result"" => [




        {




            ""path"" => ""jboss-kitchensink-ear-web.war"",




            ""directory"" => false,




            ""file-size"" => 63190L




        } 
 {




            ""path"" => ""jboss-kitchensink-ear-ejb.jar"",




            ""directory"" => false,




            ""file-size"" => 12256L




        } 
 {




            ""path"" => ""META-INF/maven/"",




            ""directory"" => true




        } 
 {




            ""path"" => ""META-INF/MANIFEST.MF"",




            ""directory"" => false,




            ""file-size"" => 130L




        } 
 {




            ""path"" => ""META-INF/application.xml"",




            ""directory"" => false,




            ""file-size"" => 802L




        } 
 {




            ""path"" => ""META-INF/kitchensink-ear-quickstart-ds.xml"",




            ""directory"" => false,




            ""file-size"" => 1955L




        } 
 {




            ""path"" => ""META-INF/"",




            ""directory"" => true




        }




     
 content(archive=true)




 {""outcome"" => ""success""}






 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




  content(archive=true)




 {




    ""outcome"" => ""success"",




    ""result"" => [




        {




            ""path"" => ""jboss-kitchensink-ear-web.war"",




            ""directory"" => false,




            ""file-size"" => 63190L




        } 
 {




            ""path"" => ""jboss-kitchensink-ear-ejb.jar"",




            ""directory"" => false,




            ""file-size"" => 12256L




        }
Deployment operation browse-content(archive=true) does not return archives in archived deployments:
result =
work with exploded deployments
result =","org.jboss.as.repository.ContentFilter
org.jboss.as.repository.PathUtil
org.jboss.as.repository.PathUtilTest"
FILE,WFCORE,WFCORE-1959,2016-11-08T16:28:30.000-06:00,Deploying an empty managed exploded deployment to server group in domain fails,"{empty=true} 
 add()
Deploying an empty exploded deployment created on domain controller fails with the following:
roll back operation on servers",org.jboss.as.domain.controller.operations.coordination.ServerOperationResolver
METHOD,tika-1.3,TIKA-1192,2013-11-06T15:31:41.000-06:00,ArrayIndexOutOfBoundsException: 9 parsing RTF,"{noformat}
  
    
    
    
    
    
    
  
    
    
  
  
  
    
  
  
  
    
  
  
  
    
    
 {noformat}
When trying to parse an RTF file I'm getting the following exception.
attach file for privacy reasons",org.apache.tika.parser.rtf.RTFParserTest:getResult(String)
CLASS,jedit-4.3,1571752,2006-10-05T21:26:12.000-05:00,,"{

\} 
 {\{\{  --&gt;
function foo\(\) \{

\} //\}\}\}
Before 'Add Explicit fold' the content of buffer looks like this \('X' means selection boundaries\):
After:
function foo \
be between &lt; function empty line work OK",org.gjt.sp.jedit.textarea.TextArea
CLASS,jedit-4.3,1599709,2006-11-20T13:17:56.000-06:00,NPE with JEditBuffer and new indenting,"lt;ENTER&gt;
introduce NullPointerExceptions
state in commit message
return main rule name
When pressing ENTER, after the ""'"", the following exception gets thrown:",org.gjt.sp.jedit.buffer.JEditBuffer
CLASS,jedit-4.3,1600401,2006-11-21T13:16:31.000-06:00,StringIndexOutOfBoundsException in TokenMarker,"lt;init&gt; 
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
      
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt; 
      
      
      
      
      
      
    
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt;
After pressing ENTER in a rather long line in a .
php file, I got the following exception.
The line appeared wrong highlighted in the first place, that why I wanted to see if splitting it would resolve the highlight issue.
After this the buffer window is not usable anymore \(e.g. I cannot enter it, nor does it get repainted\).
send buffer event to org.gjt.sp.jedit.textarea.BufferHandler@1484a8a
debug \ ] phpsidekickparser
debug \ ] workthread run in work thread",org.gjt.sp.jedit.syntax.TokenMarker
CLASS,jedit-4.3,1724940,2007-05-24T15:02:18.000-05:00,,"lt;body&gt;
  lt;p&gt;
 
 the &lt;p&gt;  
 lt;body&gt;
  lt;d&gt;
If I highlight multiple selections of text in the text area and then begin typing, only the first character of what I type is inserted in the selected areas \(except for where the cursor ended up after making the selection\).
and I highlight both p's in the &lt;p&gt; tags and then type ""div"" I end up with:
attach screenshot",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,Unnecesarry fold expantion when folded lines are edited,"{\{\{ hello

something

\}
test patch
avoid serious black hole bugs apply patch
all folds are folded.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
CLASS,jedit-4.3,2129419,2008-09-25T23:53:11.000-05:00,NPE in EditPane.setBuffer when quitting jEdit,"lt;init&gt;
When trying to quit jEdit, I get the following Null-Pointer-Exception, which is probably related to some files being changed/deleted \(due to a ""cvs up"" in the background\).
quit jEdit
dialog already failed, so that ""Close"" was the only option that worked.
The NPE:",org.gjt.sp.jedit.gui.CloseDialog.ListHandler
