Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
FILE,DATAMONGO,DATAMONGO-423,2012-03-29T06:03:26.000-05:00,Criteria.regex should use java.util.Pattern instead of $regex,"c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", new BasicDBObject(""$regex"", ""b"") ) ) );






  
  
 c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", Pattern.compile( ""b"" , Pattern.CASE_INSENSITIVE ) ) ) );
mongod complains about $regex in some cases.
DBCollection c = ...
c.find( new BasicDBObject( ""x"" ,
new BasicDBObject(""$not"", new BasicDBObject(""$regex"", ""b"") ) ) );
throws this exception:
com.mongodb.MongoException: can't use $not with $regex, use BSON regex type instead
at com.mongodb.MongoException.parse(MongoException.java:82)
at com.mongodb.DBApiLayer$MyCollection.
__find(DBApiLayer.java:312)
at com.mongodb.DBCursor.
_check(DBCursor.java:369)
at com.mongodb.DBCursor.
_hasNext(DBCursor.java:504)
at com.mongodb.DBCursor.hasNext(DBCursor.java:529)
DBCollection c = ...
c.find( new BasicDBObject( ""x"" ,
new BasicDBObject(""$not"", Pattern.compile( ""b"" , Pattern.CASE_INSENSITIVE ) ) ) );","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.QueryTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-505,2012-08-14T03:07:56.000-05:00,Conversion of associations doesn't work for collection values,"class Entity {









  Long id;




  @DBRef




  Property property;




}









 class Property {




  Long id;




}









 interface EntityRepository extends Repository<Entity, Long> {









  Entity findByPropertyIn(Property... property);




}






  findByProperty()
class Entity {
Long id;
@DBRef
Property property;
}
class Property {
Long id;
}
interface EntityRepository extends Repository<Entity, Long> {
Entity findByPropertyIn(Property... property);
}
The execution of findByProperty() will fail as the given array (or collection) is not correctly translated into a collection of DBRefs.
The reason is that ConvertingIterator treats the value as is and wants to create a DBRef from it.
We need to unwrap the elements and convert them into DBRef instances one by one.","org.springframework.data.mongodb.repository.query.ConvertingParameterAccessor
org.springframework.data.mongodb.repository.query.ConvertingParameterAccessorUnitTests"
FILE,DATAMONGO,DATAMONGO-663,2013-04-24T03:21:19.000-05:00,org.springframework.data.mongodb.core.query.Field needs an equals method,"class Field   equals()  
  
 boolean fieldsEqual = this.fieldSpec == null ? that.fieldSpec == null : this.fieldSpec.equals(that.fieldSpec);
The above class Field does not has an equals() method.
But org.springframework.data.mongodb.core.query.Query has an equals method which is using the equals method of the Field class.
boolean fieldsEqual = this.fieldSpec == null ? that.fieldSpec == null : this.fieldSpec.equals(that.fieldSpec);
Please implement an equals on the Field method.
Purpose: For unit testing the equals method is needed.",org.springframework.data.mongodb.core.query.Field
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,Updating an object does not write type information for objects to be updated,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
I used 1.0.0.
M5 version, and the type information (under _class key) was stored with object when it was necessary to be able to read it from database later.
That worked perfectly for me till my upgrade to 1.0.0.
RELEASE version that broke my application as it saves the objects without type information and later it is impossible to read it back to java model.
What I found is that MappingMongoConverter.writeInternal(...) method that in turn calls addCustomTypeIfNecessary(...) (line 330) which puts type information into DBObject.
During execution of convertToMongoType(...) (at line 851) removeTypeInfoRecursively(...) is called which clears type data saved earlier under _class key.
I had to comment out this call in order to
The first point is that there is a contradiction: why to save type information to DBObject if it is later removed by other method?
The second point is that there should be a way to persist the type information inferred from runtime along the persisted object and not just the class definition.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,"Using ""OrderBy"" in ""query by method name"" ignores the @Field annotation for field alias.","@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
@Field(""sr"")
int Score
List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,Problem with lazy loading in @DBRef when getting data using MongoTemplate,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
@Document
class Parent {
@Id
private String id;
private String name;
@DBref(lazy=true)
private Child child;
// getters and setters ommited
}
and the Child class
@Document
class Child {
@Id
private String id;
private String name;
//getters and setters ommited
}
The following situation should never happen:
Parent parent = new Parent();
parent.setName(""Daddy"");
mongoTemplate.save(parent); //ok, it is persisted like we expected.
Criteria criteria = Criteria.where(""_id"").
is(parent.getId());
Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);
// The child attribute should be null, right?
assertNull(persisted.getChild()); // it fails
The null attribute is actually an enhanced class generated by CGLib.
It should not be.
This brings a lot of problems when you, by accident, persist the same entity.
I attached a project with the JUnit test which reproduces the problem for you.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-420,2012-03-22T10:09:35.000-05:00,Extra quotes being added to @Query values and fields,"@Query 
 @Query 
 { ?0 }
 
 { ?1 }
 
 someMethod( String values, String fields ) 
 String values = ""value : 'things'"";
   
 { ""value : 'things'"" }
   
 String values = ""field : 0"";
   
 { ""field : 0"" }
   
 String values = ""field\"" : \""0"";
Quotes are being added to values passed into @Query annotations.
@Query( value = ""
{ ?
0 }
"", fields = ""
{ ?
1 }
"" )
someMethod( String values, String fields );
Whatever I pass into ""values"" or ""fields"" gets wrapped in quotes when inserted into the query sent to MongoDB.
String values = ""value : 'things'"";
-> rendered as -
{ ""value : 'things'"" }
-> This is not a valid query - the double quotes around it break it.
String values = ""field : 0"";
-> rendered as -
{ ""field : 0"" }
-> This is not a valid projection - the double quotes around it break it.
String values = ""field\"" : \""0"";
-> rendered as -
{ ""field"" : ""0"" }
-> This is not a valid projection - the double quotes around zero turn it into a string so it renders as ""true"" instead of 0/undefined.
Thanks.","org.springframework.data.mongodb.repository.query.StringBasedMongoQuery
org.springframework.data.mongodb.repository.query.StringBasedMongoQueryUnitTests"
FILE,DATAMONGO,DATAMONGO-1078,2014-10-28T02:23:26.000-05:00,@Query annotated repository query fails to map complex Id structure.,"@Query(""{'_id': {$in: ?0}}"")




List<User> findByUserIds(Collection<MyUserId> userIds) 
 {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}
StringBasedMongoQuery converts any complex object to the according mongo type including type restrictions via _class.
@Query(""{'_id': {$in: ?
0}}"")
List<User> findByUserIds(Collection<MyUserId> userIds);
end up being converted to:
{_id:  {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}, ...
So we need to check for the presence of typeKey when converting id properties.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,"@Query $in does not remove ""_class"" property on collection of embedded objects","@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
@Query(value = ""{ embedded : { $in : ?
0} }"")
List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c);
generates incorrect query.
{ ""embedded"" : { ""$in"" : [ {  ""_class"" : ""demo.EmbeddedObject"" , ""s"" : ""hello""}]}}
Query should be without _class property e.g.:
{ ""embedded"" : { ""$in"" : [ { ""s"" : ""hello""}]}}
This bug is related to https://jira.spring.io/browse/DATAMONGO-893","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,"geoNear, does not return all matching elements, it returns only a max of 100 documents","public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
Aloha,
public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {
final NearQuery nearQuery = NearQuery.near(p).
maxDistance(distance);
log.info(""{}"",nearQuery.toDBObject());
return mongoTemplate.geoNear(nearQuery, MyObject.class);
}
The geoNear method is documented like this:
Returns {@link GeoResults} for all entities matching the given {@link NearQuery}.
I expect 1000 ""matching"" documents But i only get 100.
There is some default being set, that restricts the result to 100.
That should be stated in the method.
And another method having a pageable should be added.
What do you think?",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,Repository keyword query findByInId with pageable not returning correctly,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
The query returns results but getTotalElements() and getTotalPages() always returns 0.
Also when you try to get any other page than 0, no results return.
I've tried using In with another member other than id and it works as expected.
@Document
public class Item {
@Id
private String id;
private String type;
}
public interface ItemRepository extends MongoRepository<Item, String> {
Page<Item> findByIdIn(Collection ids, Pageable pageable);
Page<Item> findByTypeIn(Collection types, Pageable pageable);
}
@RunWith(SpringJUnit4ClassRunner.class)
@ContextConfiguration(classes = {MongoDbConfig.class})
@TransactionConfiguration(defaultRollback = false)
public class TestPageableIdIn {
@Autowired
private ItemRepository itemRepository;
private List<String> allIds = new LinkedList<>();
@Before
public void setUp() {
itemRepository.deleteAll();
String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};
// 10 items per type
for (String type : types) {
for (int i = 0; i < 10; i++) {
String id = UUID.randomUUID().
toString();
allIds.add(id);
itemRepository.save(new Item(id, type));
}
}
}
@Test
public void testPageableIdIn() {
Pageable pageable = new PageRequest(0, 5);
// expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages
Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);
Assert.assertEquals(5, results.getContent().
size());
Assert.assertEquals(10, results.getTotalElements());
Assert.assertEquals(2, results.getTotalPages());
// expect 5 Items returned, total of 30 Items in 6 Pages
results = itemRepository.findByIdIn(allIds, pageable);
Assert.assertEquals(5, results.getContent().
size());
Assert.assertEquals(30, results.getTotalElements()); // this is returning 0
Assert.assertEquals(6, results.getTotalPages());     // this is returning 0
}
}","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1307,2015-10-20T12:33:45.000-05:00,Stop converting user-defined runtime exceptions to NPEs,"throw exceptionTranslator.translateExceptionIfPossible(ex);
  
 
 
 
 return null;
MongoTemplate has code like this in many places:
} catch (RuntimeException ex) { throw exceptionTranslator.translateExceptionIfPossible(ex);
MongoExceptionTranslator, however, often does NOT return an exception.
If it encounters an unknown exception it does this:
// If we get here, we have an exception that resulted from user code,
// rather than the persistence provider, so we return null to indicate
// that translation should not occur.
return null;
MongoTemplate then ""eats"" the original exception and throws a null-pointer exception instead.
MongoTemplate should throw the original exception if it gets null back from the exception translator.",org.springframework.data.mongodb.core.MongoTemplate
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,Missing indexes in associations involving generic types,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
When an association between documents involves generic types, the type information is not correctly inferred at startup time resulting in missing indexes.
Please, see https://github.com/agustisanchez/SpringDataMongoDBBug, for code samples.
class Book with index on ""ISBN"" attribute super class AbstractProduct with index on ""name"" attribute class ProductWrapper holding attribute ""content"" of generic type ""T extends AbstractProduct""
List<ProductWrapper<Book>> books2 = new ArrayList<>
The index ""name"" inherited from AbstractProduct is created (book2.content.name) inside ""catalog"" , but the index defined on the Book class itself (isbn) is not created as Spring Data Mongo is only inferring type infromation from the ProductWrapper class definition (ProductWrapper <T extends AbstractProduct>).
Spring Data MongoDB should be able to infer type information from the list declaration ( List<ProductWrapper<Book>> ), becoming aware that Catalog contains a list of Books, hence indexes defined on Book should be created.
If the wrapper class is defined as ProductWrapper<T>, then no indexes are created at all on Catalog.books2.content.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1290,2015-09-17T15:30:23.000-05:00,@Query annotation with byte[] parameter does not work,"Optional<SampleDomainObject> findBySampleData(byte[] sampleDate) 
 @Query(""{ 'sampleData' : ?0 }"")




Optional<SampleDomainObject> findBySampleDateWithAnnotation(byte[] sampleData)
In a repository, these finders should be equivalent:
Optional<SampleDomainObject> findBySampleData(byte[] sampleDate);
@Query(""{ 'sampleData' : ?
0 }"")
Optional<SampleDomainObject> findBySampleDateWithAnnotation(byte[] sampleData);
... but only the first works.","org.springframework.data.mongodb.repository.query.StringBasedMongoQuery
org.springframework.data.mongodb.repository.query.StringBasedMongoQueryUnitTests"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,Query mapper does not use @Field field name when querying nested fields in combination with nested keywords,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
@Document(collection = ""Computer"")
public class Computer
{
@Id
private String _id;
private String batchId;
@Field(""stat"")
private String status;
@Field(""disp"")
private List<Monitor> displays;
//setters and getters
}
public class Monitor {
@Field(""res"")
private String resolution;
// setters/getters
}
protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,
CursorPreparer preparer, DbObjectCallback<T> objectCallback)
DBObject mappedQuery = queryMapper.getMappedObject(query, entity);
resolves the fields to the input query to the ones in the @Field annotations, except for these in embedded arrays.
So, in the example above, resolution fields in DBObject remains resolution.
While, the status field resolves to stat.
Note the queries in the inner list, are setup as elemMatch.
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
Which doesn't get any data, because there is no field called resolution (the field in mongo is res).
{ ""$and"" : [ { ""status"" : ""A""} , { ""displays"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
Notice the status and displays fields correctly get converted to the value in the @Field annotation.
The correct query from getMappedObject should be:
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""res"" : { ""$ne"" :  null }} , { ""res"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
This basically means that any queries that operate on fields (with a name different from the peristed name) in the inner list will fail.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
CLASS,derby-10.7.1.1,DERBY-4654,2010-05-12T05:27:40.000-05:00,Restriction.toSQL() doesn't escape special characters,"org.apache.derby.vti.Restriction.toSQL()  
 Restriction.doubleQuote()   IdUtil.normalToDelimited()
org.apache.derby.vti.Restriction.toSQL() adds double quotes around column names, but it does not escape the special characters (like double quotes) in the column names, so the returned string may not be valid SQL.
This could cause problems when using the restriction to generate a query against an external database.
Restriction.doubleQuote() should use IdUtil.normalToDelimited() to get proper quoting of the names.","org.apache.derbyTesting.functionTests.tests.lang.RestrictedVTITest
org.apache.derby.vti.Restriction"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,Pig creates wrong schema after dereferencing nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
data = LOAD 'test_data.
txt' USING PigStorage() AS (f1: int, f2: int, f3:
int, f4: int);
nested = FOREACH data GENERATE f1, (f2, f3, f4) AS nested_tuple;
dereferenced = FOREACH nested GENERATE f1, nested_tuple.
(f2, f3);
DESCRIBE dereferenced;
uses_dereferenced = FOREACH dereferenced GENERATE nested_tuple.
f3;
DESCRIBE uses_dereferenced;
The schema of ""dereferenced"" should be {f1: int, nested_tuple: (f2: int,
f3: int)}.
DESCRIBE thinks it is {f1: int, f2: int} instead.
When dump is
used, the data is actually in form of the correct schema however, ex.
(1 (2,3))
(5 (6,7))
...
This is not just a problem with DESCRIBE.
Because the schema is incorrect,
the reference to ""nested_tuple"" in the ""uses_dereferenced"" statement is
considered to be invalid, and the script fails to run.
The error is:
Invalid field projection.
Projected field [nested_tuple] does not exist in
schema: f1:int,f2:int.","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2828,2012-07-19T05:03:16.000-05:00,Handle nulls in DataType.compare,"Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);
While using TOP, and if the DataBag contains null value to compare, it will generate the following exception:
Caused by: java.lang.NullPointerException
at org.apache.pig.data.DataType.compare(DataType.java:427)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:97)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:1)
at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:649)
at java.util.PriorityQueue.siftUp(PriorityQueue.java:627)
at java.util.PriorityQueue.offer(PriorityQueue.java:329)
at java.util.PriorityQueue.add(PriorityQueue.java:306)
at org.apache.pig.builtin.TOP.updateTop(TOP.java:141)
at org.apache.pig.builtin.TOP.exec(TOP.java:116)
code: (TOP.java, starts with line 91)
Object field1 = o1.get(fieldNum);
Object field2 = o2.get(fieldNum);
if (! typeFound) { datatype = DataType.findType(field1);
typeFound = true;
} return DataType.compare(field1, field2, datatype, datatype);
The reason is that if the typeFound is true , and the dataType is not null, and field1 is null, the script failed.
So we need to judge the field1 whether is null.","src.org.apache.pig.data.DataType
src.org.apache.pig.builtin.TOP
test.org.apache.pig.test.TestNull"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,"ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations","{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
Hi,
{code}
inp = LOAD '$INPUT' AS (memberId:long, shopId:long, score:int);
tuplified = FOREACH inp GENERATE (memberId, shopId) AS tuplify, score;
D1 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
D2 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
J = JOIN D1 By shopId, D2 by shopId;
K = FOREACH J GENERATE D1::memberId AS member_id1, D2::memberId AS member_id2, D1::shopId as shop;
EXPLAIN K;
DUMP K;
{code}
It is a bit weird written like that, but it provides a minimal reproduction case (in the real case, the ""tuplified"" phase came from a multi-key grouping).
This will give a wrongful output like .
.
{code}
(1 1001,1001)
(1 1002,1002)
(1 1002,1002)
(1 1002,1002)
{code}
The second column should be a member id so (1,2,3,4,5).
In the initial case, there was a FILTER (member_id1 < member_id2) after K, and computation failed because of PushUpFilter optimization mistakenly moving the LOFilter operation before the join, at a place where it tried to work on a tuple and failed.
My understanding of the issue is that when the ImplicitSplitInserter creates the LOSplitOutputs, it will correctly reset the schema, and the LOSplitOutput will regenerate uids for the fields of D1 and D2 ... but will not do that on the tuple members.
The logical plan after the ImplicitSplitINserter will look like (simplified)
{code}
|---D1: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[127]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#127:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[127]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
|---D2: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[130]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#130:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[130]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
{code}
tuplified correctly gets a new uid (127 and 130) but the members of the tuple don't.
When they get reprojected, both branches have the same uid and the join looks like:
{code}
|---J: (Name: LOJoin(HASH) Schema: D1::memberId#124:long,D1::shopId#125:long,D2::memberId#139:long,D2::shopId#132:long)ColumnPrune:InputUids=[125, 124, 132]ColumnPrune:OutputUids=[125, 124, 132]
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 0 Column: 1)
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 1 Column: 1)
{code}
If for example instead of reprojecting ""memberId"", we project ""memberId+0"", a new node is created, and ultimately the two branches of the join will correctly get separate uids.
My understanding is that LOSplitOutput.getSchema() should recurse on nested schema fields.
However, I only have a light understanding of all of the logical plan handling, so I may be completely wrong.
Attached is a draft of patch and a test reproducing the issue.
Unfortunately, I haven't been able to run all unit tests with the ""fix"" (I have some weird hangs)
I'd be happy if you could indicate if that looks like completely the wrong way to fix the issue.",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,zookeeper-3.4.5,ZOOKEEPER-1619,2013-01-11T09:57:16.000-06:00,Allow spaces in URL,"{code}
 
 {code}

 
 {code}
 
 {code}
Currently, spaces are not allowed in the url.
{code}
10.10.1.1:2181,10.10.1.2:2181/usergrid
{code}
This format will not (notice the spaces around the comma)
{code}
10.10.1.1:2181 , 10.10.1.2:2181/usergrid
{code}
Please add a trim to both the port and the hostname parsing.",src.java.main.org.apache.zookeeper.client.ConnectStringParser
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,ZooKeeper Server fails if snapCount is set to 1,"int randRoll = r.nextInt(snapCount/2);
{code}
If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:
2013-10-02 18:09:07,600 [myid:1] - ERROR [SyncThread:1:SyncRequestProcessor@151] - Severe unrecoverable error, exiting java.lang.IllegalArgumentException: n must be positive
at java.util.Random.nextInt(Random.java:300)
at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:93)
In source code,  it maybe be supposed that snapCount must be 2 or more:
{code:title=org.apache.zookeeper.server.SyncRequestProcessor.java|borderStyle=solid}
91             // we do this in an attempt to ensure that not all ofthe servers
92             // in the ensemble take a snapshot at the same time
93             int randRoll = r.nextInt(snapCount/2);
{code}
I think this supposition is not bad because snapCount = 1 is not realistic setting...
But, it may be better to mention this restriction in documentation or add a validation in the source code.",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,Unnecesarry fold expantion when folded lines are edited,"{\{\{ hello

something

\}
While testing the patch \#1999448, a problem was found.
But the patch was applied in r13404 to avoid more
serious black hole bugs.
This problem has now became a
bug.
\{\{\{ hello
something
\}\}\}
all folds are folded.
I
think it should not expand folds since the fold level
is not changed.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
METHOD,apache-nutch-1.8,NUTCH-1262,2012-01-31T03:15:33.000-06:00,Map `duplicating` content-types to a single type,"{code}
   
 {code}
Similar or duplicating content-types can end-up differently in an index.
With, for example, both application/xhtml+xml and text/html it is impossible to use a single filter to select `web pages`.
See also: http://lucene.472066.n3.nabble.com/application-xhtml-xml-gt-text-html-td3699942.html
Content-Type mapping is disabled by default and is enabled via moreIndexingFilter.mapMimeTypes.
Example mapping file is provided in conf/.
{code}
# target MIME-type <TAB> type1 [<TAB> type2 ...]
# Map XHTML to HTML
text/html       application/xhtml+xml
# Map XHTML and HTML to something else
Web page        text/html       application/xhtml+xml
# Map some office documents to each other
Office document application/vnd.
oasis.opendocument.text application/x-tika-msoffice
{code}","org.apache.nutch.indexer.more.MoreIndexingFilter:getConf()
org.apache.nutch.indexer.more.MoreIndexingFilter:setConf(Configuration)
org.apache.nutch.indexer.more.MoreIndexingFilter:filter(NutchDocument, Parse, Text, CrawlDatum, Inlinks)"
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,[resources] UnifiedTree should ensure file/folder exists,"getStat()
Build: I20030211 using natives (Linux/Windows)
When the UnifiedTree finds a new file from the file system, it assumes that if the file is not an existing file, then it is a folder.
This is not always true, because for different reasons a file returned by java.io.File.list/listFiles may not actually exist (our CoreFileSystemLibrary#getStat() returns 0).
At the first moment, the file is found in the file system and assumed to be a folder, and a corresponding resource is created in the workspace.
At the second refresh, the folder corresponding to that resource is not found in the file system, and then it is removed from the workspace.
And so on.
Bugs that revealed this problem: bug 21217 and bug 13463.","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
CLASS,openjpa-2.0.1,OPENJPA-1752,2010-07-29T23:33:45.000-05:00,TestPessimisticLocks JUNIT test produced inconsistent behavior with various backends,"testFindAfterQueryWithPessimisticLocks()
  testFindAfterQueryOrderByWithPessimisticLocks()
  testQueryAfterFindWithPessimisticLocks()
  testQueryOrderByAfterFindWithPessimisticLocks()


 
 testFindAfterQueryWithPessimisticLocks() 
  No exception;
TestPessimisticLocks JUNIT tests pass all assertions for Derby backend, but failures are seen on DB2, MySQL, Oracle.
It is likely that failures may also occur on other backends.
There could be some problem in OpenJPA code in handling pessimistic lock requests.
There is also inconsistency in reporting exceptions - lock timout or query timeout should be non-fatal; but with Derby the PessimisticLockException is reported  which is considered fatal.
It is also possible that the test scenarios are problematic.
TestPessisimiticLocks has 5 test cases, the last test case worked for all backend.
Problem test cases are listed as below:
1 testFindAfterQueryWithPessimisticLocks()
2 testFindAfterQueryOrderByWithPessimisticLocks()
3 testQueryAfterFindWithPessimisticLocks()
4 testQueryOrderByAfterFindWithPessimisticLocks()
The failure symptoms are summarized below -   Each test contains 2 variations.
The dot notation, for example, 1.1 is the first scenario in testFindAfterQueryWithPessimisticLocks() 
Each test scenario is either expecting an exception or No exception; if no exception is reported, the SELECT sql got results from database.
Tests       Derby                                        DB2V9.7                                 Oracle10gXE 10.2.0.1.0            MySQL 5.1.39/JDBC 5.1.7
====================================================================================================================================
1 1         PessimisticLockException      LockTimeoutException        LockTimeoutException              LockTimeoutException
1 2         No exception                              No exception                          No exception                                No exception
2 1         PessimisticLockException      LockTimeoutException        LockTimeoutException              LockTimeoutException
2 2         No  exception                             LockTimeoutException        No exception                                LockTimeoutException
3 1         No  exception                             QueryTimeoutException       process hang                            PersistenceException: Server shutdown [code=1053, state=08S01]
3 2         PessimisticLockException      QueryTimeoutException       process hang                            PersistenceException: Server shutdown [code=1053, state=08S01]
4 1         No  exception                             QueryTimeoutException       No exception                             QueryTimeoutException
4 2         PessimisticLockException      QueryTimeoutException       process hang                           QueryTimeoutException
NOTE: for Oracle, many test scenarios caused process to hang (test 3.1, 3.2, and 4.2) - ie.
test never run to completion
      for MySQL, Server shutdown (test 3.1 and 3.2)
      here is the  stack trace:
org.apache.openjpa.persistence.PersistenceException:Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
<openjpa-2.1.0-SNAPSHOT-rexported fatal general error> org.apache.openjpa.persistence.PersistenceException: Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
FailedObject: select e.department from Employee e where e.id < 10 [java.lang.String]
at org.apache.openjpa.jdbc.sql.DBDictionary.narrow(DBDictionary.java:4855)
at org.apache.openjpa.jdbc.sql.DBDictionary.newStoreException(DBDictionary.java:4815)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:137)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:118)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:70)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.handleCheckedException(SelectResultObjectProvider.java:155)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.handleCheckedException(QueryImpl.java:2109)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:40)
at org.apache.openjpa.kernel.QueryImpl.toResult(QueryImpl.java:1246)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:1005)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:861)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:792)
at org.apache.openjpa.kernel.DelegatingQuery.execute(DelegatingQuery.java:542)
at org.apache.openjpa.persistence.QueryImpl.execute(QueryImpl.java:288)
at org.apache.openjpa.persistence.QueryImpl.getResultList(QueryImpl.java:302)
at org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks.testQueryAfterFindWithPessimisticLocks(TestPessimisticLocks.java:271)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at junit.framework.TestCase.runTest(TestCase.java:154)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runTest(AbstractPersistenceTestCase.java:516)
at junit.framework.TestCase.runBare(TestCase.java:127)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:503)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:479)
at junit.framework.TestResult$1.protect(TestResult.java:106)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.framework.TestResult.run(TestResult.java:109)
at junit.framework.TestCase.run(TestCase.java:118)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.run(AbstractPersistenceTestCase.java:179)
at junit.framework.TestSuite.runTest(TestSuite.java:208)
at junit.framework.TestSuite.run(TestSuite.java:203)
at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:274)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:258)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$3(LoggingConnectionDecorator.java:257)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeQuery(LoggingConnectionDecorator.java:1176)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeQuery(JDBCStoreManager.java:1773)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:268)
at org.apache.openjpa.jdbc.sql.SelectImpl.executeQuery(SelectImpl.java:499)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:424)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:382)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.open(SelectResultObjectProvider.java:94)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.open(QueryImpl.java:2068)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:34)
... 30 more
NestedThrowables:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Server shutdown in progress
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
at java.lang.reflect.Constructor.newInstance(Unknown Source)
at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
at com.mysql.jdbc.Util.getInstance(Util.java:381)
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3515)
at com.mysql.jdbc.MysqlIO.nextRowFast(MysqlIO.java:1545)
at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1401)
at com.mysql.jdbc.MysqlIO.readSingleRowSet(MysqlIO.java:2829)
at com.mysql.jdbc.MysqlIO.getResultSet(MysqlIO.java:468)
at com.mysql.jdbc.MysqlIO.readResultsForQueryOrUpdate(MysqlIO.java:2534)
at com.mysql.jdbc.MysqlIO.readAllResults(MysqlIO.java:1749)
at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2159)
at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2554)
at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1761)
at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1912)
at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:93)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:280)
at org.apache.openjpa.lib.jdbc.JDBCEventConnectionDecorator$EventPreparedStatement.executeQuery(JDBCEventConnectionDecorator.java:270)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeQuery(LoggingConnectionDecorator.java:1174)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeQuery(JDBCStoreManager.java:1773)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:268)
at org.apache.openjpa.jdbc.sql.SelectImpl.executeQuery(SelectImpl.java:499)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:424)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:382)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.open(SelectResultObjectProvider.java:94)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.open(QueryImpl.java:2068)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:34)
at org.apache.openjpa.kernel.QueryImpl.toResult(QueryImpl.java:1246)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:1005)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:861)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:792)
at org.apache.openjpa.kernel.DelegatingQuery.execute(DelegatingQuery.java:542)
at org.apache.openjpa.persistence.QueryImpl.execute(QueryImpl.java:288)
at org.apache.openjpa.persistence.QueryImpl.getResultList(QueryImpl.java:302)
at org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks.testQueryAfterFindWithPessimisticLocks(TestPessimisticLocks.java:271)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at junit.framework.TestCase.runTest(TestCase.java:154)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runTest(AbstractPersistenceTestCase.java:516)
at junit.framework.TestCase.runBare(TestCase.java:127)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:503)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:479)
at junit.framework.TestResult$1.protect(TestResult.java:106)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.framework.TestResult.run(TestResult.java:109)
at junit.framework.TestCase.run(TestCase.java:118)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.run(AbstractPersistenceTestCase.java:179)
at junit.framework.TestSuite.runTest(TestSuite.java:208)
at junit.framework.TestSuite.run(TestSuite.java:203)
at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)",org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We are using openjpa inside an OSGi container together with
openjpa.MetaDataRepository"" value=""Preload=true""
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
A fix might be quite easily establihed by appending the return value of PersistenceUnitInfo.getClassLoader() to the list of claas loaders participating in the MultiClassLoader set up in
  
  MetaDataRepository.java:310ff
In the meanwhile, we are additionally setting our classloader as context loader during the creation of the EntityManagerFactory by PersistenceProvider.createContainerEntityManagerFactory(), but a fix in MetaDatRepository.preload() is highly appreciated.
TIA for fixing this,
Wolfgang
Stack trace:
org.osgi.service.blueprint.container.ComponentDefinitionException: Error when instantiating bean entityManagerFactory of class null
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:233)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:726)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:624)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:315)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)[:1.6.0_20]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)[:1.6.0_20]
at java.util.concurrent.FutureTask.run(FutureTask.java:166)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)[:1.6.0_20]
at java.lang.Thread.run(Thread.java:636)[:1.6.0_20]
Caused by: <openjpa-2.0.1-r422266:989424 fatal user error> org.apache.openjpa.persistence.ArgumentException: Unexpected error during early loading of entity metadata during initialization. See nested stacktrace for details.
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:331)
at org.apache.openjpa.persistence.PersistenceProviderImpl.preloadMetaDataRepository(PersistenceProviderImpl.java:280)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:211)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.container.AbstractServiceReferenceRecipe$JdkProxyFactory$1.invoke(AbstractServiceReferenceRecipe.java:632)
at $Proxy67.createContainerEntityManagerFactory(Unknown Source)
at org.clazzes.util.jpa.provider.EntityManagerFactoryFactory.newEntityManagerFactory(EntityManagerFactoryFactory.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:221)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:844)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:231)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
... 15 more
Caused by: java.security.PrivilegedActionException: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_20]
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:326)
... 32 more
Caused by: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at org.apache.openjpa.lib.util.MultiClassLoader.findClass(MultiClassLoader.java:216)
at java.lang.ClassLoader.loadClass(ClassLoader.java:321)[:1.6.0_20]
at java.lang.ClassLoader.loadClass(ClassLoader.java:266)[:1.6.0_20]
at java.lang.Class.forName0(Native Method)[:1.6.0_20]
at java.lang.Class.forName(Class.java:264)[:1.6.0_20]
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:233)
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:231)
... 34 more","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,Resolving factory method does not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
If a get method is annotated with @Factory then the method cannot be overridden with a method which take different parameters.
The system randomly selects one of the several methods with the same name which may or may not take the type which will be provided.
public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}
Actual results:
valueOf(String) may or may not be selected.
Expected results:
valueOf(String) should always be selected.
The provided patches fix this defect by applying the method invocation conversion rules from the Java Language Specification, 3rd Ed.
This means that widening primitive, boxing and unboxing conversions are all respected.",org.apache.openjpa.meta.FieldMetaData
METHOD,apache-nutch-2.1,NUTCH-1393,2012-06-13T18:38:39.000-05:00,Display consistent usage of GeneratorJob with 1.X,"{code}
 
  
  
  
  
  
 {code}
If we pass the generate argument to the nutch script, the Generator auto-spings into action and begins generating fetchlists.
This should not be the case, instead it should print traditional usage to stdout.
{code} lewis@lewis:~/ASF/nutchgora/runtime/local$ .
/bin/nutch generate
GeneratorJob: Selecting best-scoring urls due for fetch.
GeneratorJob: starting
GeneratorJob: filtering: true
GeneratorJob: done
GeneratorJob: generated batch id: 1339628223-1694200031
{code}
All I wanted to do was get the usage params printed to stdout but instead it generated my batch willy nilly.","org.apache.nutch.crawl.GeneratorJob:generate(long, long, boolean, boolean)
org.apache.nutch.crawl.GeneratorJob:run(String[])"
METHOD,lang,LANG-292,2006-10-31T01:45:01.000-06:00,"unescapeXml(""&12345678;"") should be ""&12345678;""","public void testNumberOverflow() throws Exception {
        doTestUnescapeEntity(""&#12345678;"", ""&#12345678;"");
        doTestUnescapeEntity(""x&#12345678;y"", ""x&#12345678;y"");
        doTestUnescapeEntity(""&#x12345678;"", ""&#x12345678;"");
        doTestUnescapeEntity(""x&#x12345678;y"", ""x&#x12345678;y"");
    }
Following test (in EntitiesTest.java) fails:
public void testNumberOverflow() throws Exception {
        doTestUnescapeEntity(""&#12345678;"", ""&#12345678;"");
        doTestUnescapeEntity(""x&#12345678;y"", ""x&#12345678;y"");
        doTestUnescapeEntity(""&#x12345678;"", ""&#x12345678;"");
        doTestUnescapeEntity(""x&#x12345678;y"", ""x&#x12345678;y"");
    }
Maximim value for char is 0xFFFF, so &#12345678; is invalid entity reference, and so should be left as is.","org.apache.commons.lang.Entities:unescape(String)
org.apache.commons.lang.Entities:unescape(Writer, String)"
METHOD,lang,LANG-315,2007-02-06T13:52:59.000-06:00,"StopWatch: suspend() acts as split(), if followed by stop()","suspend()   split()  stop();  
 StopWatch sw = new StopWatch();

        sw.start();
        Thread.sleep(1000);
        sw.suspend();
        // Time 1 (ok)
        System.out.println(sw.getTime());

        Thread.sleep(2000);
        // Time 1 (again, ok)
        System.out.println(sw.getTime());

        sw.resume();
        Thread.sleep(3000);
        sw.suspend();
        // Time 2 (ok)
        System.out.println(sw.getTime());

        Thread.sleep(4000);
        // Time 2 (again, ok)
        System.out.println(sw.getTime());

        Thread.sleep(5000);
        sw.stop();
        // Time 2 (should be, but is Time 3 => NOT ok)
        System.out.println(sw.getTime());


  stop()
In my opinion, it is a bug that suspend() acts as split(), if followed by stop(); see below:
StopWatch sw = new StopWatch();
sw.start();
Thread.sleep(1000);
sw.suspend();
// Time 1 (ok)
System.out.println(sw.getTime());
Thread.sleep(2000);
// Time 1 (again, ok)
System.out.println(sw.getTime());
sw.resume();
Thread.sleep(3000);
sw.suspend();
// Time 2 (ok)
System.out.println(sw.getTime());
Thread.sleep(4000);
// Time 2 (again, ok)
System.out.println(sw.getTime());
Thread.sleep(5000);
sw.stop();
// Time 2 (should be, but is Time 3 => NOT ok)
System.out.println(sw.getTime());
suspend/resume is like a pause, where time counter doesn't continue.
So a following stop()-call shouldn't increase the time counter, should it?",org.apache.commons.lang.time.StopWatch:stop()
METHOD,lang,LANG-346,2007-07-06T20:06:55.000-05:00,Dates.round() behaves incorrectly for minutes and seconds,"public void testRound()
{
    Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
    testCalendar.set(2007, 6, 2, 8, 9, 50);
    Date date = testCalendar.getTime();
    System.out.println(""Before round() "" + date);
    System.out.println(""After round()  "" + DateUtils.round(date, Calendar.MINUTE));
}

 
 Before round()  
 After round()   
 Before round()  
 After round()
Get unexpected output for rounding by minutes or seconds.
public void testRound()
{
Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
testCalendar.set(2007, 6, 2, 8, 9, 50);
Date date = testCalendar.getTime();
System.out.println(""Before round() "" + date);
System.out.println(""After round()  "" + DateUtils.round(date, Calendar.MINUTE));
}
--2.1 produces
Before round() Mon Jul 02 03:09:50 CDT 2007
After round()  Mon Jul 02 03:10:00 CDT 2007 -- this is what I would expect
--2.2 and 2.3 produces
Before round() Mon Jul 02 03:09:50 CDT 2007
After round()  Mon Jul 02 03:01:00 CDT 2007 -- this appears to be wrong","org.apache.commons.lang.time.DateUtils:modify(Calendar, int, boolean)"
METHOD,lang,LANG-363,2007-10-23T07:12:48.000-05:00,"StringEscapeUtils.escapeJavaScript() method did not escape '/' into '\/', it will make IE render page uncorrectly","document.getElementById(""test"")   document.getElementById(""test"") 
  
 String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);
If Javascripts including'/', IE will parse the scripts uncorrectly, actually '/' should be escaped to '\/'.
value = '<script>alert(\'aaa\');</script>';this expression will make IE render page uncorrect, it should be document.getElementById(""test"").
value = '<script>alert(\'aaa\');<\/script>';
Btw, Spring's JavascriptEscape behavor is correct.","org.apache.commons.lang.StringEscapeUtils:escapeJavaStyleString(Writer, String, boolean)"
METHOD,lang,LANG-552,2009-11-09T12:40:57.000-06:00,StringUtils replaceEach - Bug or Missing Documentation,"{code}
 import static org.junit.Assert.assertEquals;

import org.apache.commons.lang.StringUtils;
import org.junit.Test;


public class StringUtilsTest {

	@Test
	public void replaceEach(){
		String original = ""Hello World!"";
		String[] searchList = {""Hello"", ""World""};
		String[] replacementList = {""Greetings"", null};
		String result = StringUtils.replaceEach(original, searchList, replacementList);
		assertEquals(""Greetings !"", result);
		//perhaps this is ok as well
                //assertEquals(""Greetings World!"", result);
                //or even
		//assertEquals(""Greetings null!"", result);
	}

	
}
 {code}
The following Test Case for replaceEach fails with a null pointer exception.
I have expected that all StringUtils methods are ""null-friendly""
The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null.
I admit the use case is not perfect, because it is unclear what happens on the replace.
I outlined three expectations in the test case, of course only one should be met.
If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string
{code} import static org.junit.Assert.assertEquals;
import org.apache.commons.lang.StringUtils;
import org.junit.Test;
public class StringUtilsTest {
@Test public void replaceEach(){
String original = ""Hello World!""
;
String[] searchList = {""Hello"", ""World""};
String[] replacementList = {""Greetings"", null};
String result = StringUtils.replaceEach(original, searchList, replacementList);
assertEquals(""Greetings !""
, result);
//perhaps this is ok as well
//assertEquals(""Greetings World!""
, result);
//or even
//assertEquals(""Greetings null!""
, result);
}
}
{code}","org.apache.commons.lang3.StringUtils:replaceEach(String, String[], String[], boolean, int)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,SerializationUtils throws ClassNotFoundException when cloning primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.
{noformat} import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;
public class SerializationUtilsTest {
@Test public void primitiveTypeClassSerialization(){
Class<?> primitiveType = int.class;
Class<?> clone = SerializationUtils.clone(primitiveType);
assertEquals(primitiveType, clone);
}
}
{noformat}
The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4.
The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException.
I understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation.
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
String name = desc.getName();
try { return Class.forName(name, false, classLoader);
} catch (ClassNotFoundException ex) { try { return Class.forName(name, false, Thread.currentThread().
getContextClassLoader());
} catch (Exception e) { return super.resolveClass(desc);
}
}
}
{noformat}
Here is the code in ObjectInputStream that fixed the java bug.
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc)
throws IOException, ClassNotFoundException
{
String name = desc.getName();
try { return Class.forName(name, false, latestUserDefinedLoader());
} catch (ClassNotFoundException ex) {
Class cl = (Class) primClasses.get(name);
if (cl !
= null) { return cl;
} else { throw ex;
}
}
}
{noformat}","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-832,2012-09-27T00:27:58.000-05:00,FastDateParser does not handle unterminated quotes correctly,"{IsNd}
FDP does not handled unterminated quotes the same way as SimpleDateFormat
Format: 'd'd'
Date: d3
This should fail to parse the format and date but it actually works.
The format is parsed as:
Pattern: d(\p{IsNd}++)",org.apache.commons.lang3.time.FastDateParser:init()
FILE,SWARM,SWARM-863,2016-11-30T14:54:40.000-06:00,Version 2016.11.0 doesn't stop properly (with custom main class),"container = new Swarm(); // fractions being added here also




    container.start();




    container.deploy(...);






 
 container.stop();
private static org.wildfly.swarm.Swarm container
container = new Swarm(); // fractions being added here also
container.start();
container.deploy(...);
container.stop();
We now have the problem that stopping such a Swarm service in version 2016.11.0 does not properly shutdown the Swarm container (or better the underlying `Server`).
I did a debug session and found out that there remains one non-daemon thread blocking the JVM shutdown.
With version 2016.10.0 everything works fine.
The shutdown is clean and fast.
An example project can be found at https://github.com/seelenvirtuose/de.mwa.testing.wfs.
But I also have attached it as a zip.
de.mwa.testing.wfs-master.zip
Procrun can be downloaded at http://mirror.serversupportforum.de/apache//commons/daemon/binaries/windows/commons-daemon-1.0.15-bin-windows.zip
Steps to reproduce:
Tab Logging
Log path: <path-to-the-service-directory>\logs
Redirect Stdout: auto
Redirect Stderror: auto
Tab Java
Java Virtual Machine: Path to the ""jvm.dll"" of a JRE 8 (usually <path-to-jdk>\jre\bin\server\jvm.dll).Java Classpath: Full path to the uber-jar.Java Options: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=7777 (to enable remote debugging).
Tab Startup
Class: org.wildfly.swarm.bootstrap.Main
Method: main
Arguments: start
Mode: jvm
Tab Shutdown
Class: org.wildfly.swarm.bootstrap.Main
Method: main
Arguments: stop
Mode:jvm
After a succesful start you can ""GET http://localhost:8080/hello"", which should result in a ""Hello World"" response.
6) The service has many threads running.
See first attached screenshot.
Windows will hang in that stopping attempt and spit out a failure message after some time.
The process is still running afterwards.
The log file shows some output that indeed a shutdown is initalized.
The GET does not work anymore.
8) The service still has many threads (especially non-daemon threads).
See second attached screenshot.
Note, that I have other services, which show only two non-deamon threads after a shutdown attempt.
9) Killing the task ""testing-wfs.exe"" is the only way to stop the process completely.
Switching the Wildfly Swarm version to 2016.10.0 (in the POM of ""swarm"" module) makes it work great.
Starting and stopping run both smoothly.",org.wildfly.swarm.container.runtime.ServerBootstrapImpl
FILE,IO,IO-180,2008-09-08T18:03:20.000-05:00,LineIterator documentation,"LineIterator it = FileUtils.lineIterator(file, ""UTF-8"");
   try {
     while (it.hasNext()) 
{

       String line = it.nextLine();

       /// do something with line

     }
   } finally 
{

     LineIterator.closeQuietly(iterator);

   }
In the Javadoc for rg.apache.commons.io.LineIterator (in Commons IO 1.4), this code snippet is incorrect:  the last instance of ""iterator"" should be
""it"".
LineIterator it = FileUtils.lineIterator(file, ""UTF-8"");
try { while (it.hasNext())
{
String line = it.nextLine();
/// do something with line
}
} finally
{
LineIterator.closeQuietly(iterator);
}",org.apache.commons.io.LineIterator
FILE,eclipse-3.1,77249,2004-10-28T17:57:00.000-05:00,"Annotation on class cancels ""public"" modifier","@Jpf.Controller 
public class Foo {...} 
 @Jpf.Controller(
    catches={
       @Jpf.Catch(type=java.lang.Exception.class, method=""handleException""),
       @Jpf.Catch(type=PageFlowException.class, 
method=""handlePageFlowException"")
    }
)
public class Foo {
...
}
(This is in 3.1 M2.)
The org.eclipse.jdt.core.dom.CompilationUnit instance corresponding to the class Foo below has no ""public"" modifier, although it should.
Note that if the declaration is simplified to just ""@Jpf.
Controller public class Foo {...}"", then the instance does have a ""public"" modifier.
@Jpf.
Controller( catches={
@Jpf.
Catch(type=java.lang.Exception.class, method=""handleException""),
@Jpf.
Catch(type=PageFlowException.class, method=""handlePageFlowException"")
}
)
public class Foo {
...
}","org.eclipse.jdt.core.dom.ASTConverter
org.eclipse.swt.graphics.GC"
FILE,eclipse-3.1,78315,2004-11-10T12:53:00.000-06:00,org.eclipes.team.ui plugin's startup code forces compare to be loaded,"Platform.getAdapterManager()  registerAdapters(factory, DiffNode.class);

   
 startup()
3.1 M3
I wrote tests that ensure plug-ins like Search and Compare aren't loaded when opening a Java editor.
The one for compare fails because org.eclipes.team.ui forces compare to be loaded in its start(BundleContext) method:
Platform.getAdapterManager().
registerAdapters(factory, DiffNode.class);
The direct reference to DiffNode causes the compare plug-in to be loaded even if it is not needed yet.
Only when a compare will be done it needs to be loaded and the adapter being registered.
Test Case:",org.eclipse.team.internal.ui.TeamUIPlugin
FILE,eclipse-3.1,79091,2004-11-19T13:00:00.000-06:00,[compiler] Should report invalid type only on the name,"class X { Zork[] foo; }
Using latest,
class X { Zork[] foo; }
We report an error on Zork[] instead of Zork only.","org.eclipse.jdt.internal.compiler.problem.ProblemReporter
org.eclipse.jdt.internal.compiler.ast.ArrayTypeReference"
FILE,eclipse-3.1,80672,2004-12-10T04:44:00.000-06:00,[1.5] Annotation change does not trigger recompilation,"package p;
@q.Ann
public class Use {
}
  
package q;
public @interface Ann {
}


 
 
package q;
import java.lang.annotation.*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
 
 @Ann
Build 20041207
java===================================
package p;
@q.
Ann
public class Use {
}
q/Ann.
java===================================
package q;
public @interface Ann {
}
java should have got a problem due to disallowed
location for use of annotation @Ann.",org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader
FILE,eclipse-3.1,83206,2005-01-19T11:34:00.000-06:00,ICodeAssist#codeSelect(..) on implicit methods should not return a java element,"class User {
    enum Color {RED, GREEN, BLUE}
    void x() {
        Color.valueOf(""RED"");
        Color.values();
    }
}

     valueOf(String)  values()
I20050118-1015
class User { enum Color {RED, GREEN, BLUE} void x() {
Color.valueOf(""RED"");
Color.values();
}
}
ICodeAssist#codeSelect(.
.)
on implicit methods 'valueOf(String)' and 'values()' of enum Color should not return a java element.
Currently, the declaring enum is returned.",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,85397,2005-02-16T08:20:00.000-06:00,[1.5][enum] erroneous strictfp keyword on enum type produces error on constructor_,"strictfp enum Natural {
	ONE, TWO;
}

 
 strictfp enum Natural {
	ONE, TWO;
	
	private Natural() {
	}
}
I20050215-2300 (M5 test pass)
strictfp enum Natural {
ONE, TWO;
}
expected: strictfp is not allowed on the enum type actual: no error is reported
strictfp enum Natural {
ONE, TWO;
private Natural() {
}
}
expected: the wrong modifier is reported with the type name 'Natural' actual: the error is shown for the constructor_","org.eclipse.jdt.internal.compiler.lookup.SyntheticMethodBinding
org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyViewPart
org.eclipse.jdt.internal.compiler.lookup.MethodScope"
FILE,eclipse-3.1,85672,2005-02-17T05:53:00.000-06:00,[projection] Unfolding a folded region with no line delimiter on the last line selects too much,"package folding;

class Test {
    
}
I20050215-2300 (m5 test pass)
""package folding;
class Test {
}""  <-- no delimiter on last line
Note that there is a phantom line in the editor since we cannot fold that one away.
expected: caret is right after the closing brace actual: everything from after the *opening* brace is selected
Not a regression - it is like this in 3.0",org.eclipse.jface.text.source.projection.ProjectionViewer
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
I have only verified this with JPEG output.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,87211,2005-03-05T13:36:00.000-06:00,[PresentationAPI] standalone + movable stacks should remain standalone when dragged,"The dragOver()  
   
   
 PartStack.getDropTarget()
The dragOver() method for a custom presentation is not called when an entire 
stack (i.e. ViewStack) is being dragged.
Thus, you cannot prevent a ViewStack 
from being dropped onto another ViewStack.
The offending code seems to be in PartStack.getDropTarget().
I'm trying to code my presentation such that views can be dragged around and 
repositioned, but views cannot be combined with other views.","org.eclipse.ui.internal.ide.dialogs.ResourceTreeAndListGroup
org.eclipse.ui.internal.PartSashContainer"
FILE,eclipse-3.1,88295,2005-03-17T03:34:00.000-06:00,[1.5][assist] too many completion on enum case label,"public class Class3 {

	enum Color {
		BLUE, WHITE, RED;
	}
	
	void select(Color c) {
		
		switch(c){
			case BLUE :
			case WHITE:
			case R<|>
		}
	}
}
20050315
When completing in enum case label 'R<|>' only enum constants should be offered.
public class Class3 {
enum Color {
BLUE, WHITE, RED;
} void select(Color c) { switch(c){ case BLUE :
case WHITE:
case R<|>
}
}
}",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,89621,2005-03-30T12:41:00.000-06:00,[code assist] the caret position is wrong after code assist,"import java.awt.Frame;
import java.awt.event.WindowAdapter;

public class Foo extends Frame {

    public void bar() {
        addWindow<CODE ASSIST HERE>Listener(new WindowAdapter());
    }
}
Using 20030330-0500, I got this weird behavior.
import java.awt.Frame;
import java.awt.event.WindowAdapter;
public class Foo extends Frame {
public void bar() { addWindow<CODE ASSIST HERE>Listener(new WindowAdapter());
}
}
The result is:
addWindowListene<POSITION OF THE CARET>rListener
I would expect:
addWindowListener<POSITION OF THE CARET>Listener
This is pretty annoying and seems to occur only for method name proposal.","org.eclipse.jdt.ui.text.java.CompletionProposalCollector
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector
org.eclipse.jdt.internal.ui.text.java.GenericJavaTypeProposal"
FILE,eclipse-3.1,93119,2005-04-28T10:10:00.000-05:00,code assist: proposals for wildcard types,"package codeAssist;

import java.util.List ;

public class Extends {
	void m() {
		List <? |>
	}
}
I20050426-1700
package codeAssist;
import java.util.List ;
public class Extends {
	void m() {
		List <?
|>
	}
}
> actual: the only proposed item is the CU's type (Extends)
< expected: there are exactly two proposals: 'extends' and 'super'
> actual: dozens of type proposals are proposed (and two template proposals, but
that is not a jdt-core problem)
< expected: only 'extends' is proposed.",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,95096,2005-05-13T06:16:00.000-05:00,[5.0][content assist] Content assist popup disappears while completing the statically imported method name,"import static java.lang.Math
I20050513-0010
Steps to reproduce:
-> Instead of constraining the proposals to all members with prefix a, the popup closes","org.eclipse.jdt.internal.ui.text.java.JavaMethodCompletionProposal
org.eclipse.jdt.internal.ui.text.java.LazyJavaCompletionProposal"
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,[Presentations] (regression) Standalone view without title has no border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
build N20050523
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
- the history view (a regular view) has a border, but the standalone view does not
This is a regression from 3.0.2.
Standalone views should still have their border, just not the title or min/max buttons if showTitle==false.","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,97722,2005-05-31T16:41:00.000-05:00,Pref Page Ant/Runtime/Tasks/Add Task dialog problems,"@

Dialog
The dialog's error message is cropped at the bottom.
The space between Name and Location seems unneccessary.
The name text and location combo should be left aligned.
The background color of the error message is different from the dialog
background -- is this intended?
@@
Dialog font used: Trebuchet MS, size 11",org.eclipse.ant.internal.ui.preferences.AddCustomDialog
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,Container attempts to refresh children on project that is not open,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$
IProjectDescription description = ResourcesPlugin.getWorkspace
().
loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().
getRoot().
getProject
(description.getName());
project.create(description, new NullProgressMonitor());
This is the key to the issue.
A background refresh job has now been started for the closed project, but it never finishes and is stuck in an infinite loop.
I believe the offending code is in the class org.eclipse.core.internal.resources.Container.
The members() method is excuting if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
workspace.refreshManager.refresh(this);
because the projects members are not known.
Both the AliasManager and the Java
Perspective are calling members on the IProject.
If you override this method in Project and do not refresh for closed projects, the problem goes away.
On the next UI gesture, we get refresh infinite loops, one for each closed project.
We want the projects in the workspace, so we create them but do not open them, as open is very expensive.
The end user will open them by using the open project UI when needed.
This worked fine in Eclipse 3.0.","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,[1.5][compiler] Enum / Switch method is not initialized in a thread safe way,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
The synthetic method that initializes the enum/switch table is not thread safe,
that is why javac places the initialization in the static initializer of an
anonymous class.
For example, the following program should print ""1999"" 40 times
(once from each thread).
But on my machine, it prints ""default"" 22 times & 1999
18 times.
package com.bea;
public class TestEnumSwitch {
	
	public static synchronized void foo() {}
public static final void main(String args[]) {
final TestEnum e = TestEnum.A1999;
Thread[] runners = new Thread[40];
for (int i = 0; i < runners.length; i++) {
runners[i] = new Thread(new Runnable() {
public void run() {
switch (e) {
case A1:
System.err.println(""1"");
break;
case A2:
System.err.println(""2"");
break;
case A8:
System.err.println(""8"");
break;
case A13:
System.err.println(""13"");
break;
case A1999:
System.err.println(""1999"");
break;
default:
System.err.println(""default"");
break;
}
}
});
}
for (int i = 0; i < runners.length; i++) {
runners[i].start();
}
}
public enum TestEnum {
A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
A1999,
}
}","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99631,2005-06-13T09:21:00.000-05:00,[assist][5.0] Unnecessary proposals on annotation completion,"@B 
 public class Test {}
3 1 RC2
Steps to reproduce:
-> The keyword proposals byte and boolean show up
-> A list of annotations starting with 'B' would be more helpful","org.eclipse.jdt.internal.corext.refactoring.reorg.JavaMoveProcessor
org.eclipse.jdt.internal.codeassist.CompletionEngine"
CLASS,openjpa-2.2.0,OPENJPA-2163,2012-03-27T15:56:55.000-05:00,Lifecycle event callback occurs more often than expect,"final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
 
 MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
 
 ((OpenJPAEntityManagerSPI)em).addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).addLifecycleListener(l2, null);
final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
...
MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
...
((OpenJPAEntityManagerSPI)em).
addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).
addLifecycleListener(l2, null);
When life cycle event occurs for a specific entity manager, all the listeners created under the emf are being invoked.
The expected behavior is only the listener registered in the em from which the life cycle events are related should be called.","openjpa-kernel.src.main.java.org.apache.openjpa.conf.OpenJPAConfigurationImpl
openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.validation.TestValidationMode"
CLASS,openjpa-2.2.0,OPENJPA-2197,2012-05-16T17:10:22.000-05:00,MethodComparator in AnnotationPersistenceMetaDataParser should also compare parameters,"@PreUpdate
    public void updateChangeLog(Object entity)  
 private void updateChangeLog(BaseEntity he, ChangeLogEntry cle)

 
   @PreUpdate
AnnotationPersistenceMetaDataParser contains a MethodComparator which only compares the class + the method name.
Too bad I have (had...) 2 methods with the same name in my EntityListener:
@PreUpdate
    public void updateChangeLog(Object entity) { .
.
and also
private void updateChangeLog(BaseEntity he, ChangeLogEntry cle)
which is a private helper method.
Due to the bug in MethodComparator, my @PreUpdate sometimes didn't get detected.",openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.callbacks.ListenerImpl
CLASS,openjpa-2.2.0,OPENJPA-2255,2012-08-30T20:15:52.000-05:00,Couldn't load the referencedColumn definition when create the JoinTable,"@Entity 
public class Student  
 @Id @Column(name=""id"", length=128, nullable=false) private String id; 
   @Column(name=""sName"", length=255) private String sName; 
   @ManyToMany 
  @JoinTable( 
    name=""student_course_map"", 
    joinColumns={@JoinColumn(name=""student_id"", referencedColumnName=""id"", nullable=false)}, 
    inverseJoinColumns={@JoinColumn(name=""course_id"", referencedColumnName=""id"", nullable=false)} 
  ) 
  public Collection getCourses() 

   
 @Entity 
public class Courses{ 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""cName"", length=255) private String cName; 

  ... 
}
The JoinColumn couldn't have the referencedColumn's definition which includes the length definition.
and it's length  should be assigned to the default value 255.
@Entity 
public class Student { 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""sName"", length=255) private String sName; 
  @ManyToMany 
  @JoinTable( 
    name=""student_course_map"", 
    joinColumns={@JoinColumn(name=""student_id"", referencedColumnName=""id"", nullable=false)}, 
    inverseJoinColumns={@JoinColumn(name=""course_id"", referencedColumnName=""id"", nullable=false)} 
  ) 
  public Collection getCourses()
... 
}
@Entity 
public class Courses{ 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""cName"", length=255) private String cName;
... 
}
We can see the student id length has been defined to 128.
And there is no definition length in the JoinColumn student_id.
The JoinColumn should be set to the default value 255.
The warning message will occur like this
WARN  [Schema] Existing column ""student_id"" on table ""test.student_course_map"" is incompatible with the same column in the given schema definition.
Existing column: 
Full Name: student_course_map.
student_id 
Type: varchar 
Size: 128 
Default: null 
Not Null: true 
Given column: 
Full Name: student_course_map.
student_id 
Type: varchar 
Size: 255 
Default: null 
Not Null: true",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingInfo
CLASS,solr-4.4.0,SOLR-5296,2013-10-02T00:20:01.000-05:00,Creating a collection with implicit router adds shard ranges to each shard,"{quote}
 {quote}
Creating a collection with implicit router adds shard ranges to each shard.
http://localhost:8983/solr/admin/collections?action=CREATE&name=myimplicitcollection3&numShards=2&maxShardsPerNode=5&router.name=implicit&shards=s1,s2&replicationFactor=2
The following clusterstate is created:
{quote}
""myimplicitcollection3"":{
""shards"":{
""s1"":{
""range"":""80000000-ffffffff"",
""state"":""active"",
""replicas"":{
""core_node1"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s1_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node3"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s1_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}},
""s2"":{
""range"":""0-7fffffff"",
""state"":""active"",
""replicas"":{
""core_node2"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s2_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node4"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s2_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}}},
""maxShardsPerNode"":""5"",
""router"":{""name"":""implicit""},
""replicationFactor"":""2""}
{quote}
Collections with implicit router should not have shard ranges at all.
Note that the createshard API does the right thing.",solr.core.src.java.org.apache.solr.cloud.Overseer
FILE,AMQP,AMQP-516,2015-08-06T01:25:34.000-05:00,"Setting autoDelete or exclusive to anything, including ""true"" in @Queue without a queue name results in them being disabled","@RabbitListener(bindings = @QueueBinding(




    value = @Queue(autoDelete = ""true"", exclusive = ""true""),




    exchange = @Exchange(value = ""myFanout"", type = ExchangeTypes.FANOUT, durable = ""true"")




))






   
 if (!StringUtils.hasText(queueName)) {




    queueName = UUID.randomUUID().toString();




    if (!StringUtils.hasText(bindingQueue.exclusive())) {




        exclusive = true;




    }




    if (!StringUtils.hasText(bindingQueue.autoDelete())) {




        autoDelete = true;




    }




}




else {




    exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());




    autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());




}






 
 String e = bindingQueue.exclusive();




if (!StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {




    exclusive = true




}
The following queue declaration will result in a queue being declared with auto delete and exclusive set to false:
@RabbitListener(bindings = @QueueBinding(
value = @Queue(autoDelete = ""true"", exclusive = ""true""),
exchange = @Exchange(value = ""myFanout"", type = ExchangeTypes.FANOUT, durable = ""true"")
))
due to the following code in RabbitListenerAnnotationBeanProcessor:
if (!
StringUtils.hasText(queueName)) {
queueName = UUID.randomUUID().
toString();
if (!
StringUtils.hasText(bindingQueue.exclusive())) {
exclusive = true;
}
if (!
StringUtils.hasText(bindingQueue.autoDelete())) {
autoDelete = true;
}
}
else {
exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());
autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());
}
Making them exclusive and auto delete by default when using a random name seems like a good idea, but it should probably be changed to something like:
String e = bindingQueue.exclusive();
if (!
StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {
exclusive = true
}","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-633,2016-08-18T15:48:45.000-05:00,Non Transactional RabbitTemplate Uses Container Transactional Channel,"RabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager




		.getResource(connectionFactory);




if (resourceHolder != null) {




	Channel channel = resourceFactory.getChannel(resourceHolder);




	if (channel != null) {




		return resourceHolder;




	}




}






    resourceFactory.isSynchedLocalTransactionAllowed()
When running a RabbitTemplate on a transactional container thread, the container channel is used, even if the RabbitTemplate is not marked transactional.
Consider the case where you wish to publish a message when there's an error, while rejecting the inbound message.
If the container is transactional, the published message is rolled back.
If you publish with a template that is not transactional, the publish should occur on a new channel.
RabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager
.
getResource(connectionFactory);
if (resourceHolder !
= null) {
Channel channel = resourceFactory.getChannel(resourceHolder);
if (channel !
= null) {
return resourceHolder;
}
}
We should never return the resourceHolder if the resourceFactory.isSynchedLocalTransactionAllowed() is false.","org.springframework.amqp.rabbit.listener.LocallyTransactedTests
org.springframework.amqp.rabbit.core.RabbitTemplate"
METHOD,commons-math-3-3.0,MATH-905,2012-11-20T14:54:39.000-06:00,"FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts","Math.cosh(709.783)  
 FastMath.cosh(709.783)  
 Math.sinh(709.783)  
 FastMath.sinh(709.783)  
 StrictMath.log(Double.MAX_VALUE) 
 double t = exp(x*0.5);
return (0.5*t)*t;
 
 double t = exp(-x*0.5);
return (-0.5*t)*t;
As reported by Jeff Hain:
cosh(double) and sinh(double):
Math.cosh(709.783) = 8.991046692770538E307
FastMath.cosh(709.783) = Infinity
Math.sinh(709.783) = 8.991046692770538E307
FastMath.sinh(709.783) = Infinity
===> This is due to using exp( x )/2 for values of |x|
above 20: the result sometimes should not overflow,
but exp( x ) does, so we end up with some infinity.
===> for values of |x| >= StrictMath.log(Double.MAX_VALUE),
exp will overflow, so you need to use that instead:
for x positive:
double t = exp(x*0.5);
return (0.5*t)*t;
for x negative:
double t = exp(-x*0.5);
return (-0.5*t)*t;",org.apache.commons.math3.util.FastMathTest:testHyperbolicInverses()
FILE,DATACMNS,DATACMNS-114,2011-12-19T03:21:41.000-06:00,Wrong custom implementation automatically detected,"AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...)  getImplementationClassName()
When automatically scanning the repositories, and their custom implementation, the wrong custom implementation is wired to our repository bean.
Resulting in the following exception:
Caused by: java.lang.IllegalArgumentException: No property find found for type class com.myproject.Contract
at org.springframework.data.repository.query.parser.Property.<init>(Property.java:76)
at org.springframework.data.repository.query.parser.Property.<init>(Property.java:97)
at org.springframework.data.repository.query.parser.Property.create(Property.java:312)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:292)
at org.springframework.data.repository.query.parser.Property.from(Property.java:251)
at org.springframework.data.repository.query.parser.Property.from(Property.java:232)
at org.springframework.data.repository.query.parser.Part.<init>(Part.java:48)
at org.springframework.data.repository.query.parser.PartTree$OrPart.<init>(PartTree.java:242)
at org.springframework.data.repository.query.parser.PartTree.buildTree(PartTree.java:101)
at org.springframework.data.repository.query.parser.PartTree.<init>(PartTree.java:77)
at org.springframework.data.jpa.repository.query.PartTreeJpaQuery.<init>(PartTreeJpaQuery.java:56)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:92)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateIfNotFoundQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:159)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$AbstractQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:71)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.<init>(RepositoryFactorySupport.java:303)
at org.springframework.data.repository.core.support.RepositoryFactorySupport.getRepository(RepositoryFactorySupport.java:157)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:120)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:39)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:142)
... 67 more
When starting the application context, the contractRepository bean is linked to our anotherContractRepositoryImpl rather than the contractRepositoryImpl.
This behavior seems to be operating system dependent, as it only occurs on our Linux CI server.
The cause of our problem can be found at AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...).
It might be desirable to scan on getImplementationClassName() before applying the wildcard prefix.",org.springframework.data.repository.config.AbstractRepositoryConfigDefinitionParser
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,DomainClassConverter should gracefully return null for null sources or empty strings,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
I've noticed an important issue related to automatic web binding of String id to Domain class.
When posting a new Order where Order.customer == """" then a converter exception is thrown:
Failed to convert property value of type java.lang.String to required type org.mycomp.domain.Customer for property customer; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @javax.
validation.constraints.NotNull @javax.
persistence.ManyToOne org.mycomp.domain.Customer for value '; nested exception is org.springframework.dao.InvalidDataAccessApiUsageException: The given id must not be null!
; nested exception is java.lang.IllegalArgumentException: The given id must not be null!
I think it should not try to convert to Domain class if id is null or empty.
And note that for optional references this even might even cause a complete blocker?
<form:select path=""customer"">
<form:option value="""" label=""Select"" />
<form:options items=""${customers}"" itemValue=""id""></form:options>
</form:select>","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
CLASS,derby-10.9.1.0,DERBY-3024,2007-08-23T05:24:31.000-05:00,Validation of shared plans hurts scalability,"GenericPreparedStatement.upToDate()   BaseActivation.checkStatementValidity()
To investigate whether there was anything in the SQL execution layer that prevented scaling on a multi-CPU machine, I wrote a multi-threaded test which continuously executed ""VALUES 1"" using a PreparedStatement. I ran the test on a machine with 8 CPUs and expected the throughput to be proportional to the number of concurrent clients up to 8 clients (the same as the number of CPUs). However, the throughput only had a small increase from 1 to 2 clients, and adding more clients did not increase the throughput. Looking at the test in a profiler, it seems like the threads are spending a lot of time waiting to enter synchronization blocks in GenericPreparedStatement.upToDate() and BaseActivation.checkStatementValidity() (both of which are synchronized on the a GenericPreparedStatement object).
That means the threads still did the same work, but each thread got its own plan (GenericPreparedStatement object) since the statement cache didn't regard the SQL text strings as identical.
When I made that change, the test scaled more or less perfectly up to 8 concurrent threads.
We should try to find a way to make the scalability the same regardless of whether or not the threads share the same plan.","java.engine.org.apache.derby.impl.store.access.heap.HeapConglomerateFactory
java.engine.org.apache.derby.impl.store.raw.data.FileContainer
java.engine.org.apache.derby.impl.store.raw.data.RAFContainer
java.testing.org.apache.derbyTesting.functionTests.tests.lang.DBInJarTest
java.engine.org.apache.derby.impl.store.raw.data.TempRAFContainer
java.engine.org.apache.derby.impl.store.raw.data.InputStreamContainer
java.engine.org.apache.derby.impl.store.access.btree.index.B2IFactory"
CLASS,derby-10.9.1.0,DERBY-5251,2011-05-29T04:27:15.000-05:00,make ErrorCodeTest pass in non-English locale,"test_errorcode(org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
)
lang.ErrorCodeTest will fail in Chinese Locale:
D:\derby\test>java junit.textui.TestRunner org.apache.derbyTesting.functionTests
.
tests.lang.ErrorCodeTest
.
F
Time: 4.797
There was 1 failure:
1 test_errorcode(org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
)junit.framework.AssertionFailedError: Column value mismatch @ column 'MESSAGE',
row 1:
Expected: >At least one parameter to the current statement is uninitialized.
<
Found:    >当前语句中至少一个参数未初始化。<
at org.apache.derbyTesting.junit.JDBC.assertRowInResultSet(JDBC.java:121
3
at org.apache.derbyTesting.junit.JDBC.assertRowInResultSet(JDBC.java:112
5
at org.apache.derbyTesting.junit.JDBC.assertFullResultSetMinion(JDBC.jav
a:1012)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:935)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:892)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:850)
at org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest.test_e
rrorcode(ErrorCodeTest.java:88)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:
112)
FAILURES!!!
Tests run: 1,  Failures: 1,  Errors: 0
D:\derby\test>",java.testing.org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,Client should use a prepared statement rather than regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
o.a.d.client.am.Connection setTransactionIsolation() uses a Statement which  it builds up each time for setTransactionIsolation()  is called.
private Statement setTransactionIsolationStmt = null;
...
setTransactionIsolationStmt =
                    createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
....
private void setTransactionIsolationX(int level)
...
            setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);
It would be better for performance and also for avoid possible garbage collection issues, to have a single prepared statement with a parameter marker.
import java.sql.
*;
import java.net.
*;
import java.io.
*;
import org.apache.derby.drda.NetworkServerControl;
/**
* Client template starts its own NetworkServer and runs some SQL against it.
* The SQL or JDBC API calls can be modified to reproduce issues
*
*/public class SetTransactionIsolation {
public static Statement s;
public static void main(String[] args) throws Exception {
try {
// Load the driver.
Not needed for network server.
Class.forName(""org.apache.derby.jdbc.ClientDriver"");
// Start Network Server
startNetworkServer();
// If connecting to a customer database.
Change the URL
Connection conn = DriverManager
.
getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
// clean up from a previous run
s = conn.createStatement();
try {
s.executeUpdate(""DROP TABLE T"");
} catch (SQLException se) {
if (!
se.getSQLState().
equals(""42Y55""))
throw se;
}
for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
}
// rs.close();
// ps.close();
runtimeInfo();
conn.close();
// Shutdown the server
shutdownServer();
} catch (SQLException se) {
while (se !
= null) {
System.out.println(""SQLState="" + se.getSQLState()
+ se.getMessage());
se.printStackTrace();
se = se.getNextException();
}
}
}
/**
* starts the Network server
*
*/
public static void startNetworkServer() throws SQLException {
Exception failException = null;
try {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.start(new PrintWriter(System.out));
// Wait for the network server to start
boolean started = false;
int retries = 10; // Max retries = max seconds to wait
while (!
started && retries > 0) {
try {
// Sleep 1 second and then ping the network server
Thread.sleep(1000);
networkServer.ping();
// If ping does not throw an exception the server has
// started
started = true;
} catch (Exception e) {
retries--;
failException = e;
}
}
// Check if we got a reply on ping
if (!
started) {
throw failException;
}
} catch (Exception e) {
SQLException se = new SQLException(""Error starting network  server"");
se.initCause(failException);
throw se;
}
}
public static void shutdownServer() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.shutdown();
}
public static void runtimeInfo() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
System.out.println(networkServer.getRuntimeInfo());
}
}",java.client.org.apache.derby.client.am.Connection
METHOD,time,28,2013-05-31T00:52:24.000-05:00,Questionable behaviour of GJChronology when dates pass 1BC,"Chronology chronology = GJChronology.getInstance();

LocalDate start = new LocalDate(2013, 5, 31, chronology);
LocalDate expectedEnd = new LocalDate(-1, 5, 31, chronology); // 1 BC
assertThat(start.minusYears(2013), is(equalTo(expectedEnd)));
assertThat(start.plus(Period.years(-2013)), is(equalTo(expectedEnd)));
```
Chronology chronology = GJChronology.getInstance();
LocalDate start = new LocalDate(2013, 5, 31, chronology);
LocalDate expectedEnd = new LocalDate(-1, 5, 31, chronology); // 1 BC
assertThat(start.minusYears(2013), is(equalTo(expectedEnd)));
assertThat(start.plus(Period.years(-2013)), is(equalTo(expectedEnd)));
```
The error it gives is:
```
org.joda.time.IllegalFieldValueException: Value 0 for year is not supported
```
However, I never provided ""0"" for the year myself.
I thought it was the job of the framework to skip over non-existent year 0 for me to return 1 BC?","org.joda.time.chrono.GJChronology:getInstance(DateTimeZone, ReadableInstant, int)
org.joda.time.chrono.GJChronology:add(long, long)
org.joda.time.chrono.GJChronology:add(long, int)"
METHOD,time,88,2013-11-25T19:15:46.000-06:00,Constructing invalid Partials,"Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).with(hourOfDay(), 1);
assert(a == b);
 
 new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
 
 new Partial(clockhourOfDay(), 1)  with(hourOfDay(), 1)  isEqual(new Partial(hourOfDay() ,1).with(clockhourOfDay(), 1)) // throws objects must have matching field types
Partials can be constructed by invoking a constructor `Partial(DateTimeFieldType[], int[])` or by merging together a set of partials using `with`, each constructed by calling `Partial(DateTimeFieldType, int)`, e.g.:
``` java
Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).
with(hourOfDay(), 1);
assert(a == b);
```
``` java
new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).
with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
```
I suppose the Partials should not allow to be constructed in either case.
Is that right?
There's also a related issue (probably stems from the fact that the Partial is invalid):
``` java
new Partial(clockhourOfDay(), 1).
with(hourOfDay(), 1).
isEqual(new Partial(hourOfDay() ,1).
with(clockhourOfDay(), 1)) // throws objects must have matching field types
```","org.joda.time.Partial:with(DateTimeFieldType, int)"
FILE,COMPRESS,COMPRESS-245,2013-12-05T11:01:39.000-06:00,TarArchiveInputStream#getNextTarEntry returns null prematurely,"FileInputStream fin = new FileInputStream(""exampletar.tar.gz"");

GZIPInputStream gin = new GZIPInputStream(fin);

TarArchiveInputStream tin = new TarArchiveInputStream(gin);            TarArchiveEntry entry;

              tin.getNextTarEntry()
The attached archive decompressed with 1.6 only extracts part of the archive.
This does not happen with version 1.5
FileInputStream fin = new FileInputStream(""exampletar.tar.gz"");
GZIPInputStream gin = new GZIPInputStream(fin);
TarArchiveInputStream tin = new TarArchiveInputStream(gin);            TarArchiveEntry entry;
while ((entry = tin.getNextTarEntry()) !
= null) {
The file is created with
tar cvzf
in RHEL 6.5 and the contents look like this when extracted with the same tool:
topdirectory/
topdirectory/about.html
topdirectory/.
eclipseproduct
topdirectory/plugins/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/about.html
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/launcher.
gtk.linux.x86_64.
properties
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/eclipse_1206.
so
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/about.html
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/fragment.properties
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/.
api_description
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/runtime_registry_compatibility.jar
topdirectory/configuration/
topdirectory/configuration/config.ini
topdirectory/icon.
xpm
topdirectory/about_files/
topdirectory/about_files/pixman-licenses.txt
topdirectory/about_files/mpl-v11.txt
topdirectory/about_files/about_cairo.html
topdirectory/libcairo-swt.
so
with commons-compress-1.6 it looks like this:
topdirectory/
topdirectory/about.html
topdirectory/.
eclipseproduct
topdirectory/plugins
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/about.html
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/launcher.
gtk.linux.x86_64.
properties
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/eclipse_1206.
so
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/about.html
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/fragment.properties
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/.
api_description
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF","org.apache.commons.compress.archivers.tar.TarArchiveInputStream
org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest"
FILE,COMPRESS,COMPRESS-273,2014-04-11T04:13:32.000-05:00,NullPointerException when creation fields/entries from scratch,"org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
    org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();
The API has public default constructors for many data types.
However, when these 0-argument constructors are used, certain internal references are null, resulting in a NullPointerException soon after.
This also applies to some 1-argument constructors where two references should be set before get... is used later.
Either (1) these constructors should be non-public, (2) there should be documentation that certain fields need to be set later for an instance to be usable.
In the latter case, there must be public set methods for the missing data.
org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();","org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField
org.apache.commons.compress.archivers.cpio.CpioArchiveEntry
org.apache.commons.compress.archivers.zip.ExtraFieldUtils
org.apache.commons.compress.archivers.zip.UnrecognizedExtraField"
FILE,COMPRESS,COMPRESS-357,2016-05-25T17:50:50.000-05:00,BZip2CompressorOutputStream can affect output stream incorrectly,"finished()  
  
 s.close();
s = null;
  finalize()  finish()  finish()  
 finish()  
 finalize() 
 finalize()
BZip2CompressorOutputStream has an unsynchronized finished() method, and an unsynchronized finalize method.
Finish checks to see if the output stream is null, and if it is not it calls various methods, some of which write to the output stream.
Now, consider something like this sequence.
BZip2OutputStream s = ...
...
s.close();
s = null;
After the s = null, the stream is garbage.
At some point the garbage collector call finalize(), which calls finish().
But, since the GC may be on a different thread, there is no guarantee that the assignment this.out = null in finish() has actually been made visible to the GC thread, which results in bad data in the output stream.
This is not a theoretical problem; In a part of a large project I'm working on, this happens about 2% of the time.
The fixes are simple
1) synchronize finish() or
2) don't call finish from finalize().
A workaround is to derive a class and override the finalize() method.",org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream
METHOD,mahout-0.8,MAHOUT-1317,2013-08-23T13:05:58.000-05:00,Clarify some of the messages in Preconditions.checkArgument,"Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"");
In experimenting with things, I was getting some errors from RowSimilarityJob, that in looking at the source I realized were a little incomplete as to what the true issue was.
In this case, they were of the form:
Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"")
;
Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's ""incorrect"", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.
A quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.","org.apache.mahout.math.als.AlternatingLeastSquaresSolver:addLambdaTimesNuiTimesE(Matrix, double, int)
org.apache.mahout.utils.SplitInput:validate()
org.apache.mahout.math.neighborhood.FastProjectionSearch:FastProjectionSearch(DistanceMeasure, int, int)
org.apache.mahout.classifier.naivebayes.training.WeightsMapper:setup(Context)
org.apache.mahout.cf.taste.example.kddcup.KDDCupDataModel:KDDCupDataModel(File, boolean, double)
org.apache.mahout.math.neighborhood.ProjectionSearch:ProjectionSearch(DistanceMeasure, int, int)
org.apache.mahout.classifier.df.mapreduce.partial.TreeID:TreeID(int, int)
org.apache.mahout.math.neighborhood.SearchQualityTest.StripWeight:apply(WeightedThing<Vector>)
org.apache.mahout.classifier.df.data.Dataset:valueOf(int, String)
org.apache.mahout.classifier.df.mapreduce.partial.TreeID:treeId()
org.apache.mahout.classifier.df.data.DataLoader:parseString(Attribute[], Set<String>[], CharSequence, boolean)
org.apache.mahout.cf.taste.impl.common.WeightedRunningAverage:changeDatum(double, double)
org.apache.mahout.math.als.AlternatingLeastSquaresSolver:solve(Iterable<Vector>, Vector, double, int)
org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel:CassandraDataModel(String, int, String)
org.apache.mahout.math.als.AlternatingLeastSquaresSolver:createRiIiMaybeTransposed(Vector)
org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator:SamplingLongPrimitiveIterator(RandomWrapper, LongPrimitiveIterator, double)
org.apache.mahout.math.neighborhood.BruteSearch:search(Vector, int)
org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity.ItemItemSimilarity:ItemItemSimilarity(long, long, double)
org.apache.mahout.math.random.ChineseRestaurant:ChineseRestaurant(double, double)
org.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity.UserUserSimilarity:UserUserSimilarity(long, long, double)"
CLASS,bookkeeper-4.1.0,BOOKKEEPER-371,2012-08-17T05:42:02.000-05:00,NPE in hedwig hub client causes hedwig hub to shut down.,"Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .messageConsumed(messageConsumeData.msg);

  getPipeline()  getLast()   channel.close()   messageConsumed()
The hedwig client was connected to a remote region hub that restarted resulting in the channel getting disconnected.
2012-08-15 17:47:42,443 - ERROR - [pool-20-thread-1:TerminateJVMExceptionHandler@28] - Uncaught exception in thread pool-20-thread-1 java.lang.NullPointerException
at org.apache.hedwig.client.netty.HedwigClientImpl.getResponseHandlerFromChannel(HedwigClientImpl.java:323)
at org.apache.hedwig.client.handlers.MessageConsumeCallback.operationFinished(MessageConsumeCallback.java:75)
at org.apache.hedwig.client.handlers.MessageConsumeCallback.operationFinished(MessageConsumeCallback.java:41)
at org.apache.hedwig.server.regions.RegionManager$1$1$1.operationFinished(RegionManager.java:208)
at org.apache.hedwig.server.regions.RegionManager$1$1$1.operationFinished(RegionManager.java:202)
at org.apache.hedwig.server.persistence.ReadAheadCache$PersistCallback.operationFinished(ReadAheadCache.java:194)
at org.apache.hedwig.server.persistence.ReadAheadCache$PersistCallback.operationFinished(ReadAheadCache.java:171)
at org.apache.hedwig.server.persistence.BookkeeperPersistenceManager$PersistOp$1.safeAddComplete(BookkeeperPersistenceManager.java:548)
at org.apache.hedwig.zookeeper.SafeAsynBKCallback$AddCallback.addComplete(SafeAsynBKCallback.java:93)
at org.apache.bookkeeper.client.PendingAddOp.submitCallback(PendingAddOp.java:165)
at org.apache.bookkeeper.client.LedgerHandle.sendAddSuccessCallbacks(LedgerHandle.java:643)
at org.apache.bookkeeper.client.PendingAddOp.writeComplete(PendingAddOp.java:159)
at org.apache.bookkeeper.proto.PerChannelBookieClient.handleAddResponse(PerChannelBookieClient.java:577)
at org.apache.bookkeeper.proto.PerChannelBookieClient$7.safeRun(PerChannelBookieClient.java:525)
at org.apache.bookkeeper.util.SafeRunnable.run(SafeRunnable.java:31)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)
At 2012-08-15 17:47:42,443, the channel was disconnected as well.
I believe the following code in the MessageConsumeCallback is causing this problem.
Channel topicSubscriberChannel = client.getSubscriber().
getChannelForTopic(topicSubscriber);
HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).
getSubscribeResponseHandler()
.
messageConsumed(messageConsumeData.msg);
The channel was retrieved without checking if it was closed and then getPipeline().
getLast() was called which returned a null value resulting in a NPE.
Moreover, we need to check if the returned Response handler is not null because there is a race here if channel.close() is called after we retrieve the channel and before we call messageConsumed().
I guess the same applies for other instances where we use this.
Does the above explanation seem right?","hedwig-client.src.main.java.org.apache.hedwig.client.netty.WriteCallback
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.SubscribeResponseHandler
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigPublisher
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigSubscriber
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.MessageConsumeCallback
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigClientImpl"
CLASS,bookkeeper-4.1.0,BOOKKEEPER-376,2012-08-22T13:32:57.000-05:00,LedgerManagers should consider 'underreplication' node as a special Znode,"{noformat}
     
 {noformat}
Saw this while running the RW tests:
{noformat}
2012-08-22 23:59:35,649 - WARN  - [GarbageCollectorThread:HierarchicalLedgerManager@354] - Exception during garbage collecting ledgers for underreplication of /ledgers
java.io.IOException: java.lang.NumberFormatException: For input string: ""underreplicationlocks0000""
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getLedgerId(HierarchicalLedgerManager.java:236)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getStartLedgerIdByLevel(HierarchicalLedgerManager.java:254)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.doGcByLevel(HierarchicalLedgerManager.java:388)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.garbageCollectLedgers(HierarchicalLedgerManager.java:351)
at org.apache.bookkeeper.bookie.GarbageCollectorThread.doGcLedgers(GarbageCollectorThread.java:226)
at org.apache.bookkeeper.bookie.GarbageCollectorThread.run(GarbageCollectorThread.java:195)
Caused by: java.lang.NumberFormatException: For input string: ""underreplicationlocks0000""
at java.lang.NumberFormatException.forInputString(Unknown Source)
at java.lang.Long.parseLong(Unknown Source)
at java.lang.Long.parseLong(Unknown Source)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getLedgerId(HierarchicalLedgerManager.java:234)
... 5 more
{noformat}","bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.LedgerLayout
bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.AbstractZkLedgerManager"
FILE,swt-3.1,102794,2005-07-05T17:56:00.000-05:00,GridLayout has change behaviour between 3.0.2 and 3.1,"public static void main(String[] args) {
        Display display = new Display();

        Shell shell = new Shell(display);

        shell.setLayout(new FillLayout());

        ScrolledComposite sc1 = new ScrolledComposite(shell, SWT.H_SCROLL
                | SWT.V_SCROLL);
        Composite editor = new Composite(sc1, SWT.SHADOW_NONE);
        sc1.setContent(editor);
        sc1.setLayout(new FillLayout());

        GridLayout layout = new GridLayout();

        layout.numColumns = 6;
        layout.makeColumnsEqualWidth = true;
        editor.setLayout(layout);

        Label boxLabel = new Label(editor, SWT.NONE);
        boxLabel.setText(""My label"");

        Text textBox = new Text(editor, SWT.H_SCROLL | SWT.V_SCROLL | SWT.MULTI
                | SWT.BORDER);
        textBox.setText(""Some text for the text box\nAlso with a new line"");

        // do layout bits
        GridData labelData = new GridData(SWT.RIGHT, SWT.TOP, false, false);
        boxLabel.setLayoutData(labelData);

        GridData textBoxData = new GridData(SWT.FILL, SWT.CENTER, true, false,
                5, 1);
        textBoxData.widthHint = 400;
        textBox.setLayoutData(textBoxData);

        sc1.setExpandHorizontal(true);
        sc1.setExpandVertical(true);
        sc1.setMinSize(editor.computeSize(SWT.DEFAULT, SWT.DEFAULT));

        shell.pack();
        shell.open();

        while (!shell.isDisposed()) {
            if (!display.readAndDispatch())
                display.sleep();
        }
        display.dispose();
    }
Running the following problem on 3.0.2 and 3.1 shows a difference in behaviour:
public static void main(String[] args) {
Display display = new Display();
Shell shell = new Shell(display);
shell.setLayout(new FillLayout());
ScrolledComposite sc1 = new ScrolledComposite(shell, SWT.H_SCROLL
| SWT.V_SCROLL);
Composite editor = new Composite(sc1, SWT.SHADOW_NONE);
sc1.setContent(editor);
sc1.setLayout(new FillLayout());
GridLayout layout = new GridLayout();
layout.numColumns = 6;
layout.makeColumnsEqualWidth = true;
editor.setLayout(layout);
Label boxLabel = new Label(editor, SWT.NONE);
boxLabel.setText(""My label"");
Text textBox = new Text(editor, SWT.H_SCROLL | SWT.V_SCROLL | SWT.MULTI
| SWT.BORDER);
textBox.setText(""Some text for the text box\nAlso with a new line"");
// do layout bits
GridData labelData = new GridData(SWT.RIGHT, SWT.TOP, false, false);
boxLabel.setLayoutData(labelData);
GridData textBoxData = new GridData(SWT.FILL, SWT.CENTER, true, false,
5, 1);
textBoxData.widthHint = 400;
textBox.setLayoutData(textBoxData);
sc1.setExpandHorizontal(true);
sc1.setExpandVertical(true);
sc1.setMinSize(editor.computeSize(SWT.DEFAULT, SWT.DEFAULT));
shell.pack();
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch())
display.sleep();
} display.dispose();
}
Basically on 3.0.2 the window that appears has a label of about 80 pixels wide and a textbox of 400 pixels wide.
With 3.1 the label is about 400 pixels wide, with the text box being about 2000 pixels wide.
This appears to be a combination of the text box spanning 5 columns and the use of layout.makeColumnsEqualWidth = true;
Turning off makeColumnsEqualWidth helps but it means that the real app this if from ends up looking untidy.
Using minimumWidth instead of widthHint doesn't help.
Commenting out the minimumWidth line helps, but the form ends up being wider than I'd like.
The effect I'm trying to achieve is that labels are 1 column wide then text boxes are either 2 or 5 columns wide (so some rows get two labels and text boxes)  The scrolled composite is needed because in the real app the forms are actually within a TabItem, so I need the ability to scroll.",org.eclipse.swt.layout.GridLayout
FILE,swt-3.1,104150,2005-07-16T19:58:00.000-05:00,[Patch] Table cursor separated from table selection when clicking on grid lines or empty space,"table.getLinesVisible()  
 table.setLinesVisible(true)
SWT-win32, v3138 (3.1-final)
When using a table cursor, there are two kinds of table regions that have the potential to separate the table cursor from the table selection when clicked on:
1) grid lines (table.getLinesVisible() == true)
2) empty space to the left of the first cell of each row (SWT.FULL_SELECTION)
Expected behaviour:
No matter which part of the table the user clicks on, the table cursor should follow the table selection as closely as possible if the selection is changed as a result of the click.",org.eclipse.swt.custom.TableCursor
FILE,swt-3.1,104545,2005-07-20T14:21:00.000-05:00,Make default size of empty composites smaller,"static final int DEFAULT_WIDTH	= 64;
 static final int DEFAULT_HEIGHT	= 64;
Could the following two constants in Widget.java be changed to something smaller?
/* Default widths for widgets */ static final int DEFAULT_WIDTH	= 64;
static final int DEFAULT_HEIGHT	= 64;
Anything less than or equal to 16 would work for us, 0 would be OK too.
I have run our tests (JFace, UI, RCP) and they run fine when the constants are 0.
Background: When you write an RCP app and enable the cool bar, the cool bar will initially be empty, but 64x64 pixels in size.
On Windows, you cannot see the border of the empty coolbar so the user gets a big empty space at the top of their window and might be confused.
See also Bug 70049, where the same problem occurs in an RCP application that starts off with no open perspective and thus no cool bar items.",org.eclipse.swt.widgets.CoolBar
FILE,swt-3.1,78634,2004-11-15T12:29:00.000-06:00,ImageData.getTransparencyMask - incorrect javadoc or implementation wrong,"public ImageData getTransparencyMask()
The implementation of getTransparencyMask appears to return a fully opaque mask when the image has no transparency.
The javadoc seems to infer that it would return null in that case.
/**
* Returns an <code>ImageData</code> which specifies the
* transparency mask information for the receiver, or null if the
* receiver has no transparency and is not an icon.
*
* @return the transparency mask or null if none exists
*/ public ImageData getTransparencyMask()
(see implementation of ImageData.colorMask)
On a different note, should we return a transparent mask based on the the alphaData values with the 127 threshold?",org.eclipse.swt.graphics.ImageData
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,Table fails to setTopIndex after new items are added to the table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
I am working on a table viewer that keeps track of the scroll bar and loads content into the table dynamically as the user scrolls to the end of the table.
Items could be added/removed from the table as the user scrolls.
To maintain the position of the table, I call setTopIndex at the end of the update.
I have created a small testcase to simulate the process.
public static void main(String[] args) { final Display display = new Display();
Shell shell = new Shell(display);
shell.setBounds(10,10,200,200);
final Table table = new Table(shell, SWT.NONE);
table.setBounds(10,10,100,100);
for (int i = 0; i < 99; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
} table.setTopIndex(20);
shell.open();
System.out.println(""top visible index: "" + table.getTopIndex());
for (int i = 0; i < 5; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
}
table.setTopIndex(40);
System.out.println(""top visible index: "" + table.getTopIndex());
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}
Table.setTopIndex fails to position to the correct table item if new items are added to the table after the shell is opened.
The first call to setTopIndex succeeds.
The table is correctly positioned at item 20.
After adding new table items to the table, calling setTopTable(40) has no effect.
Calling getTopIndex continues to return 20.
Expected Result:
Calling setTopIndex(40) should move table item #40 to the top of the table.
Calling getTopIndex after should return 40.
If the last 5 items are added before the shell is opened, setTopIndex to 40 will also succeed.
The testcase works as expected on Windows.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
I have only verified this with JPEG output.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,87460,2005-03-08T21:22:00.000-06:00,StyledText: Caret location not updated when line style is used,"import org.eclipse.swt.*;
import org.eclipse.swt.custom.*;
import org.eclipse.swt.graphics.*;
import org.eclipse.swt.layout.*;
import org.eclipse.swt.widgets.*;

public class LineStyleCaretTest {
  public static void main(String[] args) {
    Display display = new Display();
    
    Shell shell = new Shell(display);
    shell.setLayout(new FillLayout());
    
    Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
      
    final StyledText text = new StyledText(shell, SWT.MULTI);
    text.setFont(font);
    text.setText(""Standard Widget Toolkit"");
    text.setCaretOffset(text.getText().length());
    
    text.addLineStyleListener(new LineStyleListener() {
      public void lineGetStyle(LineStyleEvent event) {
        StyleRange[] styles = new StyleRange[1];
        
        styles[0] = new StyleRange();
        styles[0].start  = 0;
        styles[0].length = text.getText().length();
        styles[0].fontStyle = SWT.BOLD;
        
        event.styles = styles;
      }
    });
    
    shell.setSize(300, 100);
    shell.open();
    
    while (!shell.isDisposed()) {
      if (!display.readAndDispatch()) {
        display.sleep();
      }
    }
    
    font.dispose();
    display.dispose();
  }
}
SWT-win32, v3124
In the line style listener, a bold font style is set, changing the width of the rendered text.
I would have expected the on-screen caret location (not the offset) to be adjusted to the change.
However, this does not happen.
For an italic style, it does not look right either.
Might be a bug?
---
import org.eclipse.swt.
*;
import org.eclipse.swt.custom.
*;
import org.eclipse.swt.graphics.
*;
import org.eclipse.swt.layout.
*;
import org.eclipse.swt.widgets.
*;
public class LineStyleCaretTest { public static void main(String[] args) {
Display display = new Display();
Shell shell = new Shell(display);
shell.setLayout(new FillLayout());
Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
final StyledText text = new StyledText(shell, SWT.MULTI);
text.setFont(font);
text.setText(""Standard Widget Toolkit"");
text.setCaretOffset(text.getText().
length());
text.addLineStyleListener(new LineStyleListener() { public void lineGetStyle(LineStyleEvent event) {
StyleRange[] styles = new StyleRange[1];
styles[0] = new StyleRange();
styles[0].
start  = 0;
styles[0].
length = text.getText().
length();
styles[0].
fontStyle = SWT.BOLD;
event.styles = styles;
}
});
shell.setSize(300, 100);
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch()) { display.sleep();
}
} font.dispose();
display.dispose();
}
}",org.eclipse.swt.custom.StyledText
FILE,swt-3.1,93724,2005-05-04T17:35:00.000-05:00,Drag-and-drop creates signal names every time,"byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);	
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
Rather than converting the names for the signals every time, these signal names should be defined in OS.java so that they can be only created once.","org.eclipse.swt.dnd.DropTarget
org.eclipse.swt.dnd.DragSource"
FILE,swt-3.1,97651,2005-05-31T14:43:00.000-05:00,tree insert mark cheese,"Tree.redraw() 
 public static void main(String[] args) {
	final Display display = new Display();
	final Shell shell = new Shell(display);
	shell.setBounds(10, 10, 300, 300);
	final Tree tree = new Tree(shell, SWT.NONE);
	tree.setBounds(10, 10, 200, 200);
	new TreeItem(tree, SWT.NONE).setText(""pre-root"");
	TreeItem root1 = new TreeItem(tree, SWT.NONE);
	root1.setText(""root"");
	TreeItem child = new TreeItem(root1, SWT.NONE);
	child.setText(""child"");
	Button button = new Button(shell, SWT.PUSH);
	button.setBounds(230,10,30,30);
	button.addSelectionListener(new SelectionAdapter() {
		public void widgetSelected(SelectionEvent e) {
			tree.redraw();
		}
	});
	root1.setExpanded(true);
	tree.setInsertMark(root1, false);
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose();
}
3.1RC1
-> problem 1: this makes most of the insert line go away, except for its pointy ends.
This line should not go away because it belongs to the ""root"" item, not to the ""child"" item, but if it really wants to go away, then its end tips should not be left
- press the button to the right of the Table: this does a Tree.redraw(), and note that the insert line reappears, so I guess it never really meant to go away
-> problem 2: now expand the root item again and its insert mark gets copied to below the child item in addition to its initial location.
This is cheese, as can be seen by damaging part of this line with another window
public static void main(String[] args) { final Display display = new Display();
final Shell shell = new Shell(display);
shell.setBounds(10, 10, 300, 300);
final Tree tree = new Tree(shell, SWT.NONE);
tree.setBounds(10, 10, 200, 200);
new TreeItem(tree, SWT.NONE).
setText(""pre-root"");
TreeItem root1 = new TreeItem(tree, SWT.NONE);
root1.setText(""root"");
TreeItem child = new TreeItem(root1, SWT.NONE);
child.setText(""child"");
Button button = new Button(shell, SWT.PUSH);
button.setBounds(230,10,30,30);
button.addSelectionListener(new SelectionAdapter() { public void widgetSelected(SelectionEvent e) { tree.redraw();
}
});
root1.setExpanded(true);
tree.setInsertMark(root1, false);
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}","org.eclipse.swt.dnd.TreeDragUnderEffect
org.eclipse.swt.widgets.Tree"
CLASS,pig-0.8.0,PIG-1188,2010-01-14T13:32:46.000-06:00,Padding nulls to the input tuple according to input schema,"{code}
  as (a0, a1);
dump a;
{code}
 
 {code}
 
 {code}
 
 {code}
 
 {code}

 
 {code}
 
 {code}
Currently, the number of fields in the input tuple is determined by the data.
When we have schema, we should generate input data according to the schema, and padding nulls if necessary.
{code}
a = load '1.
txt' as (a0, a1);
dump a;
{code}
{code}
1       2
1       2       3
1
{code}
Current result:
{code}
(1 2)
(1 2,3)
(1
{code}
Desired result:
{code}
(1 2)
(1 2)
(1 null)
{code}","test.org.apache.pig.test.TestMergeForEachOptimization
src.org.apache.pig.newplan.logical.rules.TypeCastInserter
test.org.apache.pig.test.TestNewPlanLogicalOptimizer
test.org.apache.pig.test.TestNewPlanFilterRule
test.org.apache.pig.test.TestNewPlanPushDownForeachFlatten
test.org.apache.pig.test.TestEvalPipeline2
test.org.apache.pig.test.TestMultiQueryCompiler
test.org.apache.pig.test.TestPartitionFilterPushDown
test.org.apache.pig.test.TestNewPlanFilterAboveForeach"
CLASS,pig-0.8.0,PIG-1277,2010-03-05T13:02:03.000-06:00,Pig should give error message when cogroup on tuple keys of different inner type,"UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

 
 {code}
 
  
 by (c0, c1);
dump e;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}

 
 {code}
  {(1,1)}  {(1,1)} 
 {code}

 
 {code}
  {(1,1)}  {} 
 {}  {(1,1)} 
 {code}
When we cogroup on a tuple, if the inner type of tuple does not match, we treat them as different keys.
This is confusing.
It is desirable to give error/warnings when it happens.
txt' as (a0);
b = foreach a generate a0, MapGenerate(*) as m:map[];
c = foreach b generate a0, m#'key' as key;
d = load '2.
txt' as (c0, c1);
e = cogroup c by (a0, key), d by (c0, c1);
dump e;
{code}
1 txt
{code}
1
{code}
2 txt
{code}
1 1
{code}
User expected result (which is not right):
{code}
((1 1),{(1,1)},{(1,1)})
{code}
Real result:
{code}
((1 1),{(1,1)},{})
((1 1),{},{(1,1)})
{code}
We shall give user the message that we can not merge the key due to the type mismatch.","src.org.apache.pig.impl.io.NullableBytesWritable
test.org.apache.pig.test.TestPackage
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator
src.org.apache.pig.backend.hadoop.HDataType
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestSecondarySort
src.org.apache.pig.newplan.logical.relational.LOUnion"
CLASS,pig-0.8.0,PIG-1785,2011-01-04T17:20:28.000-06:00,New logical plan: uid conflict in flattened fields,"{code}
 
 b0>b2;
dump c;
{code}

 
 {(1,2),(2,3)}
txt' as (a0:bag{t:tuple(i0:int, i1:int)});
b = foreach a generate flatten(a0) as (b0, b1), flatten(a0) as (b2, b3);
c = filter b by b0>b2;
dump c;
{code}
{(1 2),(2,3)}
Expected result:
(2 3,1,2)
We get nothing.","src.org.apache.pig.newplan.logical.rules.ImplicitSplitInserter
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
src.org.apache.pig.newplan.optimizer.PlanOptimizer
src.org.apache.pig.newplan.optimizer.Rule"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,Pig report input size -1 for empty input file,"{code}
 
 by b0;
dump c;
{code}
{code} a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1);
c = join a by a0, b by b0;
dump c;
{code}
If 1.txt is empty, Pig will report
Successfully read -1 records from: ""1.txt""
In WebUI, we can see we only have one MultiInputCounters: ""Input records from _0_2.txt"".
In this case, we should count inputs ""1.txt"" 0 instead -1.","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1910,2011-03-16T12:50:00.000-05:00,incorrect schema shown when project-star is used with other projections,"{code}
 
 describe f;
f: {a: bytearray,(null),b: bytearray}   
 {code}
{code}
grunt> l = load 'x' ;                                       
grunt> f = foreach l generate $1 as a, *, $2 as b;          
grunt> describe f;
f: {a: bytearray,(null),b: bytearray}  -- The tuple returned by * is automatically flattened, so this schema is not correct.
It is more accurate to return a null schema.
{code}","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
src.org.apache.pig.newplan.logical.expression.ProjectExpression
test.org.apache.pig.test.TestPigServer
test.org.apache.pig.test.Util"
CLASS,pig-0.8.0,PIG-1912,2011-03-16T16:11:46.000-05:00,non-deterministic output when a file is loaded multiple times,"while (( i < 10 ));  
  
 {results[*]}

 
  
  
  
 
 
  
  
 @operasolutions.com
(360)
I have a small demonstration script (actually, a directory with one main script and several other scripts that it calls) where the output (STOREd to a file) is not consistent between runs.
I will paste the files below this message, and I can also email the tarball to anybody who would like it; I wanted to just upload the tarball but I don't see a way to do that.
The problem appears to be that when a dataset X gets LOADed twice, with things other than LOADs occurring between the loads (like a FOREACH GENERATE), a FOREACH GENERATE that is later performed on X doesn't always choose the correct columns.
The correctness of the output was highly variable on my computer, for one of my co-workers it *almost* always failed, and for two other of my co-workers they didn't see any failures, so it's likely to be a race condition or something like that.
-- I will paste the name of the file as a comment, with the content of the file beneath it.
-- I will put the contents of the following files:
- 1) The Pig scripts (main.pig, calc_x_W.
pig, calc_x_Y.
pig, and load_raw_data.
pig)
- 2) The input data file (data.csv)
- 3) The correct output file (correct_output.
csv)
- 4) The shell script that runs the pig files and compares their output to what it should be
- 5) README
-- main.pig
RUN calc_x_W.
pig;
RUN calc_x_Y.
pig;
STORE x_W INTO 'output/W' USING PigStorage(',');
STORE x_Y INTO 'output/Y' USING PigStorage(',');  -- this is wrong sometimes
-- calc_x_W.
pig
RUN load_raw_data.
pig;
x_W = FOREACH raw_data GENERATE x, w;
-- calc_x_Y.
pig
RUN load_raw_data.
pig;
x_Y = FOREACH raw_data GENERATE x, y;
-- load_raw_data.
pig
raw_data = LOAD 'data.csv' USING PigStorage(',')
AS (
x,
y,
w
);
-- data.csv
x1,CORRECT  ANSWER,21148.59
x2,CORRECT  OUTPUT,27219.98
x3,RIGHT    ANSWER,10818.15
-- correct_output.
csv
x1,CORRECT  ANSWER
x2,CORRECT  OUTPUT
x3,RIGHT    ANSWER
-- testmany.sh
typeset -a results
i=0
while (( i < 10 )); do
rm -rf output/*
pig -x local -d WARN -e ""set debug off;run main.pig"" || break
diff correct_output.
csv output/Y/part-m-00000 && echo good
results[$i]=$?
i=$((i+1))
done;
echo ${results[*]}
-- README
This directory is intended to show a non-deterministic bug in pig.
Non-deterministic in the sense that the output of the script is not
the same between different times it is run on the same input; usually
the input is right, but sometimes it's wrong for no apparent reason.
The scripts and dataset included in this directory demonstrate the
issue.
The scripts load the file data.csv and write to the output
directory, but the file output/Y/part-m-00000 is sometimes different
between consecutive runs.
In particular, this file SHOULD just be
the first and third columns of data.csv, but it sometimes uses the
second column in place of the third.
The root of the problem appears to be that there is an intermediate
LOAD of data.csv, after some relations have already been defined.
The following things will make the error stop:
* commenting out ""STORE x_W INTO 'output/W' USING PigStorage(',');"" in main.pig
* making a copy of data.csv called data2.csv, and a file load_daw_data2.
pig
that loads data2.csv and having having calc_x_W.
pig use that instead.
It's possible that this isn't a bug and I'm just mis-using Pig;
if that is the case I would greatly appreciate hearing about it.
I believe this issue was also discussed here:
http://mail-archives.apache.org/mod_mbox/pig-user/201102.mbox/%3CAANLkTi=2ZtkVGJevKLYSSzSH--KCcX38+Xaw2d2STNiS@mail.gmail.com%3E
I have a shell script testmany.sh which runs my script multiple times
and reports for which runs the output agrreed with the file correct_output.
csv.
IMPORTANT NOTE: We have run this code on 4 different laptops, all running
pig 0.8.0.
On one laptop (the one I'm using) the output of this script
was highly non-deterministic, generally giving both the wrong and the right
output several times each during 10 runs.
Another laptop consistently got
the wrong output up until the 28th run, when it finally gave the right output.
The other two computer never actually observed the wrong output.
We suspect
this is likely a race condition.
Thanks!
USAGE
$ cd pigbug
$ bash testmany.sh
$ # the last line of output will be a sequence of 0s and 1s, with 1
$ # meaning that there was disagreement between the output and
$ # correct_output.
csv
Field Cady
field.cady@gmail.com
fcady@operasolutions.com
(360)621-4810","src.org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOLoad"
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.","flatten(outlinks.target);
  flatten(outlinks.target);
grunt> a = load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)});
grunt> b = union (load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)})), (load 'bar' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)}));
grunt> c = foreach a generate flatten(outlinks.target);
grunt> d = foreach b generate flatten(outlinks.target);
---> Would expect both C and D to work, but only C works.
D gives the error shown below.
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.
---> I don't care which one, but the same syntax should work for both!
2009-03-24 13:15:05,376 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing.
Invalid alias: target in {t: (target: chararray,text: chararray)}
Details at logfile: /echo/olston/data/pig_1237925683748.
log
grunt> quit
$ cat pig_1237925683748.log
ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.PigServer.parseQuery(PigServer.java:317)
at org.apache.pig.PigServer.registerQuery(PigServer.java:276)
at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
at org.apache.pig.Main.main(Main.java:321)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:6042)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5898)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5423)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4100)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3967)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3920)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3829)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3755)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3721)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3617)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3557)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3514)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2985)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2395)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1028)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:804)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:595)
at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
at org.apache.pig.PigServer.parseQuery(PigServer.java:310)
... 6 more","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,Schema reported from DESCRIBE and actual schema of inner bags are different.,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
urlContents = LOAD 'inputdir' USING BinStorage() AS (url:bytearray, pg:bytearray);
-- describe and dump are in-sync
DESCRIBE urlContents;
DUMP urlContents;
urlContentsG = GROUP urlContents BY url;
DESCRIBE urlContentsG;
urlContentsF = FOREACH urlContentsG GENERATE group,urlContents.pg;
DESCRIBE urlContentsF;
DUMP urlContentsF;
Prints for the DESCRIBE commands:
urlContents: {url: chararray,pg: chararray}
urlContentsG: {group: chararray,urlContents: {url: chararray,pg: chararray}}
urlContentsF: {group: chararray,pg: {pg: chararray}}
The reported schemas for urlContentsG and urlContentsF are wrong.
They are also against the section ""Schemas for Complex Data Types"" in http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_Schemas.
As expected, actual data observed from DUMP urlContentsG and DUMP urlContentsF do contain the tuple inside the inner bags.
The correct schema for urlContentsG is:  {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}
This may sound like a technicality, but it isn't.
For instance, a UDF that assumes an inner bag of {chararray} will not work with {(chararray)}.","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,Using materialized blobs with Postgresql causes error,"@Lob
I have entity with byte[] property annotated as @Lob and lazy fetch type, when table is createad the created column is of type oid, but when the column is read in application, the Hibernate reads the OID value instead of bytes under given oid.
It's behavior like to read / write bytea.
If i remember well, auto-creating table with Hibernate creates oid column.
The proper behavior for dealing in PostgreSQL (and this behavior is in Hibernate 3.4) is to use oids.","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,ORderBy with @ElementJoinColumn and EmbeddedId uses wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
Typical bank example, Account with Transactions.
It is a legacy db so Transaction has compound key - represented by TransactionId class.
The problem is that the order by in the generated SQL is for columns mapped in the transaction entity NOT the TransacionId as expected.
@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id.
_processDate ASC, _id.
_tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;
_processDate and _tranSequenceNumber are defined in the TransactionId class.
Transaction has the following fragment....
@EmbeddedId
	private TransactionId _id;
	
	@Column(name = ""mtrancde"")
	private int _transactionCode;
	
	@Column(name = ""mamount"")
	private BigDecimal _amount;
	
	@Column(name = ""mdesc"")
	private String _description;
@Column(name = ""mactdate"")
	private Date _actualDate;
	
	@Column(name = ""mbranch"")
	private int _branch;
And TransactionId defines the primary key columns....
@Embeddable
public class TransactionId {
	
	@Column(name = ""maccno"")
	private String _accountNumber;
	
	@Column(name = ""mprocdate"")
	private Date _processDate;
	
	@Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
However the generated SQL is doing order by on columns mapped in Transaction:
executing prepstmnt 23188098 SELECT t0.maccno, t0.mprocdate, t0.mtranseqno, t0.mactdate, t0.mamount, t0.mbranch, t0.mchqcash, t0.mdesc,
 t0.mtmnlno, t0.mtrancde, t0.mtrnfeed 
FROM transaction t0 
WHERE t0.maccno = ?
ORDER BY t0.mamount ASC, t0.mbranch ASC [params=(String) 000734123]
(no idea why it chose mamount, mbranch)
The last line should be:
ORDER BY t0.mprocdate ASC, t0.mtranseqno ASC [params=(String) 000734123]
Thanks
Michael","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-1896,2010-11-23T10:32:43.000-06:00,OpenJPA cannot store POJOs if a corresponding record already exists,"merge()  
 merge()   persist()  
      
 merge()
If a POJO is created using a java constructor, merge() cannot store the newly constructed object's data if this means updating a pre-existing record with a matching identity.
This is a major bug since it means applications where the objects have a natural key cannot use OpenJPA.
In my case the example was a filesystem; each crawl of the filesystem generates its own data objects with file path as the natural key.
These objects then need to be stored into the database.
Previous crawls may have encountered the same files, and the merge operation should cause the latest data from the POJO to be stored in the pre-existing record.
Instead, any attempt to execute either merge() or persist() on an independently constructed object with a matching record identity in the database triggers the same error in the database layer, since OpenJPA attempts to execute an insert for a pre-existing primary key, throwing...
org.apache.openjpa.lib.jdbc.ReportingSQLException: ERROR: duplicate key value violates unique constraint ""file_pkey"" {prepstmnt 32879825 INSERT INTO file (locationString, location, version, folder) VALUES (?
, ?
, ?
, ?)
[params=?
, ?
, ?
, ?]}
[code=0, state=23505]
From discussion with Rick Curtis on the users@openjpa.apache.org list, this is because the version field on a POJO which is unmanaged is not yet set.
An ASSUMPTION seems to be made that no such record exists in the database already since it wasn't loaded from the database in the first place, so a persist is attempted.
Instead, I recommend the database is QUERIED TO FIND OUT if such a record already exists, and the version field is set correspondingly before attempting the merge()
Here is the corresponding thread containing Ricks comments and links to an example in Github which can recreate the problem.
http://bit.ly/hfPjTI","org.apache.openjpa.kernel.VersionAttachStrategy:compareVersion(StateManagerImpl, PersistenceCapable)
org.apache.openjpa.persistence.relations.BasicEntity:getId()"
METHOD,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We are using openjpa inside an OSGi container together with
openjpa.MetaDataRepository"" value=""Preload=true""
We pass the appliation class loeader as part of our PersistenceUnitInfo implementation by returning it from PersistenceUnitInfo.getClassLoader().
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
A fix might be quite easily establihed by appending the return value of PersistenceUnitInfo.getClassLoader() to the list of claas loaders participating in the MultiClassLoader set up in
  
  MetaDataRepository.java:310ff
In the meanwhile, we are additionally setting our classloader as context loader during the creation of the EntityManagerFactory by PersistenceProvider.createContainerEntityManagerFactory(), but a fix in MetaDatRepository.preload() is highly appreciated.
TIA for fixing this,
Wolfgang
Stack trace:
org.osgi.service.blueprint.container.ComponentDefinitionException: Error when instantiating bean entityManagerFactory of class null
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:233)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:726)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:624)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:315)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)[:1.6.0_20]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)[:1.6.0_20]
at java.util.concurrent.FutureTask.run(FutureTask.java:166)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)[:1.6.0_20]
at java.lang.Thread.run(Thread.java:636)[:1.6.0_20]
Caused by: <openjpa-2.0.1-r422266:989424 fatal user error> org.apache.openjpa.persistence.ArgumentException: Unexpected error during early loading of entity metadata during initialization. See nested stacktrace for details.
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:331)
at org.apache.openjpa.persistence.PersistenceProviderImpl.preloadMetaDataRepository(PersistenceProviderImpl.java:280)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:211)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.container.AbstractServiceReferenceRecipe$JdkProxyFactory$1.invoke(AbstractServiceReferenceRecipe.java:632)
at $Proxy67.createContainerEntityManagerFactory(Unknown Source)
at org.clazzes.util.jpa.provider.EntityManagerFactoryFactory.newEntityManagerFactory(EntityManagerFactoryFactory.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:221)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:844)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:231)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
... 15 more
Caused by: java.security.PrivilegedActionException: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_20]
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:326)
... 32 more
Caused by: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at org.apache.openjpa.lib.util.MultiClassLoader.findClass(MultiClassLoader.java:216)
at java.lang.ClassLoader.loadClass(ClassLoader.java:321)[:1.6.0_20]
at java.lang.ClassLoader.loadClass(ClassLoader.java:266)[:1.6.0_20]
at java.lang.Class.forName0(Native Method)[:1.6.0_20]
at java.lang.Class.forName(Class.java:264)[:1.6.0_20]
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:233)
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:231)
... 34 more","org.apache.openjpa.meta.FieldMetaData:hashCode()
org.apache.openjpa.meta.FieldMetaData:compareTo(Object)"
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,Adding an integer and getting it as a long causes an exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .
getLong ( ""foo"" )
Target exception: org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object
at org.apache.commons.configuration.AbstractConfiguration.getLong(AbstractConfiguration.java:667)
The problem is that when an object in a property is not a Long, the only attempt of PropertyConverter.toLong() is that of treating it as a string.
It could try to convert it to a Number first and then try to convert it to a long.
It is a very confusing behaviour, because if you save and reload the properties everything works fine (as now the integer is a string).","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
FILE,CONFIGURATION,CONFIGURATION-481,2012-02-26T20:27:46.000-06:00,Variable interpolation across files broken in 1.7 & 1.8,"{myvar}  
 
 
 
 combinedConfig.getConfiguration(""test"")  configurationAt(""products/product[@name='abc']"", true)  getString(""desc"")

  {myvar}
global.properties:
myvar=abc
test.xml:
<products>
<product name=""abc"">
<desc>${myvar}-product</desc>
</product>
</products>
config.xml:
<properties fileName=""global.properties""/>
<xml fileName=""test.xml"" config-name=""test"">
<expressionEngine config-class=""org.apache.commons.configuration.tree.xpath.XPathExpressionEngine""/>
</xml>
combinedConfig.getConfiguration(""test"").
configurationAt(""products/product[@name='abc']"", true).
getString(""desc"")
I get ""${myvar}-product"" instead of ""abc-product"".
This was working in Commons Configuration 1.6, but seems to be broken in 1.7 and 1.8.","org.apache.commons.configuration.DefaultConfigurationBuilder
org.apache.commons.configuration.interpol.ConfigurationInterpolator
org.apache.commons.configuration.TestDefaultConfigurationBuilder"
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,HypergeometricDistribution.sample suffers from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
Hi, I have an application which broke when ported from commons math 2.2 to 3.2.
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.
{code}
import org.apache.commons.math3.distribution.HypergeometricDistribution;
public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
{code}
In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing
{code}
return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
it could do:
{code}
return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
This seemed to fix it, based on a quick test.",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-221,2008-08-29T13:31:56.000-05:00,Result of multiplying and equals for complex numbers is wrong,"class Complex  
 {code}
 import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));

                System.out.println(""res=comp: ""+res.equals(comp));
        }
}
 {code}
Hi.
The bug relates on complex numbers.
The methods ""multiply"" and ""equals"" of the class Complex are involved.
mathematic background:  (0,i) * (-1,0i) = (0,-i).
-----------------------------------------------------------------------
{code}
import org.apache.commons.math.complex.
*;
public class TestProg {
public static void main(String[] args) {
ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);
Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);
System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));
System.out.println(""res=comp: ""+res.equals(comp));
}
}
{code}
-----------------------------------------------------------------------
res:  -0 - 1i
comp: 0 - 1i
res=comp: false
-----------------------------------------------------------------------
I think the ""equals"" should return ""true"".
The problem could either be the ""multiply"" method that gives (-0,-1i) instead of (0,-1i),
or if you think thats right, the equals method has to be modified.
Good Luck
Dieter",org.apache.commons.math.complex.Complex:equals(Object)
METHOD,math,MATH-280,2009-07-06T21:26:57.000-05:00,bug in inverseCumulativeProbability() for Normal Distribution,"public class NormalDistributionImpl extends AbstractContinuousDistribution 


  
 public abstract class AbstractContinuousDistribution


 
 DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);

 
 normal.inverseCumulativeProbability(0.977249868051820);
* @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $
*/
public class NormalDistributionImpl extends AbstractContinuousDistribution
* @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $
*/
public abstract class AbstractContinuousDistribution
DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);
gives the exception below.
It should return (approx) 2.0000...
These also give errors:
0 9986501019683698 (should return 3.0000...)
0 9999683287581673 (should return 4.0000...)
org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0
at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)
at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)","org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtils:bracket(UnivariateRealFunction, double, double, double, int)"
METHOD,math,MATH-326,2009-12-29T00:09:20.000-06:00,getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways),"{code}
     public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }
 {code}

 
  
 {code}   
     public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }
 {code}

    sparseIterator() 
 {code}
   public double getLInfNorm() {
    double norm = 0;
    Iterator<Entry> it = sparseIterator();
    Entry e;
    while(it.hasNext() && (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }
 {code}
the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.
The current implementation in ArrayRealVector has a typo:
{code} public double getLInfNorm() { double max = 0;
for (double a : data) { max += Math.max(max, Math.abs(a));
} return max;
}
{code}
the += should just be an =.
There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).
Worse, the implementation in OpenMapRealVector is not even positive semi-definite:
{code} public double getLInfNorm() { double max = 0;
Iterator iter = entries.iterator();
while (iter.hasNext()) { iter.advance();
max += iter.value();
} return max;
}
{code}
I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():
{code} public double getLInfNorm() { double norm = 0;
Iterator<Entry> it = sparseIterator();
Entry e;
while(it.hasNext() && (e = it.next()) !
= null) { norm = Math.max(norm, Math.abs(e.getValue()));
} return norm;
}
{code}
Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.","org.apache.commons.math.linear.ArrayRealVector:getLInfNorm()
org.apache.commons.math.linear.OpenMapRealVector:getLInfNorm()"
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,ODE integrator goes past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
{code}
public void testMissedEvent() throws IntegratorException, DerivativeException {
final double t0 = 1878250320.0000029;
final double t =  1878250379.9999986;
FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
public int getDimension() {
return 1;
}
public void computeDerivatives(double t, double[] y, double[] yDot)
throws DerivativeException {
yDot[0] = y[0] * 1.0e-6;
}
};
DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
1 0e-10, 1.0e-10);
double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }
{code}","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-482,2011-01-17T19:52:10.000-06:00,"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f","FastMath.max(50.0f, -50.0f)  
 testMinMaxFloat()
FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.
This is because the wrong variable is returned.
The bug was not detected by the test case ""testMinMaxFloat()"" because that has a bug too - it tests doubles, not floats.","org.apache.commons.math.util.FastMath:max(float, float)"
METHOD,math,MATH-552,2011-03-31T22:36:56.000-05:00,MultidimensionalCounter.getCounts(int) returns wrong array of indices,"MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) {
    int[] x = counter.getCounts(i);
    System.out.println(i + "" "" + Arrays.toString(x));
}
MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) {
    int[] x = counter.getCounts(i);
    System.out.println(i + "" "" + Arrays.toString(x));
}
Output is:
0 [0, 0]
1 [0, 1]
2 [0, 2]
3 [0, 2]   <=== should be [0, 3]
4 [1, 0]
5 [1, 1]
6 [1, 2]
7 [1, 2]   <=== should be [1, 3]",org.apache.commons.math.util.MultidimensionalCounter:getCounts(int)
METHOD,math,MATH-554,2011-04-03T14:14:38.000-05:00,Vector3D.crossProduct is sensitive to numerical cancellation,"{code}
 Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);
Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);
System.out.println(Vector3D.crossProduct(v1, v2));
{code}

  { -1, 2, 0 }   { -1, 2, 1 }
Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...).
These formulas fail when vectors are almost colinear, like in the following example:
{code}
Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);
Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);
System.out.println(Vector3D.crossProduct(v1, v2));
{code}
The previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }","org.apache.commons.math.geometry.Vector3D:crossProduct(Vector3D, Vector3D)"
FILE,WFCORE,WFCORE-604,2015-03-18T09:19:35.000-05:00,"After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory","{standalone|domaine} 
 {standalone|domaine}  
 {standalone|domaine} 
 {standalone|domaine} 
 {standalone|domaine}
Description of problem:
===
- After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory
- Please see following reproduce steps.
How reproducible:
===
Steps to Reproduce:
4. Find ""new"" deployment info in JBOSS_HOME/{standalone|domaine}/data/content, and the old deployment info will be still there.
- I know that as we changed application in step-3, its hash value was changed.
And then, old info is remained in JBOSS_HOME/{standalone|domaine}/data/content.
But I think it always happens and should be fixed.
Actual results:
- The deployment information which created when deploy was failed remains in JBOSS_HOME/{standalone|domaine}/data/content.
Expected results:
- The deployment information which created when deploy was failed should be removed if the deploy is failed.","org.jboss.as.host.controller.mgmt.MasterDomainControllerOperationHandlerImpl
org.jboss.as.server.controller.resources.ServerRootResourceDefinition
org.jboss.as.host.controller.ManagedServerOperationsFactory
org.jboss.as.host.controller.DomainModelControllerService
org.jboss.as.host.controller.RemoteDomainConnectionService
org.jboss.as.test.shared.ModelParserUtils
org.jboss.as.server.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentAddHandler
org.jboss.as.server.logging.ServerLogger
org.jboss.as.server.deployment.DeploymentRemoveHandler
org.jboss.as.domain.controller.resources.ServerGroupResourceDefinition
org.jboss.as.repository.LocalDeploymentFileRepository
org.jboss.as.domain.controller.operations.ApplyRemoteMasterDomainModelHandler
org.jboss.as.server.deployment.DeploymentReplaceHandler
org.jboss.as.domain.controller.resources.DomainRootDefinition
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentDefinition
org.jboss.as.repository.logging.DeploymentRepositoryLogger
org.jboss.as.domain.controller.operations.deployment.DeploymentFullReplaceHandler
org.jboss.as.core.model.test.LegacyKernelServicesImpl
org.jboss.as.subsystem.test.TestModelControllerService
org.jboss.as.host.controller.HostControllerService
org.jboss.as.domain.controller.resources.DomainDeploymentResourceDefinition
org.jboss.as.core.model.test.TestModelControllerService
org.jboss.as.server.mgmt.domain.RemoteFileRepositoryService
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentAdd
org.jboss.as.server.ApplicationServerService
org.jboss.as.repository.LocalFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeAddHandler
org.jboss.as.server.test.InterfaceManagementUnitTestCase
org.jboss.as.host.controller.model.host.HostResourceDefinition
org.jboss.as.server.deployment.DeploymentAddHandlerTestCase
org.jboss.as.repository.DeploymentFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentReplaceHandler
org.jboss.as.repository.ContentRepository
org.jboss.as.domain.controller.operations.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.DeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeResource
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler
org.jboss.as.server.deployment.DeploymentFullReplaceHandler"
FILE,WFCORE,WFCORE-626,2015-04-06T15:53:19.000-05:00,Global list-get operation can inadvertently create list elements,"clear(name=attribute)
  get(name=attribute, index=0)
  add(name=attribute, value=test)
  get(name=attribute, index=0)
:list-clear(name=attribute)
:list-get(name=attribute, index=0)
:list-add(name=attribute, value=test)
:list-get(name=attribute, index=0)
#2 will return <undefined> as expected.
The expected result of #4 is ""test"".
However, it returns <undefined>.
This is because #2 will create the missing element at index 0 causing #3 to operate on index 1.","org.jboss.as.controller.operations.global.ListOperations
org.jboss.as.controller.operations.global.MapOperations"
FILE,WFCORE,WFCORE-687,2015-05-11T12:39:25.000-05:00,patches with duplicate element patch-id values should be rejected,"IdentityPatchContext.recordContentLoader(patchID, contentLoader)
Patches that contain duplicate patch-id attribute values in 'element' elements in patch.xml can be applied but can't be rolled back.
An attempt to rollback such a patch will result in an error ""Content loader already registered for patch "" + patchID, thrown from IdentityPatchContext.recordContentLoader(patchID, contentLoader).
Current implementation should reject patches with duplicate element patch-id values (unless the value is 'base').
Adding support for duplicate element patch-id values is a more difficult task at this point.
If we decide to support it after all, it'll be implemented under a different issue.","org.jboss.as.patching.metadata.PatchXml
org.jboss.as.patching.metadata.PatchBuilder
org.jboss.as.patching.metadata.PatchXmlUnitTestCase
org.jboss.as.patching.installation.LayerTestCase
org.jboss.as.patching.logging.PatchLogger
org.jboss.as.patching.metadata.PatchXmlUtils"
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,One profile can have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
Description of problem:
One profile can have more ancestors with same submodules.
It leads to WFLYCTL0212: Duplicate resource [(""subsystem"" => ""subsystem_name"")] .
Hierarchical composition of profiles was added to AS with EAP7-281 and WFCORE-382
How reproducible:
Always
Steps to Reproduce:
.
/domain.sh
.
/jboss-cli.sh -c
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)
Actual results:
No errors.
Expected results:
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'.
Overriding subsystems is not supported""},
""rolled-back"" => true
}
Workaround:
Add any subsystem to default-new profile:
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new/subsystem=jdr:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,Warnings about missing notification descriptions when an operation removes an extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
When I use migration operation the console log is filled with warning messages of type
WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
This is the same either for jacorb or web or messaging subsystem.
[standalone@localhost:9999 /] /subsystem=jacorb:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=messaging:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=we
web  webservices  weld
[standalone@localhost:9999 /] /subsystem=web
web  webservices
[standalone@localhost:9999 /] /subsystem=web:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
then I the log looks like
2015-09-24 08:41:09,729 WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
2015-09-24 08:43:13,229 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""DLQ"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""ExpiryQueue"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""pooled-connection-factory"" => ""hornetq-ra"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""RemoteConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""InVmConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""address-setting"" => ""#"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#""),
(""role"" => ""guest"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-acceptor"" => ""in-vm"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""direct-deliver"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-connector"" => ""in-vm"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""messaging"")]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""jsp-configuration"")
]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""static-resources"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""container"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""virtual-server"" => ""default-host"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""connector"" => ""http"")
]
2015-09-24 08:43:20,959 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""web"")]
I think that the migration operation should not show those warnings.","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,Inconsistent read-resource results with host scoped roles,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
When using a role which only selects the master there is no access-control response header showing the filtered resources, and the slave wrongly appears in the results:
[domain@localhost:9990 /] /host=*:read-resource{roles=master-monitor}
{
""outcome"" => ""success"",
""result"" => [
{
""address"" => [(""host"" => ""master"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""local"" => {}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => true,
""name"" => ""master"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => ""WildFly Core"",
""product-version"" => ""2.0.0.CR6-SNAPSHOT"",
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
},
{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}
]
}
When using a role that only selects the slave we get a proper access-control header
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
The same output on master with WFCORE-994 applied:
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""slave"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""remote"" => {
""protocol"" => undefined,
""port"" => undefined,
""host"" => undefined,
""username"" => undefined,
""ignore-unused-configuration"" => undefined,
""admin-only-policy"" => undefined,
""security-realm"" => ""ManagementRealm""
}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => false,
""name"" => ""slave"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => undefined,
""product-version"" => undefined,
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
}
master-monitor should behave the same as slave-maintainer.","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-1212,2015-12-11T18:04:50.000-06:00,TestModule does not clean up after itself properly,"TestModule.create()   mkdirs()   remove()  
  
 Once remove()   getModulesDir()
TestModule.create() calls mkdirs() to create its filesystem structure, but remove() only removes the dir above 'main' and below, leaving behind intermediate dirs.
The result of this is if you run the full testsuite with -Dts.basic, the dist/target/wildflyxxx/modules dir ends up with child dir 'test' in addition to the proper 'system'.
I'm not sure why this spurious dir doesn't end up in the final dists we publish.
Perhaps its just luck due to the release process not running the testsuite when the final build with the 'deploy' target is invoked.
I know my process for releasing WildFly Core doesn't re-run tests in that step.
Once remove() does its current work it should walk up the filesystem tree until it gets to the file returned by getModulesDir().
For each level in the tree it should check if that file is a dir with no children and if it is it should remove the dir.
https://issues.jboss.org/browse/JBEAP-2374",org.jboss.as.test.module.util.TestModule
FILE,WFCORE,WFCORE-1354,2016-02-03T00:19:08.000-06:00,Cannot clone a profile with a remoting subsystem but no io subsystem,"clone(to-profile=test)
The remoting subsystem added a requirement for the new io subsystem's worker capability, but it has special logic such that the requirement is only added if an endpoint resource is configured.
So, legacy configs (pre-io) won't have that resource, so there is no requirement.
This breaks down in the case of the profile 'clone' op, as a placeholder resource we add for the endpoint (to allow reads of the default endpoint config data) ends up getting 'described' and added by the cloning process.
So that added resource triggers an unmet requirement for the io worker:
[domain@localhost:9990 /] /profile=default:clone(to-profile=test)
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0369: Required capabilities are not available:
org.wildfly.io.worker.default in context 'profile=test'; There are no known registration points which can provide this capability.""}
,
""rolled-back"" => true
}
I'm not sure how to deal with this; some sort of marker is needed to disable 'describing' that placeholder resource.","org.jboss.as.remoting.RemotingExtension
org.jboss.as.subsystem.test.AbstractSubsystemBaseTest"
FILE,WFCORE,WFCORE-1028,2015-10-01T19:12:08.000-05:00,Poor handling of invalid roles,"{roles=slave-monitor}
A CLI request with an invalid value in the ""roles"" header results in improper behavior:
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-monitor}
{
""outcome"" => ""failed"",
""result"" => [],
""rolled-back"" => true
}
The op should fail because the role doesn't exist, but there is no failure-description.
The following is dumped in the HC log:
[Host Controller] 12:22:12,314 ERROR [org.jboss.as.controller.management-operation] (management-handler-thread - 3) WFLYCTL0013: Operation (""resolve"") failed - address: ([]): java.lang.IllegalArgumentException: WFLYCTL0327: Unknown role 'slave-monitor'
[Host Controller] 	at org.jboss.as.controller.access.rbac.StandardRoleMapper.canRunAs(StandardRoleMapper.java:95)
[Host Controller] 	at org.jboss.as.controller.access.rbac.RunAsRoleMapper.mapRoles(RunAsRoleMapper.java:143)
[Host Controller] 	at org.jboss.as.controller.access.rbac.RunAsRoleMapper.mapRoles(RunAsRoleMapper.java:71)
[Host Controller] 	at org.jboss.as.controller.access.rbac.DefaultPermissionFactory.getUserPermissions(DefaultPermissionFactory.java:109)
[Host Controller] 	at org.jboss.as.controller.access.permission.ManagementPermissionAuthorizer.authorize(ManagementPermissionAuthorizer.java:91)
[Host Controller] 	at org.jboss.as.controller.access.management.DelegatingConfigurableAuthorizer.authorize(DelegatingConfigurableAuthorizer.java:99)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.getBasicAuthorizationResponse(OperationContextImpl.java:1753)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.authorize(OperationContextImpl.java:1651)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.readResourceFromRoot(OperationContextImpl.java:833)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.readResource(OperationContextImpl.java:818)
[Host Controller] 	at org.jboss.as.controller.operations.global.GlobalOperationHandlers$ModelAddressResolver.execute(GlobalOperationHandlers.java:402)
[Host Controller] 	at org.jboss.as.controller.operations.global.GlobalOperationHandlers$ModelAddressResolver.execute(GlobalOperationHandlers.java:306)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.executeStep(AbstractOperationContext.java:890)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.processStages(AbstractOperationContext.java:659)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.executeOperation(AbstractOperationContext.java:370)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.executeOperation(OperationContextImpl.java:1336)
[Host Controller] 	at org.jboss.as.controller.ModelControllerImpl.internalExecute(ModelControllerImpl.java:391)
[Host Controller] 	at org.jboss.as.controller.ModelControllerImpl.execute(ModelControllerImpl.java:217)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.doExecute(ModelControllerClientOperationHandler.java:207)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.access$300(ModelControllerClientOperationHandler.java:129)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:151)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:147)
[Host Controller] 	at java.security.AccessController.doPrivileged(Native Method)
[Host Controller] 	at javax.security.auth.Subject.doAs(Subject.java:422)
[Host Controller] 	at org.jboss.as.controller.AccessAuditContext.doAs(AccessAuditContext.java:92)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1.execute(ModelControllerClientOperationHandler.java:147)
[Host Controller] 	at org.jboss.as.protocol.mgmt.AbstractMessageHandler$2$1.doExecute(AbstractMessageHandler.java:299)
[Host Controller] 	at org.jboss.as.protocol.mgmt.AbstractMessageHandler$AsyncTaskRunner.run(AbstractMessageHandler.java:519)
[Host Controller] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[Host Controller] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[Host Controller] 	at java.lang.Thread.run(Thread.java:745)
[Host Controller] 	at org.jboss.threads.JBossThread.run(JBossThread.java:320)","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.controller.access.rbac.RoleMapper
org.jboss.as.test.integration.domain.ServerManagementTestCase"
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,Saved rollout-plan 'name' or 'id' attribute discrepancy,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
When using rollout plans for EAP deployment scenarios I can create my own named rollout-plan for ease of use.
There is minor discrepancy in the way I create and use such rollout plan though.
rollout-plan add --name=my-rollout-plan --content={rollout main-server-group(rolling-to-servers=false,max-failed-servers=1),other-server-group(rolling-to-servers=true,max-failure-percentage=20) rollback-across-groups=true}
see --name attribute given to name my rollout plan
deploy /path/to/test-application.
war --all-server-groups --headers={rollout id=my-rollout-plan}
see id attribute given to rollout header operation
Yes, this is really minor issue, but I think that these two attributes used in aforementioned commands should be unified (preferably to name instead of id) as user might be confused when using it.
Note: examples are used from our documentation.
Note: I do not know whether I am missing something but I was not able to retrieve more info how to use rollout header operation in deploy command directly in CLI.","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,Better check of names of existing resources when adding '{local|remote-destination-outbound-socket-binding',"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
Then when I create some /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding or /socket-binding-group=standard-sockets/local-destination-outbound-socket-binding using same name as of already existing socket-binding resource, add operation is successful but when I perform server reload, it crashes as it is not able to parse configuration.
See:
/socket-binding-group=standard-sockets/socket-binding=myBinding:add()
/socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=myBinding:add(host=localhost,port=8765)
or
/socket-binding-group=standard-sockets/local-destination-outbound-socket-binding=myBinding:add(socket-binding-ref=http)
reload
server crashes with following stacktrace in console log:
17:31:40,447 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0019: Stopped Driver service with driver-name = h2
17:31:40,453 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0008: Undertow HTTP listener default suspending
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0007: Undertow HTTP listener default stopped, was bound to 127.0.0.1:8080
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-3) WFLYUT0004: Undertow 1.3.21.Final-redhat-1 stopping
17:31:40,458 INFO  [org.jboss.as.mail.extension] (MSC service thread 1-7) WFLYMAIL0002: Unbound mail session [java:jboss/mail/Default]
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 22ms
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0049: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) starting
17:31:40,489 ERROR [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0055: Caught exception during boot: org.jboss.as.controller.persistence.ConfigurationPersistenceException: WFLYCTL0085: Failed to parse configuration
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:131)
at org.jboss.as.server.ServerService.boot(ServerService.java:356)
at org.jboss.as.controller.AbstractControllerService$1.run(AbstractControllerService.java:299)
at java.lang.Thread.run(Thread.java:745)
Caused by: javax.xml.stream.XMLStreamException: ParseError at [row,col]:[410,9]
Message: WFLYCTL0042: A socket-binding or a outbound-socket-binding myBinding already declared has already been declared in socket-binding-group standard-sockets
at org.jboss.as.server.parsing.StandaloneXml_4.
parseSocketBindingGroup(StandaloneXml_4.java:518)
at org.jboss.as.server.parsing.StandaloneXml_4.
readServerElement(StandaloneXml_4.java:254)
at org.jboss.as.server.parsing.StandaloneXml_4.
readElement(StandaloneXml_4.java:141)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:103)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:49)
at org.jboss.staxmapper.XMLMapperImpl.processNested(XMLMapperImpl.java:110)
at org.jboss.staxmapper.XMLMapperImpl.parseDocument(XMLMapperImpl.java:69)
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:123)
... 3 more
17:31:40,490 FATAL [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0056: Server boot has failed in an unrecoverable manner; exiting.
See previous messages for details.
17:31:40,491 INFO  [org.jboss.as.server] (Thread-2) WFLYSRV0220: Server shutdown has been requested.
17:31:40,496 INFO  [org.jboss.as] (MSC service thread 1-2) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 3ms
After this occurs, one needs to fix .
/standalone/configuration/standalone.xml manually by removing duplicate resources.
If there is a problem for those resources to have same names I would welcome that names are checked during the add operation already regarding to all resources in socket-binding and {remote|local}-destination-outbound-socket-binding.
Note: not sure whether CLI component is appropriate, please change if there is better component for this.","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1607,2016-06-17T12:23:38.000-05:00,"Removing children of security-realm always finishes with {""outcome"" => ""success""}","{""outcome"" => ""success""}
Removing children of security-realm (e.g. authentication) always finishes with
{""outcome"" => ""success""}
.
This happens even if type of children of security-realm does not exist in server configuration.
It should rather finish with failure to indicate that nothing was removed.",org.jboss.as.domain.management.security.SecurityRealmChildRemoveHandler
FILE,WFCORE,WFCORE-1715,2016-08-15T19:04:54.000-05:00,HostProcessReloadHandler does not reset the HostRunningModeControl's restartMode,"The doReload()     ServerInventoryService.stop()
The ReloadContext created by HostProcessReloadHandler sets the HostRunningModeControl's restartMode but then it never gets restored to the default value.
The doReload() method of the ReloadContext should restore it.
This ensures the ServerInventoryService.stop() only uses the value set by HostProcessReloadHandler once, for that one reload.
A concern here is that ServerInventoryService only goes into its ""shutdownServers"" logic if the restartMode == RestartMode.SERVERS.
Which, due to this bug, it will be following any HC reload.
But if we restore the default value of restartMode, that is null, and null !
= RestartMode.SERVERS.
So the """"shutdownServers"" logic will no longer kick in.
But should it?
Should the default value of restartMode be ""null""?
Or should it be RestartMode.SERVERS?
If we change the default from null, then the behavior when no reload has happened will change.
Basically we need to decide whether the ""shutdownServers"" logic should happen by default.",org.jboss.as.host.controller.operations.StartServersHandler
FILE,WFCORE,WFCORE-1864,2016-10-13T09:12:31.000-05:00,Whitespaces are not removed from dependencies in module add command,"{{
...
    <dependencies>
        <module name=""org.a""/>
        <module name="" org.b ""/>
    </dependencies>
...
}}
Running module add --name=foo.bar --resources=foo.jar --dependencies=[org.a, org.b ] will result in following dependencies in module.xml
{{
...
<dependencies>
<module name=""org.a""/>
<module name="" org.b ""/>
</dependencies>
...
}}
The module name in dependencies should be stripped of leading and trailing whitespaces.","org.jboss.as.cli.handlers.module.ASModuleHandler
org.jboss.as.test.integration.management.cli.ModuleTestCase"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,Tab completion suggest writing attribute which has access type metric and is not writable,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
CLI tab completion suggests attributes that are not writable and their access-type is metric
/subsystem=messaging-activemq/server=default/jms-queue=DLQ:write-attribute(name=<TAB>
consumer-count  delivering-count  entries  legacy-entries  message-count  messages-added  scheduled-count
From executing :read-resource-description we can see, attributes consumer-count, delivering-count, message-count, messages-added, scheduled-count are of type metric.
On attempt to write metric attribute, for example message-count, non writable error is printed
[standalone@localhost:9990 jms-queue=q] :write-attribute(name=message-count, value=5)
{
""outcome"" => ""failed"",
""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",
""rolled-back"" => true
}
CLI should not suggest writing attributes that are not writable.","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,"Value of parameters ""restart-required"" for fixed-*port attributes does not match reality for socket-binding and *-destination-outbound-socket-binding in CLI","description(recursive=true)
fixed-port attribute of socket-binding and fixed-source-port attributes of *-destination-outbound-socket-binding define in its description that there is not necessary to do reload or restart for any of them.
But reality is different.
If you tries to change such attributes you are informed that reload is necessary.
The attributes are defined as ""restart-required"" => ""no-services"", see /socket-binding-group=standard-sockets:read-resource-description(recursive=true)","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
the problem is in the reduce method itself: on line 60 ( return input.getCentroid(); )
it should be input.getCentroid().
clone();
similar to line 81.
full stack trace:
java.lang.NullPointerException
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)
at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133)
at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
CLASS,mahout-0.8,MAHOUT-1317,2013-08-23T13:05:58.000-05:00,Clarify some of the messages in Preconditions.checkArgument,"Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"");
In experimenting with things, I was getting some errors from RowSimilarityJob, that in looking at the source I realized were a little incomplete as to what the true issue was.
In this case, they were of the form:
Preconditions.checkArgument(maxSimilaritiesPerRow > 0, ""Incorrect maximum number of similarities per row!"")
;
Here, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's ""incorrect"", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.
A quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.","core.src.main.java.org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluator
math.src.main.java.org.apache.mahout.math.CholeskyDecomposition
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategy
core.src.main.java.org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveImplicitFeedbackMapper
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity
core.src.main.java.org.apache.mahout.math.neighborhood.ProjectionSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.recommender.TopItems
math.src.main.java.org.apache.mahout.math.random.Empirical
core.src.main.java.org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob
core.src.test.java.org.apache.mahout.math.neighborhood.SearchQualityTest
integration.src.main.java.org.apache.mahout.utils.SplitInput
core.src.test.java.org.apache.mahout.math.neighborhood.SearchQualityTest.StripWeight
core.src.main.java.org.apache.mahout.clustering.kmeans.RandomSeedGenerator
integration.src.main.java.org.apache.mahout.utils.vectors.lucene.LuceneIterator
core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver
examples.src.main.java.org.apache.mahout.cf.taste.example.kddcup.KDDCupDataModel
math.src.main.java.org.apache.mahout.math.als.AlternatingLeastSquaresSolver
core.src.main.java.org.apache.mahout.cf.taste.hadoop.als.SolveExplicitFeedbackMapper
core.src.main.java.org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity
core.src.main.java.org.apache.mahout.classifier.df.data.DataLoader
math.src.main.java.org.apache.mahout.math.random.ChineseRestaurant
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.TreeID
core.src.main.java.org.apache.mahout.classifier.df.data.DataConverter
core.src.main.java.org.apache.mahout.math.Varint
core.src.main.java.org.apache.mahout.math.neighborhood.BruteSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.eval.AbstractDifferenceRecommenderEvaluator
core.src.main.java.org.apache.mahout.classifier.naivebayes.training.WeightsMapper
core.src.main.java.org.apache.mahout.math.neighborhood.FastProjectionSearch
core.src.main.java.org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIterator
core.src.main.java.org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper
core.src.main.java.org.apache.mahout.classifier.df.data.Dataset
integration.src.main.java.org.apache.mahout.cf.taste.impl.model.cassandra.CassandraDataModel
core.src.main.java.org.apache.mahout.common.iterator.SamplingIterator
core.src.main.java.org.apache.mahout.cf.taste.impl.common.WeightedRunningAverage"
