Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
FILE,DATAMONGO,DATAMONGO-423,2012-03-29T06:03:26.000-05:00,Criteria.regex should use java.util.Pattern instead of $regex,"c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", new BasicDBObject(""$regex"", ""b"") ) ) );






  
  
 c.find( new BasicDBObject( ""x"" ,




                new BasicDBObject(""$not"", Pattern.compile( ""b"" , Pattern.CASE_INSENSITIVE ) ) ) );
mongod complains about $regex in some cases.
DBCollection c = ...
c.find( new BasicDBObject( ""x"" ,
new BasicDBObject(""$not"", new BasicDBObject(""$regex"", ""b"") ) ) );
com.mongodb.MongoException: can't use $not with $regex, use BSON regex type instead
at com.mongodb.MongoException.parse(MongoException.java:82)
at com.mongodb.DBApiLayer$MyCollection.
__find(DBApiLayer.java:312)
at com.mongodb.DBCursor.
_check(DBCursor.java:369)
at com.mongodb.DBCursor.
_hasNext(DBCursor.java:504)
at com.mongodb.DBCursor.hasNext(DBCursor.java:529)
DBCollection c = ...
c.find( new BasicDBObject( ""x"" ,
new BasicDBObject(""$not"", Pattern.compile( ""b"" , Pattern.CASE_INSENSITIVE ) ) ) );","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.QueryTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-505,2012-08-14T03:07:56.000-05:00,not work for collection values,"class Entity {









  Long id;




  @DBRef




  Property property;




}









 class Property {




  Long id;




}









 interface EntityRepository extends Repository<Entity, Long> {









  Entity findByPropertyIn(Property... property);




}






  findByProperty()
class Entity {
Long id;
@DBRef
Property property;
}
class Property {
Long id;
}
interface EntityRepository extends Repository<Entity, Long> {
Entity findByPropertyIn(Property... property);
}
We need to unwrap the elements and convert them into DBRef instances one by one.","org.springframework.data.mongodb.repository.query.ConvertingParameterAccessor
org.springframework.data.mongodb.repository.query.ConvertingParameterAccessorUnitTests"
FILE,DATAMONGO,DATAMONGO-663,2013-04-24T03:21:19.000-05:00,org.springframework.data.mongodb.core.query.Field needs an equals method,"class Field   equals()  
  
 boolean fieldsEqual = this.fieldSpec == null ? that.fieldSpec == null : this.fieldSpec.equals(that.fieldSpec);
boolean fieldsEqual = this.fieldSpec == null ? that.fieldSpec == null : this.fieldSpec.equals(that.fieldSpec);
Please implement an equals on the Field method.
Purpose: For unit testing the equals method is needed.",org.springframework.data.mongodb.core.query.Field
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,update object not write type information for objects,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
That worked perfectly for me till my upgrade to 1.0.0.
I had to comment out this call in order to
The first point is that there is a contradiction: why to save type information to DBObject if it is later removed by other method?
The second point is that there should be a way to persist the type information inferred from runtime along the persisted object and not just the class definition.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,ignore @Field annotation for field alias,"@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
@Field(""sr"")
int Score
List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,get data use MongoTemplate problem with lazy loading,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
@Document
class Parent {
@Id
private String id;
private String name;
@DBref(lazy=true)
private Child child;
// getters and setters ommited
}
and the Child class
@Document
class Child {
@Id
private String id;
private String name;
//getters and setters ommited
}
The following situation should never happen:
Parent parent = new Parent();
parent.setName(""Daddy"");
mongoTemplate.save(parent); //ok, it is persisted like we expected.
Criteria criteria = Criteria.where(""_id"").
is(parent.getId());
Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);
// The child attribute should be null, right?
The null attribute is actually an enhanced class generated by CGLib.
It should not be.
This brings a lot of problems when you, by accident, persist the same entity.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-420,2012-03-22T10:09:35.000-05:00,add to @Query values add to fields,"@Query 
 @Query 
 { ?0 }
 
 { ?1 }
 
 someMethod( String values, String fields ) 
 String values = ""value : 'things'"";
   
 { ""value : 'things'"" }
   
 String values = ""field : 0"";
   
 { ""field : 0"" }
   
 String values = ""field\"" : \""0"";
@Query( value = ""
{ ?
0 }
"", fields = ""
{ ?
1 }
"" )
someMethod( String values, String fields );
String values = ""value : 'things'"";
{ ""value : 'things'"" }
String values = ""field : 0"";
{ ""field : 0"" }
String values = ""field\"" : \""0"";
-> rendered as -
{ ""field"" : ""0"" }
-> This is not a valid projection - the double quotes around zero turn it into a string so it renders as ""true"" instead of 0/undefined.
Thanks.","org.springframework.data.mongodb.repository.query.StringBasedMongoQuery
org.springframework.data.mongodb.repository.query.StringBasedMongoQueryUnitTests"
FILE,DATAMONGO,DATAMONGO-1078,2014-10-28T02:23:26.000-05:00,map complex id structure,"@Query(""{'_id': {$in: ?0}}"")




List<User> findByUserIds(Collection<MyUserId> userIds) 
 {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}
@Query(""{'_id': {$in: ?
0}}"")
List<User> findByUserIds(Collection<MyUserId> userIds);
{_id:  {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}, ...
So we need to check for the presence of typeKey when converting id properties.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,not remove _ class property on collection,"@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
@Query(value = ""{ embedded : { $in : ?
0} }"")
List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c);
{ ""embedded"" : { ""$in"" : [ {  ""_class"" : ""demo.EmbeddedObject"" , ""s"" : ""hello""}]}}
Query should be without _class property e.g.:
{ ""embedded"" : { ""$in"" : [ { ""s"" : ""hello""}]}}
This bug is related to https://jira.spring.io/browse/DATAMONGO-893","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,not return matching elements return max of documents,"public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
Aloha,
public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {
final NearQuery nearQuery = NearQuery.near(p).
maxDistance(distance);
log.info(""{}"",nearQuery.toDBObject());
return mongoTemplate.geoNear(nearQuery, MyObject.class);
}
The geoNear method is documented like this:
Returns {@link GeoResults} for all entities matching the given {@link NearQuery}.
I expect 1000 ""matching"" documents But i only get 100.
That should be stated in the method.
And another method having a pageable should be added.
What do you think?",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,keyword query findByInId with pageable,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
I've tried using In with another member other than id and it works as expected.
@Document
public class Item {
@Id
private String id;
private String type;
}
public interface ItemRepository extends MongoRepository<Item, String> {
Page<Item> findByIdIn(Collection ids, Pageable pageable);
Page<Item> findByTypeIn(Collection types, Pageable pageable);
}
@RunWith(SpringJUnit4ClassRunner.class)
@ContextConfiguration(classes = {MongoDbConfig.class})
@TransactionConfiguration(defaultRollback = false)
public class TestPageableIdIn {
@Autowired
private ItemRepository itemRepository;
private List<String> allIds = new LinkedList<>();
@Before
public void setUp() {
itemRepository.deleteAll();
String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};
// 10 items per type
for (String type : types) {
for (int i = 0; i < 10; i++) {
String id = UUID.randomUUID().
toString();
allIds.add(id);
itemRepository.save(new Item(id, type));
}
}
}
@Test
public void testPageableIdIn() {
Pageable pageable = new PageRequest(0, 5);
// expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages
Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);
Assert.assertEquals(5, results.getContent().
size());
Assert.assertEquals(10, results.getTotalElements());
Assert.assertEquals(2, results.getTotalPages());
// expect 5 Items returned, total of 30 Items in 6 Pages
results = itemRepository.findByIdIn(allIds, pageable);
Assert.assertEquals(5, results.getContent().
size());
}
}","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1307,2015-10-20T12:33:45.000-05:00,convert user-defined runtime exceptions to npes,"throw exceptionTranslator.translateExceptionIfPossible(ex);
  
 
 
 
 return null;
} catch (RuntimeException ex) { throw exceptionTranslator.translateExceptionIfPossible(ex);
MongoTemplate should throw the original exception if it gets null back from the exception translator.",org.springframework.data.mongodb.core.MongoTemplate
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,involve generic types,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
Please, see https://github.com/agustisanchez/SpringDataMongoDBBug, for code samples.
class Book with index on ""ISBN"" attribute super class AbstractProduct with index on ""name"" attribute class ProductWrapper holding attribute ""content"" of generic type ""T extends AbstractProduct""
List<ProductWrapper<Book>> books2 = new ArrayList<>
Spring Data MongoDB should be able to infer type information from the list declaration ( List<ProductWrapper<Book>> ), becoming aware that Catalog contains a list of Books, hence indexes defined on Book should be created.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,not use @Field field name nest fields with nested keywords nest fields in combination,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
@Document(collection = ""Computer"")
public class Computer
{
@Id
private String _id;
private String batchId;
@Field(""stat"")
private String status;
@Field(""disp"")
private List<Monitor> displays;
//setters and getters
}
public class Monitor {
@Field(""res"")
private String resolution;
// setters/getters
}
protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,
CursorPreparer preparer, DbObjectCallback<T> objectCallback)
DBObject mappedQuery = queryMapper.getMappedObject(query, entity);
Note the queries in the inner list, are setup as elemMatch.
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
{ ""$and"" : [ { ""status"" : ""A""} , { ""displays"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
The correct query from getMappedObject should be:
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""res"" : { ""$ne"" :  null }} , { ""res"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
CLASS,derby-10.7.1.1,DERBY-4654,2010-05-12T05:27:40.000-05:00,not escape special characters,"org.apache.derby.vti.Restriction.toSQL()  
 Restriction.doubleQuote()   IdUtil.normalToDelimited()
This could cause problems when using the restriction to generate a query against an external database.
Restriction.doubleQuote() should use IdUtil.normalToDelimited() to get proper quoting of the names.","org.apache.derbyTesting.functionTests.tests.lang.RestrictedVTITest
org.apache.derby.vti.Restriction"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,create wrong schema after dereferencing dereference nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
data = LOAD 'test_data.
txt' USING PigStorage() AS (f1: int, f2: int, f3:
int, f4: int);
nested = FOREACH data GENERATE f1, (f2, f3, f4) AS nested_tuple;
dereferenced = FOREACH nested GENERATE f1, nested_tuple.
(f2, f3);
DESCRIBE dereferenced;
uses_dereferenced = FOREACH dereferenced GENERATE nested_tuple.
f3;
DESCRIBE uses_dereferenced;
The schema of ""dereferenced"" should be {f1: int, nested_tuple: (f2: int,
f3: int)}.
(1 (2,3))
(5 (6,7))
...
This is not just a problem with DESCRIBE.
Invalid field projection.
Projected field [nested_tuple] does not exist in
schema: f1:int,f2:int.","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2828,2012-07-19T05:03:16.000-05:00,null in DataType.compare,"Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);
Caused by: java.lang.NullPointerException
at org.apache.pig.data.DataType.compare(DataType.java:427)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:97)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:1)
at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:649)
at java.util.PriorityQueue.siftUp(PriorityQueue.java:627)
at java.util.PriorityQueue.offer(PriorityQueue.java:329)
at java.util.PriorityQueue.add(PriorityQueue.java:306)
at org.apache.pig.builtin.TOP.updateTop(TOP.java:141)
at org.apache.pig.builtin.TOP.exec(TOP.java:116)
code: (TOP.java, starts with line 91)
Object field1 = o1.get(fieldNum);
Object field2 = o2.get(fieldNum);
if (! typeFound) { datatype = DataType.findType(field1);
typeFound = true;
} return DataType.compare(field1, field2, datatype, datatype);
So we need to judge the field1 whether is null.","src.org.apache.pig.data.DataType
src.org.apache.pig.builtin.TOP
test.org.apache.pig.test.TestNull"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,not generate new uids for nested schema fields lead to miscomputations,"{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
Hi,
{code}
inp = LOAD '$INPUT' AS (memberId:long, shopId:long, score:int);
tuplified = FOREACH inp GENERATE (memberId, shopId) AS tuplify, score;
D1 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
D2 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
J = JOIN D1 By shopId, D2 by shopId;
K = FOREACH J GENERATE D1::memberId AS member_id1, D2::memberId AS member_id2, D1::shopId as shop;
EXPLAIN K;
DUMP K;
{code}
It is a bit weird written like that, but it provides a minimal reproduction case (in the real case, the ""tuplified"" phase came from a multi-key grouping).
The second column should be a member id so (1,2,3,4,5).
In the initial case, there was a FILTER (member_id1 < member_id2) after K, and computation failed because of PushUpFilter optimization mistakenly moving the LOFilter operation before the join, at a place where it tried to work on a tuple and failed.
My understanding of the issue is that when the ImplicitSplitInserter creates the LOSplitOutputs, it will correctly reset the schema, and the LOSplitOutput will regenerate uids for the fields of D1 and D2 ... but will not do that on the tuple members.
The logical plan after the ImplicitSplitINserter will look like (simplified)
{code}
|---D1: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[127]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#127:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[127]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
|---D2: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[130]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#130:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[130]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
{code}
tuplified correctly gets a new uid (127 and 130) but the members of the tuple don't.
When they get reprojected, both branches have the same uid and the join looks like:
{code}
|---J: (Name: LOJoin(HASH) Schema: D1::memberId#124:long,D1::shopId#125:long,D2::memberId#139:long,D2::shopId#132:long)ColumnPrune:InputUids=[125, 124, 132]ColumnPrune:OutputUids=[125, 124, 132]
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 0 Column: 1)
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 1 Column: 1)
{code}
If for example instead of reprojecting ""memberId"", we project ""memberId+0"", a new node is created, and ultimately the two branches of the join will correctly get separate uids.
My understanding is that LOSplitOutput.getSchema() should recurse on nested schema fields.
However, I only have a light understanding of all of the logical plan handling, so I may be completely wrong.
Attached is a draft of patch and a test reproducing the issue.
Unfortunately, I haven't been able to run all unit tests with the ""fix"" (I have some weird hangs)
I'd be happy if you could indicate if that looks like completely the wrong way to fix the issue.",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,zookeeper-3.4.5,ZOOKEEPER-1619,2013-01-11T09:57:16.000-06:00,Allow spaces in URL,"{code}
 
 {code}

 
 {code}
 
 {code}
{code}
10.10.1.1:2181,10.10.1.2:2181/usergrid
{code}
{code}
10.10.1.1:2181 , 10.10.1.2:2181/usergrid
{code}
Please add a trim to both the port and the hostname parsing.",src.java.main.org.apache.zookeeper.client.ConnectStringParser
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,set snapCount,"int randRoll = r.nextInt(snapCount/2);
{code}
2013-10-02 18:09:07,600 [myid:1] - ERROR [SyncThread:1:SyncRequestProcessor@151] - Severe unrecoverable error, exiting java.lang.IllegalArgumentException: n must be positive
at java.util.Random.nextInt(Random.java:300)
at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:93)
In source code,  it maybe be supposed that snapCount must be 2 or more:
{code:title=org.apache.zookeeper.server.SyncRequestProcessor.java|borderStyle=solid}
91             // we do this in an attempt to ensure that not all ofthe servers
92             // in the ensemble take a snapshot at the same time
93             int randRoll = r.nextInt(snapCount/2);
{code}
I think this supposition is not bad because snapCount = 1 is not realistic setting...
But, it may be better to mention this restriction in documentation or add a validation in the source code.",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,fold expantion,"{\{\{ hello

something

\}
While testing the patch \#1999448, a problem was found.
But the patch was applied in r13404 to avoid more
serious black hole bugs.
This problem has now became a
bug.
\{\{\{ hello
something
\}\}\}
I
think it should not expand folds since the fold level
is not changed.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
METHOD,apache-nutch-1.8,NUTCH-1262,2012-01-31T03:15:33.000-06:00,Map `duplicating` content-types to a single type,"{code}
   
 {code}
See also: http://lucene.472066.n3.nabble.com/application-xhtml-xml-gt-text-html-td3699942.html
Example mapping file is provided in conf/.
{code}
# target MIME-type <TAB> type1 [<TAB> type2 ...]
# Map XHTML to HTML
text/html       application/xhtml+xml
# Map XHTML and HTML to something else
Web page        text/html       application/xhtml+xml
# Map some office documents to each other
Office document application/vnd.
oasis.opendocument.text application/x-tika-msoffice
{code}","org.apache.nutch.indexer.more.MoreIndexingFilter:getConf()
org.apache.nutch.indexer.more.MoreIndexingFilter:setConf(Configuration)
org.apache.nutch.indexer.more.MoreIndexingFilter:filter(NutchDocument, Parse, Text, CrawlDatum, Inlinks)"
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,[resources] UnifiedTree should ensure file/folder exists,"getStat()
Build: I20030211 using natives (Linux/Windows)
Bugs that revealed this problem: bug 21217 and bug 13463.","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
CLASS,openjpa-2.0.1,OPENJPA-1752,2010-07-29T23:33:45.000-05:00,produce inconsistent behavior with various backends,"testFindAfterQueryWithPessimisticLocks()
  testFindAfterQueryOrderByWithPessimisticLocks()
  testQueryAfterFindWithPessimisticLocks()
  testQueryOrderByAfterFindWithPessimisticLocks()


 
 testFindAfterQueryWithPessimisticLocks() 
  No exception;
It is likely that failures may also occur on other backends.
There could be some problem in OpenJPA code in handling pessimistic lock requests.
There is also inconsistency in reporting exceptions - lock timout or query timeout should be non-fatal; but with Derby the PessimisticLockException is reported  which is considered fatal.
It is also possible that the test scenarios are problematic.
TestPessisimiticLocks has 5 test cases, the last test case worked for all backend.
Problem test cases are listed as below:
1 testFindAfterQueryWithPessimisticLocks()
2 testFindAfterQueryOrderByWithPessimisticLocks()
3 testQueryAfterFindWithPessimisticLocks()
4 testQueryOrderByAfterFindWithPessimisticLocks()
The dot notation, for example, 1.1 is the first scenario in testFindAfterQueryWithPessimisticLocks() 
Each test scenario is either expecting an exception or No exception; if no exception is reported, the SELECT sql got results from database.
Tests       Derby                                        DB2V9.7                                 Oracle10gXE 10.2.0.1.0            MySQL 5.1.39/JDBC 5.1.7
====================================================================================================================================
1 1         PessimisticLockException      LockTimeoutException        LockTimeoutException              LockTimeoutException
1 2         No exception                              No exception                          No exception                                No exception
2 1         PessimisticLockException      LockTimeoutException        LockTimeoutException              LockTimeoutException
2 2         No  exception                             LockTimeoutException        No exception                                LockTimeoutException
3 1         No  exception                             QueryTimeoutException       process hang                            PersistenceException: Server shutdown [code=1053, state=08S01]
3 2         PessimisticLockException      QueryTimeoutException       process hang                            PersistenceException: Server shutdown [code=1053, state=08S01]
4 1         No  exception                             QueryTimeoutException       No exception                             QueryTimeoutException
4 2         PessimisticLockException      QueryTimeoutException       process hang                           QueryTimeoutException
org.apache.openjpa.persistence.PersistenceException:Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
<openjpa-2.1.0-SNAPSHOT-rexported fatal general error> org.apache.openjpa.persistence.PersistenceException: Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
FailedObject: select e.department from Employee e where e.id < 10 [java.lang.String]
at org.apache.openjpa.jdbc.sql.DBDictionary.narrow(DBDictionary.java:4855)
at org.apache.openjpa.jdbc.sql.DBDictionary.newStoreException(DBDictionary.java:4815)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:137)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:118)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:70)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.handleCheckedException(SelectResultObjectProvider.java:155)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.handleCheckedException(QueryImpl.java:2109)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:40)
at org.apache.openjpa.kernel.QueryImpl.toResult(QueryImpl.java:1246)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:1005)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:861)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:792)
at org.apache.openjpa.kernel.DelegatingQuery.execute(DelegatingQuery.java:542)
at org.apache.openjpa.persistence.QueryImpl.execute(QueryImpl.java:288)
at org.apache.openjpa.persistence.QueryImpl.getResultList(QueryImpl.java:302)
at org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks.testQueryAfterFindWithPessimisticLocks(TestPessimisticLocks.java:271)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at junit.framework.TestCase.runTest(TestCase.java:154)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runTest(AbstractPersistenceTestCase.java:516)
at junit.framework.TestCase.runBare(TestCase.java:127)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:503)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:479)
at junit.framework.TestResult$1.protect(TestResult.java:106)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.framework.TestResult.run(TestResult.java:109)
at junit.framework.TestCase.run(TestCase.java:118)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.run(AbstractPersistenceTestCase.java:179)
at junit.framework.TestSuite.runTest(TestSuite.java:208)
at junit.framework.TestSuite.run(TestSuite.java:203)
at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: Server shutdown in progress {prepstmnt 33525219 SELECT t1.id, t1.name FROM Employee t0 LEFT OUTER JOIN Department t1 ON t0.FK_DEPT = t1.id WHERE (t0.id < ?) LIMIT ?, ? FOR UPDATE [params=?, ?, ?]} [code=1053, state=08S01]
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:274)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:258)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$3(LoggingConnectionDecorator.java:257)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeQuery(LoggingConnectionDecorator.java:1176)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeQuery(JDBCStoreManager.java:1773)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:268)
at org.apache.openjpa.jdbc.sql.SelectImpl.executeQuery(SelectImpl.java:499)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:424)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:382)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.open(SelectResultObjectProvider.java:94)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.open(QueryImpl.java:2068)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:34)
... 30 more
NestedThrowables:
com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Server shutdown in progress
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
at java.lang.reflect.Constructor.newInstance(Unknown Source)
at com.mysql.jdbc.Util.handleNewInstance(Util.java:406)
at com.mysql.jdbc.Util.getInstance(Util.java:381)
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:984)
at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3515)
at com.mysql.jdbc.MysqlIO.nextRowFast(MysqlIO.java:1545)
at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1401)
at com.mysql.jdbc.MysqlIO.readSingleRowSet(MysqlIO.java:2829)
at com.mysql.jdbc.MysqlIO.getResultSet(MysqlIO.java:468)
at com.mysql.jdbc.MysqlIO.readResultsForQueryOrUpdate(MysqlIO.java:2534)
at com.mysql.jdbc.MysqlIO.readAllResults(MysqlIO.java:1749)
at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2159)
at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2554)
at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1761)
at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1912)
at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:93)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:280)
at org.apache.openjpa.lib.jdbc.JDBCEventConnectionDecorator$EventPreparedStatement.executeQuery(JDBCEventConnectionDecorator.java:270)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeQuery(LoggingConnectionDecorator.java:1174)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:278)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeQuery(JDBCStoreManager.java:1773)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:268)
at org.apache.openjpa.jdbc.sql.SelectImpl.executeQuery(SelectImpl.java:499)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:424)
at org.apache.openjpa.jdbc.sql.SelectImpl.execute(SelectImpl.java:382)
at org.apache.openjpa.jdbc.kernel.SelectResultObjectProvider.open(SelectResultObjectProvider.java:94)
at org.apache.openjpa.kernel.QueryImpl$PackingResultObjectProvider.open(QueryImpl.java:2068)
at org.apache.openjpa.lib.rop.EagerResultList.<init>(EagerResultList.java:34)
at org.apache.openjpa.kernel.QueryImpl.toResult(QueryImpl.java:1246)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:1005)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:861)
at org.apache.openjpa.kernel.QueryImpl.execute(QueryImpl.java:792)
at org.apache.openjpa.kernel.DelegatingQuery.execute(DelegatingQuery.java:542)
at org.apache.openjpa.persistence.QueryImpl.execute(QueryImpl.java:288)
at org.apache.openjpa.persistence.QueryImpl.getResultList(QueryImpl.java:302)
at org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks.testQueryAfterFindWithPessimisticLocks(TestPessimisticLocks.java:271)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at junit.framework.TestCase.runTest(TestCase.java:154)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runTest(AbstractPersistenceTestCase.java:516)
at junit.framework.TestCase.runBare(TestCase.java:127)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:503)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.runBare(AbstractPersistenceTestCase.java:479)
at junit.framework.TestResult$1.protect(TestResult.java:106)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.framework.TestResult.run(TestResult.java:109)
at junit.framework.TestCase.run(TestCase.java:118)
at org.apache.openjpa.persistence.test.AbstractPersistenceTestCase.run(AbstractPersistenceTestCase.java:179)
at junit.framework.TestSuite.runTest(TestSuite.java:208)
at junit.framework.TestSuite.run(TestSuite.java:203)
at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)",org.apache.openjpa.persistence.lockmgr.TestPessimisticLocks
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,ignore class loader,"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We are using openjpa inside an OSGi container together with
openjpa.MetaDataRepository"" value=""Preload=true""
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
A fix might be quite easily establihed by appending the return value of PersistenceUnitInfo.getClassLoader() to the list of claas loaders participating in the MultiClassLoader set up in
  
  MetaDataRepository.java:310ff
In the meanwhile, we are additionally setting our classloader as context loader during the creation of the EntityManagerFactory by PersistenceProvider.createContainerEntityManagerFactory(), but a fix in MetaDatRepository.preload() is highly appreciated.
TIA for fixing this,
Wolfgang
Stack trace:
org.osgi.service.blueprint.container.ComponentDefinitionException: Error when instantiating bean entityManagerFactory of class null
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:233)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:726)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:624)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:315)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)[:1.6.0_20]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)[:1.6.0_20]
at java.util.concurrent.FutureTask.run(FutureTask.java:166)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)[:1.6.0_20]
at java.lang.Thread.run(Thread.java:636)[:1.6.0_20]
Caused by: <openjpa-2.0.1-r422266:989424 fatal user error> org.apache.openjpa.persistence.ArgumentException: Unexpected error during early loading of entity metadata during initialization. See nested stacktrace for details.
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:331)
at org.apache.openjpa.persistence.PersistenceProviderImpl.preloadMetaDataRepository(PersistenceProviderImpl.java:280)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:211)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.container.AbstractServiceReferenceRecipe$JdkProxyFactory$1.invoke(AbstractServiceReferenceRecipe.java:632)
at $Proxy67.createContainerEntityManagerFactory(Unknown Source)
at org.clazzes.util.jpa.provider.EntityManagerFactoryFactory.newEntityManagerFactory(EntityManagerFactoryFactory.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:221)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:844)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:231)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
... 15 more
Caused by: java.security.PrivilegedActionException: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_20]
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:326)
... 32 more
Caused by: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at org.apache.openjpa.lib.util.MultiClassLoader.findClass(MultiClassLoader.java:216)
at java.lang.ClassLoader.loadClass(ClassLoader.java:321)[:1.6.0_20]
at java.lang.ClassLoader.loadClass(ClassLoader.java:266)[:1.6.0_20]
at java.lang.Class.forName0(Native Method)[:1.6.0_20]
at java.lang.Class.forName(Class.java:264)[:1.6.0_20]
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:233)
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:231)
... 34 more","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}
Expected results:
valueOf(String) should always be selected.
The provided patches fix this defect by applying the method invocation conversion rules from the Java Language Specification, 3rd Ed.
This means that widening primitive, boxing and unboxing conversions are all respected.",org.apache.openjpa.meta.FieldMetaData
METHOD,lang,LANG-346,2007-07-06T20:06:55.000-05:00,behave for minutes behave for seconds,"public void testRound()
{
    Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
    testCalendar.set(2007, 6, 2, 8, 9, 50);
    Date date = testCalendar.getTime();
    System.out.println(""Before round() "" + date);
    System.out.println(""After round()  "" + DateUtils.round(date, Calendar.MINUTE));
}

 
 Before round()  
 After round()   
 Before round()  
 After round()
Get unexpected output for rounding by minutes or seconds.
public void testRound()
{
Calendar testCalendar = Calendar.getInstance(TimeZone.getTimeZone(""GMT""));
testCalendar.set(2007, 6, 2, 8, 9, 50);
Date date = testCalendar.getTime();
System.out.println(""Before round() "" + date);
System.out.println(""After round()  "" + DateUtils.round(date, Calendar.MINUTE));
}
After round()  Mon Jul 02 03:10:00 CDT 2007 -- this is what I would expect","org.apache.commons.lang.time.DateUtils:modify(Calendar, int, boolean)"
METHOD,lang,LANG-363,2007-10-23T07:12:48.000-05:00,not escape /' into '\/ make IE render page,"document.getElementById(""test"")   document.getElementById(""test"") 
  
 String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);
If Javascripts including'/', IE will parse the scripts uncorrectly, actually '/' should be escaped to '\/'.
value = '<script>alert(\'aaa\');</script>';this expression will make IE render page uncorrect, it should be document.getElementById(""test"").
value = '<script>alert(\'aaa\');<\/script>';
Btw, Spring's JavascriptEscape behavor is correct.","org.apache.commons.lang.StringEscapeUtils:escapeJavaStyleString(Writer, String, boolean)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,throw ClassNotFoundException clone primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
{noformat} import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;
public class SerializationUtilsTest {
@Test public void primitiveTypeClassSerialization(){
Class<?> primitiveType = int.class;
Class<?> clone = SerializationUtils.clone(primitiveType);
assertEquals(primitiveType, clone);
}
}
{noformat}
The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4.
I understand the intention of the ClassLoaderAwareObjectInputStream, but this implementation should also implement a fallback to the original implementation.
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
String name = desc.getName();
try { return Class.forName(name, false, classLoader);
} catch (ClassNotFoundException ex) { try { return Class.forName(name, false, Thread.currentThread().
getContextClassLoader());
} catch (Exception e) { return super.resolveClass(desc);
}
}
}
{noformat}
Here is the code in ObjectInputStream that fixed the java bug.
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc)
throws IOException, ClassNotFoundException
{
String name = desc.getName();
try { return Class.forName(name, false, latestUserDefinedLoader());
} catch (ClassNotFoundException ex) {
Class cl = (Class) primClasses.get(name);
if (cl !
= null) { return cl;
} else { throw ex;
}
}
}
{noformat}","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-832,2012-09-27T00:27:58.000-05:00,not handle unterminated quotes,"{IsNd}
Format: 'd'd'
Date: d3
This should fail to parse the format and date but it actually works.
Pattern: d(\p{IsNd}++)",org.apache.commons.lang3.time.FastDateParser:init()
FILE,eclipse-3.1,77249,2004-10-28T17:57:00.000-05:00,cancel public modifier,"@Jpf.Controller 
public class Foo {...} 
 @Jpf.Controller(
    catches={
       @Jpf.Catch(type=java.lang.Exception.class, method=""handleException""),
       @Jpf.Catch(type=PageFlowException.class, 
method=""handlePageFlowException"")
    }
)
public class Foo {
...
}
(This is in 3.1 M2.)
The org.eclipse.jdt.core.dom.CompilationUnit instance corresponding to the class Foo below has no ""public"" modifier, although it should.
@Jpf.
Controller( catches={
@Jpf.
Catch(type=java.lang.Exception.class, method=""handleException""),
@Jpf.
Catch(type=PageFlowException.class, method=""handlePageFlowException"")
}
)
public class Foo {
...
}","org.eclipse.jdt.core.dom.ASTConverter
org.eclipse.swt.graphics.GC"
FILE,eclipse-3.1,79091,2004-11-19T13:00:00.000-06:00,[compiler] Should report invalid type only on the name,"class X { Zork[] foo; }
Using latest,
class X { Zork[] foo; }
We report an error on Zork[] instead of Zork only.","org.eclipse.jdt.internal.compiler.problem.ProblemReporter
org.eclipse.jdt.internal.compiler.ast.ArrayTypeReference"
FILE,eclipse-3.1,80672,2004-12-10T04:44:00.000-06:00,not trigger recompilation,"package p;
@q.Ann
public class Use {
}
  
package q;
public @interface Ann {
}


 
 
package q;
import java.lang.annotation.*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
 
 @Ann
Build 20041207
java===================================
package p;
@q.
Ann
public class Use {
}
q/Ann.
java===================================
package q;
public @interface Ann {
}
java should have got a problem due to disallowed
location for use of annotation @Ann.",org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader
FILE,eclipse-3.1,83206,2005-01-19T11:34:00.000-06:00,ICodeAssist#codeSelect(..) on implicit methods should not return a java element,"class User {
    enum Color {RED, GREEN, BLUE}
    void x() {
        Color.valueOf(""RED"");
        Color.values();
    }
}

     valueOf(String)  values()
I20050118-1015
class User { enum Color {RED, GREEN, BLUE} void x() {
Color.valueOf(""RED"");
Color.values();
}
}
ICodeAssist#codeSelect(.
.)
on implicit methods 'valueOf(String)' and 'values()' of enum Color should not return a java element.",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,85397,2005-02-16T08:20:00.000-06:00,produce error on constructor _,"strictfp enum Natural {
	ONE, TWO;
}

 
 strictfp enum Natural {
	ONE, TWO;
	
	private Natural() {
	}
}
I20050215-2300 (M5 test pass)
strictfp enum Natural {
ONE, TWO;
}
expected: strictfp is not allowed on the enum type actual: no error is reported
strictfp enum Natural {
ONE, TWO;
private Natural() {
}
}
expected: the wrong modifier is reported with the type name 'Natural' actual: the error is shown for the constructor_","org.eclipse.jdt.internal.compiler.lookup.SyntheticMethodBinding
org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyViewPart
org.eclipse.jdt.internal.compiler.lookup.MethodScope"
FILE,eclipse-3.1,85672,2005-02-17T05:53:00.000-06:00,unfold folded region with line delimiter,"package folding;

class Test {
    
}
I20050215-2300 (m5 test pass)
""package folding;
class Test {
}""  <-- no delimiter on last line
expected: caret is right after the closing brace actual: everything from after the *opening* brace is selected
Not a regression - it is like this in 3.0",org.eclipse.jface.text.source.projection.ProjectionViewer
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,produce invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
I have only verified this with JPEG output.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,88295,2005-03-17T03:34:00.000-06:00,assist ] many completion on enum case label,"public class Class3 {

	enum Color {
		BLUE, WHITE, RED;
	}
	
	void select(Color c) {
		
		switch(c){
			case BLUE :
			case WHITE:
			case R<|>
		}
	}
}
20050315
When completing in enum case label 'R<|>' only enum constants should be offered.
public class Class3 {
enum Color {
BLUE, WHITE, RED;
} void select(Color c) { switch(c){ case BLUE :
case WHITE:
case R<|>
}
}
}",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,95096,2005-05-13T06:16:00.000-05:00,assist popup complete imported method name,"import static java.lang.Math
I20050513-0010
Steps to reproduce:
-> Instead of constraining the proposals to all members with prefix a, the popup closes","org.eclipse.jdt.internal.ui.text.java.JavaMethodCompletionProposal
org.eclipse.jdt.internal.ui.text.java.LazyJavaCompletionProposal"
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,have border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
build N20050523
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
This is a regression from 3.0.2.
Standalone views should still have their border, just not the title or min/max buttons if showTitle==false.","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,97722,2005-05-31T16:41:00.000-05:00,dialog problems,"@

Dialog
The name text and location combo should be left aligned.
@@
Dialog font used: Trebuchet MS, size 11",org.eclipse.ant.internal.ui.preferences.AddCustomDialog
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,refresh children on project,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$
IProjectDescription description = ResourcesPlugin.getWorkspace
().
loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().
getRoot().
getProject
(description.getName());
project.create(description, new NullProgressMonitor());
This is the key to the issue.
I believe the offending code is in the class org.eclipse.core.internal.resources.Container.
because the projects members are not known.
If you override this method in Project and do not refresh for closed projects, the problem goes away.
We want the projects in the workspace, so we create them but do not open them, as open is very expensive.
The end user will open them by using the open project UI when needed.
This worked fine in Eclipse 3.0.","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,not initialize switch method in thread safe way,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
The synthetic method that initializes the enum/switch table is not thread safe,
that is why javac places the initialization in the static initializer of an
anonymous class.
For example, the following program should print ""1999"" 40 times
(once from each thread).
package com.bea;
public class TestEnumSwitch {
	
	public static synchronized void foo() {}
public static final void main(String args[]) {
final TestEnum e = TestEnum.A1999;
Thread[] runners = new Thread[40];
for (int i = 0; i < runners.length; i++) {
runners[i] = new Thread(new Runnable() {
public void run() {
switch (e) {
case A1:
System.err.println(""1"");
break;
case A2:
System.err.println(""2"");
break;
case A8:
System.err.println(""8"");
break;
case A13:
System.err.println(""13"");
break;
case A1999:
System.err.println(""1999"");
break;
default:
System.err.println(""default"");
break;
}
}
});
}
for (int i = 0; i < runners.length; i++) {
runners[i].start();
}
}
public enum TestEnum {
A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
A1999,
}
}","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99631,2005-06-13T09:21:00.000-05:00,assist [ ] unnecessary proposals on annotation completion,"@B 
 public class Test {}
3 1 RC2
Steps to reproduce:
-> A list of annotations starting with 'B' would be more helpful","org.eclipse.jdt.internal.corext.refactoring.reorg.JavaMoveProcessor
org.eclipse.jdt.internal.codeassist.CompletionEngine"
CLASS,openjpa-2.2.0,OPENJPA-2197,2012-05-16T17:10:22.000-05:00,MethodComparator in AnnotationPersistenceMetaDataParser should also compare parameters,"@PreUpdate
    public void updateChangeLog(Object entity)  
 private void updateChangeLog(BaseEntity he, ChangeLogEntry cle)

 
   @PreUpdate
Too bad I have (had...) 2 methods with the same name in my EntityListener:
@PreUpdate
    public void updateChangeLog(Object entity) { .
.
and also
private void updateChangeLog(BaseEntity he, ChangeLogEntry cle)
which is a private helper method.",openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.callbacks.ListenerImpl
CLASS,openjpa-2.2.0,OPENJPA-2255,2012-08-30T20:15:52.000-05:00,not load referencedColumn definition create JoinTable,"@Entity 
public class Student  
 @Id @Column(name=""id"", length=128, nullable=false) private String id; 
   @Column(name=""sName"", length=255) private String sName; 
   @ManyToMany 
  @JoinTable( 
    name=""student_course_map"", 
    joinColumns={@JoinColumn(name=""student_id"", referencedColumnName=""id"", nullable=false)}, 
    inverseJoinColumns={@JoinColumn(name=""course_id"", referencedColumnName=""id"", nullable=false)} 
  ) 
  public Collection getCourses() 

   
 @Entity 
public class Courses{ 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""cName"", length=255) private String cName; 

  ... 
}
and it's length  should be assigned to the default value 255.
@Entity 
public class Student { 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""sName"", length=255) private String sName; 
  @ManyToMany 
  @JoinTable( 
    name=""student_course_map"", 
    joinColumns={@JoinColumn(name=""student_id"", referencedColumnName=""id"", nullable=false)}, 
    inverseJoinColumns={@JoinColumn(name=""course_id"", referencedColumnName=""id"", nullable=false)} 
  ) 
  public Collection getCourses()
... 
}
@Entity 
public class Courses{ 
  @Id @Column(name=""id"", length=128, nullable=false) private String id; 
  @Column(name=""cName"", length=255) private String cName;
... 
}
We can see the student id length has been defined to 128.
And there is no definition length in the JoinColumn student_id.
The JoinColumn should be set to the default value 255.
WARN  [Schema] Existing column ""student_id"" on table ""test.student_course_map"" is incompatible with the same column in the given schema definition.
Existing column: 
Full Name: student_course_map.
student_id 
Type: varchar 
Size: 128 
Default: null 
Not Null: true 
Given column: 
Full Name: student_course_map.
student_id 
Type: varchar 
Size: 255 
Default: null 
Not Null: true",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingInfo
CLASS,solr-4.4.0,SOLR-5296,2013-10-02T00:20:01.000-05:00,create collection with implicit router add shard ranges to shard,"{quote}
 {quote}
http://localhost:8983/solr/admin/collections?action=CREATE&name=myimplicitcollection3&numShards=2&maxShardsPerNode=5&router.name=implicit&shards=s1,s2&replicationFactor=2
{quote}
""myimplicitcollection3"":{
""shards"":{
""s1"":{
""range"":""80000000-ffffffff"",
""state"":""active"",
""replicas"":{
""core_node1"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s1_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node3"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s1_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}},
""s2"":{
""range"":""0-7fffffff"",
""state"":""active"",
""replicas"":{
""core_node2"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s2_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node4"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s2_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}}},
""maxShardsPerNode"":""5"",
""router"":{""name"":""implicit""},
""replicationFactor"":""2""}
{quote}
Collections with implicit router should not have shard ranges at all.",solr.core.src.java.org.apache.solr.cloud.Overseer
FILE,AMQP,AMQP-633,2016-08-18T15:48:45.000-05:00,non transactional RabbitTemplate uses Container transactional channel,"RabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager




		.getResource(connectionFactory);




if (resourceHolder != null) {




	Channel channel = resourceFactory.getChannel(resourceHolder);




	if (channel != null) {




		return resourceHolder;




	}




}






    resourceFactory.isSynchedLocalTransactionAllowed()
Consider the case where you wish to publish a message when there's an error, while rejecting the inbound message.
If the container is transactional, the published message is rolled back.
RabbitResourceHolder resourceHolder = (RabbitResourceHolder) TransactionSynchronizationManager
.
getResource(connectionFactory);
if (resourceHolder !
= null) {
Channel channel = resourceFactory.getChannel(resourceHolder);
if (channel !
= null) {
return resourceHolder;
}
}
We should never return the resourceHolder if the resourceFactory.isSynchedLocalTransactionAllowed() is false.","org.springframework.amqp.rabbit.listener.LocallyTransactedTests
org.springframework.amqp.rabbit.core.RabbitTemplate"
METHOD,commons-math-3-3.0,MATH-905,2012-11-20T14:54:39.000-06:00,not support same range as Math counterparts not support same range of values,"Math.cosh(709.783)  
 FastMath.cosh(709.783)  
 Math.sinh(709.783)  
 FastMath.sinh(709.783)  
 StrictMath.log(Double.MAX_VALUE) 
 double t = exp(x*0.5);
return (0.5*t)*t;
 
 double t = exp(-x*0.5);
return (-0.5*t)*t;
As reported by Jeff Hain:
cosh(double) and sinh(double):
Math.cosh(709.783) = 8.991046692770538E307
FastMath.cosh(709.783) = Infinity
Math.sinh(709.783) = 8.991046692770538E307
FastMath.sinh(709.783) = Infinity
===> This is due to using exp( x )/2 for values of |x|
above 20: the result sometimes should not overflow,
but exp( x ) does, so we end up with some infinity.",org.apache.commons.math3.util.FastMathTest:testHyperbolicInverses()
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,DomainClassConverter should gracefully return null for null sources or empty strings,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
I've noticed an important issue related to automatic web binding of String id to Domain class.
Failed to convert property value of type java.lang.String to required type org.mycomp.domain.Customer for property customer; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @javax.
validation.constraints.NotNull @javax.
persistence.ManyToOne org.mycomp.domain.Customer for value '; nested exception is org.springframework.dao.InvalidDataAccessApiUsageException: The given id must not be null!
; nested exception is java.lang.IllegalArgumentException: The given id must not be null!
I think it should not try to convert to Domain class if id is null or empty.
And note that for optional references this even might even cause a complete blocker?
<form:select path=""customer"">
<form:option value="""" label=""Select"" />
<form:options items=""${customers}"" itemValue=""id""></form:options>
</form:select>","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
CLASS,derby-10.9.1.0,DERBY-5251,2011-05-29T04:27:15.000-05:00,make ErrorCodeTest pass in non-English locale,"test_errorcode(org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
)
D:\derby\test>java junit.textui.TestRunner org.apache.derbyTesting.functionTests
.
tests.lang.ErrorCodeTest
.
F
Time: 4.797
There was 1 failure:
1 test_errorcode(org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
)junit.framework.AssertionFailedError: Column value mismatch @ column 'MESSAGE',
row 1:
Expected: >At least one parameter to the current statement is uninitialized.
<
Found:    ><
at org.apache.derbyTesting.junit.JDBC.assertRowInResultSet(JDBC.java:121
3
at org.apache.derbyTesting.junit.JDBC.assertRowInResultSet(JDBC.java:112
5
at org.apache.derbyTesting.junit.JDBC.assertFullResultSetMinion(JDBC.jav
a:1012)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:935)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:892)
at org.apache.derbyTesting.junit.JDBC.assertFullResultSet(JDBC.java:850)
at org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest.test_e
rrorcode(ErrorCodeTest.java:88)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:25)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:
112)
FAILURES!!!
Tests run: 1,  Failures: 1,  Errors: 0
D:\derby\test>",java.testing.org.apache.derbyTesting.functionTests.tests.lang.ErrorCodeTest
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,Client should use a prepared statement rather than regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
o.a.d.client.am.Connection setTransactionIsolation() uses a Statement which  it builds up each time for setTransactionIsolation()  is called.
private Statement setTransactionIsolationStmt = null;
...
setTransactionIsolationStmt =
                    createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
....
private void setTransactionIsolationX(int level)
...
            setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);
It would be better for performance and also for avoid possible garbage collection issues, to have a single prepared statement with a parameter marker.
import java.sql.
*;
import java.net.
*;
import java.io.
*;
import org.apache.derby.drda.NetworkServerControl;
/**
* Client template starts its own NetworkServer and runs some SQL against it.
* The SQL or JDBC API calls can be modified to reproduce issues
*
*/public class SetTransactionIsolation {
public static Statement s;
public static void main(String[] args) throws Exception {
try {
// Load the driver.
Not needed for network server.
Class.forName(""org.apache.derby.jdbc.ClientDriver"");
// Start Network Server
startNetworkServer();
// If connecting to a customer database.
Change the URL
Connection conn = DriverManager
.
getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
// clean up from a previous run
s = conn.createStatement();
try {
s.executeUpdate(""DROP TABLE T"");
} catch (SQLException se) {
if (!
se.getSQLState().
equals(""42Y55""))
throw se;
}
for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
}
// rs.close();
// ps.close();
runtimeInfo();
conn.close();
// Shutdown the server
shutdownServer();
} catch (SQLException se) {
while (se !
= null) {
System.out.println(""SQLState="" + se.getSQLState()
+ se.getMessage());
se.printStackTrace();
se = se.getNextException();
}
}
}
/**
* starts the Network server
*
*/
public static void startNetworkServer() throws SQLException {
Exception failException = null;
try {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.start(new PrintWriter(System.out));
// Wait for the network server to start
boolean started = false;
int retries = 10; // Max retries = max seconds to wait
while (!
started && retries > 0) {
try {
// Sleep 1 second and then ping the network server
Thread.sleep(1000);
networkServer.ping();
// If ping does not throw an exception the server has
// started
started = true;
} catch (Exception e) {
retries--;
failException = e;
}
}
// Check if we got a reply on ping
if (!
started) {
throw failException;
}
} catch (Exception e) {
SQLException se = new SQLException(""Error starting network  server"");
se.initCause(failException);
throw se;
}
}
public static void shutdownServer() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.shutdown();
}
public static void runtimeInfo() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
System.out.println(networkServer.getRuntimeInfo());
}
}",java.client.org.apache.derby.client.am.Connection
METHOD,time,28,2013-05-31T00:52:24.000-05:00,questionable behaviour of GJChronology pass 1BC,"Chronology chronology = GJChronology.getInstance();

LocalDate start = new LocalDate(2013, 5, 31, chronology);
LocalDate expectedEnd = new LocalDate(-1, 5, 31, chronology); // 1 BC
assertThat(start.minusYears(2013), is(equalTo(expectedEnd)));
assertThat(start.plus(Period.years(-2013)), is(equalTo(expectedEnd)));
```
Chronology chronology = GJChronology.getInstance();
LocalDate start = new LocalDate(2013, 5, 31, chronology);
LocalDate expectedEnd = new LocalDate(-1, 5, 31, chronology); // 1 BC
assertThat(start.minusYears(2013), is(equalTo(expectedEnd)));
assertThat(start.plus(Period.years(-2013)), is(equalTo(expectedEnd)));
```
```
org.joda.time.IllegalFieldValueException: Value 0 for year is not supported
```
However, I never provided ""0"" for the year myself.
I thought it was the job of the framework to skip over non-existent year 0 for me to return 1 BC?","org.joda.time.chrono.GJChronology:getInstance(DateTimeZone, ReadableInstant, int)
org.joda.time.chrono.GJChronology:add(long, long)
org.joda.time.chrono.GJChronology:add(long, int)"
FILE,COMPRESS,COMPRESS-245,2013-12-05T11:01:39.000-06:00,tararchiveinputstream #getce null,"FileInputStream fin = new FileInputStream(""exampletar.tar.gz"");

GZIPInputStream gin = new GZIPInputStream(fin);

TarArchiveInputStream tin = new TarArchiveInputStream(gin);            TarArchiveEntry entry;

              tin.getNextTarEntry()
This does not happen with version 1.5
FileInputStream fin = new FileInputStream(""exampletar.tar.gz"");
GZIPInputStream gin = new GZIPInputStream(fin);
TarArchiveInputStream tin = new TarArchiveInputStream(gin);            TarArchiveEntry entry;
while ((entry = tin.getNextTarEntry()) !
= null) {
The file is created with
tar cvzf
in RHEL 6.5 and the contents look like this when extracted with the same tool:
topdirectory/
topdirectory/about.html
topdirectory/.
eclipseproduct
topdirectory/plugins/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/about.html
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/launcher.
gtk.linux.x86_64.
properties
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/eclipse_1206.
so
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/about.html
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/fragment.properties
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/.
api_description
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/runtime_registry_compatibility.jar
topdirectory/configuration/
topdirectory/configuration/config.ini
topdirectory/icon.
xpm
topdirectory/about_files/
topdirectory/about_files/pixman-licenses.txt
topdirectory/about_files/mpl-v11.txt
topdirectory/about_files/about_cairo.html
topdirectory/libcairo-swt.
so
topdirectory/
topdirectory/about.html
topdirectory/.
eclipseproduct
topdirectory/plugins
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/about.html
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/eclipse.
inf
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
SF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/MANIFEST.
MF
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/META-INF/ECLIPSEF.
RSA
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/launcher.
gtk.linux.x86_64.
properties
topdirectory/plugins/org.
eclipse.equinox.launcher.gtk.linux.x86_64_1.0.200.
v20090519/eclipse_1206.
so
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/about.html
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/fragment.properties
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/.
api_description
topdirectory/plugins/org.
eclipse.core.runtime.compatibility.registry_3.2.200.v20090429-1800/META-INF","org.apache.commons.compress.archivers.tar.TarArchiveInputStream
org.apache.commons.compress.archivers.tar.TarArchiveInputStreamTest"
FILE,COMPRESS,COMPRESS-357,2016-05-25T17:50:50.000-05:00,affect output stream,"finished()  
  
 s.close();
s = null;
  finalize()  finish()  finish()  
 finish()  
 finalize() 
 finalize()
Now, consider something like this sequence.
BZip2OutputStream s = ...
...
s.close();
s = null;
After the s = null, the stream is garbage.
But, since the GC may be on a different thread, there is no guarantee that the assignment this.out = null in finish() has actually been made visible to the GC thread, which results in bad data in the output stream.
This is not a theoretical problem; In a part of a large project I'm working on, this happens about 2% of the time.
The fixes are simple
1) synchronize finish() or
2) don't call finish from finalize().
A workaround is to derive a class and override the finalize() method.",org.apache.commons.compress.compressors.bzip2.BZip2CompressorOutputStream
CLASS,bookkeeper-4.1.0,BOOKKEEPER-371,2012-08-17T05:42:02.000-05:00,cause hedwig hub,"Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .messageConsumed(messageConsumeData.msg);

  getPipeline()  getLast()   channel.close()   messageConsumed()
2012-08-15 17:47:42,443 - ERROR - [pool-20-thread-1:TerminateJVMExceptionHandler@28] - Uncaught exception in thread pool-20-thread-1 java.lang.NullPointerException
at org.apache.hedwig.client.netty.HedwigClientImpl.getResponseHandlerFromChannel(HedwigClientImpl.java:323)
at org.apache.hedwig.client.handlers.MessageConsumeCallback.operationFinished(MessageConsumeCallback.java:75)
at org.apache.hedwig.client.handlers.MessageConsumeCallback.operationFinished(MessageConsumeCallback.java:41)
at org.apache.hedwig.server.regions.RegionManager$1$1$1.operationFinished(RegionManager.java:208)
at org.apache.hedwig.server.regions.RegionManager$1$1$1.operationFinished(RegionManager.java:202)
at org.apache.hedwig.server.persistence.ReadAheadCache$PersistCallback.operationFinished(ReadAheadCache.java:194)
at org.apache.hedwig.server.persistence.ReadAheadCache$PersistCallback.operationFinished(ReadAheadCache.java:171)
at org.apache.hedwig.server.persistence.BookkeeperPersistenceManager$PersistOp$1.safeAddComplete(BookkeeperPersistenceManager.java:548)
at org.apache.hedwig.zookeeper.SafeAsynBKCallback$AddCallback.addComplete(SafeAsynBKCallback.java:93)
at org.apache.bookkeeper.client.PendingAddOp.submitCallback(PendingAddOp.java:165)
at org.apache.bookkeeper.client.LedgerHandle.sendAddSuccessCallbacks(LedgerHandle.java:643)
at org.apache.bookkeeper.client.PendingAddOp.writeComplete(PendingAddOp.java:159)
at org.apache.bookkeeper.proto.PerChannelBookieClient.handleAddResponse(PerChannelBookieClient.java:577)
at org.apache.bookkeeper.proto.PerChannelBookieClient$7.safeRun(PerChannelBookieClient.java:525)
at org.apache.bookkeeper.util.SafeRunnable.run(SafeRunnable.java:31)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)
At 2012-08-15 17:47:42,443, the channel was disconnected as well.
I believe the following code in the MessageConsumeCallback is causing this problem.
Channel topicSubscriberChannel = client.getSubscriber().
getChannelForTopic(topicSubscriber);
HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).
getSubscribeResponseHandler()
.
messageConsumed(messageConsumeData.msg);
Moreover, we need to check if the returned Response handler is not null because there is a race here if channel.close() is called after we retrieve the channel and before we call messageConsumed().
I guess the same applies for other instances where we use this.
Does the above explanation seem right?","hedwig-client.src.main.java.org.apache.hedwig.client.netty.WriteCallback
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.SubscribeResponseHandler
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigPublisher
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigSubscriber
hedwig-client.src.main.java.org.apache.hedwig.client.handlers.MessageConsumeCallback
hedwig-client.src.main.java.org.apache.hedwig.client.netty.HedwigClientImpl"
CLASS,bookkeeper-4.1.0,BOOKKEEPER-376,2012-08-22T13:32:57.000-05:00,LedgerManagers should consider 'underreplication' node as a special Znode,"{noformat}
     
 {noformat}
{noformat}
2012-08-22 23:59:35,649 - WARN  - [GarbageCollectorThread:HierarchicalLedgerManager@354] - Exception during garbage collecting ledgers for underreplication of /ledgers
java.io.IOException: java.lang.NumberFormatException: For input string: ""underreplicationlocks0000""
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getLedgerId(HierarchicalLedgerManager.java:236)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getStartLedgerIdByLevel(HierarchicalLedgerManager.java:254)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.doGcByLevel(HierarchicalLedgerManager.java:388)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.garbageCollectLedgers(HierarchicalLedgerManager.java:351)
at org.apache.bookkeeper.bookie.GarbageCollectorThread.doGcLedgers(GarbageCollectorThread.java:226)
at org.apache.bookkeeper.bookie.GarbageCollectorThread.run(GarbageCollectorThread.java:195)
Caused by: java.lang.NumberFormatException: For input string: ""underreplicationlocks0000""
at java.lang.NumberFormatException.forInputString(Unknown Source)
at java.lang.Long.parseLong(Unknown Source)
at java.lang.Long.parseLong(Unknown Source)
at org.apache.bookkeeper.meta.HierarchicalLedgerManager.getLedgerId(HierarchicalLedgerManager.java:234)
... 5 more
{noformat}","bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.LedgerLayout
bookkeeper-server.src.main.java.org.apache.bookkeeper.meta.AbstractZkLedgerManager"
FILE,swt-3.1,102794,2005-07-05T17:56:00.000-05:00,have change behaviour between 3.0.2,"public static void main(String[] args) {
        Display display = new Display();

        Shell shell = new Shell(display);

        shell.setLayout(new FillLayout());

        ScrolledComposite sc1 = new ScrolledComposite(shell, SWT.H_SCROLL
                | SWT.V_SCROLL);
        Composite editor = new Composite(sc1, SWT.SHADOW_NONE);
        sc1.setContent(editor);
        sc1.setLayout(new FillLayout());

        GridLayout layout = new GridLayout();

        layout.numColumns = 6;
        layout.makeColumnsEqualWidth = true;
        editor.setLayout(layout);

        Label boxLabel = new Label(editor, SWT.NONE);
        boxLabel.setText(""My label"");

        Text textBox = new Text(editor, SWT.H_SCROLL | SWT.V_SCROLL | SWT.MULTI
                | SWT.BORDER);
        textBox.setText(""Some text for the text box\nAlso with a new line"");

        // do layout bits
        GridData labelData = new GridData(SWT.RIGHT, SWT.TOP, false, false);
        boxLabel.setLayoutData(labelData);

        GridData textBoxData = new GridData(SWT.FILL, SWT.CENTER, true, false,
                5, 1);
        textBoxData.widthHint = 400;
        textBox.setLayoutData(textBoxData);

        sc1.setExpandHorizontal(true);
        sc1.setExpandVertical(true);
        sc1.setMinSize(editor.computeSize(SWT.DEFAULT, SWT.DEFAULT));

        shell.pack();
        shell.open();

        while (!shell.isDisposed()) {
            if (!display.readAndDispatch())
                display.sleep();
        }
        display.dispose();
    }
Running the following problem on 3.0.2 and 3.1 shows a difference in behaviour:
public static void main(String[] args) {
Display display = new Display();
Shell shell = new Shell(display);
shell.setLayout(new FillLayout());
ScrolledComposite sc1 = new ScrolledComposite(shell, SWT.H_SCROLL
| SWT.V_SCROLL);
Composite editor = new Composite(sc1, SWT.SHADOW_NONE);
sc1.setContent(editor);
sc1.setLayout(new FillLayout());
GridLayout layout = new GridLayout();
layout.numColumns = 6;
layout.makeColumnsEqualWidth = true;
editor.setLayout(layout);
Label boxLabel = new Label(editor, SWT.NONE);
boxLabel.setText(""My label"");
Text textBox = new Text(editor, SWT.H_SCROLL | SWT.V_SCROLL | SWT.MULTI
| SWT.BORDER);
textBox.setText(""Some text for the text box\nAlso with a new line"");
// do layout bits
GridData labelData = new GridData(SWT.RIGHT, SWT.TOP, false, false);
boxLabel.setLayoutData(labelData);
GridData textBoxData = new GridData(SWT.FILL, SWT.CENTER, true, false,
5, 1);
textBoxData.widthHint = 400;
textBox.setLayoutData(textBoxData);
sc1.setExpandHorizontal(true);
sc1.setExpandVertical(true);
sc1.setMinSize(editor.computeSize(SWT.DEFAULT, SWT.DEFAULT));
shell.pack();
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch())
display.sleep();
} display.dispose();
}
This appears to be a combination of the text box spanning 5 columns and the use of layout.makeColumnsEqualWidth = true;
Turning off makeColumnsEqualWidth helps but it means that the real app this if from ends up looking untidy.
Using minimumWidth instead of widthHint doesn't help.
Commenting out the minimumWidth line helps, but the form ends up being wider than I'd like.
The effect I'm trying to achieve is that labels are 1 column wide then text boxes are either 2 or 5 columns wide (so some rows get two labels and text boxes)  The scrolled composite is needed because in the real app the forms are actually within a TabItem, so I need the ability to scroll.",org.eclipse.swt.layout.GridLayout
FILE,swt-3.1,104150,2005-07-16T19:58:00.000-05:00,click on empty space click on grid lines,"table.getLinesVisible()  
 table.setLinesVisible(true)
SWT-win32, v3138 (3.1-final)
Expected behaviour:
No matter which part of the table the user clicks on, the table cursor should follow the table selection as closely as possible if the selection is changed as a result of the click.",org.eclipse.swt.custom.TableCursor
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,fail to setTopIndex add new items to table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
I am working on a table viewer that keeps track of the scroll bar and loads content into the table dynamically as the user scrolls to the end of the table.
Items could be added/removed from the table as the user scrolls.
To maintain the position of the table, I call setTopIndex at the end of the update.
I have created a small testcase to simulate the process.
public static void main(String[] args) { final Display display = new Display();
Shell shell = new Shell(display);
shell.setBounds(10,10,200,200);
final Table table = new Table(shell, SWT.NONE);
table.setBounds(10,10,100,100);
for (int i = 0; i < 99; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
} table.setTopIndex(20);
shell.open();
System.out.println(""top visible index: "" + table.getTopIndex());
for (int i = 0; i < 5; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
}
table.setTopIndex(40);
System.out.println(""top visible index: "" + table.getTopIndex());
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}
Expected Result:
Calling setTopIndex(40) should move table item #40 to the top of the table.
Calling getTopIndex after should return 40.
If the last 5 items are added before the shell is opened, setTopIndex to 40 will also succeed.
The testcase works as expected on Windows.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,produce invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
I have only verified this with JPEG output.
Many files were tested and the majority 
 did produced the proper JPG images as expected.
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,93724,2005-05-04T17:35:00.000-05:00,create signal names,"byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);	
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
Rather than converting the names for the signals every time, these signal names should be defined in OS.java so that they can be only created once.","org.eclipse.swt.dnd.DropTarget
org.eclipse.swt.dnd.DragSource"
FILE,swt-3.1,97651,2005-05-31T14:43:00.000-05:00,mark cheese,"Tree.redraw() 
 public static void main(String[] args) {
	final Display display = new Display();
	final Shell shell = new Shell(display);
	shell.setBounds(10, 10, 300, 300);
	final Tree tree = new Tree(shell, SWT.NONE);
	tree.setBounds(10, 10, 200, 200);
	new TreeItem(tree, SWT.NONE).setText(""pre-root"");
	TreeItem root1 = new TreeItem(tree, SWT.NONE);
	root1.setText(""root"");
	TreeItem child = new TreeItem(root1, SWT.NONE);
	child.setText(""child"");
	Button button = new Button(shell, SWT.PUSH);
	button.setBounds(230,10,30,30);
	button.addSelectionListener(new SelectionAdapter() {
		public void widgetSelected(SelectionEvent e) {
			tree.redraw();
		}
	});
	root1.setExpanded(true);
	tree.setInsertMark(root1, false);
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose();
}
3.1RC1
This line should not go away because it belongs to the ""root"" item, not to the ""child"" item, but if it really wants to go away, then its end tips should not be left
public static void main(String[] args) { final Display display = new Display();
final Shell shell = new Shell(display);
shell.setBounds(10, 10, 300, 300);
final Tree tree = new Tree(shell, SWT.NONE);
tree.setBounds(10, 10, 200, 200);
new TreeItem(tree, SWT.NONE).
setText(""pre-root"");
TreeItem root1 = new TreeItem(tree, SWT.NONE);
root1.setText(""root"");
TreeItem child = new TreeItem(root1, SWT.NONE);
child.setText(""child"");
Button button = new Button(shell, SWT.PUSH);
button.setBounds(230,10,30,30);
button.addSelectionListener(new SelectionAdapter() { public void widgetSelected(SelectionEvent e) { tree.redraw();
}
});
root1.setExpanded(true);
tree.setInsertMark(root1, false);
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}","org.eclipse.swt.dnd.TreeDragUnderEffect
org.eclipse.swt.widgets.Tree"
CLASS,pig-0.8.0,PIG-1188,2010-01-14T13:32:46.000-06:00,null to input tuple,"{code}
  as (a0, a1);
dump a;
{code}
 
 {code}
 
 {code}
 
 {code}
 
 {code}

 
 {code}
 
 {code}
When we have schema, we should generate input data according to the schema, and padding nulls if necessary.
{code}
a = load '1.
txt' as (a0, a1);
dump a;
{code}
{code}
1       2
1       2       3
1
{code}
Desired result:
{code}
(1 2)
(1 2)
(1 null)
{code}","test.org.apache.pig.test.TestMergeForEachOptimization
src.org.apache.pig.newplan.logical.rules.TypeCastInserter
test.org.apache.pig.test.TestNewPlanLogicalOptimizer
test.org.apache.pig.test.TestNewPlanFilterRule
test.org.apache.pig.test.TestNewPlanPushDownForeachFlatten
test.org.apache.pig.test.TestEvalPipeline2
test.org.apache.pig.test.TestMultiQueryCompiler
test.org.apache.pig.test.TestPartitionFilterPushDown
test.org.apache.pig.test.TestNewPlanFilterAboveForeach"
CLASS,pig-0.8.0,PIG-1277,2010-03-05T13:02:03.000-06:00,Pig should give error message when cogroup on tuple keys of different inner type,"UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

 
 {code}
 
  
 by (c0, c1);
dump e;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}

 
 {code}
  {(1,1)}  {(1,1)} 
 {code}

 
 {code}
  {(1,1)}  {} 
 {}  {(1,1)} 
 {code}
This is confusing.
It is desirable to give error/warnings when it happens.
txt' as (a0);
b = foreach a generate a0, MapGenerate(*) as m:map[];
c = foreach b generate a0, m#'key' as key;
d = load '2.
txt' as (c0, c1);
e = cogroup c by (a0, key), d by (c0, c1);
dump e;
{code}
1 txt
{code}
1
{code}
2 txt
{code}
1 1
{code}
User expected result (which is not right):
{code}
((1 1),{(1,1)},{(1,1)})
{code}
We shall give user the message that we can not merge the key due to the type mismatch.","src.org.apache.pig.impl.io.NullableBytesWritable
test.org.apache.pig.test.TestPackage
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator
src.org.apache.pig.backend.hadoop.HDataType
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestSecondarySort
src.org.apache.pig.newplan.logical.relational.LOUnion"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,report input size for empty input file,"{code}
 
 by b0;
dump c;
{code}
{code} a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1);
c = join a by a0, b by b0;
dump c;
{code}
In WebUI, we can see we only have one MultiInputCounters: ""Input records from _0_2.txt"".
In this case, we should count inputs ""1.txt"" 0 instead -1.","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1910,2011-03-16T12:50:00.000-05:00,use project-star with other projections,"{code}
 
 describe f;
f: {a: bytearray,(null),b: bytearray}   
 {code}
It is more accurate to return a null schema.
{code}","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.expression.ExpToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
src.org.apache.pig.newplan.logical.expression.ProjectExpression
test.org.apache.pig.test.TestPigServer
test.org.apache.pig.test.Util"
CLASS,pig-0.8.0,PIG-1935,2011-03-24T20:33:38.000-05:00,New logical plan: Should not push up filter in front of Bincond,"{code}
  PigStorage()    
        
   ;
dump filtered;
{code}
{code}
data = LOAD 'data.txt' using PigStorage() as (referrer:chararray, canonical_url:chararray, ip:chararray);
best_url = FOREACH data GENERATE ((canonical_url !
= '' and canonical_url is not null) ?
canonical_url : referrer) AS url, ip;
filtered = FILTER best_url BY url == 'badsite.com';
dump filtered;
{code}
data.txt:
badsite.com             127.0.0.1
goodsite.com/1?foo=true goodsite.com    127.0.0.1
Expected:
(badsite.com,127.0.0.1)
Thanks Corbin Hoenes for reporting.","test.org.apache.pig.test.TestNewPlanFilterAboveForeach
src.org.apache.pig.newplan.logical.expression.BinCondExpression"
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,combine schema with nested bag combine schema from union,"flatten(outlinks.target);
  flatten(outlinks.target);
grunt> a = load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)});
grunt> b = union (load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)})), (load 'bar' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)}));
grunt> c = foreach a generate flatten(outlinks.target);
grunt> d = foreach b generate flatten(outlinks.target);
---> Would expect both C and D to work, but only C works.
---> I don't care which one, but the same syntax should work for both!
2009-03-24 13:15:05,376 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing.
Invalid alias: target in {t: (target: chararray,text: chararray)}
Details at logfile: /echo/olston/data/pig_1237925683748.
log
grunt> quit
$ cat pig_1237925683748.log
ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.PigServer.parseQuery(PigServer.java:317)
at org.apache.pig.PigServer.registerQuery(PigServer.java:276)
at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
at org.apache.pig.Main.main(Main.java:321)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:6042)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5898)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5423)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4100)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3967)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3920)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3829)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3755)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3721)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3617)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3557)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3514)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2985)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2395)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1028)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:804)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:595)
at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
at org.apache.pig.PigServer.parseQuery(PigServer.java:310)
... 6 more","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,report from DESCRIBE,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
urlContents = LOAD 'inputdir' USING BinStorage() AS (url:bytearray, pg:bytearray);
-- describe and dump are in-sync
DESCRIBE urlContents;
DUMP urlContents;
urlContentsG = GROUP urlContents BY url;
DESCRIBE urlContentsG;
urlContentsF = FOREACH urlContentsG GENERATE group,urlContents.pg;
DESCRIBE urlContentsF;
DUMP urlContentsF;
urlContents: {url: chararray,pg: chararray}
urlContentsG: {group: chararray,urlContents: {url: chararray,pg: chararray}}
urlContentsF: {group: chararray,pg: {pg: chararray}}
As expected, actual data observed from DUMP urlContentsG and DUMP urlContentsF do contain the tuple inside the inner bags.
The correct schema for urlContentsG is:  {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}
This may sound like a technicality, but it isn't.
For instance, a UDF that assumes an inner bag of {chararray} will not work with {(chararray)}.","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,use materialized blobs with Postgresql cause error,"@Lob
It's behavior like to read / write bytea.
The proper behavior for dealing in PostgreSQL (and this behavior is in Hibernate 3.4) is to use oids.","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,use wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
Typical bank example, Account with Transactions.
It is a legacy db so Transaction has compound key - represented by TransactionId class.
The problem is that the order by in the generated SQL is for columns mapped in the transaction entity NOT the TransacionId as expected.
@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id.
_processDate ASC, _id.
_tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;
_processDate and _tranSequenceNumber are defined in the TransactionId class.
Transaction has the following fragment....
@EmbeddedId
	private TransactionId _id;
	
	@Column(name = ""mtrancde"")
	private int _transactionCode;
	
	@Column(name = ""mamount"")
	private BigDecimal _amount;
	
	@Column(name = ""mdesc"")
	private String _description;
@Column(name = ""mactdate"")
	private Date _actualDate;
	
	@Column(name = ""mbranch"")
	private int _branch;
And TransactionId defines the primary key columns....
@Embeddable
public class TransactionId {
	
	@Column(name = ""maccno"")
	private String _accountNumber;
	
	@Column(name = ""mprocdate"")
	private Date _processDate;
	
	@Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
executing prepstmnt 23188098 SELECT t0.maccno, t0.mprocdate, t0.mtranseqno, t0.mactdate, t0.mamount, t0.mbranch, t0.mchqcash, t0.mdesc,
 t0.mtmnlno, t0.mtrancde, t0.mtrnfeed 
FROM transaction t0 
WHERE t0.maccno = ?
ORDER BY t0.mamount ASC, t0.mbranch ASC [params=(String) 000734123]
(no idea why it chose mamount, mbranch)
The last line should be:
ORDER BY t0.mprocdate ASC, t0.mtranseqno ASC [params=(String) 000734123]
Thanks
Michael","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-1896,2010-11-23T10:32:43.000-06:00,not store pojos,"merge()  
 merge()   persist()  
      
 merge()
This is a major bug since it means applications where the objects have a natural key cannot use OpenJPA.
In my case the example was a filesystem; each crawl of the filesystem generates its own data objects with file path as the natural key.
These objects then need to be stored into the database.
Previous crawls may have encountered the same files, and the merge operation should cause the latest data from the POJO to be stored in the pre-existing record.
, ?
, ?
, ?)
[params=?
, ?
, ?
, ?]}
[code=0, state=23505]
From discussion with Rick Curtis on the users@openjpa.apache.org list, this is because the version field on a POJO which is unmanaged is not yet set.
An ASSUMPTION seems to be made that no such record exists in the database already since it wasn't loaded from the database in the first place, so a persist is attempted.
Instead, I recommend the database is QUERIED TO FIND OUT if such a record already exists, and the version field is set correspondingly before attempting the merge()
Here is the corresponding thread containing Ricks comments and links to an example in Github which can recreate the problem.
http://bit.ly/hfPjTI","org.apache.openjpa.kernel.VersionAttachStrategy:compareVersion(StateManagerImpl, PersistenceCapable)
org.apache.openjpa.persistence.relations.BasicEntity:getId()"
METHOD,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,ignore class loader,"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We are using openjpa inside an OSGi container together with
openjpa.MetaDataRepository"" value=""Preload=true""
We pass the appliation class loeader as part of our PersistenceUnitInfo implementation by returning it from PersistenceUnitInfo.getClassLoader().
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.
A fix might be quite easily establihed by appending the return value of PersistenceUnitInfo.getClassLoader() to the list of claas loaders participating in the MultiClassLoader set up in
  
  MetaDataRepository.java:310ff
In the meanwhile, we are additionally setting our classloader as context loader during the creation of the EntityManagerFactory by PersistenceProvider.createContainerEntityManagerFactory(), but a fix in MetaDatRepository.preload() is highly appreciated.
TIA for fixing this,
Wolfgang
Stack trace:
org.osgi.service.blueprint.container.ComponentDefinitionException: Error when instantiating bean entityManagerFactory of class null
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:233)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:726)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:624)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:315)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)[:1.6.0_20]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)[:1.6.0_20]
at java.util.concurrent.FutureTask.run(FutureTask.java:166)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)[:1.6.0_20]
at java.lang.Thread.run(Thread.java:636)[:1.6.0_20]
Caused by: <openjpa-2.0.1-r422266:989424 fatal user error> org.apache.openjpa.persistence.ArgumentException: Unexpected error during early loading of entity metadata during initialization. See nested stacktrace for details.
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:331)
at org.apache.openjpa.persistence.PersistenceProviderImpl.preloadMetaDataRepository(PersistenceProviderImpl.java:280)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:211)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.container.AbstractServiceReferenceRecipe$JdkProxyFactory$1.invoke(AbstractServiceReferenceRecipe.java:632)
at $Proxy67.createContainerEntityManagerFactory(Unknown Source)
at org.clazzes.util.jpa.provider.EntityManagerFactoryFactory.newEntityManagerFactory(EntityManagerFactoryFactory.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:221)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:844)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:231)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
... 15 more
Caused by: java.security.PrivilegedActionException: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_20]
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:326)
... 32 more
Caused by: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at org.apache.openjpa.lib.util.MultiClassLoader.findClass(MultiClassLoader.java:216)
at java.lang.ClassLoader.loadClass(ClassLoader.java:321)[:1.6.0_20]
at java.lang.ClassLoader.loadClass(ClassLoader.java:266)[:1.6.0_20]
at java.lang.Class.forName0(Native Method)[:1.6.0_20]
at java.lang.Class.forName(Class.java:264)[:1.6.0_20]
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:233)
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:231)
... 34 more","org.apache.openjpa.meta.FieldMetaData:hashCode()
org.apache.openjpa.meta.FieldMetaData:compareTo(Object)"
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,add integer cause exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .
getLong ( ""foo"" )
Target exception: org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object
at org.apache.commons.configuration.AbstractConfiguration.getLong(AbstractConfiguration.java:667)
It could try to convert it to a Number first and then try to convert it to a long.
It is a very confusing behaviour, because if you save and reload the properties everything works fine (as now the integer is a string).","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,suffer from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
Hi, I have an application which broke when ported from commons math 2.2 to 3.2.
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.
{code}
import org.apache.commons.math3.distribution.HypergeometricDistribution;
public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
{code}
In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing
{code}
return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
it could do:
{code}
return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
This seemed to fix it, based on a quick test.",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-221,2008-08-29T13:31:56.000-05:00,multiply for complex numbers equal for complex numbers,"class Complex  
 {code}
 import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));

                System.out.println(""res=comp: ""+res.equals(comp));
        }
}
 {code}
Hi.
The bug relates on complex numbers.
The methods ""multiply"" and ""equals"" of the class Complex are involved.
mathematic background:  (0,i) * (-1,0i) = (0,-i).
-----------------------------------------------------------------------
{code}
import org.apache.commons.math.complex.
*;
public class TestProg {
public static void main(String[] args) {
ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);
Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);
System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));
System.out.println(""res=comp: ""+res.equals(comp));
}
}
{code}
-----------------------------------------------------------------------
-----------------------------------------------------------------------
I think the ""equals"" should return ""true"".
The problem could either be the ""multiply"" method that gives (-0,-1i) instead of (0,-1i),
or if you think thats right, the equals method has to be modified.
Good Luck
Dieter",org.apache.commons.math.complex.Complex:equals(Object)
METHOD,math,MATH-326,2009-12-29T00:09:20.000-06:00,use wrong formula in ArrayRealVector use wrong formula in OpenMapRealVector,"{code}
     public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }
 {code}

 
  
 {code}   
     public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }
 {code}

    sparseIterator() 
 {code}
   public double getLInfNorm() {
    double norm = 0;
    Iterator<Entry> it = sparseIterator();
    Entry e;
    while(it.hasNext() && (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }
 {code}
the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.
{code} public double getLInfNorm() { double max = 0;
for (double a : data) { max += Math.max(max, Math.abs(a));
} return max;
}
{code}
the += should just be an =.
There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).
{code} public double getLInfNorm() { double max = 0;
Iterator iter = entries.iterator();
while (iter.hasNext()) { iter.advance();
max += iter.value();
} return max;
}
{code}
I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():
{code} public double getLInfNorm() { double norm = 0;
Iterator<Entry> it = sparseIterator();
Entry e;
while(it.hasNext() && (e = it.next()) !
= null) { norm = Math.max(norm, Math.abs(e.getValue()));
} return norm;
}
{code}
Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.","org.apache.commons.math.linear.ArrayRealVector:getLInfNorm()
org.apache.commons.math.linear.OpenMapRealVector:getLInfNorm()"
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,go past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
{code}
public void testMissedEvent() throws IntegratorException, DerivativeException {
final double t0 = 1878250320.0000029;
final double t =  1878250379.9999986;
FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
public int getDimension() {
return 1;
}
public void computeDerivatives(double t, double[] y, double[] yDot)
throws DerivativeException {
yDot[0] = y[0] * 1.0e-6;
}
};
DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
1 0e-10, 1.0e-10);
double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }
{code}","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-552,2011-03-31T22:36:56.000-05:00,return wrong array of indices,"MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) {
    int[] x = counter.getCounts(i);
    System.out.println(i + "" "" + Arrays.toString(x));
}
MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) {
    int[] x = counter.getCounts(i);
    System.out.println(i + "" "" + Arrays.toString(x));
}
Output is:
0 [0, 0]
1 [0, 1]
2 [0, 2]
3 [0, 2]   <=== should be [0, 3]
4 [1, 0]
5 [1, 1]
6 [1, 2]
7 [1, 2]   <=== should be [1, 3]",org.apache.commons.math.util.MultidimensionalCounter:getCounts(int)
FILE,WFCORE,WFCORE-626,2015-04-06T15:53:19.000-05:00,create list elements,"clear(name=attribute)
  get(name=attribute, index=0)
  add(name=attribute, value=test)
  get(name=attribute, index=0)
:list-clear(name=attribute)
:list-get(name=attribute, index=0)
:list-add(name=attribute, value=test)
:list-get(name=attribute, index=0)
The expected result of #4 is ""test"".","org.jboss.as.controller.operations.global.ListOperations
org.jboss.as.controller.operations.global.MapOperations"
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
Description of problem:
Hierarchical composition of profiles was added to AS with EAP7-281 and WFCORE-382
How reproducible:
Always
Steps to Reproduce:
.
/domain.sh
.
/jboss-cli.sh -c
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)
Actual results:
No errors.
Expected results:
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'.
Overriding subsystems is not supported""},
""rolled-back"" => true
}
Workaround:
Add any subsystem to default-new profile:
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new/subsystem=jdr:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,remove extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
This is the same either for jacorb or web or messaging subsystem.
[standalone@localhost:9999 /] /subsystem=jacorb:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=messaging:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=we
web  webservices  weld
[standalone@localhost:9999 /] /subsystem=web
web  webservices
[standalone@localhost:9999 /] /subsystem=web:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
2015-09-24 08:41:09,729 WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
2015-09-24 08:43:13,229 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""DLQ"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""ExpiryQueue"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""pooled-connection-factory"" => ""hornetq-ra"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""RemoteConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""InVmConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""address-setting"" => ""#"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#""),
(""role"" => ""guest"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-acceptor"" => ""in-vm"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""direct-deliver"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-connector"" => ""in-vm"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""messaging"")]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""jsp-configuration"")
]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""static-resources"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""container"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""virtual-server"" => ""default-host"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""connector"" => ""http"")
]
2015-09-24 08:43:20,959 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""web"")]
I think that the migration operation should not show those warnings.","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,scop roles,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
[domain@localhost:9990 /] /host=*:read-resource{roles=master-monitor}
{
""outcome"" => ""success"",
""result"" => [
{
""address"" => [(""host"" => ""master"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""local"" => {}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => true,
""name"" => ""master"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => ""WildFly Core"",
""product-version"" => ""2.0.0.CR6-SNAPSHOT"",
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
},
{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}
]
}
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
The same output on master with WFCORE-994 applied:
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""slave"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""remote"" => {
""protocol"" => undefined,
""port"" => undefined,
""host"" => undefined,
""username"" => undefined,
""ignore-unused-configuration"" => undefined,
""admin-only-policy"" => undefined,
""security-realm"" => ""ManagementRealm""
}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => false,
""name"" => ""slave"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => undefined,
""product-version"" => undefined,
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
}
master-monitor should behave the same as slave-maintainer.","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-1354,2016-02-03T00:19:08.000-06:00,not clone profile with remoting subsystem,"clone(to-profile=test)
So, legacy configs (pre-io) won't have that resource, so there is no requirement.
[domain@localhost:9990 /] /profile=default:clone(to-profile=test)
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0369: Required capabilities are not available:
org.wildfly.io.worker.default in context 'profile=test'; There are no known registration points which can provide this capability.""}
,
""rolled-back"" => true
}
I'm not sure how to deal with this; some sort of marker is needed to disable 'describing' that placeholder resource.","org.jboss.as.remoting.RemotingExtension
org.jboss.as.subsystem.test.AbstractSubsystemBaseTest"
FILE,WFCORE,WFCORE-1028,2015-10-01T19:12:08.000-05:00,handle of invalid roles,"{roles=slave-monitor}
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-monitor}
{
""outcome"" => ""failed"",
""result"" => [],
""rolled-back"" => true
}
The op should fail because the role doesn't exist, but there is no failure-description.
[Host Controller] 12:22:12,314 ERROR [org.jboss.as.controller.management-operation] (management-handler-thread - 3) WFLYCTL0013: Operation (""resolve"") failed - address: ([]): java.lang.IllegalArgumentException: WFLYCTL0327: Unknown role 'slave-monitor'
[Host Controller] 	at org.jboss.as.controller.access.rbac.StandardRoleMapper.canRunAs(StandardRoleMapper.java:95)
[Host Controller] 	at org.jboss.as.controller.access.rbac.RunAsRoleMapper.mapRoles(RunAsRoleMapper.java:143)
[Host Controller] 	at org.jboss.as.controller.access.rbac.RunAsRoleMapper.mapRoles(RunAsRoleMapper.java:71)
[Host Controller] 	at org.jboss.as.controller.access.rbac.DefaultPermissionFactory.getUserPermissions(DefaultPermissionFactory.java:109)
[Host Controller] 	at org.jboss.as.controller.access.permission.ManagementPermissionAuthorizer.authorize(ManagementPermissionAuthorizer.java:91)
[Host Controller] 	at org.jboss.as.controller.access.management.DelegatingConfigurableAuthorizer.authorize(DelegatingConfigurableAuthorizer.java:99)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.getBasicAuthorizationResponse(OperationContextImpl.java:1753)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.authorize(OperationContextImpl.java:1651)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.readResourceFromRoot(OperationContextImpl.java:833)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.readResource(OperationContextImpl.java:818)
[Host Controller] 	at org.jboss.as.controller.operations.global.GlobalOperationHandlers$ModelAddressResolver.execute(GlobalOperationHandlers.java:402)
[Host Controller] 	at org.jboss.as.controller.operations.global.GlobalOperationHandlers$ModelAddressResolver.execute(GlobalOperationHandlers.java:306)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.executeStep(AbstractOperationContext.java:890)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.processStages(AbstractOperationContext.java:659)
[Host Controller] 	at org.jboss.as.controller.AbstractOperationContext.executeOperation(AbstractOperationContext.java:370)
[Host Controller] 	at org.jboss.as.controller.OperationContextImpl.executeOperation(OperationContextImpl.java:1336)
[Host Controller] 	at org.jboss.as.controller.ModelControllerImpl.internalExecute(ModelControllerImpl.java:391)
[Host Controller] 	at org.jboss.as.controller.ModelControllerImpl.execute(ModelControllerImpl.java:217)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.doExecute(ModelControllerClientOperationHandler.java:207)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.access$300(ModelControllerClientOperationHandler.java:129)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:151)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:147)
[Host Controller] 	at java.security.AccessController.doPrivileged(Native Method)
[Host Controller] 	at javax.security.auth.Subject.doAs(Subject.java:422)
[Host Controller] 	at org.jboss.as.controller.AccessAuditContext.doAs(AccessAuditContext.java:92)
[Host Controller] 	at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1.execute(ModelControllerClientOperationHandler.java:147)
[Host Controller] 	at org.jboss.as.protocol.mgmt.AbstractMessageHandler$2$1.doExecute(AbstractMessageHandler.java:299)
[Host Controller] 	at org.jboss.as.protocol.mgmt.AbstractMessageHandler$AsyncTaskRunner.run(AbstractMessageHandler.java:519)
[Host Controller] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[Host Controller] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[Host Controller] 	at java.lang.Thread.run(Thread.java:745)
[Host Controller] 	at org.jboss.threads.JBossThread.run(JBossThread.java:320)","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.controller.access.rbac.RoleMapper
org.jboss.as.test.integration.domain.ServerManagementTestCase"
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,save name id attribute discrepancy,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
There is minor discrepancy in the way I create and use such rollout plan though.
rollout-plan add --name=my-rollout-plan --content={rollout main-server-group(rolling-to-servers=false,max-failed-servers=1),other-server-group(rolling-to-servers=true,max-failure-percentage=20) rollback-across-groups=true}
deploy /path/to/test-application.
war --all-server-groups --headers={rollout id=my-rollout-plan}
Yes, this is really minor issue, but I think that these two attributes used in aforementioned commands should be unified (preferably to name instead of id) as user might be confused when using it.
Note: examples are used from our documentation.
Note: I do not know whether I am missing something but I was not able to retrieve more info how to use rollout header operation in deploy command directly in CLI.","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,add local | remote-destination-outbound-socket-binding,"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
See:
/socket-binding-group=standard-sockets/socket-binding=myBinding:add()
/socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=myBinding:add(host=localhost,port=8765)
or
/socket-binding-group=standard-sockets/local-destination-outbound-socket-binding=myBinding:add(socket-binding-ref=http)
reload
17:31:40,447 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0019: Stopped Driver service with driver-name = h2
17:31:40,453 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0008: Undertow HTTP listener default suspending
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0007: Undertow HTTP listener default stopped, was bound to 127.0.0.1:8080
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-3) WFLYUT0004: Undertow 1.3.21.Final-redhat-1 stopping
17:31:40,458 INFO  [org.jboss.as.mail.extension] (MSC service thread 1-7) WFLYMAIL0002: Unbound mail session [java:jboss/mail/Default]
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 22ms
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0049: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) starting
17:31:40,489 ERROR [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0055: Caught exception during boot: org.jboss.as.controller.persistence.ConfigurationPersistenceException: WFLYCTL0085: Failed to parse configuration
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:131)
at org.jboss.as.server.ServerService.boot(ServerService.java:356)
at org.jboss.as.controller.AbstractControllerService$1.run(AbstractControllerService.java:299)
at java.lang.Thread.run(Thread.java:745)
Caused by: javax.xml.stream.XMLStreamException: ParseError at [row,col]:[410,9]
Message: WFLYCTL0042: A socket-binding or a outbound-socket-binding myBinding already declared has already been declared in socket-binding-group standard-sockets
at org.jboss.as.server.parsing.StandaloneXml_4.
parseSocketBindingGroup(StandaloneXml_4.java:518)
at org.jboss.as.server.parsing.StandaloneXml_4.
readServerElement(StandaloneXml_4.java:254)
at org.jboss.as.server.parsing.StandaloneXml_4.
readElement(StandaloneXml_4.java:141)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:103)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:49)
at org.jboss.staxmapper.XMLMapperImpl.processNested(XMLMapperImpl.java:110)
at org.jboss.staxmapper.XMLMapperImpl.parseDocument(XMLMapperImpl.java:69)
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:123)
... 3 more
17:31:40,490 FATAL [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0056: Server boot has failed in an unrecoverable manner; exiting.
See previous messages for details.
17:31:40,491 INFO  [org.jboss.as.server] (Thread-2) WFLYSRV0220: Server shutdown has been requested.
17:31:40,496 INFO  [org.jboss.as] (MSC service thread 1-2) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 3ms
After this occurs, one needs to fix .
/standalone/configuration/standalone.xml manually by removing duplicate resources.
If there is a problem for those resources to have same names I would welcome that names are checked during the add operation already regarding to all resources in socket-binding and {remote|local}-destination-outbound-socket-binding.
Note: not sure whether CLI component is appropriate, please change if there is better component for this.","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1607,2016-06-17T12:23:38.000-05:00,finish with {,"{""outcome"" => ""success""}
.
This happens even if type of children of security-realm does not exist in server configuration.
It should rather finish with failure to indicate that nothing was removed.",org.jboss.as.domain.management.security.SecurityRealmChildRemoveHandler
FILE,WFCORE,WFCORE-1715,2016-08-15T19:04:54.000-05:00,not reset restartMode,"The doReload()     ServerInventoryService.stop()
The doReload() method of the ReloadContext should restore it.
This ensures the ServerInventoryService.stop() only uses the value set by HostProcessReloadHandler once, for that one reload.
Which, due to this bug, it will be following any HC reload.
But if we restore the default value of restartMode, that is null, and null !
= RestartMode.SERVERS.
So the """"shutdownServers"" logic will no longer kick in.
But should it?
Should the default value of restartMode be ""null""?
Or should it be RestartMode.SERVERS?
If we change the default from null, then the behavior when no reload has happened will change.
Basically we need to decide whether the ""shutdownServers"" logic should happen by default.",org.jboss.as.host.controller.operations.StartServersHandler
FILE,WFCORE,WFCORE-1864,2016-10-13T09:12:31.000-05:00,add command not remove whitespaces from dependencies,"{{
...
    <dependencies>
        <module name=""org.a""/>
        <module name="" org.b ""/>
    </dependencies>
...
}}
{{
...
<dependencies>
<module name=""org.a""/>
<module name="" org.b ""/>
</dependencies>
...
}}
The module name in dependencies should be stripped of leading and trailing whitespaces.","org.jboss.as.cli.handlers.module.ASModuleHandler
org.jboss.as.test.integration.management.cli.ModuleTestCase"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,write attribute have access type metric have attribute,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
/subsystem=messaging-activemq/server=default/jms-queue=DLQ:write-attribute(name=<TAB>
consumer-count  delivering-count  entries  legacy-entries  message-count  messages-added  scheduled-count
From executing :read-resource-description we can see, attributes consumer-count, delivering-count, message-count, messages-added, scheduled-count are of type metric.
[standalone@localhost:9990 jms-queue=q] :write-attribute(name=message-count, value=5)
{
""outcome"" => ""failed"",
""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",
""rolled-back"" => true
}
CLI should not suggest writing attributes that are not writable.","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,not match reality for socket-binding not match reality for *,"description(recursive=true)
fixed-port attribute of socket-binding and fixed-source-port attributes of *-destination-outbound-socket-binding define in its description that there is not necessary to do reload or restart for any of them.
But reality is different.
The attributes are defined as ""restart-required"" => ""no-services"", see /socket-binding-group=standard-sockets:read-resource-description(recursive=true)","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
FILE,WFCORE,WFCORE-2021,2016-11-21T08:50:23.000-06:00,"""ls /socket-binding-group=*"" should print proper error message","{""WFLYCTL0062: Composite operation failed and was rolled back. Steps that failed:"" => {""Operation step-1"" => ""WFLYCTL0030: No resource definition is registered for address [(\""subsystem\"" => \""*\"")]""}
""ls /socket-binding-group="" should print proper error message.
Similar operation ""ls /subsystem="" print good error message.
[standalone@localhost:9990 /] ls /subsystem=*
{""WFLYCTL0062: Composite operation failed and was rolled back.
Steps that failed:"" => {""Operation step-1"" => ""WFLYCTL0030: No resource definition is registered for address [(\""subsystem\"" => \""*\"")]""}}
[standalone@localhost:9990 /] ls /socket-binding-group=*
Exception for ls /socket-binding-group=*: java.lang.IllegalArgumentException
[standalone@localhost:9990 /]",org.jboss.as.cli.handlers.LsHandler
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,throw NullPointerException set REDUCE_STREAMING_KMEANS to true,"return input.getCentroid();  
 input.getCentroid()  clone();
the problem is in the reduce method itself: on line 60 ( return input.getCentroid(); )
it should be input.getCentroid().
clone();
similar to line 81.
java.lang.NullPointerException
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)
at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133)
at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
