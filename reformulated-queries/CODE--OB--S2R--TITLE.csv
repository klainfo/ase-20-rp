Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
CLASS,tika-1.3,TIKA-1070,2013-01-31T05:43:33.000-06:00,StackOverflow error in org.apache.tika.sax.ToXMLContentHandler$ElementInfo.getPrefix(ToXMLContentHandler.java:58),"new ElementInfo(currentElement, namespaces);
The error occurs when parsing big ""XLS"" files and is caused by the ElementInfo stored in ""currentElement"".
Each time a new element is started (method startElement) the current elment is newly overwritten with
currentElement = new ElementInfo(currentElement, namespaces);
where the existing element is used as the parent element.
Since the currentElement is not reset to the parent element after finishing the element (method: endElement) the method getPrefix recursively traverses the parents and finally causes the StackOverFlowError",tika-core.src.main.java.org.apache.tika.sax.ToXMLContentHandler
CLASS,tika-1.3,TIKA-1152,2013-07-23T08:45:11.000-05:00,Process loops infinitely on parsing of a CHM file,"{code}
 
    
     
    
    
    
    
    
    
    
    
 {code}
By parsing [the attachment CHM file|^eventcombmt.chm] (MS Microsoft Help Files), Java process stuck.",tika-parsers.src.main.java.org.apache.tika.parser.chm.lzx.ChmLzxBlock
FILE,DATAMONGO,DATAMONGO-467,2012-06-24T08:58:49.000-05:00,"String @id field is not mapped to ObjectId when using QueryDSL "".id"" path","@Document 
 @Id String id;






 
 QUser.id.eq(""4f43b6a384aea4e77d403709"")
Using this entity (@Document) definition with a String as the declared ID:
and the following query
Always returns null for a repository find.
Looking at the mongoDb query log (mongod -v) it appears that the spring-data-mongodb/QueryDSL layers are not translating the String 4f43b6a384aea4e77d403709 to ObjectId(""4f43b6a384aea4e77d403709"") as one would normally do in the mongo shell.",org.springframework.data.mongodb.repository.support.SpringDataMongodbSerializerUnitTests
FILE,DATAMONGO,DATAMONGO-505,2012-08-14T03:07:56.000-05:00,Conversion of associations doesn't work for collection values,"class Entity {









  Long id;




  @DBRef




  Property property;




}









 class Property {




  Long id;




}









 interface EntityRepository extends Repository<Entity, Long> {









  Entity findByPropertyIn(Property... property);




}






  findByProperty()
Assume you have the following scenario:
The execution of findByProperty() will fail as the given array (or collection) is not correctly translated into a collection of DBRefs.
The reason is that ConvertingIterator treats the value as is and wants to create a DBRef from it.","org.springframework.data.mongodb.repository.query.ConvertingParameterAccessor
org.springframework.data.mongodb.repository.query.ConvertingParameterAccessorUnitTests"
FILE,DATAMONGO,DATAMONGO-523,2012-09-01T03:39:51.000-05:00,@TypeAlias annotation not used with AbstractMongoConfiguration,"@TypeAlias      @Document  @TypeAlias
When using the AbstractMongoConfiguration without any further modifications regarding the converter (afterMappingMongoConverterCreation) the @TypeAlias annotation is not used when writing the _class property.
Seems like it always uses the SimpleTypeInformationMapper.",org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
FILE,DATAMONGO,DATAMONGO-585,2012-12-01T08:28:43.000-06:00,Exception during authentication in multithreaded access,"class which implements Runnable.  
Those
Specifically what can be used for a test case is use a ThreadPoolExecutor, set a minpool and maxpool value of 5, add 25 objects to be executed.
Those objects are a class which implements Runnable.
Those objects each then perform a bunch of inserts into mongoDB using a DocumentDao which calls mongoOperations.insert.",org.springframework.data.mongodb.core.MongoDbUtils
FILE,DATAMONGO,DATAMONGO-629,2013-03-22T04:08:25.000-05:00,Different results when using count and find with the same criteria with 'id' field,"Query q = query 
    
  
 
 
 {




		""id"" : /zzz/




	} 
 
 
 
 
 
  
  
 
 
 {




		""count"" : ""test"",




		""query"" : {




			""_id"" : /zzz/




		}




	 
 
 
 
 
 
     find()     count()
Assume we have following query:
mongoTemplate.find(q,java.util.HashMap.class,'test') gives following query (peeked in mongo console):
The same query, when used in count (mongoTemplate.count(q,'test')) gives:
This is inconsistent since we could have records with field id and they will be retrieved properly from the db.
Count on the other hand will give bad results since it uses _id field.
This is probably the cause of the strange behaviour because find() method does not use QueryMapper and count() does.","org.springframework.data.mongodb.core.mapping.MongoMappingContext
org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-571,2012-11-09T08:00:10.000-06:00,Spring Data for MongoDb doesn't save null values when @Version is added to domain class,"Scenario 
 CrudRepository.findOne()  
 @Version 
 CrudRepository.save()  
 @Version
1. Domain class is loaded from mongodb using CrudRepository.findOne() method.
2. The loaded instances any non id nor @Version annotated field is set to null.
3. The loaded instance is saved to same mongodb using CrudRepository.save() method.
4. The field that has been set to null doesnt write to database, its unchanged.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.Update"
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,Updating an object does not write type information for objects to be updated,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
I'm using quite complex domain model, that consist of instantiable domain classes as well as of abstract ones.
I used 1.0.0.
M5 version, and the type information (under _class key) was stored with object when it was necessary to be able to read it from database later.
RELEASE version that broke my application as it saves the objects without type information and later it is impossible to read it back to java model.
What I found is that MappingMongoConverter.writeInternal(...) method that in turn calls addCustomTypeIfNecessary(...) (line 330) which puts type information into DBObject.
During execution of convertToMongoType(...) (at line 851) removeTypeInfoRecursively(...) is called which clears type data saved earlier under _class key.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-702,2013-06-20T00:12:30.000-05:00,"Spring Data MongoDB projection search, is not properly configured with respective Java Pojo","@Field 
 query.fields() 
  
 @Field
If Model Java POJO, is annotated with @Field, then during search with query.fields().
include(field)
uning mongoTemplate it does not java property field as field parameter, rather it needs the name declared with name defined with @Field annotation.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-717,2013-07-10T11:13:46.000-05:00,Application context is not properly distributed to persistent entities,"@Override




	protected <T> BasicMongoPersistentEntity<T> createPersistentEntity(TypeInformation<T> typeInformation) {









		BasicMongoPersistentEntity<T> entity = new BasicMongoPersistentEntity<T>(typeInformation);









		if (context != null) {




			entity.setApplicationContext(context);




		}









		return entity;




	}









	




	 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




	}






 
  
 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




                // Send the application context to ALL the PersistentEntities, not just ones created after this point




               for (BasicMongoPersistentEntity entity : getPersistentEntities()) {




                   entity.setApplicationContext(applicationContext);




               }




	}






      testMultiTenantSave()  
   initialize()    
 
 @Bean




	public MongoMappingContext mongoMappingContext() throws ClassNotFoundException {









		MongoMappingContext mappingContext = new MongoMappingContext();




		mappingContext.setInitialEntitySet(getInitialEntitySet());




		mappingContext.setSimpleTypeHolder(customConversions().getSimpleTypeHolder());




		mappingContext.initialize(); // <----









		return mappingContext;




	}
The MongoMappingContext does not properly distribute the application context when set to persistent entities that have already been added.
Current code:",org.springframework.data.mongodb.config.AbstractMongoConfigurationUnitTests
FILE,DATAMONGO,DATAMONGO-721,2013-07-11T11:36:06.000-05:00,Polymorphic attribute type not persisted on update operations,"@Document
public class ParentClass {
   private List<ChildClass> list;
}
    @Document   
        
  
 mongoTemplate.updateFirst(Query.query(criteria), 
  new Update().push(""list"", child));
Here is our situation: we have an entity which have an attribute which is a list of another kind of entity, like the code below.
When using MongoTemplate class with code such as below, the _class attribute is not inserted on the embedded document, so, if one of the items of the list attribute is a subclass of ChildClass, and ChildClass is an abstract class, we begin to face instantiation problems.
Here is one example of usage of MongoTemplate in which we found a problem.
If child is a subclass of ChildClass, the _class attribute is not added to the embedded document.",org.springframework.data.mongodb.core.convert.QueryMapper
FILE,DATAMONGO,DATAMONGO-602,2013-01-30T02:22:53.000-06:00,Querying with $in operator on the id field of type BigInteger returns zero results,"List<BigInteger> profileIds = findProfileIds();




Predicate predicate = QProfileDocument.profileDocument.id.in(profileIds);




Iterable<ProfileDocument> profiles = profileRepository.findAll(predicate);
There is a problem when trying to query for documents with id field is in a given list of BigInteger values.
For example, these lines will work only if given id list contains only one item.
The query looks like this for a profileIds with multiple elements
and it looks like this when there is only one item in the list.
In the first case, query will return no results and in the second it will work and return an Iterable with one item.",org.springframework.data.mongodb.core.MongoTemplateTests
FILE,DATAMONGO,DATAMONGO-805,2013-12-02T06:34:36.000-06:00,Excluding DBRef field in a query causes a MappingException,"Query query = new Query(Criteria.where(""parentField"").is(""test""));
        query.fields().exclude(""children"");
        ParentClass parentClass = mongoOperations.findOne(query, ParentClass.class);
Excluding a field in a query where the field is a DBRef as below throws a MappingException.
Exception trace:
I've attached a simple test case that throws the MappingException.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.mapping.MappingTests
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-897,2014-04-01T04:38:51.000-05:00,FindAndUpdate broken when using @DbRef and interface as target,"MongoTemplate.findAndModify(...)   @DbRef  @DbRef
NullPointerException is thrown when using MongoTemplate.findAndModify(...) with @DbRef and interface as @DbRef target.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.QueryMapper"
FILE,DATAMONGO,DATAMONGO-892,2014-03-28T09:08:03.000-05:00,<mongo:mapping-converter> can't be configured as nested bean definition,"parserContext.isNested()
The sample config which worked in the 1.1.1 version, but doesn't work now:
That's because MappingMongoConverterParser doesn't check if the parserContext.isNested(), registers BeanDefinition and returns null","org.springframework.data.mongodb.config.MappingMongoConverterParser
org.springframework.data.mongodb.config.MappingMongoConverterParserIntegrationTests"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,"Using ""OrderBy"" in ""query by method name"" ignores the @Field annotation for field alias.","@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
I created a method using the ""query by method name"" approach:
Inside my Answer object, I have a field called ""Score"" that is annotated with
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-745,2013-09-03T02:15:08.000-05:00,@Query($in) and Pageable in result Page total = 0,"@Query(""{'snapshotId' : ?0 ,'defects.id':{ $in:?1}}"")
Page<Test> findBySnapshotIdAndDefects(ObjectId snapshotId, List<Integer> defectIds, Pageable pageable) 
 {5225a5ece4b0a01629fce9c6={ ""_id"" : 
{ ""$oid"" : ""5225a5ece4b0a01629fce9c6""}
Hi If I used MongoRepository and anotation Quary and Pageable in response I get true result but getTotalElements == 0","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.AbstractPersonRepositoryIntegrationTests
org.springframework.data.mongodb.repository.PersonRepository"
FILE,DATAMONGO,DATAMONGO-938,2014-05-21T06:09:48.000-05:00,Exception when creating geo within Criteria using MapReduce,"Criteria.where(""location"")  within(new Box(lowerLeft, upperRight));
I am getting an IllegalArgumentException when I try to query a MongoDB collection using a Criteria.within and a Box.
The exception reads:","org.springframework.data.mongodb.core.mapreduce.MapReduceTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-952,2014-06-10T22:45:47.000-05:00,@Query annotation does not work with only field restrictions,"@Query 
 @Query(fields = ""{ 'email' : 1 }"")




User findByEmail(String email)






  @Query
If you are using repository based queries and try to use @Query annotation to limit the fetched fields, it has zero effect.
For example repository query:
The query above returns all fields of the User and the fields definition has no effect at all.
If you are using @Query with value attribute to define the query, then the fields limitation is applied though.",org.springframework.data.mongodb.repository.query.PartTreeMongoQuery
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,Problem with lazy loading in @DBRef when getting data using MongoTemplate,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
The situation is simple: if we reference on an entity class another entity (both annotated with @Document) called Parent and Child.
Here is the code:
// Than we try to load this same entity from the database
assertNull(persisted.getChild()); // it fails
I attached a project with the JUnit test which reproduces the problem for you.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-1068,2014-10-08T19:43:32.000-05:00,elemMatch of Class Criteria fails to build special cirteria,"public class Room {




		private String name;




		private List<Date> occupied;




	}






 
 {




		occupied : {




			$not : {




				$elemMatch : {




					$gte : start,




					$lte : end




				}




			}




		}




	}






 
 Criteria c1 = new Criteria().gte(start).lte(end);




	Criteria c = Criteria.where(""occupied"").not().elemMatch(c1);






 
 {




	occupied : {




		$not : {




			$elemMatch : {




			}




		}




	}




}






  elemMatch(Criteria)
There is an entity like this:
I need to issue a criteria to fetch all documents in which none of elements of occupied falls into a specified date range.
The query JSON is:
Then, I tried on
But the serialization to JSON for c is:
It seems that c1 will be explained to empty Map by invoking elemMatch(Criteria) because I haven't assign a key to it.","org.springframework.data.mongodb.core.query.CriteriaTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-1078,2014-10-28T02:23:26.000-05:00,@Query annotated repository query fails to map complex Id structure.,"@Query(""{'_id': {$in: ?0}}"")




List<User> findByUserIds(Collection<MyUserId> userIds) 
 {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}
StringBasedMongoQuery converts any complex object to the according mongo type including type restrictions via _class.
Therefore annotated queries like:
end up being converted to:","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,"@Query $in does not remove ""_class"" property on collection of embedded objects","@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
Following method on repository
generates incorrect query.
I attached test project demonstrating this bug.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,"geoNear, does not return all matching elements, it returns only a max of 100 documents","public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
I have the following query:
I expect 1000 ""matching"" documents But i only get 100.
There is some default being set, that restricts the result to 100.",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,Repository keyword query findByInId with pageable not returning correctly,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
I've been trying to use the In-keyword with identifiers and making the query pageable.
The query returns results but getTotalElements() and getTotalPages() always returns 0.
Also when you try to get any other page than 0, no results return.
Below is a strip down example I used for testing;
I've created 3 types and 10 items per those types, results in a total of 30 items.
Assert.assertEquals(30, results.getTotalElements()); // this is returning 0
Assert.assertEquals(6, results.getTotalPages());     // this is returning 0","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1202,2015-04-14T02:36:40.000-05:00,Indexed annotation problems under generics,"@Indexed
Under simple scenarios with reflexive DBRef relations works as expected, but if used in conjunction with generics it doesn't create the index expected.
I provide a github project with two scenarios:
Employer: Very simple reflexive relation that works nicely (EmployerTest)
Customer: A little more complex scenario using GenericCustomer as a base class to allow having many different kinds of customers.
This scenario fails at creating the index (CustomerTest).
If you run Application it will just create indexes and put some data in customer collection assuming a MongoD is running locally at 27017","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreator
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreatorIntegrationTests
org.springframework.data.mongodb.core.index.IndexResolver"
FILE,DATAMONGO,DATAMONGO-1250,2015-07-03T21:07:44.000-05:00,Custom converter implementation not used in updates,"@Document 
 
 
 @Document




public class MyPersistantObject  
 public Allocation allocation;




     public BigDecimal value;









     
 private final String code;









         Allocation(String code) {




            this.code = code;




        }









         public static Converter<Allocation, String> writer() {




            return new Converter<Allocation, String>() {




                public String convert(Allocation allocation) {




                    return allocation.getCode();




                }




            };




        }









         public static Converter<String, Allocation> reader() {




            return new Converter<String, Allocation>() {




                public Allocation convert(String source) {




                    return Allocation.getByCode(source);




                }




            };




        }









         public static Allocation getByCode(String code)  
 return AVAILABLE;




                 
 return ALLOCATED;




             
 throw new IllegalArgumentException(""Unable to get Allocation from: "" + code);




         
 public String getCode() {




            return code;




        }




     
 @Bean




    public CustomConversions customConversions() {




        return new CustomConversions(Arrays.asList(




                MyPersistantObject.Allocation.reader(),




                MyPersistantObject.Allocation.writer()




        ));




    }






 
 @Test




    public void testConversion() {




        Update update;




        Query query;




        MyPersistantObject returned;




        MyPersistantObject myPersistantObject = new MyPersistantObject();




        myPersistantObject.allocation = AVAILABLE;




        myPersistantObject.value = new BigDecimal(1234567);









        mongoTemplate.save(myPersistantObject);









        // Check it was saved correctly - first with invalid allocation to confirm conversion in query




        query = query(where(""allocation"").is(ALLOCATED));




        assertThat(mongoTemplate.findOne(query, MyPersistantObject.class), is(nullValue()));









        // Check it was saved correctly - now with valid allocation to confirm conversion in query




        query = query(where(""allocation"").is(AVAILABLE));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(AVAILABLE));




        assertThat(returned.value.longValue(), is(1234567L));









        try {




            // Update allocation from constant - will fail




            update = update(""allocation"", ALLOCATED);




            mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        } catch (Exception e) {




            System.err.println(""failed to convert allocation: java.lang.IllegalArgumentException: can't serialize class converter_test.MyPersistantObject$Allocation"");




        }









        // Update allocation from string value - succeeds




        update = update(""allocation"", ALLOCATED.getCode());




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check allocation update




        query = query(where(""allocation"").is(ALLOCATED));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(ALLOCATED));









        // Update value only - will fail: Caused by: java.lang.IllegalArgumentException: Unable to get MyPersistantObject.Allocation from: 54321




        // Tries to use MyPersistantObject.Allocation converter to String




        update = update(""value"", new BigDecimal(54321));




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check value update




        returned = mongoTemplate.findAll(MyPersistantObject.class).get(0);




        assertThat(returned.value.longValue(), is(54321L));




    }
I have a custom (de)serialiser for an enumerated type, and it works perfectly when saving and loading a @Document annotated POJO.
It also works when building and executing a Query object.
However when used in an Update, it is either ignored, or called in situations where it shouldn't.
Please clone https://github.com/patrickherrera/converter_test.git for a full test application.
In brief there is a POJO and for the purposes of the test it has a static enum with the desired converters:
There is a unit test that drives a few scenarios:
By use of a positive and negative case, it appears that the converter is being called correctly when used in the Query builder.
When it comes to an Update, the Enum is unable to be serialised correctly, and an exception is thrown to that effect.
So it appears that the customer converter for converting from my Enum is not called in this situation.
The BigDecimal is converted to a String by an existing converter I assume, but then my customer converter is called to try and convert the numeric String into an Allocation Enum which of course fails.
That second variant never seems to be called in MappingMongoConverter, but perhaps if that type information was passed then it would not use my Allocation converter.
Without type information it seems the default is just to use the first converter that can handle the input type, in this case a String.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.UpdateMapper"
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,Missing indexes in associations involving generic types,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
When an association between documents involves generic types, the type information is not correctly inferred at startup time resulting in missing indexes.
Given:
When defining a class Catalog with a list of ""wrapped"" books:
The index ""name"" inherited from AbstractProduct is created (book2.content.name) inside ""catalog"" , but the index defined on the Book class itself (isbn) is not created as Spring Data Mongo is only inferring type infromation from the ProductWrapper class definition (ProductWrapper <T extends AbstractProduct>).
If the wrapper class is defined as ProductWrapper<T>, then no indexes are created at all on Catalog.books2.content.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1290,2015-09-17T15:30:23.000-05:00,@Query annotation with byte[] parameter does not work,"Optional<SampleDomainObject> findBySampleData(byte[] sampleDate) 
 @Query(""{ 'sampleData' : ?0 }"")




Optional<SampleDomainObject> findBySampleDateWithAnnotation(byte[] sampleData)
... but only the first works.
Please find an example project with test attached!","org.springframework.data.mongodb.repository.query.StringBasedMongoQuery
org.springframework.data.mongodb.repository.query.StringBasedMongoQueryUnitTests"
FILE,DATAMONGO,DATAMONGO-1360,2016-01-16T07:47:34.000-06:00,Cannot query with JSR310,"query.addCriteria(where(""createdDate"").lte(LocalDateTime.now()));
I have a MongoDb document I successfully store using Spring Data MongoDb.
It looks like this:
When I create a custom Criteria query that looks like this:
The resulting MongoDb query looks like this:
It consequently fails with this message:
It does not fail when I use a java.util.Date in my query even though I have stilled persisted my document with a java.time.LocalDateTime object.
The query then looks slightly different like this:","org.springframework.data.mongodb.core.Venue
org.springframework.data.mongodb.core.geo.AbstractGeoSpatialTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1288,2015-09-16T23:51:16.000-05:00,"Update.inc(String, Number) method fails to work with AtomicInteger","org.springframework.data.mongodb.core.query.Update.inc(String, Number)
Even though the org.springframework.data.mongodb.core.query.Update.inc(String, Number) method takes the parameter as Number, it doesn't actually work right with things that aren't simple primitive object wrappers.
For example, if you pass in an AtomicInteger, the update fails to execute, as the generated json isn't correct.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters
org.springframework.data.mongodb.core.convert.CustomConversions"
FILE,DATAMONGO,DATAMONGO-1438,2016-05-26T14:01:14.000-05:00,I get a warning in my logs since switched to Spring Data MongoDB Hopper-SR1 Release Train in Spring Boot 1.3.5,"@Document
When I start my Spring Boot 1.3.5 application with no custom conversions and with Spring Data MongoDB Release Train Hopper-SR1 I get following warning in my logs:
I have alle my Domain classes they are saved in MongoDB annotated with @Document (see DATAMONGO-1413)","org.springframework.data.mongodb.core.convert.MongoConvertersUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters"
FILE,DATAMONGO,DATAMONGO-1394,2016-03-09T10:36:46.000-06:00,References not handled correctly when using QueryDSL,"public class Book {




     @DBRef




     private Library library;




} 












 
 public class Library {




     @Id




     private String id;




}












  
   ;




QBook book = QBook.book;




BooleanExpression exp = book.library.id.eq(library_id);




    




List<Book> list = bookRepository.findAll(exp);  // EMPTY






 
  
 Library library = libraryRepository.findById(library_id);




QBook book = QBook.book;




BooleanExpression exp = book.library.eq(library);









List<Book> list = bookRepository.findAll(exp);  // EXPECTED ITEMS






 
  
 List<Book> list = bookRepository.findByLibraryId(library_id) // EXPECTED ITEMS
Trying to use eq on reference's $id, with no luck.
This is an example code:
List<Book> list = bookRepository.findAll(exp);  // EMPTY
Doing:
or:","org.springframework.data.mongodb.repository.support.SpringDataMongodbSerializer
org.springframework.data.mongodb.repository.support.QuerydslRepositorySupportTests"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,Query mapper does not use @Field field name when querying nested fields in combination with nested keywords,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
we have a document class;
In MongoTemplate.java, the call to :
resolves the fields to the input query to the ones in the @Field annotations, except for these in embedded arrays.
So, in the example above, resolution fields in DBObject remains resolution.
While, the status field resolves to stat.
The query submitted to mongo after getMappedObject is called:
Which doesn't get any data, because there is no field called resolution (the field in mongo is res).
Note: The query input to getMappedObject is:
Notice the status and displays fields correctly get converted to the value in the @Field annotation.
This basically means that any queries that operate on fields (with a name different from the peristed name) in the inner list will fail.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-1486,2016-09-07T16:46:53.000-05:00,Changes to MappingMongoConverter Result in Class Cast Exception,"Map<Integer, Map<Platform, String>> descriptions = new HashMap<>();






 
 public void setAlternateDescriptionMap(int compositeId, Map<Integer, Map<Platform, String>> alternateDescriptionsMap) {




		Query query = new Query();




		query.addCriteria(Criteria.where(""_id"").is(compositeId));




		Update update = new Update();




		update.set(""alternateDescriptionMap"", alternateDescriptionsMap);









		coreMongoTemplate.updateFirst(query, update, ""product"");




	}






   
    
 
 MappingMongoConverter.convertMongoType()
We have a situation where we have a model object that looks like the following:
When we execute the following:
We end up getting the following exception:","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests"
CLASS,derby-10.7.1.1,DERBY-4835,2010-10-06T11:05:13.000-05:00,Trigger plan does not recompile with upgrade from 10.5.3.0 to 10.6.1.0 causing  java.lang.NoSuchMethodError,"tidlggls(blt_number,create_date,update_date,propagation_date,glossary_status,
     time_stamp,min_max_size )
    
      
 
  
 tidlrblt(BLT,BLT_SIZE,MIN_MAX_SIZE)  
 
     
  
   GeneratedMe
thod;    
  
  
 if (fromVersion.majorVersionNumber >= DataDictionary.DD_VERSION_DERBY_10_5)
				bootingDictionary.updateMetadataSPSes(tc);
			else
				bootingDictionary.clearSPSPlans();

  clearSPSPlans()
Trigger plan does not recompile on upgrade from 10.5.3.0 to 10.6.1.0  causing the following exception  the first time the trigger is fired after upgrade.
To reproduce, run the attached script 10_5_3_work.sql with the 10.5.3.0  release and then connect with 10.6.1.0 and insert into the table with the trigger:","org.apache.derby.impl.sql.catalog.DD_Version
org.apache.derbyTesting.functionTests.tests.upgradeTests.BasicSetup"
CLASS,derby-10.7.1.1,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:",org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.7.1.1,DERBY-4889,2010-11-05T20:06:56.000-05:00,Different byte to boolean conversion on embedded and client,"PreparedStatement ps = c.prepareStatement(""values cast(? as boolean)"");
        ps.setByte(1, (byte) 32);
        ResultSet rs = ps.executeQuery();
        rs.next();
        System.out.println(rs.getBoolean(1));

 If setByte()   setInt()
The following code prints ""true"" with the embedded driver and ""false"" with the client driver:
If setByte() is replaced with setInt(), they both print ""true"".","org.apache.derbyTesting.functionTests.tests.jdbcapi.ParameterMappingTest
org.apache.derby.impl.drda.DRDAConnThread"
CLASS,derby-10.7.1.1,DERBY-4892,2010-11-06T04:14:51.000-05:00,Unsafe use of BigDecimal constructors,"test_06_casts(org.apache.derbyTesting.functionTests.tests.lang.UDTTest)
The problem appears when the build does not use the Java 1.4 libraries.
The compiled byte-code will therefore use those Java 5 constructors, and the code will fail at run-time if ever executed on a Java 1.4 JVM.
To reproduce, build Derby without ant.properties on a system where PropertySetter doesn't find JDK 1.4.
Verify with -DprintCompilerProperties=true that java14compile.classpath is built up of jar files from a Java 5 or Java 6 directory.
Then run org.apache.derbyTesting.functionTests.tests.lang.UDTTest using a Java 1.4 JVM.
You'll see two errors of this kind:
The problem in client.am.Cursor can be seen if you follow the same procedure as above, and instead of UDTTest run ParameterMappingTest with the patch for DERBY-4891 that enables testing of booleans.","org.apache.derbyTesting.functionTests.tests.lang.Price
org.apache.derby.client.am.Cursor
org.apache.derbyTesting.system.oe.client.Submitter"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,Pig creates wrong schema after dereferencing nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
The following script fails:
DESCRIBE thinks it is {f1: int, f2: int} instead.
When dump is
used, the data is actually in form of the correct schema however, ex.
Because the schema is incorrect,
the reference to ""nested_tuple"" in the ""uses_dereferenced"" statement is
considered to be invalid, and the script fails to run.
The error is:","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2828,2012-07-19T05:03:16.000-05:00,Handle nulls in DataType.compare,"Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);
While using TOP, and if the DataBag contains null value to compare, it will generate the following exception:
The reason is that if the typeFound is true , and the dataType is not null, and field1 is null, the script failed.","src.org.apache.pig.data.DataType
src.org.apache.pig.builtin.TOP
test.org.apache.pig.test.TestNull"
CLASS,pig-0.11.1,PIG-3114,2013-01-03T19:49:42.000-06:00,Duplicated macro name error when using pigunit,"{code:title=test.pig|borderStyle=solid}
    {
    $C = ORDER $QUERY BY total DESC, $A;
}  
  
     AS total;

queries_ordered = my_macro_1(queries_count, query);

    
   ;
{code}
I'm using PigUnit to test a pig script within which a macro is defined.
Pig runs fine on cluster but getting parsing error with pigunit.
Pig script which is failing :","src.org.apache.pig.PigServer
test.org.apache.pig.test.pigunit.TestPigTest
test.org.apache.pig.pigunit.PigTest
test.org.apache.pig.pigunit.pig.PigServer"
CLASS,pig-0.11.1,PIG-3267,2013-04-03T16:14:30.000-05:00,HCatStorer fail in limit query,"{code}
 
  
  
     ;
{code}

 
 {code}
  {code}
The following query fail:
Error happens before launching the second job. Error message:","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.11.1,PIG-3292,2013-04-24T03:06:41.000-05:00,Logical plan invalid state: duplicate uid in schema during self-join to get cross product,"{code}
 
  
   {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}

 
 {code}
   
 {code}

 
 {code}
 
  
   {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}
The problem:
We want to do self join to get cross-product
{code}
a = load '/input' as (key, x);
And an error:
{code}
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2270: Logical plan invalid state: duplicate uid in schema : 1-7::x#16:bytearray,y::x#16:bytearray
{code}","test.org.apache.pig.test.TestEvalPipelineLocal
src.org.apache.pig.newplan.logical.relational.LOCross"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,"ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations","{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
Consider the following example
On input data:
{code}
1       1001    101
1       1002    103
1       1003    102
1       1004    102
2       1005    101
2       1003    101
2       1002    123
3       1042    101
3       1005    101
3       1002    133
{code}
This will give a wrongful output like .
.
{code}
(1 1001,1001)
(1 1002,1002)
(1 1002,1002)
(1 1002,1002)
{code}",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,pig-0.11.1,PIG-3316,2013-05-08T14:01:36.000-05:00,Pig failed to interpret DateTime values in some special cases,";
dump A;
For the query
with input data
1 1970-01-01
2 1970-01
pig generates the following output
(1 1970-01-01T00:00:00.000-01:00)
(2 1970-01-01T00:00:00.000-01:00)
which seemingly incorrectly interprets the day or month part as time zone.","test.org.apache.pig.test.TestDefaultDateTimeZone
src.org.apache.pig.builtin.ToDate"
CLASS,pig-0.11.1,PIG-3329,2013-05-16T22:44:41.000-05:00,RANK operator failed when working with SPLIT,"RANK b;
dump d;
input.txt:
1 2 3
4 5 6
7 8 9
script:
a = load 'input.txt' using PigStorage(' ') as (a:int, b:int, c:int);
SPLIT a into b if a > 0, c if a > 5;
d = RANK b;
dump d;
job will fail with error message:","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.11.1,PIG-3379,2013-07-16T13:37:26.000-05:00,Alias reuse in nested foreach causes PIG script to fail,"{code:title=temp.pig}
       
      
    
    {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);

  DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);

  GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
        
  GENERATE timeStamp;
describe A;
{code}
 
 {code}
   
    
 {code}
The following script fails:","src.org.apache.pig.parser.LogicalPlanBuilder
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.expression.ProjectExpression"
CLASS,pig-0.11.1,PIG-3495,2013-10-02T16:40:25.000-05:00,Streaming udf e2e tests failures on Windows,"{code}
 
 PigStorage()  
 myfuncs.square(age);
dump b;
{code}
Register a jython script with an absolute path fail.
For Example:
{code}
register 'D:\scriptingudf.py' using jython as myfuncs;
a = load 'studenttab10k' using PigStorage() as (name, age:int, gpa:double);
b = foreach a generate myfuncs.square(age);
dump b;
{code}",src.org.apache.pig.scripting.ScriptEngine
CLASS,pig-0.11.1,PIG-3510,2013-10-09T18:02:45.000-05:00,New filter extractor fails with more than one filter statement,";
{code}
{code:title=one filter}
      ;
{code}
Here is an example that demonstrates the problem:
{code:title=two filters}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001);
c = FILTER b BY (event_id == 419 OR event_id == 418);
{code}
{code:title=one filter}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001) AND (event_id == 419 OR event_id == 418);
{code}
Both dateint and event_id are partition columns.
For the 1 filter case, the whole expression is pushed down whereas for the 2 filter case, only (event_id == 419 OR event_id == 418) is pushed down.",src.org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
full stack trace:
it happens every time the REDUCE_STREAMING_KMEANS is set to true.",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
CLASS,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
I hashed the features with max size 5M (because I didn't know how many
features were in the dataset and wanted to minimize collisions).
The kmeans ran fine and generate sensible looking results, but when I tried
to run ClusterDumper I got the following error:
It worked fine when I reduced the hash size to be <= than the total number
of features, but this is not desirable in general (for me) since I don't
know the number of features before I run the job (and if I guess too high
then ClusterDumper crashes)",integration.src.main.java.org.apache.mahout.utils.vectors.VectorHelper
CLASS,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}

 {Code}

  StreamingKMeansThread.call()

 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }

    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
The issue is caused by the following code in StreamingKMeansThread.call()",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread
CLASS,zookeeper-3.4.5,ZOOKEEPER-1535,2012-08-14T18:56:40.000-05:00,ZK Shell/Cli re-executes last command on exit,"{{ctrl+d}}   {{ls}}   {{ctrl+d}}   {{ls}}  
 {noformat}
 
 {noformat}
In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell.
In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run.",src.java.main.org.apache.zookeeper.ZooKeeperMain
CLASS,zookeeper-3.4.5,ZOOKEEPER-1619,2013-01-11T09:57:16.000-06:00,Allow spaces in URL,"{code}
 
 {code}

 
 {code}
 
 {code}
Currently, spaces are not allowed in the url.
This format will work.
This format will not (notice the spaces around the comma)",src.java.main.org.apache.zookeeper.client.ConnectStringParser
CLASS,zookeeper-3.4.5,ZOOKEEPER-1700,2013-05-07T19:43:31.000-05:00,FLETest consistently failing - setLastSeenQuorumVerifier seems to be hanging,"{noformat}
   
  
    
    
          
      
    
    
  
  
  
  
      
  
  
     
      
      
    
    
 {noformat}
I'm consistently seeing a failure on my laptop when running the FLETest ""testJoin"" test.
What seems to be happening is that the call to setLastSeenQuorumVerifier is hanging.
See the following log from the test, notice 17:35:57 for the period in question.
Note that I turned on debug logging and added a few log messages around the call to setLastSeenQuorumVerifier (you can see the code enter but never leave)",src.java.test.org.apache.zookeeper.test.FLETest
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,ZooKeeper Server fails if snapCount is set to 1,"int randRoll = r.nextInt(snapCount/2);
{code}
If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
METHOD,bookkeeper-4.1.0,BOOKKEEPER-387,2012-09-04T04:27:35.000-05:00,BookKeeper Upgrade is not working.,"{code}
     
 {code}
I am trying to upgrade BK from 4.1.0 to 4.2.0, but it will log as ""Directory is current, no need to upgrade? even then it will continue and fail.
and throwing following exception.",org.apache.bookkeeper.bookie.UpgradeTest:testCommandLine()
CLASS,argouml-0.22,3923,2006-02-07T13:17:48.000-06:00,Problem importing Poseidon activity diagrams from XMI,"Collection actionStates = getModel().getAllActionStates();
  Iterator iterActionState = actionStates.iterator();
iterActionState.hasNext(); 
 ActionStateFacade actionState =
(ActionStateFacade) iterActionState.next();
1) Import an XMI from Poseidon, which works well with AndroMDA (the
PiggyBank example).
2) If I add my activity diagram under the use case diagram I always get a new activity graph, so I have 2 activity graphs alltogether.
I cannot add an activity diagram under the imported activity graph.
Please see the screenshot I attached.
Screenshot:
3) This code, which works with Poseidon, won't work with ArgoUML:
actionState is always ""null"".
4) Importing the activity diagram from Poseidon works and the result can be processed by AndroMDA but if you are making the activity diagram from the beginning with ArgoUML, it won't work because of the error above",org.argouml.persistence.XMIParser
CLASS,argouml-0.22,4200,2006-05-11T23:30:25.000-05:00,Activity diagrams vanish in a new Package,"Model folder;
Activity diagrams vanish in a Package.
That is how you can get the defect:
stand on the main Model folder;
right mouse click and choose from droped meniu ""Add Package"";
new package appears;
rename the package;
there are some diagrams below, so i jus drag them one by one using right
mouse button and releasing over the package;
the package remains unchanged (i expected it should become expandable like a
diagram in the tree);
in selected package Properties Owned Elements I can see my diagrams, but
double click does not allow me to review them.","org.argouml.ui.explorer.PerspectiveManager
org.argouml.ui.explorer.rules.GoModelElementToBehavior"
CLASS,lucene-4.0,LUCENE-4461,2012-10-05T10:21:38.000-05:00,Multiple FacetRequest with the same path creates inconsistent results,"FacetSearchParams facetSearchParams = new FacetSearchParams();
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
Multiple FacetRequest are getting merged into one creating wrong results in this case:",org.apache.lucene.facet.search.StandardFacetsAccumulator
CLASS,lucene-4.0,LUCENE-4561,2012-11-15T10:06:09.000-06:00,DWPT assert tripped again,"{noformat}
   
 {noformat}

 
 {noformat}
   
  
    
    
    
            
    
    
            
    
    
               {field=DFR I(ne)1} 
                                                                                                                                                                                                                                                                                                                          
 {noformat}
TestBagOfPositions tripped the spooky DWPT ram used on flush assert in http://jenkins.sd-datasolutions.de/job/Lucene-Solr-4.x-Linux/2472/
It reproduces for me:
Full failure:",org.apache.lucene.index.DocumentsWriterFlushControl
CLASS,jedit-4.3,1193683,2005-05-02T09:22:25.000-05:00,"folding bug, text is in a black hole","{\{\{ test
aaaa
bbbb
cccc
\}
Hi, when you have some folded text \(folding closed\)
Close it you'll get
Delete one \{
You'll have
and no text fold anymore.
But the weird thing is that the text is not completely lost because you can type another \{ \(no need to undo\)
and the fold will reappear magically.
So the text was still here but hidden by jEdit's text area",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1571752,2006-10-05T21:26:12.000-05:00,'Add Explicit Fold'  in PHP mode - wrong comments,"{

\} 
 {\{\{  --&gt;
function foo\(\) \{

\} //\}\}\}
Before 'Add Explicit fold' the content of buffer looks like this \('X' means selection boundaries\):
After:",org.gjt.sp.jedit.textarea.TextArea
CLASS,jedit-4.3,1599709,2006-11-20T13:17:56.000-06:00,NPE with JEditBuffer and new indenting,"lt;ENTER&gt;
To reproduce:
Save an empty buffer as test.php.
Type:
\---------------
&lt;?
php
$foo = '&lt;ENTER&gt;
\---------------
When pressing ENTER, after the ""'"", the following exception gets thrown:",org.gjt.sp.jedit.buffer.JEditBuffer
CLASS,jedit-4.3,1600401,2006-11-21T13:16:31.000-06:00,StringIndexOutOfBoundsException in TokenMarker,"lt;init&gt; 
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
      
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt; 
      
      
      
      
      
      
    
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt;
After pressing ENTER in a rather long line in a .
php file, I got the following exception.
The line appeared wrong highlighted in the first place, that why I wanted to see if splitting it would resolve the highlight issue.
After this the buffer window is not usable anymore \(e.g. I cannot enter it, nor does it get repainted\).",org.gjt.sp.jedit.syntax.TokenMarker
CLASS,jedit-4.3,1658252,2007-02-12T17:48:03.000-06:00,C mode: incorrect bracket matching in multi-line defines,"{ \
code;                         \
more code;                    \
even more code;               \
\}
Try this define:
The brackets don't match.","org.gjt.sp.jedit.syntax.ParserRule
org.gjt.sp.jedit.syntax.XModeHandler
org.gjt.sp.jedit.syntax.XModeHandler.TagDecl
org.gjt.sp.jedit.syntax.TokenMarker"
CLASS,jedit-4.3,1724940,2007-05-24T15:02:18.000-05:00,typing in multiple select,"lt;body&gt;
  lt;p&gt;
 
 the &lt;p&gt;  
 lt;body&gt;
  lt;d&gt;
If I highlight multiple selections of text in the text area and then begin typing, only the first character of what I type is inserted in the selected areas \(except for where the cursor ended up after making the selection\).
For example if I have the text:
and I highlight both p's in the &lt;p&gt; tags and then type ""div"" I end up with:",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,Unnecesarry fold expantion when folded lines are edited,"{\{\{ hello

something

\}
\(Quoted from Matthieu's comment for patch \#1999448\)
if I use explicit fold, with this buffer
all folds are folded.
I remove one ""l"" from hello.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
CLASS,jedit-4.3,2129419,2008-09-25T23:53:11.000-05:00,NPE in EditPane.setBuffer when quitting jEdit,"lt;init&gt;
When trying to quit jEdit, I get the following Null-Pointer-Exception, which is probably related to some files being changed/deleted \(due to a ""cvs up"" in the background\).
dialog already failed, so that ""Close"" was the only option that worked.
The NPE:",org.gjt.sp.jedit.gui.CloseDialog.ListHandler
CLASS,jabref-2.6,1631548,2007-01-09T14:20:57.000-06:00,"""Open last edited DB at startup"" depends on the working dir","{HOME\}
another little bug/feature: The JabRef option ""Open last edited database at startup"" depends on the working directory at which JabRef is started.
Example:
results in an empty JabRef not opening $\{HOME\}/at-work/Bibliography/my\_documents.bib.",net.sf.jabref.JabRefFrame
METHOD,apache-nutch-2.1,NUTCH-1393,2012-06-13T18:38:39.000-05:00,Display consistent usage of GeneratorJob with 1.X,"{code}
 
  
  
  
  
  
 {code}
If we pass the generate argument to the nutch script, the Generator auto-spings into action and begins generating fetchlists.
An example is below
All I wanted to do was get the usage params printed to stdout but instead it generated my batch willy nilly.","org.apache.nutch.crawl.GeneratorJob:generate(long, long, boolean, boolean)
org.apache.nutch.crawl.GeneratorJob:run(String[])"
CLASS,openjpa-2.0.1,OPENJPA-1787,2010-09-10T11:23:51.000-05:00,Bean validation fails merging a new entity,"EntityManager em = entityManagerFactory.createEntityManager();
        Person person = new Person();
        person.setName(""Oliver"");                               // Employee.name is annotated @NotNull 
        person = em.merge(person);
If you try to merge a new entity.
you get a ConstraintValidationException, although name is set.","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.kernel.AttachStrategy
org.apache.openjpa.integration.validation.TestValidationGroups"
CLASS,openjpa-2.0.1,OPENJPA-1903,2010-12-06T13:05:34.000-06:00,Some queries only work the first time they are executed,"@Entity
@IdClass(MandantAndNameIdentity.class)
public class Website {
    @Id
    private String mandant;
   
    @Id
    private String name;
...
}

 @Entity
@IdClass(WebsiteProduktDatumIdentity.class)
public class Preis {
    @Id
    @ManyToOne(cascade = CascadeType.MERGE)
    private Website website;

    @Id
    @Basic
    private String datum;
...
}

 
 em.getTransaction().begin();

        Website website = em.merge(new Website(""Mandant"", ""Website""));

        em.merge(new Preis(website, DATUM));
       
        em.getTransaction().commit();

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website.name = :website "", Preis.class);
       q.setParameter(""website"", website.getName());

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website = :website "", Preis.class);
        q.setParameter(""website"", website);
I have a problem in my application where a query that sometimes returns data and sometimes not.
Basically I have two Entities which both use multiple Ids to produce the Primary Key, ""Preis"" contains a foreign key on ""Website"":
I use the following to set up a website and a Preis:
Afterwards, if I run the query as follows:
this query works all the time, note that it uses website.name for matching, not the full Website-object.
However if I put the query as
it only works ONCE and then does not return any results any more!!",org.apache.openjpa.jdbc.kernel.PreparedQueryImpl
CLASS,openjpa-2.0.1,OPENJPA-1912,2011-01-03T13:48:09.000-06:00,enhancer generates invalid code if fetch-groups is activated,"@Entity
public abstract class AbstractGroup {
   ...
    @Temporal(TemporalType.TIMESTAMP)
    @TrackChanges
    private Date applicationBegin;
 ...
}

 
 @Entity
public class Group extends AbstractGroup {
...
}

 
 public void writeExternal(ObjectOutput objectoutput)
        throws IOException
     
 pcWriteUnmanaged(objectoutput);
        if(pcStateManager != null)
        {
            if(pcStateManager.writeDetached(objectoutput))
                return;
        } else
        {
            objectoutput.writeObject(pcGetDetachedState());
            objectoutput.writeObject(null);
        }
        objectoutput.writeObject(applicationBegin);
        objectoutput.writeObject(applicationEnd);
        objectoutput.writeObject(applicationLocked);
        objectoutput.writeObject(approvalRequired);
If openjpa.DetachState =fetch-groups is used, the enhancer will add a 'implements Externalizable' + writeExternal + readExternal.
The problem is, that writeExternal and readExternal will also try to externalize the private members of any given superclass.
Thus we get a runtime Exception that we are not allowed to access those fields.
Example:
will result in the following code (decompiled with jad):",org.apache.openjpa.enhance.PCEnhancer
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We pass the appliation class loeader as part of our PersistenceUnitInfo implementation by returning it from PersistenceUnitInfo.getClassLoader().
However, the code in MetaDataRepository.preload() only uses the context class loader and not the class loader from PersistenceUnitInfo, which leades to ClassNotFoundExpcetions like mentioned at the end of this report.","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,Resolving factory method does not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
If a get method is annotated with @Factory then the method cannot be overridden with a method which take different parameters.
The system randomly selects one of the several methods with the same name which may or may not take the type which will be provided.
For example:
        @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}
Actual results:
valueOf(String) may or may not be selected.",org.apache.openjpa.meta.FieldMetaData
CLASS,openjpa-2.0.1,OPENJPA-1986,2011-04-27T11:44:53.000-05:00,Extra queries being generated when cascading a persist,"@Entity
public class CascadePersistEntity implements Serializable {
    private static final long serialVersionUID = -8290604110046006897L;

    @Id
    long id;

    @OneToOne(cascade = CascadeType.ALL)
    CascadePersistEntity other;
...
}

 
 CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);
I found a scenario where extra queries were being generated while cascading a persist to a new Entity.
See the following example:
and the following scenario:
CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);
This results in two inserts and one select.","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.conf.Compatibility
org.apache.openjpa.kernel.SingleFieldManager"
METHOD,lang,LANG-292,2006-10-31T01:45:01.000-06:00,"unescapeXml(""&12345678;"") should be ""&12345678;""","public void testNumberOverflow() throws Exception {
        doTestUnescapeEntity(""&#12345678;"", ""&#12345678;"");
        doTestUnescapeEntity(""x&#12345678;y"", ""x&#12345678;y"");
        doTestUnescapeEntity(""&#x12345678;"", ""&#x12345678;"");
        doTestUnescapeEntity(""x&#x12345678;y"", ""x&#x12345678;y"");
    }
Following test (in EntitiesTest.java) fails:","org.apache.commons.lang.Entities:unescape(String)
org.apache.commons.lang.Entities:unescape(Writer, String)"
METHOD,lang,LANG-300,2006-12-19T17:47:43.000-06:00,NumberUtils.createNumber throws NumberFormatException for one digit long,"isDigits(numeric.substring(1))  numeric.substring(1)
NumberUtils.createNumber throws a NumberFormatException when parsing ""1l"", ""2l"" .
.
etc..
.
It works fine if you try to parse ""01l"" or ""02l"".
The condition isDigits(numeric.substring(1)), line 455 return false as numeric.substring(1) is an empty string for ""1l""",org.apache.commons.lang.math.NumberUtils:createNumber(String)
METHOD,lang,LANG-363,2007-10-23T07:12:48.000-05:00,"StringEscapeUtils.escapeJavaScript() method did not escape '/' into '\/', it will make IE render page uncorrectly","document.getElementById(""test"")   document.getElementById(""test"") 
  
 String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);
For example, document.getElementById(""test"").
value = '<script>alert(\'aaa\');</script>';this expression will make IE render page uncorrect, it should be document.getElementById(""test"").
Try  to run below codes, you will find the difference:
  String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);","org.apache.commons.lang.StringEscapeUtils:escapeJavaStyleString(Writer, String, boolean)"
METHOD,lang,LANG-477,2009-01-09T10:05:53.000-06:00,ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes,"{code:title=ExtendedMessageFormatTest.java|borderStyle=solid}

 private static Map<String, Object> formatRegistry = new HashMap<String, Object>();    
     static {
        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());
    }
    
     public static void main(String[] args) {
        ExtendedMessageFormat mf = new ExtendedMessageFormat(""it''s a {dummy} 'test'!"", formatRegistry);
        String formattedPattern = mf.format(new String[] {""great""});
        System.out.println(formattedPattern);
    }
 
 {code}

 
 {code:title=ExtendedMessageFormat.java|borderStyle=solid}
 
 if (escapingOn && c[start] == QUOTE) {
        return appendTo == null ? null : appendTo.append(QUOTE);
}

WORKING:
if (escapingOn && c[start] == QUOTE) {
        next(pos);
        return appendTo == null ? null : appendTo.append(QUOTE);
}
{code}
When using ExtendedMessageFormat with a custom format registry and a pattern conatining single quotes, an OutOfMemoryError will occur.
Example that will cause error:","org.apache.commons.lang.text.ExtendedMessageFormat:appendQuotedString(String, ParsePosition, StringBuffer, boolean)"
METHOD,lang,LANG-480,2009-01-20T17:36:44.000-06:00,StringEscapeUtils.escapeHtml incorrectly converts unicode characters above U+00FFFF into 2 characters,"import org.apache.commons.lang.*;

public class J2 {
    public static void main(String[] args) throws Exception {
        // this is the utf8 representation of the character:
        // COUNTING ROD UNIT DIGIT THREE
        // in unicode
        // codepoint: U+1D362
        byte[] data = new byte[] { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 };

        //output is: &amp;#55348;&amp;#57186;
        // should be: &amp;#119650;
        System.out.println(""'"" + StringEscapeUtils.escapeHtml(new String(data, ""UTF8"")) + ""'"");
    }
}
Characters that are represented as a 2 characters internaly by java are incorrectly converted by the function.
The following test displays the problem quite nicely:","org.apache.commons.lang.Entities:escape(Writer, String)"
METHOD,lang,LANG-538,2009-10-16T16:47:39.000-05:00,DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations,"Calenar.getTime()    
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";

    // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)
    // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);


    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}

 
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
    cal.getTime();

    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}
If a Calendar object is constructed in certain ways a call to Calendar.setTimeZone does not correctly change the Calendars fields.
For example, the following unit test fails:
However, this unit test passes:","org.apache.commons.lang3.time.FastDateFormat:format(Calendar, StringBuffer)"
METHOD,lang,LANG-552,2009-11-09T12:40:57.000-06:00,StringUtils replaceEach - Bug or Missing Documentation,"{code}
 import static org.junit.Assert.assertEquals;

import org.apache.commons.lang.StringUtils;
import org.junit.Test;


public class StringUtilsTest {

	@Test
	public void replaceEach(){
		String original = ""Hello World!"";
		String[] searchList = {""Hello"", ""World""};
		String[] replacementList = {""Greetings"", null};
		String result = StringUtils.replaceEach(original, searchList, replacementList);
		assertEquals(""Greetings !"", result);
		//perhaps this is ok as well
                //assertEquals(""Greetings World!"", result);
                //or even
		//assertEquals(""Greetings null!"", result);
	}

	
}
 {code}
The following Test Case for replaceEach fails with a null pointer exception.","org.apache.commons.lang3.StringUtils:replaceEach(String, String[], String[], boolean, int)"
METHOD,lang,LANG-645,2010-08-20T14:11:08.000-05:00,FastDateFormat.format() outputs incorrect week of year because locale isn't respected,"format()     
  
 {code}
 import java.util.Calendar;
import java.util.Date;
import java.util.Locale;
import java.text.SimpleDateFormat;

import org.apache.commons.lang.time.FastDateFormat;

public class FastDateFormatWeekBugDemo {
    public static void main(String[] args) {
        Locale.setDefault(new Locale(""en"", ""US""));
        Locale locale = new Locale(""sv"", ""SE"");

        Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome
        cal.set(2010, 0, 1, 12, 0, 0);
        Date d = cal.getTime();
        System.out.println(""Target date: "" + d);

        FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
 {code}
  Locale.setDefault()
FastDateFormat apparently doesn't respect the locale it was sent on creation when outputting week in year (e.g. ""ww"") in format().
It seems to use the settings of the system locale for firstDayOfWeek and minimalDaysInFirstWeek, which (depending on the year) may result in the incorrect week number being output.
Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:
FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
{code}",org.apache.commons.lang3.time.FastDateFormat:format(Date)
METHOD,lang,LANG-662,2010-12-06T22:40:30.000-06:00,"org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)","class Fraction    
    
 
  public void testReducedFactory_int_int()  
 
  f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2);
		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator());
		assertEquals(1, f.getDenominator());

	 public void testReduce()  
 
  f = Fraction.getFraction(Integer.MIN_VALUE, 2);
		result = f.reduce();
		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator());
		assertEquals(1, result.getDenominator());
{code}
The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator.","org.apache.commons.lang3.math.Fraction:greatestCommonDivisor(int, int)"
METHOD,lang,LANG-710,2011-07-01T20:57:30.000-05:00,"StringIndexOutOfBoundsException when calling unescapeHtml4(""&#03"")","unescapeHtml4()
When calling unescapeHtml4() on the String ""&#03"" (or any String that contains these characters) an Exception is thrown:","org.apache.commons.lang3.text.translate.NumericEntityUnescaper:translate(CharSequence, int, Writer)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,SerializationUtils throws ClassNotFoundException when cloning primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
If a serializable object contains a reference to a primitive class, e.g. int.class or int[].class, the SerializationUtils throw a ClassNotFoundException when trying to clone that object.
The SerializationUtils problem arises because the SerializationUtils internally use the ClassLoaderAwareObjectInputStream that overrides the ObjectInputStream's resoleClass method without delegating to the super method in case of a ClassNotFoundException.
For example:","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-832,2012-09-27T00:27:58.000-05:00,FastDateParser does not handle unterminated quotes correctly,"{IsNd}
FDP does not handled unterminated quotes the same way as SimpleDateFormat
For example:
The format is parsed as:",org.apache.commons.lang3.time.FastDateParser:init()
METHOD,lang,LANG-857,2012-11-20T12:36:14.000-06:00,StringIndexOutOfBoundsException in CharSequenceTranslator,"{code:java}
 @Test
public void testEscapeSurrogatePairs() throws Exception {
    assertEquals(""\uD83D\uDE30"", StringEscapeUtils.escapeCsv(""\uD83D\uDE30""));
}
 {code}

 
 {code}
 {code}

 
 public final void translate(CharSequence input, Writer out) throws IOException
This is a simple test case for this problem.
You'll get the exception as shown below.","org.apache.commons.lang3.text.translate.CharSequenceTranslator:translate(CharSequence, Writer)"
METHOD,lang,LANG-879,2013-03-18T21:46:29.000-05:00,"LocaleUtils test fails with new Locale ""ja_JP_JP_#u-ca-japanese"" of JDK7","import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;

import java.util.Locale;

import org.testng.annotations.Test;

import com.scispike.foundation.i18n.StringToLocaleConverter;

public class LocaleStringConverterTest {

	StringToLocaleConverter converter = new StringToLocaleConverter();

	public void testStringToLocale(Locale l) {
		String s = l.toString();

		assertThat(converter.convert(s), equalTo(l));
	}

	@Test
	public void testAllLocales() {

		Locale[] locales = Locale.getAvailableLocales();
		for (Locale l : locales) {
			testStringToLocale(l);
		}
	}
}


  
 import java.util.Locale;

import org.apache.commons.lang3.LocaleUtils;
import org.springframework.core.convert.converter.Converter;

public class StringToLocaleConverter implements Converter<String, Locale> {

	@Override
	public Locale convert(String source) {
		if (source == null) {
			return LocaleToStringConverter.DEFAULT;
		}
		return LocaleUtils.toLocale(source);
	}
}
The Test below fails with the following error on JDK7, but succeeds on JDK6:",org.apache.commons.lang3.LocaleUtils:toLocale(String)
FILE,SWARM,SWARM-528,2016-06-22T02:53:46.000-05:00,swarm.http.port and swarm.port.offset do not work with @ArquillianResource URL baseURL,"@ArquillianResource 
  
 
 
 
 @ArquillianResource 
  
 
 
 
 @ArquillianResource
If you set the swarm port using either swarm.http.port or swarm.port.offset via arquillian.xml e.g.
the arquillian swarm container is correctly started on the specified port/offset.
The problem is that if you use:
to retrieve the url the swarm container is accessible via it always returns http://localhost:8080.
If you set the port property in arquillian.xml
it starts the swarm container on 8080 and
returns http://localhost:8081
Attempting to combine the port property and the offset does not work either e.g.
the port/offset is ignored and the container is started on 8080, while
returns http:localhost:8081 note: while the examples above use swarm.port.offset, the same issue occurs if you use swarm.http.port",org.wildfly.swarm.arquillian.resources.SwarmURLResourceProvider
FILE,SWARM,SWARM-486,2016-05-28T18:25:37.000-05:00,Can't load project-stages.yml on classpath with Arq,"classpath(src/main/resources)  
 
 
 container.withStageConfig(Paths.get(""/tmp"", ""external-project-stages.yml"").toUri().toURL())
problem project-stages.
yml on classpath(src/main/resources) is not loaded with Arquillian tests.
I attached the error log and the reproducer in 'Steps to Reproduce' section.
Though -swarm try to load it, apparently can't see it when Arq tests.",org.wildfly.swarm.container.ProjectStagesTest
FILE,SWARM,SWARM-863,2016-11-30T14:54:40.000-06:00,Version 2016.11.0 doesn't stop properly (with custom main class),"container = new Swarm(); // fractions being added here also




    container.start();




    container.deploy(...);






 
 container.stop();
We are using a custom main class whose main method reacts on a single argument: ""start"" or ""stop"".
Actually we are feeding that argument through Procrun (https://commons.apache.org/proper/commons-daemon/procrun.html).
Inside that main class we hold a field
which we handle as following during startup:
The reaction on the ""stop"" signal is as easy as following:
We now have the problem that stopping such a Swarm service in version 2016.11.0 does not properly shutdown the Swarm container (or better the underlying `Server`).
I did a debug session and found out that there remains one non-daemon thread blocking the JVM shutdown.
The shutdown is clean and fast.
1) Clone the github project and start a ""mvn package"" to produce the uber-jar, which is located in the ""swarm"" sub-module of that project.
2) Copy that uber-jar and both procrun executables (""prunsrv.exe"" and ""prunmgr.exe"") into a test directory.
For 64-bit OS's use the file ""amd64/prunsrv.exe"" (inside that procrun zip file).
Rename the file ""prunsrv.exe"" to ""testing-wfs.exe"" and the file ""prunmgr.exe"" to ""testing-wfsw.exe"".
3) Install a Windows service by running the command ""testing-wfs.exe //IS"".
Now this service can be configured by running the ""testing-wfsw.exe"" (a Windows GUI Tool for that purpose).
4) Configure the service as following:
5) Now start the service.
After a succesful start you can ""GET http://localhost:8080/hello"", which should result in a ""Hello World"" response.
6) The service has many threads running.
See first attached screenshot.
7) Now stop the service.
Windows will hang in that stopping attempt and spit out a failure message after some time.
The process is still running afterwards.
The log file shows some output that indeed a shutdown is initalized.
The GET does not work anymore.
8) The service still has many threads (especially non-daemon threads).
See second attached screenshot.",org.wildfly.swarm.container.runtime.ServerBootstrapImpl
METHOD,derby-10.9.1.0,DERBY-5951,2012-10-16T10:33:53.000-05:00,Missing method exception raised when using Clobs with territory based collation,"db;create=true; 
   varchar( 32672 )  
  
  
  
 clobTable( a )   makeClob( 'a' )  
   varchar( 32672 )  
  
  
 clobTable( a )   makeClob( 'a' )  
     Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;   
   Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;     
  
 clobTable( a )   makeClob( 'a' )
When using territory-based collation with Clobs, Derby raises an error trying to invoke a missing method.
The following script shows this problem:
-- fails with a java.lang.NoSuchMethodError exception
Here is the error:",org.apache.derby.iapi.types.CollatorSQLClob:getNewNull()
FILE,IO,IO-481,2015-06-19T18:19:48.000-05:00,org.apache.commons.io.FileUtils#waitFor waits too long,"public void testRealWallTime() 
{

        long start = System.currentTimeMillis();

        FileUtils.waitFor(new File(""""), 2);

        System.out.println(""elapsed = "" + (System.currentTimeMillis() - start));

    }
The following testcase will never run in less than 4 seconds on my machine public void testRealWallTime()",org.apache.commons.io.FileUtils
FILE,eclipse-3.1,100137,2005-06-15T04:29:00.000-05:00,Variables view: code assist does not work in details pane,"public class A {
	String dog1 = ""Max"", dog2 = ""Bailey"", dog3 = ""Harriet"";
	public static void main(String[] args) {
		new A().foo();
	}
	
	void foo() {
		String p= """";
	}
}
1. create a fresh Java project
2. delete the JRE System Library
3. add a new User Library which is marked as system library and has the rt.jar as single library (I used JDK 1.5.0_03)
4. add the following class:
5. add breakpoint on line 8
6. debug
- source is not found",org.eclipse.jdt.launching.StandardClasspathProvider
FILE,eclipse-3.1,100807,2005-06-20T09:30:00.000-05:00,Source not found,"JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
  
 JavaModelManager.getZipFile(IPath) 
 
   
 JavaModelManager.closeZipFile(ZipFile) 
    
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile)
The source files in the project are not being found.
options file and only the following archives were searched for the source file when a breakpoint was reached.
The source lookup path was set by using the
""Restore default"".",org.eclipse.debug.internal.core.sourcelookup.containers.ContainerSourceContainer
FILE,eclipse-3.1,102427,2005-06-30T20:45:00.000-05:00,Cannot inspect/display static import methods,"public class Helper {
    public static int getValue() {...}
}
  
import static Helper.*;

public class Doer {
    public void doit() {
        int i = getValue();
    }
}
 
 getValue() 
 getValue()
Consider:
When debugging, if you select 'getValue()' in the method 'doit' and execute
display (or inspect) you get an error indicating that the method 'getValue()' is
not undefined for type Doer.",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,102778,2005-07-05T15:40:00.000-05:00,Scrapbook page doesn't work with enhanced for statement,"int[] tab = new int[] {1, 2, 3, 4, 5, 6, 7, 8, 9 };
int sum = 0;
for (int i : tab) {
	sum += i;
}
Using 3.1, create a new java project.
Add a new scrapbook page that contains this source:
You get an error about syntax error.",org.eclipse.jdt.internal.eval.CodeSnippetParser
FILE,eclipse-3.1,103379,2005-07-11T15:37:00.000-05:00,[MPE] [EditorMgmt] An editor instance is being leaked each time an editor is open and closed,"dispose()
Every we open and close an editor.
That editor instance is being leaked.
It creates a new simple project and a new file.
It opens up the new file in the editor that comes with the testcase, then close the editor.
Repeat 500 times.
What's interesting is that the editor, upon open, will allocate a 200000 size
String array as a private field.
If you run this testcase with -Xmx256M, you will run out of memory.
However, if you explicitly set the String array to null in the dispose() method of the editor, then the same testcase will not run out of memory.
This leads us to believe that the editor instance is being leaked.",org.eclipse.ui.operations.OperationHistoryActionHandler
FILE,eclipse-3.1,103918,2005-07-14T17:25:00.000-05:00,100% CPU load while creating dynamic proxy in rich client app,"public void start(BundleContext context) throws Exception {
  super.start(context);
  XmlBeanFactory bf = new XmlBeanFactory(
     new ClassPathResource(""/bug/beans.xml""));
  bf.getBean(""hang"");
}

  bf.getBean(""hang"")  
 bf.getBean()
I've
noticed that when spring tries to instantiate any dynamic proxy RCP falls into
infinit loop, CPU gets 100% load and the application needs to be killed.
my start method contains the following code:
When bf.getBean(""hang"") is executed the application hangs.",org.eclipse.core.runtime.internal.adaptor.ContextFinder
FILE,eclipse-3.1,106492,2005-08-09T11:01:00.000-05:00,NPE on console during debug session,"name.equals(""IResourceTest.testDelete"")  
  
  
          
       
  
       
  
       
   testDelete()  
  
   
    
 
  
   
  
   
    
 
   runTest()  
   runBare()  
   protect()
While debugging, I noticed the attached stack trace on my Java console (not in the log file).
There was nothing in the log file.
I see from the stack that it occurred during evaluation of a conditional breakpoint.
I have a single breakpoint with this condition:
After this error occurred, the debug process hung, and ""Terminate"" and
""Terminate All"" had no effect.
I was still able to ""Suspend"" the process, and it resulted in a debug view showing:",org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
FILE,eclipse-3.1,108466,2005-08-31T09:39:00.000-05:00,Dups from (Eclipse)ClassLoader.getResources(String),"EclipseClassLoader.getResources(String)  
   
 getClass()  getClassLoader()  getResources(""file.txt"")
When running runtime workspace, EclipseClassLoader.getResources(String) returns duplicate results for files found in plugin jarfiles (i.e. jar files listed in manifest.mf Bundle-ClassPath: entry or plugin.xml <library/> element).
1. create plugin project
2. add a jar with ""file.txt"" entry to the project
3. add the jar to plugin runtime classpath using plugin manifest editor
4. update plugin classpath (make sure the jar got added to java build path)
5. add code that counts number of entries returned by getClass().
5. start eclipse application, see that getResources returns two entries
Apparently, eclipse adds the jar on plugin's classpath twice -- as a regular
OSGi classpath entry and as a development entry.",org.eclipse.pde.internal.core.ClasspathHelper
FILE,eclipse-3.1,110837,2005-09-27T13:16:00.000-05:00,javax.crypto.KeyAgreement.getInstance(String) throws exception in IDE,"KeyAgreement.getInstance(""DiffieHellman"")  
  
 
import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;

public class KeyAgreementProblem
{
    public static void main(String[] args) throws NoSuchAlgorithmException
    {
        KeyAgreement ka = KeyAgreement.getInstance(""DiffieHellman"");
        System.out.println(ka);
    }
}
 
 
  
  
 javax.crypto.KeyAgreement.getInstance(DashoA12275)
A call to KeyAgreement.getInstance(""DiffieHellman"") will throw a
NoSuchAlgorithmException if run from Eclipse but not if it's run directly from the command line.
Here's the code:
Running in Eclipse with the same JDK and environment produces this exception:","org.eclipse.jdt.launching.AbstractJavaLaunchConfigurationDelegate
org.eclipse.jdt.internal.launching.JRERuntimeClasspathEntryResolver
org.eclipse.jdt.internal.launching.StandardVMType"
FILE,eclipse-3.1,113455,2005-10-22T11:32:00.000-05:00,[Markers] Some error markers do not appear,"problemView.getCurrentMarkers()
+ When I saw Bug 113454, I started up Eclipse and synchronized with HEAD.
Even with all the code from HEAD, no errors showed up in my Problems view.
+ So, I opened the file referenced in the first compile error
(ResourceMappingMarkersTest), and it had several errors in it.
+ I clicked on one of the methods that was an error
(problemView.getCurrentMarkers()), and hit ""F3"".
Now one error appears in the
Problems view: ""ModelProvider cannot be resolved"" in CompositeResourceMapping.
+ I tried cleaning all projects, to see if that would motivate the errors to appear.
The one error that was there previously then disappeared.
I had a hard time deciding whether to make this ""blocker"" or ""major"".","org.eclipse.ui.views.markers.internal.Util
org.eclipse.ui.views.markers.internal.MarkerView"
FILE,eclipse-3.1,115363,2005-11-07T13:28:00.000-06:00,"java.lang.VerifyError in org.eclipse.ui.workbench from HEAD, using N20051107","Ljava/lang/String;Ljava/lang/String;Lorg/eclipse/jface/action/IContributionManager;
I created a new ""RCP application with intro"" using the wizard, deleted its
Activator class, and tried to run it from within Eclipse.",org.eclipse.jdt.internal.compiler.codegen.Label
FILE,eclipse-3.1,133072,2006-03-23T16:58:00.000-06:00,"Cannot launch an ""Eclipse Application"" without the -ws argument","package Fred;

import javax.swing.JFrame;
import javax.swing.SwingUtilities;

import org.eclipse.core.runtime.IPlatformRunnable;

public class Main implements IPlatformRunnable {

       public Object run(Object args) throws Exception {
               SwingUtilities.invokeLater(new Runnable() {
                       public void run() {
                               new JFrame(""Fred"").setVisible(true);
                       }
               });
               synchronized(this)
               {
                       wait();
               }
               return IPlatformRunnable.EXIT_OK;
       }

}
When running against 1.4.2_09, you get the dreaded ""2006-03-23
The application then stops.
When running against 1.5.0_06 you get these messages on startup:
Under 1.5.0_06, the app appears to run, but there is no menu-bar,
dock-icon, or window that shows up.
Basically nothing happens, but
the event thread is running and you have to kill it.
Here's the test class we used:","org.eclipse.pde.internal.ui.IPDEUIConstants
org.eclipse.pde.internal.ui.launcher.LaunchAction"
FILE,eclipse-3.1,300054,2010-01-19T10:12:00.000-06:00,Unexpected 'Save Resource' dialog appears when copying changes from right to left,"public class Bug {
	void bar() {
		System.out.println();
	}
}
  System.out.println();
1 start with new workspace
2 paste this into the Package Explorer:
4 delete ""System.out.println();"" and save
5 compare the current state with the previous one
6 double-click on 'bar' in the compare editor's upper pane
7 click 'Copy Current Change from Right to Left' button
==> 'Save Resource' dialog appears which is a major interruption of my workflow.",org.eclipse.compare.internal.Utilities
FILE,eclipse-3.1,76472,2004-10-18T11:31:00.000-05:00,Duplicate entries in the constant pool for some methods,"public class X {
	public static void main(String[] args) {
		long[] tab = new long[] {};
		System.out.println(tab.clone());
		System.out.println(tab.clone());
	}
}

  clone()
Compile this example:
Disassemble it and you can see that the call to clone() creates two entries in the constant pool.","org.eclipse.jdt.internal.compiler.ast.BreakStatement
org.eclipse.jdt.internal.compiler.flow.FlowContext
org.eclipse.jdt.internal.compiler.flow.LoopingFlowContext"
FILE,eclipse-3.1,76534,2004-10-18T22:57:00.000-05:00,Can't perform evaluations inside inner class with constructor_ parameters,"createViewer(...)
We currently disallow evaluations in inner classes that take parameters in the referenced constructor_.
For example, see the CheckBoxTreeViewer that's created in BreakpointsView#createViewer(...).",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,76677,2004-10-20T13:18:00.000-05:00,Console Input incorrect,"public class ConsoleTest {
    public static void main(String[] args) {
        try {
	        byte[] b = new byte[100];
	        for(;;) {
	            int read = System.in.read(b);
	            System.out.write(b, 0, read);
	        }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
1. Run the following program
2. Enter '123' in the console
3. replace '2' with 'x'
4. hit 'enter'
console output is 13x instead of 1x3.","org.eclipse.ui.internal.console.IOConsolePartition
org.eclipse.ui.internal.console.IOConsolePartitioner"
FILE,eclipse-3.1,77234,2004-10-28T15:41:00.000-05:00,Detail formatter doesn't see inherited method,"getTypeName() 
  
  
  
 getTypeName()   JavaExceptionBreakpoint

getTypeName()
1 Create a detail formatter for the type JavaExceptionBreakpoint.
2 Set the contents to ""getTypeName()""
3 Debug to a breakpoint with a JavaExceptionBreakpoint in the variables view.
I'm debugging RemoveBreakpointAction and I delete an exception breakpoint to see
this.
4 Select the JavaExceptionBreakpoint.
I get the following in the details pane:",org.eclipse.jdt.internal.debug.ui.JavaDetailFormattersManager
FILE,eclipse-3.1,77573,2004-11-03T04:43:00.000-06:00,[1.5][assist] Code assist does not propose static fields,"import static java.lang.Math
- Write ""import static java.lang.Math."" in a cu
- Press Ctrl+Space
->No proposals","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.core.CompletionProposal
org.eclipse.jdt.core.CompletionRequestor"
FILE,eclipse-3.1,78201,2004-11-09T13:43:00.000-06:00,ClassCastException on Refresh in the AntView,"RefreshBuildFilesAction$1.run(IProgressMonitor)  
 ModalContext$ModalContextThread.run()
Select a target in the AntView.
Use the context menu refresh action:",org.eclipse.ant.internal.ui.model.AntProjectNodeProxy
FILE,eclipse-3.1,78245,2004-11-09T18:34:00.000-06:00,Breakpoints in enums not correctly created.,"public enum TestEnum {
  a;
  public static void main(String[] args) {
    System.out.println();   // <- add a breakpoint here
  }
}
We are now able to add breakpoint in enum classes, but they're not correctly created, the associated type is wrong.
It's working OK if the enum is an inner type, but not if it's a top level type.
System.out.println();   // <- add a breakpoint here
The breakpoint is created, but displayed in the breakpoint view as 'null [line
XX] - main(String[])', and the program doesn't stop on the breakpoint.",org.eclipse.jdt.internal.debug.ui.actions.ValidBreakpointLocationLocator
FILE,eclipse-3.1,78315,2004-11-10T12:53:00.000-06:00,org.eclipes.team.ui plugin's startup code forces compare to be loaded,"Platform.getAdapterManager()  registerAdapters(factory, DiffNode.class);

   
 startup()
The one for compare fails because org.eclipes.team.ui forces compare to be loaded in its start(BundleContext) method:
The direct reference to DiffNode causes the compare plug-in to be loaded even if it is not needed yet.
1. add startup() method to CompareUIPlugin
2. put a breakpoint there",org.eclipse.team.internal.ui.TeamUIPlugin
FILE,eclipse-3.1,78740,2004-11-16T10:57:00.000-06:00,IDOMType.getFlags() fails to represent interface flags correctly.,"becomeDetailed()   

package org.example.jdom;

import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;

public class Test implements IPlatformRunnable
{
  public Object run(Object object)
  {
    DOMFactory factory = new DOMFactory();
    IDOMCompilationUnit jCompilationUnit =
factory.createCompilationUnit(""package x; /** @model */ interface X  {}"", ""NAME"");
    IDOMType jType = (IDOMType)jCompilationUnit.getFirstChild().getNextNode(); 
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    jType.getComment();
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    return new Integer(0);
  }
}
This code demonstrates that calling getComment on an IDOMType will change the flags from correctly encoding the type as being interface to incorrectly encoding it (because during becomeDetailed() that information is lost):",org.eclipse.jdt.internal.compiler.DocumentElementParser
FILE,eclipse-3.1,79545,2004-11-26T05:58:00.000-06:00,Eclipse vs Sun JDK: different class files from the same source code,"public class CharIntTest
{
    /**
     * Eclipse value: "" ""
     * JDK value:     ""32""
     */
    public static String C = """" + +' ';
    /**
     * Eclipse value: ""32""
     * JDK value:     ""32""
     */
    public static String I = """" + +32;

    public static void main(String[] args)
    {
        System.out.println(C);
        System.out.println(I);
    }
}
Compile the source code below in Eclipse and run it.
Do the same using Sun JDK (1.4.1 or 1.5).
The problem is connected with +' ': Eclipse treats it as ' ' but Sun JDK converts that space into 32 (+' ' => + (int) ' ' => +32 => 32) (which IMHO is correct).
The source code:","org.eclipse.jdt.internal.compiler.impl.Constant
org.eclipse.jdt.internal.compiler.ast.EqualExpression"
FILE,eclipse-3.1,79957,2004-12-02T00:47:00.000-06:00,[Viewers] NPE changing input usingTableViewer and virtual,"Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
 
 tv.setInput(model1);
i've straight forward code ... 
<code>
Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
.
Same code works fine without the SWT.VIRTUAL style bit,but when VIRTUAL is set
it throws a null pointer exception...",org.eclipse.jface.viewers.TableViewer
FILE,eclipse-3.1,80672,2004-12-10T04:44:00.000-06:00,[1.5] Annotation change does not trigger recompilation,"package p;
@q.Ann
public class Use {
}
  
package q;
public @interface Ann {
}


 
 
package q;
import java.lang.annotation.*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
 
 @Ann
Define 2 files:
p/Use.
Build - all is fine
Now incrementally change q/Ann.
java to:
package q;
import java.lang.annotation.
*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
Build - still fine though p/Use.",org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader
FILE,eclipse-3.1,81045,2004-12-14T20:13:00.000-06:00,ClassNotLoadedException when trying to change a value,"public class Test {
	static class Inner {
	}
	public static void main(String[] args) {
		Inner inner= null;
		System.out.println(1);  //  <- breakpoint here
	}
}
Debug to the breakpoint.
Right-click on the 'inner' variable > change value ...
A ClassNotLoadedException dialog appears.","org.eclipse.jdt.internal.debug.ui.actions.JavaVariableValueEditor
org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
org.eclipse.jdt.internal.debug.core.model.JDILocalVariable"
FILE,eclipse-3.1,82712,2005-01-12T15:54:00.000-06:00,[1.5] Code assist does not show method parameters from static imports,"import static java.lang.Math.*; 
 public class Test {

    void t() {
        abs(<CTRL+SPACE>);
    }
}
Test:",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,83205,2005-01-19T11:25:00.000-06:00,[osgi] shutdown did not complete,"System.exit()  
  
    
  
  
    
   
  
 
  
   Object.wait()  
   
  
  
  
  
  
  
  
 
 it()  
    
 
 
 
 
  
 Object.wait()  
   
  Object.wait()
Accidentally shut down Eclipse while exporting a plug-in project as deployable feature was in progress.
The console window stayed open, and responsive (could use console).
No further activity seemed to be happening.
It seems System.exit() was not called.
The console and an event dispatching threads were left behind.",org.eclipse.core.launcher.Main
FILE,eclipse-3.1,83383,2005-01-21T06:39:00.000-06:00,IllegalArgumentException in Signature.getParameterCount,"String signature= ""foo(+Ljava.lang.Comparable;)"";
Signature.getParameterCount(signature);
When completing a METHOD_REF proposal for a method that has a parameter with an open type bound, getParameterPackages I get an IAE:",org.eclipse.jdt.internal.core.util.Util
FILE,eclipse-3.1,83489,2005-01-22T17:33:00.000-06:00,[select] Code select returns IType instead of ITypeParameter on method parameters types,"class Test<T> {
  void foo(T t) {}
}
Consider following test case:
When I select ""T"" in method declaration, selection engine returns an IType
""Test"" instead of expected ITypeParameter ""T"".",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,83536,2005-01-24T10:02:00.000-06:00,"""Incompatible argument to function"" at vararg function","package t1;
public class Test {
    public static void main (String[] args) {
        new Test ().test (new byte[5]);
    }
    private void test (Object... params) {
    }
}

 
  
 new Test ()  new Object[] {new byte[5]}
Platform: Eclipse 3.1 M4
Code example:
The code generates the following error at runtime when started from Eclipse (and
only when started from Eclipse): 
java.lang.VerifyError: (class: t1/Test, method: main signature:
([Ljava/lang/String;)V) Incompatible argument to function","org.eclipse.jdt.internal.compiler.ast.Statement
org.eclipse.ui.internal.WorkbenchWindow
org.eclipse.ui.internal.Workbench"
FILE,eclipse-3.1,84194,2005-02-01T18:05:00.000-06:00,[content assist] Code assist in import statements insert at the end,"import org.eclipse.core.runtime.*;
Open any Java file that contains > 1 import statements.
Let's say the first import statement reads:
delete the '*;' from the end and try to use code assist to insert
IRunnableWithProgress for example.
You will see that upon pressing Enter to select, the text gets inserted several lines down under all the import statements.
the cursor is now in a random position also.","org.eclipse.jdt.internal.ui.text.java.JavaTypeCompletionProposal
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector"
FILE,eclipse-3.1,84724,2005-02-08T13:41:00.000-06:00,[1.5][search] fails to find call sites for varargs constructor_s,"public class Test {
    public void foo() {
        Cell c= new Cell("""", """"); // calls Cell.Cell(String...)
    }
}
 class Cell {
    public Cell(String... args) { }
}
The search engine fails to find the call to the varargs constructor_ in the example below.
Simply highlight the constructor_'s name and invoke ""References"" -
> ""Workspace"" from the Java editor context menu; no occurrences will be found.",org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
FILE,eclipse-3.1,84770,2005-02-09T06:46:00.000-06:00,Formatter fails in specific case (.class in code),"public class FormatterTest {
  void doTest(
      ) {
     System.out.println(""("" + 
         Object.class + "")"");
  }
}
 
 toString()
1 Make a new class in default package:
2 Try to format it by Ctrl+Shift+F  - nothing happens
3 Now change the 'Object.class' to 'Object.class.toString()'
(You can also delete the last ' + "")""'
4 Try to format it by Ctrl+Shift+F  - everything is ok
It seems that formatter crashes, when it has some string operatation (like + ) 
after the keyword class",org.eclipse.jdt.internal.formatter.BinaryExpressionFragmentBuilder
FILE,eclipse-3.1,84944,2005-02-10T16:49:00.000-06:00,[1.5][builder] Parameterized return type is sometimes not visible.,"package parser;

public interface ValueParser<T> {
	T parse(final String string);
}
I defined an interface with a parameterized return type:
However the return type seems to be not visible for some of the implementations.
The strange thing about this behavior is that a ""clean project"" may clean the
error until next compile, sometimes the error did not occur in the different
implementation.","org.eclipse.jdt.internal.compiler.lookup.BinaryTypeBinding
org.eclipse.jdt.internal.compiler.lookup.ParameterizedTypeBinding
org.eclipse.jdt.internal.compiler.lookup.WildcardBinding"
FILE,eclipse-3.1,85344,2005-02-15T17:36:00.000-06:00,Error evaluating logical structure value for Map in Java 5.0,"public class Test {
  public static void main(String[] args) {
    Map<String, Integer> map= new HashMap<String, Integer>();
    System.out.println();     // <-- breakpoint here
  }
}

  entrySet()
System.out.println();     // <-- breakpoint here
I get ""Error: The method entrySet() is undefined for the type Map__"" when I expand map in the variables view.",org.eclipse.jdt.internal.debug.eval.ast.engine.BinaryBasedSourceGenerator
FILE,eclipse-3.1,85397,2005-02-16T08:20:00.000-06:00,[1.5][enum] erroneous strictfp keyword on enum type produces error on constructor_,"strictfp enum Natural {
	ONE, TWO;
}

 
 strictfp enum Natural {
	ONE, TWO;
	
	private Natural() {
	}
}
- have this code:
expected: strictfp is not allowed on the enum type actual: no error is reported
- alternatively, have this code:
expected: the wrong modifier is reported with the type name 'Natural' actual: the error is shown for the constructor_","org.eclipse.jdt.internal.compiler.lookup.SyntheticMethodBinding
org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyViewPart
org.eclipse.jdt.internal.compiler.lookup.MethodScope"
FILE,eclipse-3.1,85402,2005-02-16T08:50:00.000-06:00,[1.5][assist] NPE while trying to complete on empty annotation,"import e.Team;
   @Author(name={Team.DAVID, Team.JEROME})
    
  public class Test {
	@Author(name=Team.PHILIPPE) void foo() {}
	@Author int t;
  }
  
  import e.Team;
  public @interface Author {
	Team[] name() default Team.FREDERIC;
  }
  
  package e;
  public enum Team {
	PHILIPPE, DAVID, JEROME, FREDERIC;
  }

 
 ResultCollector.accept(CompletionProposal)
Having following test case:
If you try to complete at caret position, then you get an Error Excuting Command
dialog.
Debug shows that there's a NPE in ResultCollector.accept(CompletionProposal)
method due to name==null for CompletionProposal","org.eclipse.jdt.internal.codeassist.complete.CompletionOnAnnotationOfType
org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.codeassist.complete.CompletionParser"
FILE,eclipse-3.1,85672,2005-02-17T05:53:00.000-06:00,[projection] Unfolding a folded region with no line delimiter on the last line selects too much,"package folding;

class Test {
    
}
Have this code:
Put the caret right after the closing brace and fold the region.
Note that there is a phantom line in the editor since we cannot fold that one away.
Put the caret on the last line and unfold the type.
expected: caret is right after the closing brace actual: everything from after the *opening* brace is selected",org.eclipse.jface.text.source.projection.ProjectionViewer
FILE,eclipse-3.1,85734,2005-02-17T12:28:00.000-06:00,Debug view flickers excessively,"Runtime.exec(...)
The debug view flickers excessively when debugging.
In particular, I have set a
breakpoint on ""Runtime.exec(...)"" and started a debugging session on Eclipse.","org.eclipse.debug.internal.ui.views.RemoteTreeViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewEventHandler
org.eclipse.debug.internal.ui.views.RemoteTreeContentManager"
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
Simple test case below loads
 PNG Files and Saves them as JPEG.
Many files were tested and the majority 
 did produced the proper JPG images as expected.","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,87171,2005-03-04T14:19:00.000-06:00,Find declaring node doesn't work for methods/fields using type parameters,"public class Inline<T> {
	void foo(T t) {
		System.out.println(t);
	}
}

 class Use {
	public static void main(String[] args) {
		Inline<String> i= null;
		i.foo(""Eclipse"");
	}
}

  i.foo(""Eclipse"");
 
 root.findDeclaringNode(methodBinding);
The code below is all in compilation unit Inline.java
- take the method binding of the invocation foo in i.foo(""Eclipse"");
- take the root node representing the whole CU
- call root.findDeclaringNode(methodBinding);
observe: null is returned although the CU contains the corresponding declaration.
Please note that the same happens for fields using type parameters.","org.eclipse.jdt.core.dom.CompilationUnit
org.eclipse.jdt.core.dom.DefaultBindingResolver"
FILE,eclipse-3.1,87569,2005-03-09T16:41:00.000-06:00,Infinte loop obtaining image when switching to Debug Perspective,"class which implements java.io.Serializable
Create a Java Project using Java 5 (not sure it matters)
Create a class which implements java.io.Serializable
Use the ""Add generated serial version ID"" quickfix
Switch to the Debug perspective",org.eclipse.debug.internal.ui.DefaultLabelProvider
FILE,eclipse-3.1,87665,2005-03-10T11:38:00.000-06:00,Clicking on x on performance page opens details with no errors,"testOpenJavaEditor1()
Take a look at:
Scroll down to performance.OpenJavaEditorStressTest#testOpenJavaEditor1()""
Click on the red x ==> details show up all green.","org.eclipse.swt.printing.PrintDialog
org.eclipse.swt.widgets.MessageBox"
FILE,eclipse-3.1,89621,2005-03-30T12:41:00.000-06:00,[code assist] the caret position is wrong after code assist,"import java.awt.Frame;
import java.awt.event.WindowAdapter;

public class Foo extends Frame {

    public void bar() {
        addWindow<CODE ASSIST HERE>Listener(new WindowAdapter());
    }
}
Select addWindowListener in the list of proposal.
The result is:","org.eclipse.jdt.ui.text.java.CompletionProposalCollector
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector
org.eclipse.jdt.internal.ui.text.java.GenericJavaTypeProposal"
FILE,eclipse-3.1,89632,2005-03-30T13:10:00.000-06:00,Exception when trying to evaluate in Snippet Editor,"Collection<String> c = new ArrayList<String>();
        c.add(""a"");
        c.add(""b"");
        c.add(""c"");

        for (Iterator<String> i = c.iterator(); i.hasNext(); )
            if (i.next().length() == 4)
            {
                String x = i.next();
                System.out.println(x);
            }
        
        return c;

 
   
  run()
Testcase:
I added the testcase to the snippet editor.
I then did a ""Set Imports..."" to include java.util.
* to resolve collection and iterator.
Trying a ""Display"" or ""Inspect"" resulted in the following error in the console:",org.eclipse.jdt.internal.eval.CodeSnippetMessageSend
FILE,eclipse-3.1,90283,2005-04-05T08:56:00.000-05:00,[WorkbenchParts]IPartListener2#partInputChanged is not being sent,"partActivated(IWorkbenchPartReference ref)  
 ref.getId()  
 ref.getPart(true)  getSite()  
 ASTProvider.ActivationListener.isJavaEditor()
0. enable mark occurrences
1. enable search to reuse the editor (see Search preference page)
2. do a Java Search where you have matches in more than one file
3. select the first match
4. step through the matches until the second file gets opened
5. activate the editor by clicking on the tab
==> occurrence marking not working
To see the null value you can put a breakpoint in
ASTProvider.ActivationListener.isJavaEditor().",org.eclipse.ui.texteditor.AbstractTextEditor
FILE,eclipse-3.1,90289,2005-04-05T09:17:00.000-05:00,>1 debug worker thread calling IStackFrame.getVariables(),"IStackFrame.getVariables()
If I add a Suspsend VM breakpoint on the call to IStackFrame.getVariables() in 
my StackFrame object, I am seeing it hit 2 or more times.
As a result the Variables view shows duplicates of my local variables.","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.registers.RegistersView
org.eclipse.debug.internal.ui.views.registers.RegistersViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
FILE,eclipse-3.1,90600,2005-04-07T09:32:00.000-05:00,[model] CreateElementInCUOperation.apply: should use project options for rewriter,"rewriter.rewriteAST(document, null)
When executing the rewriter, (rewriter.rewriteAST(document, null))
CreateElementInCUOperation.apply should pass in the current projects options instead of null","org.eclipse.jface.text.source.SourceViewer
org.eclipse.jdt.internal.core.CreateElementInCUOperation
org.eclipse.jface.text.source.OverviewRuler"
FILE,eclipse-3.1,91098,2005-04-12T06:07:00.000-05:00,The Mark Occurrences feature does not mark all occurrences,"String a;
String[] b;
String[][] c;
The precise test case is the following:
Put the cursor on String or String[].
All occurrences of String get highlighted.
Now put the cursor on String[][].
No occurrence of String is highlighted.",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,91346,2005-04-13T16:43:00.000-05:00,available property reference not found for marking occurrences,"{buildDirectory}
Cursor in property=""${buildDirectory}"" does not mark the property declaration 
occurrence",org.eclipse.ant.internal.ui.model.AntPropertyNode
FILE,eclipse-3.1,91786,2005-04-18T16:07:00.000-05:00,"top frame annotated as ""secondary""","toStrings()
I have noticed that stepping with a lot of ""toStrings()""/evaluations occurring causes the editor annotation for the top frame to appear as a non-top frame.
I assume this is because the time at which the annotation draws, the thread is resumed for an evaluations and the ""top frame"" test fails.",org.eclipse.debug.internal.ui.InstructionPointerManager
FILE,eclipse-3.1,92236,2005-04-21T11:12:00.000-05:00,ConcurrentModificationException on shutdown,"org.eclipse.team.internal.core.TeamPlugin.stop()
I found the following exception in my log.
It occurred while I was shutting down my workspace last night:",org.eclipse.team.internal.core.ResourceVariantCache
FILE,eclipse-3.1,92451,2005-04-22T16:36:00.000-05:00,code assist failure: new+cast+arrays,"public class Test {
	public static void main(String[] args) {
		java.util.List elements = null;
		// code assist works on this line
		new Test(Test.toStrings((Test[])elements.toArray(new Test
[0])));
		//code assist fails on this line
	}
	public Test(Object object) {
	}
	public static Object toStrings(Test[] objects) {
		return null;
	}
}
Code assist fails in the following (self-contained) class (see comments for line of error)",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,93119,2005-04-28T10:10:00.000-05:00,code assist: proposals for wildcard types,"package codeAssist;

import java.util.List ;

public class Extends {
	void m() {
		List <? |>
	}
}
- have this code (caret at |):
- invoke code assist
> actual: the only proposed item is the CU's type (Extends)
- alternatively, press 'e' and invoke code assist
> actual: dozens of type proposals are proposed (and two template proposals, but
that is not a jdt-core problem)
< expected: only 'extends' is proposed.",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,93249,2005-04-29T05:49:00.000-05:00,Code assist doesn't propose full method stub,"IRunnableWithProgress runnable= new IRunnableWithProgress() {
};

  
  
 public void run(org.eclipse.core.runtime.IProgressMonitor monitor) throws
InvocationTargetException, InterruptedException
- take revision 1.8 of BuildPathAction.
- in run method add the following
- inside the runnable type run<code assist> and select run
observe: only the following method signature gets inserted.
No method body.
Additionally IProgressMonitor is fully qualified.","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.ui.text.java.OverrideCompletionProposal"
FILE,eclipse-3.1,93727,2005-05-04T17:43:00.000-05:00,Code Formatter fails with Method Parameter Annotations,"import org.drools.semantics.annotation.DroolsParameter;

public class Test
{
  public Object passthrough( @DroolsParameter(""parameter"") Object parameter ) {
    return parameter;
  }
}
It fails silently, and I don't see an error in
<Workspace>/.
Example:
import org.drools.semantics.annotation.DroolsParameter;",org.eclipse.jdt.internal.formatter.CodeFormatterVisitor
FILE,eclipse-3.1,94201,2005-05-09T17:08:00.000-05:00,Applet Contextual Launch Action broken,"public class MyApplet extends Applet {
	private static final long serialVersionUID = 1L;

	public void paint(Graphics graphics) {
		graphics.drawString(""Hello World"", 50, 100);
	}
}
Example Code:
public class MyApplet extends Applet {
	private static final long serialVersionUID = 1L;
When run from the context Menu an error message is display ""No Applet Found""
Manually creating a launch config via the lcd works fine.",org.eclipse.jdt.internal.debug.ui.launcher.AppletLaunchConfigurationUtils
FILE,eclipse-3.1,94216,2005-05-09T20:04:00.000-05:00,Open type does not work for generic types,"interface IGeneric<T> {
}
 public class Generic<T> implements IGeneric<T> {
    public static void main(String[] args) {
        IGeneric<String> gen= new Generic<String>();
        System.out.println();  // <-- breakpoint here
    }
}
Try to do 'open declaring type' or 'open concrete type' for 'gen' at the breakpoint, nothing happens.","org.eclipse.jdt.internal.debug.ui.actions.OpenVariableDeclaredTypeAction
org.eclipse.jdt.internal.debug.ui.actions.OpenVariableConcreteTypeAction"
FILE,eclipse-3.1,94465,2005-05-10T14:33:00.000-05:00,Java Core Dump where modifying value in the Variables View.,"String [] elms= { ""abc"", ""cde"", ""xyz"" };
Test case:
1. In the variables, expand the array, and then expand the first element, [0]
=""abc"".
2. Select the ""value=char[3]"" field.
RMC->Change Value.
3. In the Change Primitive Value dialog, type in a new string value.
4. Click ok and it will result in a java dump.","org.eclipse.jdt.internal.debug.ui.JDIModelPresentation
org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.ActionMessages"
FILE,eclipse-3.1,95096,2005-05-13T06:16:00.000-05:00,[5.0][content assist] Content assist popup disappears while completing the statically imported method name,"import static java.lang.Math
- Create a new Class ""Foo""
- Type ""import static java.lang.Math.""
- Press Ctrl+Space
- Type ""a""
-> Instead of constraining the proposals to all members with prefix a, the popup closes","org.eclipse.jdt.internal.ui.text.java.JavaMethodCompletionProposal
org.eclipse.jdt.internal.ui.text.java.LazyJavaCompletionProposal"
FILE,eclipse-3.1,95152,2005-05-13T12:14:00.000-05:00,[search] F3 can't find synthetic constructor_,"InputReadJob readJob = new InputReadJob(streamsProxy);
1) Add org.eclipse.debug.ui to the search path (i.e., by clicking ""Add to Java
Search"" in the plugins view.
2) Open type on ""ProcessConsole"" (class file with source attached)
3) Go to line 483:
4) Highlight the InputReadJob constructor_ and hit F3.
Clicking this entry in the outline view does not jump to the constructor_ in the editor.
The mapping of class file to source is not handling the synthetic addition of the enclosing class by the compiler.
This breaks any kind of navigation to the corresponding constructor_ in the source attachment.","org.eclipse.ant.internal.ui.views.AntViewDropAdapter
org.eclipse.ant.internal.ui.launchConfigurations.AntLaunchShortcut
org.eclipse.ant.internal.ui.AntUtil
org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
org.eclipse.jdt.internal.core.search.indexing.BinaryIndexer
org.eclipse.jdt.internal.core.index.DiskIndex
org.eclipse.jdt.internal.core.search.matching.ConstructorPattern"
FILE,eclipse-3.1,95505,2005-05-17T02:56:00.000-05:00,Can not use code completion,"{cursor}
It was very convinient in Eclipse that when it already knows type and I write
""new "" and press Ctrl+Space, it shows this type.
For example:",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,96440,2005-05-24T11:11:00.000-05:00,Tables laying out 3 times when trying to determine sizes,"table.getClientArea()
STEPS
1 Put a breakpoint in the JFace TableLayout class
2 Launch a self hosted workspace
3 Open Preferences-> Ant Runtime
4 You will see a client area size of about 81
5 Do the same in M6 - it will be about 320 or so.",org.eclipse.jface.preference.PreferencePage
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,[Presentations] (regression) Standalone view without title has no border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
- change the browser example's BrowserPerspectiveFactory to have the following instead of the regular addView layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
- run the example, and show the history view
- the history view (a regular view) has a border, but the standalone view does not","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,96766,2005-05-26T07:47:00.000-05:00,Console hyperlinks broken by 3.1M7,"public class Tst {
    
    public static void main(String[] args) throws Exception {
        
        System.out.println(""Log: Tst.main(Tst.java:5) Some message"");
        System.out.println(""Log: Tst.main(Tst.java:6)"");
    }
}
Example:
Run the above in Eclipse.
In the console the 'Tst.java:6' hyperlink will work,
but the 'Tst.java:5' hyperlink will give 'Hyperlink Error' with reason 'Unable
to parse line number from hyperlink'.",org.eclipse.jdt.internal.debug.ui.console.JavaStackTraceHyperlink
FILE,eclipse-3.1,96820,2005-05-26T12:27:00.000-05:00,JME during Source lookup,"enable()
I work in a full source workspace (ZRH plugins from HEAD, all other plugins imported as source).
I set a breakpoint in ContentAssistHandler#enable()
(in plugin org.eclipse.ui.workbench.texteditor), and started a run-time workbench.
After opening the Find/Replace dialog and enabling ""Regular Expressions"", I got a ""Source not found."" editor and the JME below.",org.eclipse.jdt.internal.launching.JavaSourceLookupUtil
FILE,eclipse-3.1,97674,2005-05-31T15:16:00.000-05:00,Changing value did not report error,"VectorTest.testCapacity()
 
 fEmpty.toString()  charAt(100000)
* Suspended in VectorTest.testCapacity()
* selected fFull in variables viwe
* selecetd ""change value"" from context menu
* entered expression ""fEmpty.toString().
charAt(100000)""
* pressed OK
> no error, no change","org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.EvaluateAction"
FILE,eclipse-3.1,97722,2005-05-31T16:41:00.000-05:00,Pref Page Ant/Runtime/Tasks/Add Task dialog problems,"@

Dialog
The dialog's error message is cropped at the bottom.
The space between Name and Location seems unneccessary.
The background color of the error message is different from the dialog
background -- is this intended?",org.eclipse.ant.internal.ui.preferences.AddCustomDialog
FILE,eclipse-3.1,98147,2005-06-02T13:09:00.000-05:00,Variables View does not show all children if same instance is expanded twice,"package xy;
public class Try {
	String fName;
	int fID;
	
	public Try(String name, int id) {
		fName= name;
		fID= id;
	}
	
	public static void main(String[] args) {
		Try t= new Try(""Hello"", 5);
		callee(t, t);
	}
	
	static void callee(Try t1, Try t2) {
		boolean same= t1.equals(t2); //breakpoint here
	}
	
}
- Debug the class below with the breakpoint where indicated.
- Expand t1 in the Variables view -> expands fine and shows fID and fName.
- Expand t2 -> only child fID is shown",org.eclipse.debug.internal.ui.views.RemoteTreeViewer
FILE,eclipse-3.1,98621,2005-06-06T22:04:00.000-05:00,[refactoring] [rename] Rename Type hangs,"class I18L  
 public class I18N {

	protected static void loadMessages(Class clazz, String name) {
		...
	}
}

 
 public class Messages extends I18L {
  public static String unexpectedException;
  ...
  static {
    loadMessages(Messages.class, ""messages.properties"");
  }
}
I renamed a (foolishly misspelled) class I18L to I18N.
The class looked like this:
The class was extended in 10 subclasses in 8 projects, like this:
I clicked OK in the dialog.
After about 5 minutes, I clicked Cancel.
The only
noticeable effect was the Cancel button was disabled.
Clicking the window exit
box had no effect.
When I restarted Eclipse, 7 of the references had been changed to I18N and 3 had
not.
Open Type (after re-indexing its database) still shows the non-existant
I18L type, though if you try to open it, the path cannot be found.","org.eclipse.core.internal.jobs.ImplicitJobs
org.eclipse.core.internal.jobs.ThreadJob"
FILE,eclipse-3.1,98686,2005-06-07T10:39:00.000-05:00,CVS Project pref page mnemonics,"@

Click
context menu of shared project -> properties; cvs page
""Enable watch/edit for this project"" has no mnemonic.
""Host"" has a mnemonic ... is this intended?
Click on ""Change Sharing...""
The ""Show only..."" check box has no mnemonic.","org.eclipse.pde.internal.ui.IHelpContextIds
org.eclipse.team.internal.ccvs.ui.CVSProjectPropertiesPage"
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,Container attempts to refresh children on project that is not open,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
Take an existing simple project on disk and import the project into the workspace by performing a simple create with code like:
Do not open the project with the project.open() API.
Now create a project either by API or UI and open it.
Or simply switch to the
Java perspective.
A background refresh job has now been started for the closed project, but it never finishes and is stuck in an infinite loop.
The members() method is excuting if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
workspace.refreshManager.refresh(this);
Both the AliasManager and the Java
Perspective are calling members on the IProject.
Our particular use case is that we are loading existing Java projects on disk by performing a create, but never an open.
On the next UI gesture, we get refresh infinite loops, one for each closed project.","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,[1.5][compiler] Enum / Switch method is not initialized in a thread safe way,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
For example, the following program should print ""1999"" 40 times
(once from each thread).
But on my machine, it prints ""default"" 22 times & 1999
18 times.","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99355,2005-06-10T09:48:00.000-05:00,extract method trips up with generics and final variables,"package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      final Container<String> container = c.get();
      final String string = container.get();
      //to here
   }
}
 
 

package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      extractedMethod(c);
      //to here
   }

   private void extractedMethod(final final final Container<Container<String>> c)
   {
      final Container<String> container = c.get();
      final String string = container.get();
   }
}
if you extract method where indicated below.
you will see that the extracted method declares its paramater with too many final modifiers:
notice the 3 final modifiers in the extractedMethod signature.",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,99631,2005-06-13T09:21:00.000-05:00,[assist][5.0] Unnecessary proposals on annotation completion,"@B 
 public class Test {}
@B<Ctrl+Space>
public class Test {}
-> The keyword proposals byte and boolean show up","org.eclipse.jdt.internal.corext.refactoring.reorg.JavaMoveProcessor
org.eclipse.jdt.internal.codeassist.CompletionEngine"
FILE,eclipse-3.1,99693,2005-06-13T11:29:00.000-05:00,Invalid stack frames during display,"private static void doGenerics() {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < 1000; i++) {
			int num = rand.nextInt(10000) + 1;
			list.add(num);
		}
		
		int max = 0;
//start eval
		for (Integer integer : list) { // BREAKPOINT HERE
			max = Math.max(max, integer);
		}
		System.out.println(max);
//end eval
	}
Debug the following method to a breakpoint:
Select everything between start eval and end eval comments and ctrl-shift-d.
Watching the Variables View I see a lot of invalid stack frames.
They are not destructive (and nothing is logged).","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
CLASS,openjpa-2.2.0,OPENJPA-2163,2012-03-27T15:56:55.000-05:00,Lifecycle event callback occurs more often than expect,"final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
 
 MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
 
 ((OpenJPAEntityManagerSPI)em).addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).addLifecycleListener(l2, null);
A problem was uncovered in a scenario where multiple EntityManager instances created from the same EntityManagerFactory, and each instance is initialized with a new instance of a LifecycleListener instance, i.e.
When life cycle event occurs for a specific entity manager, all the listeners created under the emf are being invoked.","openjpa-kernel.src.main.java.org.apache.openjpa.conf.OpenJPAConfigurationImpl
openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.validation.TestValidationMode"
CLASS,openjpa-2.2.0,OPENJPA-2227,2012-07-09T14:24:05.000-05:00,OpenJPA doesn't find custom SequenceGenerators,"{code}
 @Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable  
 @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
 {code}

 
     JavaTypes.classForName()     Class.forName()
When defining a custom Sequence a ClassNotFoundException (for the Sequence class) will be thrown when trying to insert data into the database.
ExampleConfiguration:
{code}
@Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable {
    @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
{code}
The example will produce the stacktrace attached.
It seems that the wrong class loader is used to instantiate the custom sequence class.
With JavaSE (JUnit) all is working fine, but after deploying into WAS the Exception will occur.",openjpa-kernel.src.main.java.org.apache.openjpa.meta.SequenceMetaData
CLASS,openjpa-2.2.0,OPENJPA-2247,2012-08-03T10:29:58.000-05:00,JoinColumn annotation is ignored when mapping a unidirectional owned OneToOne that is in a SecondaryTable,"@Entity
@SecondaryTable(name = ""ParentSecondaryTable"", pkJoinColumns = 
    { @PrimaryKeyJoinColumn(name = ""idParent"", referencedColumnName = ""idParent"") })
public class Parent {

    @Id
    @GeneratedValue
    int idParent;

    String child_ref;

    @OneToOne
    @JoinColumn(name = ""CHILD_REF"", table = ""ParentSecondaryTable"", referencedColumnName = ""idChild"")
    PChild child;

}
The runtime incorrectly ignores @JoinColumn.
name when mapping a unidirectional owned OneToOne that is in a SecondaryTable.
For example:
The column ""CHILD_REF"" will be ignored and the runtime will look for the fk in non-existent column ParentSecondaryTable.CHILD_IDCHILD.",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingRepository
CLASS,openjpa-2.2.0,OPENJPA-428,2007-11-01T12:39:40.000-05:00,"Bad error message regarding ""openjpa.Id""","@Id 
 @Id  @Id 
 @Id 
 @Entity
@Table(name=""TAX"", schema=""JPA_SC"")
public class Tax  {
	
	// Class variables  
	protected double taxamount;
 
	public Tax(){
		
	}
	
	public Tax(double taxamount){
		this.taxamount = taxamount;
	}
//plus getter and setter for taxamount

}
When running my project with OpenJPA, I get the following error message:
As you can see, the two property names printed are the same, not different or similar.
Furthermore, I was able to identify that the error message was being printed only when I removed the @Id annotation from one of my classes (all the other classes still have @Id).
Here is a sample of my class without @Id annotation:
@Entity
@Table(name=""TAX"", schema=""JPA_SC"")
public class Tax  {
	
	// Class variables  
	protected double taxamount;
 
	public Tax(){
		
	}
	
	public Tax(double taxamount){
		this.taxamount = taxamount;
	}
//plus getter and setter for taxamount","openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.Configurations
openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.ConfigurationImpl"
METHOD,atunes-1.10.0,231,2008-10-04T18:31:26.000-05:00,Can't add image if repository was read by older app version,"public boolean isSupportsInternalImage()
1. Read repository by older version of aTunes
2. Update to latest SVN
3. Try to add an image (assuming supported file format)
Adding image not possible.
public boolean isSupportsInternalImage() returns false as was never set to true (as not supported in old versions).",net.sourceforge.atunes.kernel.modules.repository.audio.AudioFile:supportsInternalPicture()
CLASS,solr-4.4.0,SOLR-5295,2013-10-02T00:09:02.000-05:00,The createshard collection API creates maxShardsPerNode number of replicas if replicationFactor is not specified,"{quote}
 
  
  
  
 {quote}
I've just created a very simple test collection on 3 machines where I set maxShardsPerNode at collection creation time to 1, and
I made 3 shards.
So I try again -- I create a collection with 3 shards and set maxShardsPerNode to 1000 (just as a silly test).
Now I add shard4 and it immediately tries to add 1000 replicas of shard4...",solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor
CLASS,solr-4.4.0,SOLR-5296,2013-10-02T00:20:01.000-05:00,Creating a collection with implicit router adds shard ranges to each shard,"{quote}
 {quote}
Creating a collection with implicit router adds shard ranges to each shard.
Using the Example A from SolrCloud wiki:
bq.
The following clusterstate is created:
Note that the createshard API does the right thing.",solr.core.src.java.org.apache.solr.cloud.Overseer
FILE,AMQP,AMQP-190,2011-09-10T20:24:17.000-05:00,CachingConnectionFactory leaks channels when synchronized with a TransactionManager,"convertAndSend()
It seems that when I use RabbitTemplate, channelTransacted=true, to convertAndSend() a message to an exchange within the context of a synchronized TransactionManager (e.g. an active transaction on the current thread), the channel is never closed, hence new publishes will always get their ""own"", shiny, new channel (that is never closed or released to the channel pool) until Rabbit can't handle any more channels.","org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer
org.springframework.amqp.rabbit.core.RabbitTemplatePerformanceIntegrationTests
org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils
org.springframework.amqp.rabbit.connection.RabbitResourceHolder"
FILE,AMQP,AMQP-340,2013-11-18T09:41:01.000-06:00,Wrong RabbitMQ credentials are not considered as fatal error by SimpleMessageListenerContainer,"start()  
 SimpleMessageListenerContainer.run()
If I connect an AmqpInboundChannelAdapter to the RabbitMQ (3.1.0) with wrong credentials (wrong password) the SimpleMessageListenerContainer doesn't detect the real reason and continues to restart the consumer in a seemingly endless loop.
As AmqpInboundChannelAdapter user, I don't notice (except if I turn on debug logging level) any problem, its start() method returns after a pause with no error, but the connection remains not established.
It would be nice to have a notification (exception) about wrong credentials right away (there is a comprehensive rabbit client exception that gets swallowed in SimpleMessageListenerContainer.run())","org.springframework.amqp.rabbit.listener.BlockingQueueConsumer
org.springframework.amqp.rabbit.listener.MessageListenerContainerLifecycleIntegrationTests
org.springframework.amqp.rabbit.support.RabbitExceptionTranslator"
FILE,AMQP,AMQP-502,2015-06-19T03:02:33.000-05:00,Fanout binding is not created due to missing routing key,"@RabbitListener(




      bindings = @QueueBinding(




          value = @Queue(




              autoDelete = ""true""




          ),




          exchange = @Exchange(




              type = ""fanout"",




              value = ""mytest.broadcast"",




              autoDelete = ""true""




          ),




          key = ""#""




      )




  )




  public void processBroadcast(String data) {




    int i = 0;




  }






 
  
  
  
     
 
     
 
  
  
  
  
  
   {}   
     
 
     
 
     
 
  
     
 
     
 
     
 
     
 
     
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   {}   
  
     
 
      
   {}
Currently i am using spring-cloud-starter-bus-amqp which in terms references spring-amqp 1.4.3.
when i declare a rabbitlistener like this:
I will get an error, that the binding can not be created.
The following is the debug logout and the stacktrace:","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-516,2015-08-06T01:25:34.000-05:00,"Setting autoDelete or exclusive to anything, including ""true"" in @Queue without a queue name results in them being disabled","@RabbitListener(bindings = @QueueBinding(




    value = @Queue(autoDelete = ""true"", exclusive = ""true""),




    exchange = @Exchange(value = ""myFanout"", type = ExchangeTypes.FANOUT, durable = ""true"")




))






   
 if (!StringUtils.hasText(queueName)) {




    queueName = UUID.randomUUID().toString();




    if (!StringUtils.hasText(bindingQueue.exclusive())) {




        exclusive = true;




    }




    if (!StringUtils.hasText(bindingQueue.autoDelete())) {




        autoDelete = true;




    }




}




else {




    exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());




    autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());




}






 
 String e = bindingQueue.exclusive();




if (!StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {




    exclusive = true




}
The following queue declaration will result in a queue being declared with auto delete and exclusive set to false:
Making them exclusive and auto delete by default when using a random name seems like a good idea, but it should probably be changed to something like:","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-653,2016-10-08T02:53:08.000-05:00,RabbitMessagingTemplate doesn't take advantage of RabbitTemplate's registered converters,"@Bean




Jackson2JsonMessageConverter jackson2JsonMessageConverter() {




	return new Jackson2JsonMessageConverter();




}
When using RabbitTemplate in a Spring Boot application, it's very easy to register a Spring AMQP Message Converter.
Just add this to your code:
However, if you switch to RabbitMessagingTemplate, that bean no longer works, because RabbitMessagingTemplate doesn't offer to look up RabbitTemplate's converters, and instead relies on its own.","org.springframework.amqp.rabbit.core.RabbitMessagingTemplateTests
org.springframework.amqp.rabbit.core.RabbitMessagingTemplate"
FILE,AMQP,AMQP-656,2016-10-15T00:25:46.000-05:00,Unable to refer to the default exchange using @Argument within a @RabbitListener,"@Argument 
 @RabbitListener(bindings =




        @QueueBinding(




            value = @Queue(




                value = ""app.events.myEvent"",




                durable = ""true"",




                exclusive = ""false"",




                autoDelete = ""false"",




                arguments = {




                        @Argument(name=""x-dead-letter-exchange"", value = """"),




                        @Argument(name=""x-dead-letter-routing-key"", value=""app.dlq"")




                }),




            exchange = @Exchange(value=""amq.topic"", durable = ""true"", type = ""topic""),




            key=""event.app.myEvent.v1""




        ))






 
 @Bean




    public Queue appMyEventQueue() {




        return QueueBuilder.durable(""app.events.myEvent"")




            .withArgument(""x-dead-letter-exchange"", """")




            .withArgument(""x-dead-letter-routing-key"", deadLetterQueue().getName())




            .build();




    }
It seems you are unable to use @Argument annotations that use empty strings to refer to the default exchange.
For example, you should be able to configure a queue to use the default exchange as part of the dead letter config similar to the following:
This fails though as spring seems to not send the empty string.","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
METHOD,commons-math-3-3.0,MATH-718,2011-12-03T18:40:44.000-06:00,inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.,"{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}
The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.
Following code will be reproduce the problem.
This returns 499525, though it should be 499999.","org.apache.commons.math3.util.ContinuedFraction:evaluate(double, double, int)"
METHOD,commons-math-3-3.0,MATH-841,2012-08-05T04:27:07.000-05:00,gcd speed up,"public void testApache(){
        Random rng=new Random(0);
        long checksum=0;
        long start=System.nanoTime();
        checksum+=gcd(0,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,0);
        checksum+=gcd(Integer.MAX_VALUE,rng.nextInt());
        for(int i=0;i<10000;i++) checksum+=gcd(rng.nextInt(),Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MIN_VALUE,1<<30);
        checksum+=gcd(1<<30,1<<30);
        checksum+=gcd(3 * (1<<20),9 * (1<<15));
        for(int i=0;i<30000000;i++) checksum+=gcd(rng.nextInt(),rng.nextInt());
        long end=System.nanoTime();
        long tns=end-start;
        long tms=(tns+500000)/1000000;
        long ts=(tms+500)/1000;
        System.out.println(""exec time=""+ts+""s, (""+tms+""ms), checksum=""+checksum);
        assertEquals(9023314441L,checksum);
    }
The gcd(int,int) method of ArithmeticUtils seems 2 times slower than the naive approach using modulo operator.
The following test code runs in 11s with current version and in 6s with the patch.","org.apache.commons.math3.util.ArithmeticUtils:gcd(int, int)"
FILE,DATACMNS,DATACMNS-68,2011-08-26T08:05:09.000-05:00,NullPointerException in AbstractPersistentProperty::getComponentType(),"class TestClassSet extends TreeSet<Object> { }









 class TestClassComplex {




    private String id;




    private TestClassSet testClassSet;









    public String getId() {




        return id;




    }









    public TestClassSet getTestClassSet() {




        return testClassSet;




    }









    public void setTestClassSet(TestClassSet testClassSet) {




        this.testClassSet = testClassSet;




    }




}






 
 List<TestClassSet> o = mongoTemplate.findAll(TestClassSet.class);






 
 List<TestClassComplex> o = mongoTemplate.findAll(TestClassComplex.class);
The following code appears to work fine:
But this fails with the NPE below:",org.springframework.data.util.ClassTypeInformation
FILE,DATACMNS,DATACMNS-114,2011-12-19T03:21:41.000-06:00,Wrong custom implementation automatically detected,"AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...)  getImplementationClassName()
We have two repositories with a similar name suffix.
Both repositories have a custom interface and implementation, also ending with a similar name suffix.
When automatically scanning the repositories, and their custom implementation, the wrong custom implementation is wired to our repository bean.
Resulting in the following exception:
For example:
We have a repository named ContractRepository with a custom interface ContractRepositoryCustom and an implementation ContractRepositoryImpl, all defined inside the same package.
In another package we have a repository, for another entity type, named AnotherContractRepository with a custom interface AnotherContractRepositoryCustom and an implementation AnotherContractRepositoryImpl.
When starting the application context, the contractRepository bean is linked to our anotherContractRepositoryImpl rather than the contractRepositoryImpl.",org.springframework.data.repository.config.AbstractRepositoryConfigDefinitionParser
FILE,DATACMNS,DATACMNS-157,2012-04-20T01:24:38.000-05:00,@Query in extending interface is not picked up correctly,"@Query 
 @NoRepositoryBean




public interface EntityRepository<T> extends JpaRepository<T, Long> {









	T findByDealer(Dealer dealer);




}









 public interface CarRepository extends EntityRepository<PersonalSiteVehicle> {









	@Override




	@Query(""select p from PersonalSiteVehicle p join p.detail d join d.enrichable e where e.dealer = ?1"")




	PersonalSiteVehicle findByDealer(Dealer dealer);




}






 
  @Query
I try to define an interface method in a super repository interface and 'implement' this in an extending interface with @Query.
Tested in the latest nightly build:
Results in
It looks like Spring Data does not use the @Query annotation in the sub interface.","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-160,2012-04-21T08:47:35.000-05:00,Regression of Repository instances with only delete* methods,"public interface DeleteOnlyRepository<T, ID extends Serializable> extends Repository<T, ID>{









    public void delete(ID paramID);









    public void delete(T paramT);









    public void delete(Iterable<? extends T> paramIterable);









    public void deleteAll();









}
A repository which only defines delete methods is not created by the Spring Data code with the exception:
I attach a sample Maven project to reproduce the issue","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-176,2012-05-21T11:47:05.000-05:00,StackOverflowError when inserted object is a CGLIB proxy,"@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)
When trying to persist an object [to MongoDB (spring-data-mongodb v1.1.0.
M1)] that is in ""session"" scope and using a CGLIB proxy (ie: ""@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)"") I receive a StackOverflowError.",org.springframework.data.util.ClassTypeInformation
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,DomainClassConverter should gracefully return null for null sources or empty strings,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
Imagine the use case where you have an Order domain class which has a ManyToOne reference to Customer.
When posting a new Order where Order.customer == """" then a converter exception is thrown:
This is the code I used:","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-257,2012-11-29T02:29:27.000-06:00,PropertyPath cannot deal with all uppercase fields,"@Id 
 class Foo{




  




  @Id




  private String UID;









  //code omitted




}
Cannot execute MongoOperations.findOne method if my model entity contains @Id field which name is uppercase, like UID.
Here an example:
1) create an entity like in example code snippet in MongoDb
2) try to perform find by id operation by calling MongoOperations.findOne
3) at this step you will get an exception","org.springframework.data.mapping.PropertyPath
org.springframework.data.mapping.PropertyPathUnitTests"
FILE,DATACMNS,DATACMNS-509,2014-05-08T08:39:02.000-05:00,NullableWrapper Breaks JSON Conversion,"{




    final Set<Pos> allPos = posService.findAll();




    return ImmutableSortedSet.copyOf(allPos);




}






 
 {""name: ""pos1""}  {""name: ""pos2""} 
  
     {""name: ""pos1""}  {""name: ""pos2""}
Since an upgrade to JPA 1.6 RC1, Spring MVC fails to properly address a NullableWrapper and this is returned, with the contents contained with the NullableWrapper.
I have a MVC method that is returning:
With Spring Data JPA 1.5.
, I get on the wire a set of Pos's in JSON format, i.e.,
With Spring Data JPA 1.6 RC1, I now get the NullableWrapper with Contents:","org.springframework.data.repository.core.support.DummyRepositoryFactory
org.springframework.data.repository.core.support.RepositoryFactorySupport
org.springframework.data.repository.core.support.RepositoryFactorySupportUnitTests"
FILE,DATACMNS,DATACMNS-511,2014-05-22T00:04:43.000-05:00,AbstractMappingContext.addPersistentEntity causes infinite loop,"public class User extends AbstractTenantUser<User, Role, Permission, Tenant> {




    ...




}




 public abstract class AbstractTenantUser<USER extends AbstractTenantUser<USER, ROLE, PERMISSION, TENANT>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>, TENANT extends AbstractTenant<USER>> extends AbstractUser<USER, ROLE, PERMISSION> implements TenantEntity<TENANT> {




    ...




}




 public abstract class AbstractUser<USER extends AbstractUser<USER, ROLE, PERMISSION>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AbstractPermission<USER extends AbstractUser<USER, ?, ?>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AuditingDateBaseEntity<USER extends AbstractUser<USER, ?, ?>> extends AbstractDateBaseEntity implements AuditingEntity<USER> {




    ...




}




 public abstract class AbstractDateBaseEntity extends AbstractBaseEntity implements DateEntity {




    ...




}




 public abstract class AbstractBaseEntity implements BaseEntity {




    ...




}
We use quite a few abstract MappedSuperclasses that have circular references and apparently this does not work.
An example:",org.springframework.data.util.TypeVariableTypeInformation
FILE,DATACMNS,DATACMNS-562,2014-08-19T01:25:20.000-05:00,MappingContext fails to resolve TreeMap as Map value type,"public class ClassC extends ClassA {




	private ClassB b;









	public ClassB getB() {




		return b;




	}









	public void setB(ClassB b) {




		this.b = b;




	}




}









 class ClassA {









	private String name;









	private ClassD dObject;









	public String getName() {




		return name;




	}









	public void setName(String name) {




		this.name = name;




	}









	public ClassD getdObject() {




		return dObject;




	}









	public void setdObject(ClassD dObject) {




		this.dObject = dObject;




	}




}









 class ClassB extends ClassA {




}









 class ClassD {









	private TreeMap<String, TreeMap<String, String>> map = new TreeMap<>();









	public TreeMap<String, TreeMap<String, String>> getMap() {




		return map;




	}









	public void setMap(TreeMap<String, TreeMap<String, String>> map) {




		this.map = map;




	}









}






 
 
 
 
 
 
 
 ClassC cObject = new ClassC();




cObject.setName(""Jon"");




try {




	mongoTemplate.save(cObject, ""c"");




} catch (Exception e) {




	e.printStackTrace();




}






 
 
     private transient EntrySet entrySet = null;
This is the Class of entity used to save to mongodb.
This is the call entry:
Exception caught as below:","org.springframework.data.mapping.model.AbstractPersistentPropertyUnitTests
org.springframework.data.mapping.model.AbstractPersistentProperty"
FILE,DATACMNS,DATACMNS-609,2014-11-30T07:31:51.000-06:00,Multiple usage of repository setup means (XML or annotation) creates multiple bean definitions for RepositoryInterfaceAwareBeanPostProcessor,"predictBeanType()
Apparently every time the tag jpa:repositories is used in XML files, a new instance of RepositoryInterfaceAwareBeanPostProcessor is created and registered with the bean factory.
This is mainly a performance problem because predictBeanType()-method is then called N times for every relevant bean.
This can easily be reproduced with the version 51d1c5d of git@github.com:spring-projects/spring-data-jpa-examples.git by adding an extra stanza like <jpa:repositories base-package=""org.springframework.dummy"" /> to simple-repository-context.xml and running XmlConfigCachingRepositoryTests.","org.springframework.data.repository.config.RepositoryConfigurationExtensionSupportUnitTests
org.springframework.data.repository.config.RepositoryConfigurationExtensionSupport"
FILE,DATACMNS,DATACMNS-616,2014-12-17T02:25:54.000-06:00,AnnotationRevisionMetadata can't access private fields,"@Entity




@RevisionEntity(ExtendedRevisionListener.class)




@Table(name = ""revinfo"")




public class ExtendedRevision implements Serializable  
 @Id




	@GeneratedValue




	@Column(name = ""REV"")




	@RevisionNumber




	private Integer id;









	 @RevisionTimestamp




	@Temporal(TemporalType.TIMESTAMP)




	@Column(name = ""REVTSTMP"", nullable = false)




	private Date date;









	 @Column(nullable = false, length = 15)




	private String username;









	 public Integer getId() {




		return id;




	}









	 public Date getDate() {




		return date;




	}









	 public String getUsername() {




		return username;




	}









	 public void setUsername(String username) {




		this.username = username;




	}
Trying to use a custom Envers revision class:
triggers this error:",org.springframework.data.util.AnnotationDetectionFieldCallback
FILE,DATACMNS,DATACMNS-683,2015-04-13T05:31:25.000-05:00,Enabling Spring Data web support breaks @ModelAttribute binding in Spring MVC,"package be.vdab.web;









import org.springframework.context.annotation.ComponentScan;




import org.springframework.context.annotation.Configuration;




import org.springframework.data.web.config.EnableSpringDataWebSupport;




import org.springframework.web.servlet.config.annotation.EnableWebMvc;




import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;









// enkele imports




@Configuration




@EnableWebMvc




@EnableSpringDataWebSupport




@ComponentScan




public class CreateControllerBeans extends WebMvcConfigurerAdapter {




}






  






package be.vdab.web;









import org.springframework.stereotype.Controller;




import org.springframework.web.bind.annotation.ModelAttribute;




import org.springframework.web.bind.annotation.RequestMapping;




import org.springframework.web.bind.annotation.RequestMethod;




import org.springframework.web.servlet.ModelAndView;









import be.vdab.entities.Person;









@Controller




@RequestMapping(value = ""/"")




public class PersonController {




	private static final String TOEVOEGEN_VIEW = ""/WEB-INF/JSP/index.jsp"";














	@RequestMapping(method=RequestMethod.GET)




	ModelAndView get() {




		return new ModelAndView(TOEVOEGEN_VIEW).addObject(new Person());




	}




	




	@RequestMapping(method = RequestMethod.POST)




	String post(@ModelAttribute Person person) {




	  if (person == null) {




		  throw new IllegalArgumentException(""person IS NULL"");




	  }




	  return ""redirect:/"";




	}



















}






 
    
 
 
 
    
 @EnableSpringDataWebSupport   
 @ModelAttribute
Given following Java config class
, following Controller class
and following JSP
the method post in PersonController throws the InvalidArgumentException because the person parameter is null.","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-695,2015-05-13T09:08:15.000-05:00,Potential NullPointerException in AbstractMappingContext,"public class External{




 ..




 private Optional<Internal> field = new Optional<Internal>();




 ..




}
The issue is triggered querying a nested generic field qualified with a custom class (not primitive).
Following snippet shows the nested field we are trying to query:
The call to mongoOperations throws a NullPointerException originating from AbstractMappingContext.","org.springframework.data.mapping.context.AbstractMappingContext
org.springframework.data.mapping.context.AbstractMappingContextUnitTests"
FILE,DATACMNS,DATACMNS-943,2016-10-04T21:54:22.000-05:00,Redeclared save(Iterable) results in wrong method overload to be invoked eventually,"myRepository.save(Arrays.asList(new MyEntity(1, ""foo""), new MyEntity(2, ""bar"")));
But in jenkins server (on linux) with the same java version the code breaks with this exception
The invoking call is
In linux and with the upgrade to spring boot 1.4.1 it seems the save(Iterable) method is not invoked, but the save(entity) method is invoked and causes this exception.","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
CLASS,derby-10.9.1.0,DERBY-3024,2007-08-23T05:24:31.000-05:00,Validation of shared plans hurts scalability,"GenericPreparedStatement.upToDate()   BaseActivation.checkStatementValidity()
To investigate whether there was anything in the SQL execution layer that prevented scaling on a multi-CPU machine, I wrote a multi-threaded test which continuously executed ""VALUES 1"" using a PreparedStatement. I ran the test on a machine with 8 CPUs and expected the throughput to be proportional to the number of concurrent clients up to 8 clients (the same as the number of CPUs). However, the throughput only had a small increase from 1 to 2 clients, and adding more clients did not increase the throughput. Looking at the test in a profiler, it seems like the threads are spending a lot of time waiting to enter synchronization blocks in GenericPreparedStatement.upToDate() and BaseActivation.checkStatementValidity() (both of which are synchronized on the a GenericPreparedStatement object).
I then changed the test slightly, appending a comment with a unique thread id to the ""VALUES 1"" statement.
When I made that change, the test scaled more or less perfectly up to 8 concurrent threads.","java.engine.org.apache.derby.impl.store.access.heap.HeapConglomerateFactory
java.engine.org.apache.derby.impl.store.raw.data.FileContainer
java.engine.org.apache.derby.impl.store.raw.data.RAFContainer
java.testing.org.apache.derbyTesting.functionTests.tests.lang.DBInJarTest
java.engine.org.apache.derby.impl.store.raw.data.TempRAFContainer
java.engine.org.apache.derby.impl.store.raw.data.InputStreamContainer
java.engine.org.apache.derby.impl.store.access.btree.index.B2IFactory"
CLASS,derby-10.9.1.0,DERBY-4647,2010-05-07T13:34:26.000-05:00,BaseTestCase.execJavaCmd() does not work with weme 6.2,"BaseTestCase.execJavaCmd()
Spawning a java process with BaseTestCase.execJavaCmd() does not work with weme 6.2, I think because the boot classpath does not get passed.
The error is actually
.
execJavaProcess does pick up the j9 executable but does not pass on the other settings.
This is how my script invokes the test with j9.","java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockMinion
java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockTest"
CLASS,derby-10.9.1.0,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:",java.engine.org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.9.1.0,DERBY-5407,2011-09-12T08:50:38.000-05:00,"When run across the network, dblook produces unusable DDL for VARCHAR FOR BIT DATA columns.","varchar( 20 )  
 
 
 VARCHAR ()
In private correspondence, Mani Afschar Yazdi reports that dblook omits the length specification for VARCHAR FOR BIT DATA columns when run across the network.
Embedded dblook runs fine.
I can reproduce this problem as follows:
1 Bring up a server (here I am using port 8246).
2 Create a database with the following ij script:
3 Now run dblook across the network:
This produces the following DDL for the table:
A similar experiment using an embedded database produces usable DDL which includes a length specification for the VARCHAR FOR BIT DATA column.","java.testing.org.apache.derbyTesting.functionTests.tests.lang.SystemCatalogTest
java.engine.org.apache.derby.catalog.types.BaseTypeIdImpl"
CLASS,derby-10.9.1.0,DERBY-5567,2012-01-05T07:35:04.000-06:00,AlterTableTest#testDropColumn fails: drop view cannot be performed due to dependency,"testDropColumn(org.apache.derbyTesting.functionTests.tests.lang.AlterTableTest)
Saw this when running suitesAll on 10.8.2.2:
Prior to this, though, I saw this on the console, but no error/failure.",java.engine.org.apache.derby.iapi.sql.dictionary.ViewDescriptor
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,Client should use a prepared statement rather than regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
The program below shows repeated calls to setTransactionIsolation.",java.client.org.apache.derby.client.am.Connection
CLASS,derby-10.9.1.0,DERBY-6131,2013-03-27T08:17:37.000-05:00,"select from view with ""upper"" and ""in"" list throws a ClassCastException","varchar(1000) 
   varchar(1000)
the issue can be reproduced
1 create table myTbl1 (name varchar(1000));
2 create table myTbl2 (name varchar(1000));
3 create view myView (name) as select t1.name from myTbl1 t1 union all select t2.name from myTbl2 t2;
4 select name from myView where upper(name) in ('AA', 'BB');
#4 failed with ""org.apache.derby.impl.sql.compile.SimpleStringOperatorNode incompatible with org.apache.derby.impl.sql.compile.ColumnReference: java.lang.ClassCastException""
If the view is created as ""create myView (name) as select t1.name from myTbl1 t1"", the query worked fine.","java.testing.org.apache.derbyTesting.functionTests.tests.lang._Suite
java.testing.org.apache.derbyTesting.junit.BaseJDBCTestCase
java.engine.org.apache.derby.impl.sql.compile.PredicateList"
METHOD,time,18,2013-04-19T08:28:47.000-05:00,NPE in DateTimeZoneBuilder,"PrecalculatedZone.create()  ZoneInfoCompiler.verbose() 
    
 static {
 cVerbose.set(Boolean.FALSE);
 }
 
 public static boolean verbose() {
 return cVerbose.get();
 }
 
 public static boolean verbose(){
 Boolean verbose = cVerbose.get();
 return (verbose != null) ? verbose : false;
}
 
 @Test
 public void testDateTimeZoneBuilder() throws Exception {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ1"", true);
 Thread t = new Thread(new Runnable() {
 @Override
 public void run() {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ2"", true);
 }
 });
 t.start();
 t.join();
 }

  private DateTimeZoneBuilder getTestDataTimeZoneBuilder() {
 return new DateTimeZoneBuilder()
 .addCutover(1601, 'w', 1, 1, 1, false, 7200000)
 .setStandardOffset(3600000)
 .addRecurringSavings("""", 3600000, 1601, Integer.MAX_VALUE, 'w', 3, -1, 1, false, 7200000)
 .addRecurringSavings("""", 0, 1601, Integer.MAX_VALUE, 'w', 10, -1, 1, false, 10800000);
 }
When a DateTimeZone is build with duplicate-named 'recurring saving time' in a first thread, all goes Ok: a warning message is generated and an identifier is automatically generated in PrecalculatedZone.create().
When a second thread does the same, an NPE is generated in ZoneInfoCompiler.verbose().
Here follows a test case:",org.joda.time.tz.ZoneInfoCompiler:<clinit0>
METHOD,time,21,2013-05-03T21:03:46.000-05:00,DateTimeFormat.parseInto sometimes miscalculates year (2.2),"public void testParseInto_monthDay_feb29_startOfYear() {
 DateTimeFormatter f = DateTimeFormat.forPattern(""M d"").withLocale(Locale.UK);
 MutableDateTime result = new MutableDateTime(2000, 1, 1, 0, 0, 0, 0, NEWYORK);
 assertEquals(4, f.parseInto(result, ""2 29"", 0));
 assertEquals(new MutableDateTime(2000, 2, 29, 0, 0, 0, 0, NEWYORK), result);
 }
The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999:","org.joda.time.format.DateTimeFormatter:parseInto(ReadWritableInstant, String, int)"
METHOD,time,28,2013-05-31T00:52:24.000-05:00,Questionable behaviour of GJChronology when dates pass 1BC,"Chronology chronology = GJChronology.getInstance();

LocalDate start = new LocalDate(2013, 5, 31, chronology);
LocalDate expectedEnd = new LocalDate(-1, 5, 31, chronology); // 1 BC
assertThat(start.minusYears(2013), is(equalTo(expectedEnd)));
assertThat(start.plus(Period.years(-2013)), is(equalTo(expectedEnd)));
I expect the following test to pass:
The error it gives is:","org.joda.time.chrono.GJChronology:getInstance(DateTimeZone, ReadableInstant, int)
org.joda.time.chrono.GJChronology:add(long, long)
org.joda.time.chrono.GJChronology:add(long, int)"
METHOD,time,77,2013-10-16T15:36:22.000-05:00,addDays(0) changes value of MutableDateTime,"final MutableDateTime mdt = new MutableDateTime(2011, 10, 30, 3, 0, 0, 0, DateTimeZone.forID(""Europe/Berlin""));
System.out.println(""Start date: "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(-1);
System.out.println(""addHours(-1): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(0);
System.out.println(""addHours(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addDays(0);
System.out.println(""addDays(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
 
 
 addHours(-1)  
 addHours(0)  
 addDays(0)  
 
          
 addDays(0)
Upon DST transition from summer to winter time zone, adding the amount of zero days to a mutable date time object changes the value of the object.
The code
prints
The methods addMonths and addYears show the same problem; addSeconds, addMinutes and addHours are ok.","org.joda.time.MutableDateTime:add(DurationFieldType, int)
org.joda.time.MutableDateTime:addWeeks(int)
org.joda.time.MutableDateTime:addYears(int)
org.joda.time.MutableDateTime:addMonths(int)
org.joda.time.MutableDateTime:addMinutes(int)
org.joda.time.MutableDateTime:addHours(int)
org.joda.time.MutableDateTime:addWeekyears(int)
org.joda.time.MutableDateTime:addDays(int)
org.joda.time.MutableDateTime:addSeconds(int)
org.joda.time.MutableDateTime:addMillis(int)"
METHOD,time,79,2013-10-17T20:36:31.000-05:00,none standard PeriodType without year throws exception,"Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()})).normalizedStandard(PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()}));
return p.getMonths();
 
    
      
   
 withYearsRemoved() throws the  
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard()).normalizedStandard(PeriodType.standard());
return p.getMonths();
 
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard().withYearsRemoved()).normalizedStandard(PeriodType.standard().withYearsRemoved());
return p.getMonths();
I tried to get a Period only for months and weeks with following code:
This throws following exception:
Even removing the year component with .
withYearsRemoved() throws the same exception:",org.joda.time.Period:normalizedStandard(PeriodType)
METHOD,time,88,2013-11-25T19:15:46.000-06:00,Constructing invalid Partials,"Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).with(hourOfDay(), 1);
assert(a == b);
 
 new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
 
 new Partial(clockhourOfDay(), 1)  with(hourOfDay(), 1)  isEqual(new Partial(hourOfDay() ,1).with(clockhourOfDay(), 1)) // throws objects must have matching field types
However, the above doesn't work in all cases:
``` java
new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).
with(clockhourOfDay(), 1)) // throws objects must have matching field types
```","org.joda.time.Partial:with(DateTimeFieldType, int)"
METHOD,time,93,2013-12-01T09:33:58.000-06:00,Partial.with fails with NPE,"new Partial(yearOfCentury(), 1)  with(weekyear(), 1);
// NullPointerException
// org.joda.time.Partial.with (Partial.java:447)
With the latest master:
Fails with yearOfCentury, year and yearOfEra.","org.joda.time.Partial:Partial(DateTimeFieldType[], int[], Chronology)
org.joda.time.field.UnsupportedDurationField:compareTo(DurationField)"
FILE,COMPRESS,COMPRESS-131,2011-06-03T16:25:45.000-05:00,ArrayOutOfBounds while decompressing bz2,"recvDecodingTables()
Decompressing a bz2 file generated by bzip2 utility throws an ArrayIndexOutOfBounds at method recvDecodingTables() line 469.",org.apache.commons.compress.compressors.BZip2TestCase
FILE,COMPRESS,COMPRESS-189,2012-06-26T21:30:39.000-05:00,ZipArchiveInputStream may read 0 bytes when reading from a nested Zip file,"ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
      ZipArchiveEntry entry = iterator.nextElement();
      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
      ZipArchiveEntry innerEntry;
      while ((innerEntry = zipInput.getNextZipEntry()) != null){
        if (innerEntry.getName().endsWith(""XML""))
{

          //zipInput.read();

          System.out.println(IOUtils.toString(zipInput));

        }
      }
    }
When the following code is run an error ""Underlying input stream returned zero bytes"" is produced.
If the commented line is uncommented it can be seen that the ZipArchiveInputStream returned 0 bytes.
The zip file used to produce this behavious is available at http://wwmm.ch.cam.ac.uk/~dl387/test.ZIP
Also I believe whilst ZipFile can iterate over entries fast due to being able to look at the master table whilst ZipArchiveInputStream cannot.","org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream"
FILE,COMPRESS,COMPRESS-273,2014-04-11T04:13:32.000-05:00,NullPointerException when creation fields/entries from scratch,"org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
    org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();
However, when these 0-argument constructors are used, certain internal references are null, resulting in a NullPointerException soon after.
The attachment contains a number of similar test cases that show the same issue in a couple of classes.
An example:","org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField
org.apache.commons.compress.archivers.cpio.CpioArchiveEntry
org.apache.commons.compress.archivers.zip.ExtraFieldUtils
org.apache.commons.compress.archivers.zip.UnrecognizedExtraField"
FILE,COMPRESS,COMPRESS-278,2014-04-18T16:09:05.000-05:00,Incorrect handling of NUL username and group Tar.gz entries,"package TestBed;



import java.io.File;

import java.io.FileInputStream;

import java.io.FileNotFoundException;

import java.io.IOException;



import org.apache.commons.compress.archivers.tar.TarArchiveEntry;

import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;

import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;

import org.junit.Test;



/**

 * Unit test for simple App.

 */

public class AppTest

{



    @Test

    public void extractNoFileOwner()

    {

        TarArchiveInputStream tarInputStream = null;



        try

        {

            tarInputStream =

                new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(

                    ""/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz"" ) ) ) );

            TarArchiveEntry entry;

            while ( ( entry = tarInputStream.getNextTarEntry() ) != null )

            {

                System.out.println( entry.getName() );

                System.out.println(entry.getUserName()+""/""+entry.getGroupName());

            }



        }

        catch ( FileNotFoundException e )

        {

            e.printStackTrace();

        }

        catch ( IOException e )

        {

            e.printStackTrace();

        }

    }



}
With version 1.8 of commons-compress it's no longer possible to decompress  files from an archive if the archive contains entries having null (or being empty?)
set as username and/or usergroup.
With version 1.7 this still worked now I get this exception:
Some test code you can run to verify it:
With 1.7 the TestCase outputed this:
With 1.8 it's failing once it reaches the null valued entry, which is the first.","org.apache.commons.compress.archivers.tar.TarUtilsTest
org.apache.commons.compress.archivers.tar.TarUtils"
FILE,COMPRESS,COMPRESS-309,2015-02-18T17:22:16.000-06:00,BZip2CompressorInputStream return value wrong when told to read to a full buffer.,"BZip2CompressorInputStream.read(buffer, offset, length)  
 @Test

	public void testApacheCommonsBZipUncompression () throws Exception {

		// Create a big random piece of data

		byte[] rawData = new byte[1048576];

		for (int i=0; i<rawData.length; ++i) {

			rawData[i] = (byte) Math.floor(Math.random()*256);

		}



		// Compress it

		ByteArrayOutputStream baos = new ByteArrayOutputStream();

		BZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);

		bzipOut.write(rawData);

		bzipOut.flush();

		bzipOut.close();

		baos.flush();

		baos.close();



		// Try to read it back in

		ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());

		BZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);

		byte[] buffer = new byte[1024];

		// Works fine

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		// Fails, returns -1 (indicating the stream is complete rather than that the buffer 

		// was full)

		Assert.assertEquals(0, bzipIn.read(buffer, 1024, 0));

		// But if you change the above expected value to -1, the following line still works

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		bzipIn.close();

	}
BZip2CompressorInputStream.read(buffer, offset, length) returns -1 when given an offset equal to the length of the buffer.
This indicates, not that the buffer was full, but that the stream was finished.
It seems like a pretty stupid thing to do - but I'm getting this when trying to use Kryo serialization (which is probably a bug on their part, too), so it does occur and has negative affects.
Here's a JUnit test that shows the problem specifically:
// Fails, returns -1 (indicating the stream is complete rather than that the buffer
// was full)",org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
METHOD,mahout-0.8,MAHOUT-1301,2013-08-01T09:31:21.000-05:00,toString() method of SequentialAccessSparseVector has excess comma at the end,"SequentialAccessSparseVector toString()   toString()  
 {code:java}
 Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
  v.toString()  
 {code:java}
 {1:0.1,3:0.3}
 {code}
 
 {code:java}
 {1:0.1,3:0.3,}
 {code}
Unfortunately, that patch introduced new bug: output of the toString() method had been changed - extra comma added at the end of the string
Example: 
Consider following sparse vector
{code:java}
Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
In 0.7 v.toString() returns following string:
{code:java}
{1:0.1,3:0.3}
{code}
but in 0.8 it returns
{code:java}
{1:0.1,3:0.3,}
{code}
As you can see, there is extra comma at the end of the string.","org.apache.mahout.math.SequentialAccessSparseVector:toString()
org.apache.mahout.math.RandomAccessSparseVector:toString()"
METHOD,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
it happens every time the REDUCE_STREAMING_KMEANS is set to true.","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:reduce(IntWritable, Iterable<CentroidWritable>, Context)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:getBestCentroids(List<Centroid>, Configuration)"
METHOD,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
I had a repository of 500K documents, for which I generated the input vectors and a dictionary using some custom code (not seq2sparse etc).
I hashed the features with max size 5M (because I didn't know how many features were in the dataset and wanted to minimize collisions).
The kmeans ran fine and generate sensible looking results, but when I tried to run ClusterDumper I got the following error:
The error is when it tries to access the dictionary for the feature with index 698948
It looks like the dictionary array is sized for the number of unique keywords, not the highest index:","org.apache.mahout.utils.vectors.VectorHelper:loadTermDictionary(Configuration, String)
org.apache.mahout.utils.vectors.VectorHelperTest:testJsonFormatting()"
METHOD,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}


 {Code}


  StreamingKMeansThread.call()


 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }


    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Path, Configuration)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Iterable<Centroid>, Configuration)"
FILE,swt-3.1,104150,2005-07-16T19:58:00.000-05:00,[Patch] Table cursor separated from table selection when clicking on grid lines or empty space,"table.getLinesVisible()  
 table.setLinesVisible(true)
When using a table cursor, there are two kinds of table regions that have the potential to separate the table cursor from the table selection when clicked on:
1) grid lines (table.getLinesVisible() == true)
2) empty space to the left of the first cell of each row (SWT.FULL_SELECTION)
To reproduce the problem, use snippet 96 with an added table.setLinesVisible(true).",org.eclipse.swt.custom.TableCursor
FILE,swt-3.1,104545,2005-07-20T14:21:00.000-05:00,Make default size of empty composites smaller,"static final int DEFAULT_WIDTH	= 64;
 static final int DEFAULT_HEIGHT	= 64;
Background: When you write an RCP app and enable the cool bar, the cool bar will initially be empty, but 64x64 pixels in size.
On Windows, you cannot see the border of the empty coolbar so the user gets a big empty space at the top of their window and might be confused.",org.eclipse.swt.widgets.CoolBar
FILE,swt-3.1,117574,2005-11-22T15:22:00.000-06:00,RIGHT_TO_LEFT |  DOUBLE_BUFFERED don't get along,"public static void main(String[] args) {
	Display display = new Display();
	Shell shell = new Shell(display, SWT.SHELL_TRIM | SWT.RIGHT_TO_LEFT | SWT.DOUBLE_BUFFERED);
	shell.addListener(SWT.Paint, new Listener() {
		public void handleEvent(Event event) {
			System.out.println(event.gc.getClipping());
			event.gc.drawString(""This is broken "" + event.gc.getClipping(), 10, 10);
		}
	});
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose ();
}
Example:
Doesn't draw anything.","org.eclipse.swt.widgets.Composite
org.eclipse.swt.graphics.GC"
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,Table fails to setTopIndex after new items are added to the table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
Here's my testcase to demonstrate the problem:
Table.setTopIndex fails to position to the correct table item if new items are added to the table after the shell is opened.
The first call to setTopIndex succeeds.
The table is correctly positioned at item 20.
After adding new table items to the table, calling setTopTable(40) has no effect.
Calling getTopIndex continues to return 20.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,84609,2005-02-07T13:35:00.000-06:00,TableColumn has NPE while calling pack()  on last column,"lvtTable.getColumn(0).pack();
lvtTable.getColumn(1).pack();
lvtTable.getColumn(2).pack();

   
 parent.getColumns()
Consider followed code, table has only 3 columns:
On third call I get caught NPE (in debugger) in TableColumn (line 356), because parent.getColumns() (in TableColumn:354) returns array with 4 elements (always one more as existing in the table), and the last element is always null.","org.eclipse.swt.widgets.TableColumn
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
The ImageLoader Save function appears to be producing bad JPG images.
Simple test case below loads
 PNG Files and Saves them as JPEG.
Many files were tested and the majority 
 did produced the proper JPG images as expected.",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,87460,2005-03-08T21:22:00.000-06:00,StyledText: Caret location not updated when line style is used,"import org.eclipse.swt.*;
import org.eclipse.swt.custom.*;
import org.eclipse.swt.graphics.*;
import org.eclipse.swt.layout.*;
import org.eclipse.swt.widgets.*;

public class LineStyleCaretTest {
  public static void main(String[] args) {
    Display display = new Display();
    
    Shell shell = new Shell(display);
    shell.setLayout(new FillLayout());
    
    Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
      
    final StyledText text = new StyledText(shell, SWT.MULTI);
    text.setFont(font);
    text.setText(""Standard Widget Toolkit"");
    text.setCaretOffset(text.getText().length());
    
    text.addLineStyleListener(new LineStyleListener() {
      public void lineGetStyle(LineStyleEvent event) {
        StyleRange[] styles = new StyleRange[1];
        
        styles[0] = new StyleRange();
        styles[0].start  = 0;
        styles[0].length = text.getText().length();
        styles[0].fontStyle = SWT.BOLD;
        
        event.styles = styles;
      }
    });
    
    shell.setSize(300, 100);
    shell.open();
    
    while (!shell.isDisposed()) {
      if (!display.readAndDispatch()) {
        display.sleep();
      }
    }
    
    font.dispose();
    display.dispose();
  }
}
In the snippet below, there is a StyledText with a line style listener.
In the line style listener, a bold font style is set, changing the width of the rendered text.
However, this does not happen.",org.eclipse.swt.custom.StyledText
FILE,swt-3.1,87997,2005-03-14T19:21:00.000-06:00,TableEditor.dispose( ) causes NPE if linked Table is being disposed,"TableEdtior.dispose( )  
  
   

import org.eclipse.swt.custom.TableEditor;
import org.eclipse.swt.events.*;
import org.eclipse.swt.widgets.*;

public class Test
{
    public static void main( String[ ] args )
    {
        Shell shell = new Shell( );
        Table table = new Table( shell, 0 );
        new TableColumn( table, 0 );
        TableItem item = new TableItem( table, 0 );
        final TableEditor editor = new TableEditor( table );
        final Text text = new Text( table, 0 );
        editor.setEditor( text, item, 0 );
        item.addDisposeListener( new DisposeListener( ) {
            public void widgetDisposed( DisposeEvent e )
            {
                text.dispose( );
                editor.dispose( ); // Triggers a NPE
            }
        } );
        shell.dispose( );
    }
}
If the table is in the process of being
disposed, the columns will have already been disposed and this will result in a
NPE.
Specifically this prevents one from adding a dispose listener on the Table
or a TableItem and trying to dispose of the associated editor, as in the code
below.
Further if the dispose listener is set on the parent of the Table, a
""Widget is disposed"" exception will be thrown instead of the NPE.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,88829,2005-03-22T20:41:00.000-06:00,Table.setColumnOrder() may not fire enough Move events,"Table.setColumnOrder(new int[] {4,1,2,3,0});
- start with five columns, all different widths
- do Table.setColumnOrder(new int[] {4,1,2,3,0});
- SWT.Move events are fired for columns 0 and 4 because they swapped positions",org.eclipse.swt.widgets.Table
FILE,swt-3.1,90258,2005-04-05T04:56:00.000-05:00,Table item not updated when item count == 1,"clearAll() 
 table.setItemCount(1);
table.clearAll();

 
 private void handleSetData(Event event) {

	TableItem item= (TableItem) event.item;
	int index= fProposalTable.indexOf(item);
	
	ICompletionProposal current= fFilteredProposals[index];
	
	item.setText(current.getDisplayString());
	item.setImage(current.getImage());
	item.setData(current);
}
Everything works fine, except for the case
I set the item count to 1, in which case I do not receive an SWT.SetData notification.
One funny thing is that in the variable view, the debugger displays the updated contents of table.items[0] after calling clearAll(), but I have verified that the data is never ever set.
The display fails to update.
My code looks like this:",org.eclipse.swt.widgets.Table
FILE,swt-3.1,93724,2005-05-04T17:35:00.000-05:00,Drag-and-drop creates signal names every time,"byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);	
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
Here is an example of some code in DragSource.java for GTK+:
Rather than converting the names for the signals every time, these signal names should be defined in OS.java so that they can be only created once.","org.eclipse.swt.dnd.DropTarget
org.eclipse.swt.dnd.DragSource"
FILE,swt-3.1,97651,2005-05-31T14:43:00.000-05:00,tree insert mark cheese,"Tree.redraw() 
 public static void main(String[] args) {
	final Display display = new Display();
	final Shell shell = new Shell(display);
	shell.setBounds(10, 10, 300, 300);
	final Tree tree = new Tree(shell, SWT.NONE);
	tree.setBounds(10, 10, 200, 200);
	new TreeItem(tree, SWT.NONE).setText(""pre-root"");
	TreeItem root1 = new TreeItem(tree, SWT.NONE);
	root1.setText(""root"");
	TreeItem child = new TreeItem(root1, SWT.NONE);
	child.setText(""child"");
	Button button = new Button(shell, SWT.PUSH);
	button.setBounds(230,10,30,30);
	button.addSelectionListener(new SelectionAdapter() {
		public void widgetSelected(SelectionEvent e) {
			tree.redraw();
		}
	});
	root1.setExpanded(true);
	tree.setInsertMark(root1, false);
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose();
}
- run the snippet below
- the insert line is set to be under the ""root"" item
- collapse the root item
-> problem 1: this makes most of the insert line go away, except for its pointy ends.
- press the button to the right of the Table: this does a Tree.redraw(), and note that the insert line reappears, so I guess it never really meant to go away
-> problem 2: now expand the root item again and its insert mark gets copied to below the child item in addition to its initial location.
This is cheese, as can be seen by damaging part of this line with another window","org.eclipse.swt.dnd.TreeDragUnderEffect
org.eclipse.swt.widgets.Tree"
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,Adding an integer and getting it as a long causes an exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
Try this in a BeanShell:
The problem is that when an object in a property is not a Long, the only attempt of PropertyConverter.toLong() is that of treating it as a string.","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
FILE,CONFIGURATION,CONFIGURATION-241,2006-12-02T00:03:48.000-06:00,clearProperty() does not generate events,"clearProperty() 
 ConfigurationFactory configurationFactory = new ConfigurationFactory();
   
 configurationFactory.setConfigurationURL(configFileURL);
Configuration configuration = ConfigurationFactory.getConfiguration();
configuration.addConfigurationListener(new ConfigurationListener() {
    public void configurationChanged(ConfigurationEvent e) 
{
        System.out.println(e.getPropertyName() + "": "" + e.getPropertyValue());
    }
});
System.out.println(configuration.getProperty(""name.first"")); // prints ""Mike""
 configuration.claerProperty(""name.first"")  ; // no output whatsoever
System.out.println(configuration.getProperty(""name.first"")); // prints ""null""
I am loading configuration information from multiple sources and have registered a listener with the resulting configuration object.
Unfortunately the listener does not receive ""clear property"" events.
I've confirmed that it can properly receive other events (like ""set property""), and that calls to ""clearProperty()"" do actually clear the property, so I believe this may be a bug in commons-configuration.
Below is a watered down version of what I am doing (note, my configuration file simply pulls in a property file containing this property: name.first=Mike):","org.apache.commons.configuration.TestCompositeConfiguration
org.apache.commons.configuration.CompositeConfiguration"
FILE,CONFIGURATION,CONFIGURATION-259,2007-03-28T08:47:56.000-05:00,ConfigurationFactory Merge is broken,"URL configURL = getClass().getResource(configFile);
ConfigurationFactory factory = new ConfigurationFactory();
factory.setConfigurationURL(configURL);
myConfig = factory.getConfiguration();
 
 
 DefaultConfigurationBuilder builder = new DefaultConfigurationBuilder();
            builder.setURL(configURL);
            myConfig = builder.getConfiguration();
I am trying to merge two Configuration using the ConfigurationFactory and the additional tag.
In particular, after creating a particular subset from a loaded configuration, the subset is empty.
So when initializing the configuration as follows, I get the following error:",org.apache.commons.configuration.ConfigurationFactory
FILE,CONFIGURATION,CONFIGURATION-332,2008-07-04T15:54:10.000-05:00,PropertiesConfiguration.save() doesn't persist properties added through a DataConfiguration,"public void testSaveWithDataConfiguration() throws ConfigurationException
{
    File file = new File(""target/testsave.properties"");
    if (file.exists()) {
        assertTrue(file.delete());
    }

    PropertiesConfiguration config = new PropertiesConfiguration(file);

    DataConfiguration dataConfig = new DataConfiguration(config);

    dataConfig.setProperty(""foo"", ""bar"");
    assertEquals(""bar"", config.getProperty(""foo""));
    config.save();

    // reload the file
    PropertiesConfiguration config2 = new PropertiesConfiguration(file);
    assertFalse(""empty configuration"", config2.isEmpty());
}
The properties added through a DataConfiguration aren't persisted when the configuration is saved, but they can be queried normally.
The following test fails on the last assertion :","org.apache.commons.configuration.TestPropertiesConfiguration
org.apache.commons.configuration.DataConfiguration"
FILE,CONFIGURATION,CONFIGURATION-347,2008-11-05T21:06:22.000-06:00,Iterating over the keys of a file-based configuration can cause a ConcurrentModificationException,"getKeys()
Some implementations of FileConfiguration return an iterator in their getKeys() method that is directly connected to the underlying data store.
When now a reload is performed (which can happen at any time) the data store is modified, and the iterator becomes invalid.
But even if the code performing the iteration is the only instance that accesses the configuration, the exception can be thrown.","org.apache.commons.configuration.TestFileConfiguration
org.apache.commons.configuration.AbstractFileConfiguration"
FILE,CONFIGURATION,CONFIGURATION-408,2010-02-11T01:01:05.000-06:00,"When I save a URL as a property value, the forward slashes are getting escaped","public static void main(String[] args)
  {
    try
    {

      PropertiesConfiguration config = new PropertiesConfiguration();     

      File newProps = new File(""foo.properties"");

      config.setProperty(""foo"", ""http://www.google.com/"");     

      config.save(newProps);

      

    }
    catch (Exception e){}
  }
When I save a URL as a property value, the forward slashes are getting escaped.
Example Code :",org.apache.commons.configuration.TestPropertiesConfiguration
FILE,CONFIGURATION,CONFIGURATION-481,2012-02-26T20:27:46.000-06:00,Variable interpolation across files broken in 1.7 & 1.8,"{myvar}  
 
 
 
 combinedConfig.getConfiguration(""test"")  configurationAt(""products/product[@name='abc']"", true)  getString(""desc"")

  {myvar}
With Commons Configuration 1.6, I was able to declare a variable in a properties file, and then reference it in a XML file using the ${myvar} syntax.
For example:
When I try to retrieve the value, like so:
I get ""${myvar}-product"" instead of ""abc-product"".","org.apache.commons.configuration.DefaultConfigurationBuilder
org.apache.commons.configuration.interpol.ConfigurationInterpolator
org.apache.commons.configuration.TestDefaultConfigurationBuilder"
FILE,CONFIGURATION,CONFIGURATION-627,2016-05-04T22:54:12.000-05:00,BeanHelper exception on XMLConfiguration builder.getConfiguration(),"builder =

        new FileBasedConfigurationBuilder<XMLConfiguration>(

                XMLConfiguration.class)

                        .configure(params.xml()

                                .setFileName(

                                        propsFile.getCanonicalPath())

                                .setValidating(false));



config = builder.getConfiguration();



   
 private static boolean isPropertyWriteable(Object bean, String propName)    
  
   org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)
Creating an XMLConfiguration from a file with a builder:
Causes a non-halting exception originating in org.apache.commons.configuration2.beanutils.BeanHelper, method private static boolean isPropertyWriteable(Object bean, String propName) with parameters XMLConfiguration, ""validating"".
The exception:",org.apache.commons.configuration2.builder.TestPropertiesBuilderParametersImpl
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,Using materialized blobs with Postgresql causes error,"@Lob
I have entity with byte[] property annotated as @Lob and lazy fetch type, when table is createad the created column is of type oid, but when the column is read in application, the Hibernate reads the OID value instead of bytes under given oid.
If i remember well, auto-creating table with Hibernate creates oid column.","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
CLASS,hibernate-3.5.0b2,HHH-5042,2010-03-26T05:06:09.000-05:00,TableGenerator does not increment hibernate_sequences.next_hi_value anymore after having exhausted the current lo-range,"class MultipleHiLoPerTableGenerator 
 IntegralDataTypeHolder value;
 
 int lo;

 
  
  
 IntegralDataTypeHolder hiVal = (IntegralDataTypeHolder) doWorkInNewTransaction( session );

   
  
 varchar(255) 
     varchar(255)
The problem in the new code is that only value get's incremented whilst variable lo is still used to check when a new hiVal must be obtained.
as lo is never incremented, MultipleHiLoPerTableGenerator continues to deliver numbers without ever update hibernate_sequences.
next_hi_value on the database (only one unique update is propagates at the first insert)
This lead to duplicate keys as soon another session from another sessionfactory tries to insert new objects on the concerning table.
Please see attached testcase.
as the testcase uses 2 sessionfactories hibernate.hbm2ddl.auto=create cannot be used!!
Schema has to be exported separately and the testcase must run without hbm2ddl.auto property!
Here the schema for HSQLDB:","org.hibernate.id.SequenceHiLoGenerator
org.hibernate.id.enhanced.OptimizerFactory
org.hibernate.id.SequenceGenerator
org.hibernate.id.MultipleHiLoPerTableGenerator"
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,ORderBy with @ElementJoinColumn and EmbeddedId uses wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
The problem is that the order by in the generated SQL is for columns mapped in the transaction entity NOT the TransacionId as expected.
So the Account class has the following fragment....
However the generated SQL is doing order by on columns mapped in Transaction:","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-1784,2010-09-08T08:31:29.000-05:00,Map value updates not flushed,"@Embeddable
public class LocalizedString {


    private String language;
    private String string;


    // getters and setters omitted
}


 


 @Entity
public class MultilingualString {


    @Id
    private long id;


    @ElementCollection(fetch=FetchType.EAGER)
    private Map<String, LocalizedString> map = new HashMap<String, LocalizedString>();
}



 
   ;
    em.getTransaction().begin();
    m.getMap().get(""en"").setString(""foo"");
     em.merge(m)
     em.getTransaction()  commit();
   
 
   ;
    em.getTransaction().begin();
     m.getMap()  put(""en"")  new LocalizedString(""en"", ""foo"") 
 em.merge(m)
     em.getTransaction()  commit();


 
 hashCode()   equals()   equal()
I have an entity with a map element collection where the map value is an Embeddable.
Given a persistent instance m of my entity, I update a member of a given map value and then merge the modified entity:
commit();
   
The problem is, the state change of the map does not get saved to the database.
With DEBUG logging on, I can see that the flush on commit does not trigger any SQL UPDATE.
After this change, I do see the expected UPDATE.","org.apache.openjpa.util.ProxyMaps:beforePut(ProxyMap, Object, Object)"
METHOD,openjpa-2.0.1,OPENJPA-526,2008-02-27T13:28:05.000-06:00,Insert text more than 4K bytes to Clob column causes SQLException: Exhausted Resultset,"public class Exam 
 @Lob 
 @Column(name = ""text"", nullable = false)  
 private String text;
 
 With nullable = false
Here's the persistence class:
public class Exam... {
    @Lob
    @Column(name = ""text"", nullable = false) ***** NOTE: set nullable = true will fix the problem but it leads to bug OPENJPA-525 *****
    private String text;
}","org.apache.openjpa.persistence.kernel.common.apps.Lobs:getId()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:getLob()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:Lobs(String, int)
org.apache.openjpa.persistence.kernel.common.apps.Lobs:setLob(String)
org.apache.openjpa.jdbc.sql.OracleDictionary:setNull(PreparedStatement, int, int, Column)"
METHOD,adempiere-3.1.0,1240,2008-05-16T03:03:55.000-05:00,Posting not balanced when is producing more than 1 produc,"Production Quantity= 2
The accounting is not balanced  when more that 1 product BOM is produced.
1. En Gardenworld , Production windows , create a row in the tab Production Header for  ""Production 2 Patio  set"".
2. in the tab Production Plan create a row for the Patio Furniture Set product, Production Quantity= 2
3. Then click on  ""Create/post Production"" button in the Production header tab, this create the production line.
4. the first line is necesary divide in two for give a serial each one.
then in the Patio Furniture Set product set in movement quantity 1 and give a serial  in the attribute set instance field, then create other row similar but the other serial
5. then click on  ""Create/post Production"" button in the Production header tab
6. click on ""Not Postet"" Button, then there the botton label is changed to ""Dont Balanced""",org.compiere.acct.Doc_Production:createFacts(MAcctSchema)
CLASS,pig-0.8.0,PIG-1188,2010-01-14T13:32:46.000-06:00,Padding nulls to the input tuple according to input schema,"{code}
  as (a0, a1);
dump a;
{code}
 
 {code}
 
 {code}
 
 {code}
 
 {code}

 
 {code}
 
 {code}
Currently, the number of fields in the input tuple is determined by the data.
Here is one example:
Pig script:
Input file:
Current result:
{code}
(1 2)
(1 2,3)
(1
{code}","test.org.apache.pig.test.TestMergeForEachOptimization
src.org.apache.pig.newplan.logical.rules.TypeCastInserter
test.org.apache.pig.test.TestNewPlanLogicalOptimizer
test.org.apache.pig.test.TestNewPlanFilterRule
test.org.apache.pig.test.TestNewPlanPushDownForeachFlatten
test.org.apache.pig.test.TestEvalPipeline2
test.org.apache.pig.test.TestMultiQueryCompiler
test.org.apache.pig.test.TestPartitionFilterPushDown
test.org.apache.pig.test.TestNewPlanFilterAboveForeach"
CLASS,pig-0.8.0,PIG-1277,2010-03-05T13:02:03.000-06:00,Pig should give error message when cogroup on tuple keys of different inner type,"UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

 
 {code}
 
  
 by (c0, c1);
dump e;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}

 
 {code}
  {(1,1)}  {(1,1)} 
 {code}

 
 {code}
  {(1,1)}  {} 
 {}  {(1,1)} 
 {code}
When we cogroup on a tuple, if the inner type of tuple does not match, we treat them as different keys.
Here is one example:
UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}
Pig script: 
{code}
a = load '1.
Real result:
{code}
((1 1),{(1,1)},{})
((1 1),{},{(1,1)})
{code}","src.org.apache.pig.impl.io.NullableBytesWritable
test.org.apache.pig.test.TestPackage
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator
src.org.apache.pig.backend.hadoop.HDataType
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestSecondarySort
src.org.apache.pig.newplan.logical.relational.LOUnion"
CLASS,pig-0.8.0,PIG-1771,2010-12-16T14:45:37.000-06:00,"New logical plan: Merge schema fail if LoadFunc.getSchema return different schema with ""Load...AS""","{code}
 
 BinStorage() 
         tuple()  ;
dump auxData;
{code}
The following script fail:
Error message:","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1776,2010-12-17T16:28:09.000-06:00,"changing statement corresponding to alias after explain , then doing dump gives incorrect result","{code}
 
  
 {code}
/* but dumping c after following steps gives incorrect results */","src.org.apache.pig.PigServer
src.org.apache.pig.newplan.logical.relational.LOLoad
test.org.apache.pig.test.TestUDFContext"
CLASS,pig-0.8.0,PIG-1785,2011-01-04T17:20:28.000-06:00,New logical plan: uid conflict in flattened fields,"{code}
 
 b0>b2;
dump c;
{code}

 
 {(1,2),(2,3)}
The following script produce wrong result:
{code}
a = load '1.
1 txt:
We get nothing.","src.org.apache.pig.newplan.logical.rules.ImplicitSplitInserter
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
src.org.apache.pig.newplan.optimizer.PlanOptimizer
src.org.apache.pig.newplan.optimizer.Rule"
CLASS,pig-0.8.0,PIG-1808,2011-01-17T08:50:48.000-06:00,Error message in 0.8 not much helpful as compared to 0.7,"null;
DUMP D;
The below script fails both in 0.7 and 0.8 since A requires a valid schema to be defined.
But the error message in 0.8 is not helpful.
Error message in 0.8
Error message in 0.7","test.org.apache.pig.test.TestPushUpFilter
src.org.apache.pig.newplan.logical.rules.PushUpFilter"
CLASS,pig-0.8.0,PIG-1812,2011-01-19T20:06:36.000-06:00,Problem with DID_NOT_FIND_LOAD_ONLY_MAP_PLAN,"{t:(id:chararray, wht:float)} 
    
 flatten(cat_bag.id)    
    
 {
        I = order M by ts;
        J = order B by ts;
        generate flatten(group) as (pkg:chararray, cat_id:chararray), J.ts as tsorig, I.ts as tsmap;
}
I have the following input files:
My script is listed below:
When running this script, I got a warning about ""Encountered Warning DID_NOT_FIND_LOAD_ONLY_MAP_PLAN 1 time(s)"" and pig error log as below:
But, when I removed the DISTINCT statement before COGROUP, i.e. ""B = distinct B;""  this script can run smoothly.
I have also tried other reducer side operations like ORDER, it seems that they will also trigger above error.","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LimitAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.KeyTypeDiscoveryVisitor
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.RearrangeAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1813,2011-01-20T10:25:01.000-06:00,Pig 0.8 throws ERROR 1075 while trying to refer a map in the result of  eval udf.Works with 0.7,"flatten(org.myudf.GETFIRST(value))  
 PigStorage()
The above script fails when run with Pig 0.8 but runs fine with Pig 0.7 or if pig.usenewlogicalplan=false.
The below is the exception thrown in 0.8 :","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-1831,2011-01-28T04:02:31.000-06:00,Indeterministic behavior in local mode due to static variable PigMapReduce.sJobConf,"PigStorage()
The below script when run in local mode gives me a different output.
For example consider the below script :
And input is 
abcd    label1  11      feature1
acbd    label2  22      feature2
adbc    label3  33      feature3
Here if I store relation B and D then everytime i get the result  :
acbd            3
abcd            3
adbc            3
But if i dont store relations B and D then I get an empty output.
Here again I have observed that this behaviour is random ie sometimes like 1out of 5 runs there will be output.","src.org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange
src.org.apache.pig.builtin.Distinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage
src.org.apache.pig.data.InternalSortedBag
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin
src.org.apache.pig.impl.builtin.DefaultIndexableLoader
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce.Map
src.org.apache.pig.impl.io.FileLocalizer
test.org.apache.pig.test.TestFinish
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner
src.org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager
src.org.apache.pig.data.InternalCachedBag
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestPruneColumn
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase
test.org.apache.pig.test.TestFRJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup
test.org.apache.pig.test.utils.FILTERFROMFILE
src.org.apache.pig.data.InternalDistinctBag"
CLASS,pig-0.8.0,PIG-1843,2011-02-04T20:42:39.000-06:00,NPE in schema generation,"{code}
   
   ;
{code}
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(getSchemaName(""parselong"", input), DataType.MAP));
    }
}
{code}
Hit NPE in following script:
{code}
a = load 'table_testBagDereferenceInMiddle2' as (a0:chararray);
b = foreach a generate MapGenerate(STRSPLIT(a0).
Error message:","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.EvalFunc"
CLASS,pig-0.8.0,PIG-1856,2011-02-15T17:26:16.000-06:00,Custom jar is not packaged with the new job created by LimitAdjuster,"{code}
 
  
 {code}
The script:
The script, however,  fails since the piggybank jar isn't shipped to the backend with the additional job created by the LimitAdjuster.","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1858,2011-02-17T02:27:48.000-06:00,UDF in nested plan results frontend exception,"{code}
 
 PigStorage()  
 {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        } 
   ;
{code}
The below is my script :
{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).
The below is the exception that I get :
When i trun off new logical plan the script executes successfully.",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-1866,2011-02-23T14:01:13.000-06:00,Dereference a bag within a tuple does not work,"{code}
     
 t.b1;
dump b;
{code}
The following script does not work (both in new and old logical plan):
Error from old logical plan:
Error from new logical plan:
If we change ""b = foreach a generate t.b1;"" to ""b = foreach a generate t.i;"", it works fine, only refer to a bag does not work.","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.8.0,PIG-1868,2011-02-24T00:42:05.000-06:00,New logical plan fails when I have complex data types from udf,"{code}
 
 {
 Tuples = order B1 by ts;
 generate Tuples;
} 
   { t: ( previous, current, next ) } 
 as id;
dump C3;
{code}

 
 {code}
 
 {code}

  on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The below is my script :
On C3 it fails with below message :
{code}
Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 45 Input: 0 Column: 1)
{code}
The below is the describe on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The script works if I turn off new logical plan or use Pig 0.7.","src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-1892,2011-03-10T02:44:12.000-06:00,Bug in new logical plan : No output generated even though there are valid records,"Maploader()
I have the below script which provides me no output even though there are valid records in relation B which is used for the left out join.
For the script to work I have to turn off the coloumn prune optimizer by -t ColumnMapKeyPrune or rearrange the script such that;
B0 = filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'c') );
C =  filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'p') );","test.org.apache.pig.test.TestPruneColumn.PigStorageWithTrace
src.org.apache.pig.newplan.logical.rules.MapKeysPruneHelper
test.org.apache.pig.test.TestPruneColumn"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,Pig report input size -1 for empty input file,"{code}
 
 by b0;
dump c;
{code}
In the following script:
If 1.txt is empty, Pig will report
Successfully read -1 records from: ""1.txt""","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1912,2011-03-16T16:11:46.000-05:00,non-deterministic output when a file is loaded multiple times,"while (( i < 10 ));  
  
 {results[*]}

 
  
  
  
 
 
  
  
 @operasolutions.com
(360)
I have a small demonstration script (actually, a directory with one main script and several other scripts that it calls) where the output (STOREd to a file) is not consistent between runs.
The problem appears to be that when a dataset X gets LOADed twice, with things other than LOADs occurring between the loads (like a FOREACH GENERATE), a FOREACH GENERATE that is later performed on X doesn't always choose the correct columns.
The correctness of the output was highly variable on my computer, for one of my co-workers it *almost* always failed, and for two other of my co-workers they didn't see any failures, so it's likely to be a race condition or something like that.
-- FILES FOR REPLICATING THE PROBLEM
-- correct_output.
Non-deterministic in the sense that the output of the script is not
the same between different times it is run on the same input; usually
the input is right, but sometimes it's wrong for no apparent reason.
The scripts load the file data.csv and write to the output
directory, but the file output/Y/part-m-00000 is sometimes different
between consecutive runs.","src.org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOLoad"
CLASS,pig-0.8.0,PIG-1927,2011-03-21T19:23:54.000-05:00,Dereference partial name failed,"{code}
 
 generate e0;
describe f;
{code}
The following script fail:","test.org.apache.pig.test.TestUnionOnSchema
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1963,2011-04-04T17:18:24.000-05:00,"in nested foreach, accumutive udf taking input from order-by does not get results in order","{code}
 
 explain d;
dump d;
{code}
This happens only when secondary sort is not being used for the order-by.
For example -
{code}
a1 = load 'fruits.txt' as (f1:int,f2);
a2 = load 'fruits.txt' as (f1:int,f2);","test.org.apache.pig.test.TestAccumulator
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.AccumulatorOptimizer"
CLASS,pig-0.8.0,PIG-1979,2011-04-08T02:24:01.000-05:00,New logical plan failing with ERROR 2229: Couldn't find matching uid -1,"{code}
 
  
    
    
      
     PigStorage() 
  
  
  
   PigStorage() ;
{code}

   
  
    
 {code}

 import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;

public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}

  @Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().newTuple(1);
      tp2.set(0, (inputTuple.get(0).toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}

 {code}
The below is my script 
{code}
register myudf.jar;
c01 = LOAD 'input'  USING org.test.MyTableLoader('');
c02 = FILTER c01  BY result == 'OK'  AND formatted IS NOT NULL  AND formatted !
The script is failing in building the plan, while applying for logical optimization rule for AddForEach.
The problem is happening when I try to include doc_005::category in the projection for relation finalresult.
The script goes through fine if I disable AddForEach rule by -t AddForEach","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression"
CLASS,pig-0.8.0,PIG-1993,2011-04-12T19:47:41.000-05:00,PigStorageSchema throw NPE with ColumnPruning,"{code}
 
  
  
 GENERATE a1;
dump b;
{code}
The following script fail:
Error message:","contrib.piggybank.java.src.test.java.org.apache.pig.piggybank.test.TestPigStorageSchema
contrib.piggybank.java.src.main.java.org.apache.pig.piggybank.storage.PigStorageSchema"
CLASS,pig-0.8.0,PIG-313,2008-07-14T19:20:04.000-05:00,Error handling aggregate of a computation,"{code}
 
 {code}

 
 {quote}
   
   
   
   
   
    
    
    
    
 {quote}
Query which fails:
Error output:",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.","flatten(outlinks.target);
  flatten(outlinks.target);
---> Would expect both C and D to work, but only C works.
D gives the error shown below.
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,Schema reported from DESCRIBE and actual schema of inner bags are different.,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
The following script:
Prints for the DESCRIBE commands:
The reported schemas for urlContentsG and urlContentsF are wrong.
They are also against the section ""Schemas for Complex Data Types"" in http://wiki.apache.org/pig-data/attachments/FrontPage/attachments/plrm.htm#_Schemas.","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-946,2009-09-04T17:51:37.000-05:00,"Combiner optimizer does not optimize when limit follow group, foreach","10;
dump d;
The following script is combinable but is not optimized:
a = load '/user/pig/tests/data/singlefile/studenttab10k';
b = group a by $1;
c = foreach b generate group, AVG(a.$2);
d = limit c 10;
dump d;","test.org.apache.pig.test.TestCombiner
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.CombinerOptimizer"
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,HypergeometricDistribution.sample suffers from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-221,2008-08-29T13:31:56.000-05:00,Result of multiplying and equals for complex numbers is wrong,"class Complex  
 {code}
 import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));

                System.out.println(""res=comp: ""+res.equals(comp));
        }
}
 {code}
little java program + output that shows the bug:
res:  -0 - 1i
comp: 0 - 1i
res=comp: false",org.apache.commons.math.complex.Complex:equals(Object)
METHOD,math,MATH-280,2009-07-06T21:26:57.000-05:00,bug in inverseCumulativeProbability() for Normal Distribution,"public class NormalDistributionImpl extends AbstractContinuousDistribution 


  
 public abstract class AbstractContinuousDistribution


 
 DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);

 
 normal.inverseCumulativeProbability(0.977249868051820);
This code:
gives the exception below.
normal.inverseCumulativeProbability(0.977249868051820); works fine
0 9986501019683698 (should return 3.0000...)
0 9999683287581673 (should return 4.0000...)","org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtils:bracket(UnivariateRealFunction, double, double, double, int)"
METHOD,math,MATH-305,2009-10-22T06:35:08.000-05:00,NPE in  KMeansPlusPlusClusterer unittest,"package org.fao.fisheries.chronicles.calcuation.cluster;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.Arrays;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;

public class ClusterAnalysisTest {


	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());

	}

}
When running this unittest, I am facing this NPE:
This is the unittest:","org.apache.commons.math.util.MathUtils:distance(int[], int[])"
METHOD,math,MATH-318,2009-11-06T15:09:36.000-06:00,wrong result in eigen decomposition,"{code}
     public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }
 {code}
Some results computed by EigenDecompositionImpl are wrong.
The following case computed by Fortran Lapack fails with version 2.0
{code}
    public void testMathpbx02() {","org.apache.commons.math.linear.EigenDecompositionImpl:flipIfWarranted(int, int)"
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,ODE integrator goes past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-369,2010-05-03T15:48:27.000-05:00,"BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException","new BisectionSolver()  solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);
invoke:
NullPointerException will be thrown.","org.apache.commons.math.analysis.solvers.BisectionSolver:solve(UnivariateRealFunction, double, double, double)"
METHOD,math,MATH-482,2011-01-17T19:52:10.000-06:00,"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f","FastMath.max(50.0f, -50.0f)  
 testMinMaxFloat()
FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.
The bug was not detected by the test case ""testMinMaxFloat()"" because that has a bug too - it tests doubles, not floats.","org.apache.commons.math.util.FastMath:max(float, float)"
METHOD,math,MATH-567,2011-05-05T17:49:00.000-05:00,"class Dfp toDouble method return -inf whan Dfp value is 0 ""zero""","toDouble()  
 toDouble()  
 import org.apache.commons.math.dfp.DfpField;


 
 
 
 public static void main(String[] args)  
 DfpField field = new DfpField(100);
		    getZero()   field.getZero()  toDouble() 
   newDfp(0.0)  
 field.newDfp(0.0)  toDouble() 
 toDouble()
If the Dfp's value is 0 ""zero"", the toDouble() method returns a  negative infini.
A simple test case is :","org.apache.commons.math.dfp.Dfp:Dfp(DfpField, double)
org.apache.commons.math.dfp.Dfp:toDouble()"
METHOD,math,MATH-60,2006-05-14T04:20:21.000-05:00,"[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result","Fraction parse(String source, 
ParsePostion pos)  class ProperFractionFormat  
 ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);

//Test 1 : fail 
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

 
 parse(String, ParsePosition)
Please see the following code segment for more details:
Note: Similarly, when I passed in the following inputs:
input 2: (source = 1 2 / -3, pos = 0)
input 3: ( source =  -1 -2 / 3, pos = 0)
Function ""Fraction parse(String, ParsePosition)"" returned Fraction 1/3 (means the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs above.
I think the function does not handle parsing the numberator/ denominator properly incase input string provide invalid numerator/denominator.","org.apache.commons.math.fraction.ProperFractionFormat:parse(String, ParsePosition)"
METHOD,math,MATH-631,2011-07-23T23:48:27.000-05:00,"""RegulaFalsiSolver"" failure","{code}
 @Test
public void testBug() {
    final UnivariateRealFunction f = new UnivariateRealFunction() {
            @Override
            public double value(double x) {
                return Math.exp(x) - Math.pow(Math.PI, 3.0);
            }
        };

    UnivariateRealSolver solver = new RegulaFalsiSolver();
    double root = solver.solve(100, f, 1, 10);
}
 {code}
 
 {noformat}
 
 {noformat}
The following unit test:
fails with
{noformat}
illegal state: maximal count (100) exceeded: evaluations
{noformat}
Using ""PegasusSolver"", the answer is found after 17 evaluations.",org.apache.commons.math.analysis.solvers.BaseSecantSolver:doSolve()
METHOD,math,MATH-645,2011-08-13T16:18:48.000-05:00,MathRuntimeException with simple ebeMultiply on OpenMapRealVector,"{code:java}
 import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}
 {code}
 
 {noformat}
  
 {noformat}
The following piece of code
{code} raises an exception","org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(RealVector)
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(RealVector)"
METHOD,math,MATH-691,2011-10-16T17:18:34.000-05:00,Statistics.setVarianceImpl makes getStandardDeviation produce NaN,"new Variance(true/false)    
 {code:java}
 int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use ""population variance""
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);
{code}

 
 {code:java}
   double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
{code}
Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN.
The code to reproduce it:",org.apache.commons.math.stat.descriptive.SummaryStatistics:addValue(double)
METHOD,math,MATH-713,2011-11-25T16:35:02.000-06:00,Negative value with restrictNonNegative,"SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);

 
 x = 1; y = -1;
A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:
SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);
Function
1 * x + 1 * y + 0
Constraints:
1 * x + 0 * y = 1
Result:
x = 1; y = -1;",org.apache.commons.math.optimization.linear.SimplexTableau:getSolution()
METHOD,math,MATH-836,2012-07-31T17:04:25.000-05:00,"Fraction(double, int) constructor_ strange behaviour","public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }
The Fraction constructor_ Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction.
When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:
1: the constructor_ returns a positive Fraction.
Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value
2: the constructor_ does not manage to reduce the Fraction properly.
Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.","org.apache.commons.math3.fraction.Fraction:Fraction(double, double, int, int)"
METHOD,math,MATH-929,2013-01-15T11:45:28.000-06:00,MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd,"{code}
 Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);
{code}
To reproduce:
{code}
Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).",org.apache.commons.math3.distribution.MultivariateNormalDistribution:density(double[])
METHOD,math,MATH-942,2013-03-09T15:05:04.000-06:00,DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type,"Array.newInstance(singletons.get(0).getClass(), sampleSize)   
 singleons.get(0) 
 {{DiscreteDistribution.sample()}}  
 {code}
 List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).sample(1);
{code}
An exception will be thrown if:
* {{singleons.get(0)}} is of type T1, an sub-class of T, and
* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.
To reproduce:
{code}
List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).",org.apache.commons.math3.distribution.DiscreteDistribution:sample(int)
METHOD,math,MATH-949,2013-03-15T18:11:56.000-05:00,LevenbergMarquardtOptimizer reports 0 iterations,"LevenbergMarquardtOptimizer.getIterations()     BaseOptimizer.incrementEvaluationsCount()

 
 {noformat}
     @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }

 {noformat}
The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0.
I've put a test case below.
Notice how the evaluations count is correctly incremented, but the iterations count is not.","org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer:doOptimize()
org.apache.commons.math3.optim.BaseOptimizer:BaseOptimizer(ConvergenceChecker<PAIR>)
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer:doOptimize()"
FILE,WFCORE,WFCORE-267,2014-11-19T19:47:31.000-06:00,CLI prints output twice if using cli client jar,"INFO: {




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}




{




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}
If you are using the CLI client jar, all output is printed twice.
This is because JBoss logging is not set up and by default CommandContextImpl is printing log messages to standard out.
The output will look something like this:",org.jboss.as.cli.CommandLineMain
FILE,WFCORE,WFCORE-495,2015-01-12T08:48:29.000-06:00,"WFLY won't startup due to ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""","file(standalone.xml)  file(standalone.xml)
WFLY won't startup due to ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""
If you firstly deploy a .
war application by CLI command, then deploy 2nd time same .
war application by copying it to /depolyment directory.
After server restart, it shows:
1. start wildfly and deploy a .
war application by CLI.
2. copy same .
war application to /deployment directory
3. restart wildfly to see the error message.
This happens because step 2 does a ""full-replace-deployment"" operation which does not remove content from standalone/data/content/aa/2d0425dd53572294d591b56efdee2680539eaf/content and deployment info from configuration file(standalone.xml).
Therefore, you will have xxx.war in standalone/data/content and configuration file(standalone.xml), also a xxx.war and xxx.war.deployed file inside /standalone/deployments.
A second time server restart will cause a duplicate resource error.",org.jboss.as.server.deployment.DeploymentFullReplaceHandler
FILE,WFCORE,WFCORE-604,2015-03-18T09:19:35.000-05:00,"After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory","{standalone|domaine} 
 {standalone|domaine}  
 {standalone|domaine} 
 {standalone|domaine} 
 {standalone|domaine}
- After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory
1. Fail to deploy application via jboss-cli
2. Find deployment info in JBOSS_HOME/{standalone|domaine}/data/content, but there are no standalone.xml in <deployments> tag.
3. Fix deployment and success to deploy.
4. Find ""new"" deployment info in JBOSS_HOME/{standalone|domaine}/data/content, and the old deployment info will be still there.
- The deployment information which created when deploy was failed remains in JBOSS_HOME/{standalone|domaine}/data/content.","org.jboss.as.host.controller.mgmt.MasterDomainControllerOperationHandlerImpl
org.jboss.as.server.controller.resources.ServerRootResourceDefinition
org.jboss.as.host.controller.ManagedServerOperationsFactory
org.jboss.as.host.controller.DomainModelControllerService
org.jboss.as.host.controller.RemoteDomainConnectionService
org.jboss.as.test.shared.ModelParserUtils
org.jboss.as.server.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentAddHandler
org.jboss.as.server.logging.ServerLogger
org.jboss.as.server.deployment.DeploymentRemoveHandler
org.jboss.as.domain.controller.resources.ServerGroupResourceDefinition
org.jboss.as.repository.LocalDeploymentFileRepository
org.jboss.as.domain.controller.operations.ApplyRemoteMasterDomainModelHandler
org.jboss.as.server.deployment.DeploymentReplaceHandler
org.jboss.as.domain.controller.resources.DomainRootDefinition
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentDefinition
org.jboss.as.repository.logging.DeploymentRepositoryLogger
org.jboss.as.domain.controller.operations.deployment.DeploymentFullReplaceHandler
org.jboss.as.core.model.test.LegacyKernelServicesImpl
org.jboss.as.subsystem.test.TestModelControllerService
org.jboss.as.host.controller.HostControllerService
org.jboss.as.domain.controller.resources.DomainDeploymentResourceDefinition
org.jboss.as.core.model.test.TestModelControllerService
org.jboss.as.server.mgmt.domain.RemoteFileRepositoryService
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentAdd
org.jboss.as.server.ApplicationServerService
org.jboss.as.repository.LocalFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeAddHandler
org.jboss.as.server.test.InterfaceManagementUnitTestCase
org.jboss.as.host.controller.model.host.HostResourceDefinition
org.jboss.as.server.deployment.DeploymentAddHandlerTestCase
org.jboss.as.repository.DeploymentFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentReplaceHandler
org.jboss.as.repository.ContentRepository
org.jboss.as.domain.controller.operations.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.DeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeResource
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler
org.jboss.as.server.deployment.DeploymentFullReplaceHandler"
FILE,WFCORE,WFCORE-626,2015-04-06T15:53:19.000-05:00,Global list-get operation can inadvertently create list elements,"clear(name=attribute)
  get(name=attribute, index=0)
  add(name=attribute, value=test)
  get(name=attribute, index=0)
Consider the following sequence of operations:
#2 will return <undefined> as expected.
However, it returns <undefined>.
This is because #2 will create the missing element at index 0 causing #3 to operate on index 1.","org.jboss.as.controller.operations.global.ListOperations
org.jboss.as.controller.operations.global.MapOperations"
FILE,WFCORE,WFCORE-442,2014-12-02T19:15:13.000-06:00,AbstractMultiTargetHandler-based handlers do not propagate failures to the top level failure-description,"{read-only=1} 
 {




                ""name"" => ""jboss.server.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""user.home"",




                ""path"" => ""/Users/hbraun"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.base.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
  
 {




                ""name"" => ""user.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.data.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/data"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.home.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.log.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/log"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.controller.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            }
An example is worth a thousand words:
One item in the set has a failure description but the overall response does not.",org.jboss.as.controller.operations.global.GlobalOperationHandlers
FILE,WFCORE,WFCORE-716,2015-05-27T10:21:36.000-05:00,Once server in reload-required state capabilities no longer checked at stage Model.,"attribute(name=security-realm)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }




 
 attribute(name=security-domain, value=MgMtDom)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }
Once a server is in the state reload-required capabilities and requirements are no longer checked e.g.: -
The following command is referencing a non-existent capability: -
When I execute :reload it will fail: -",org.jboss.as.controller.CapabilityReferenceRecorder
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,One profile can have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
One profile can have more ancestors with same submodules.
It leads to WFLYCTL0212: Duplicate resource [(""subsystem"" => ""subsystem_name"")] .
get fresh EAP","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-955,2015-08-27T14:34:07.000-05:00,Server is not responding after attempt to set parent of profile to non-existent profile,"add()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""java.lang.NullPointerException:null""




}




 
 add()
Server is not responding after attempt to set parent of profile to non-existent profile.
Server is not responding also after attempt to set parent of socket-binding-group to non-existent socket-binding-group.
Get fresh EAP
Actual results:
Get fresh EAP","org.jboss.as.controller.OperationContextImpl
org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.SocketCapabilityResolutionUnitTestCase
org.jboss.as.controller.capability.registry.IncludingResourceCapabilityScope
org.jboss.as.controller.AbstractCapabilityResolutionTestCase"
FILE,WFCORE,WFCORE-876,2015-08-12T07:52:43.000-05:00,Reload or Shutdown inside IF statement is performed before the if/else block batch is executed,"resource()
Executing a reload or shutdown inside an if/else block results in the reload or restart occurring before the other commands in the batch are executed.
This command will actually leave the server in a reload-required state.
The shutdown, or reload, will occur before the write-attribute is executed.","org.jboss.as.cli.handlers.trycatch.TryCatchFinallyControlFlow
org.jboss.as.test.integration.management.cli.ifelse.NonExistingPathComparisonTestCase
org.jboss.as.test.integration.management.cli.TryCatchFinallyTestCase
org.jboss.as.cli.handlers.ifelse.IfElseControlFlow
org.jboss.as.test.integration.management.cli.ifelse.BasicIfElseTestCase"
FILE,WFCORE,WFCORE-949,2015-09-03T05:08:35.000-05:00,Removing http-interface autocompletes but fails,"remove()
Executing /core-service=management/management-interface=http-interface:remove() autocompletes but does not succeed.","org.jboss.as.server.operations.NativeRemotingManagementRemoveHandler
org.jboss.as.host.controller.operations.HttpManagementRemoveHandler
org.jboss.as.jmx.RemotingConnectorResource
org.jboss.as.test.integration.domain.HTTPSManagementInterfaceTestCase
org.jboss.as.server.mgmt.NativeManagementResourceDefinition
org.jboss.as.subsystem.test.AdditionalInitialization
org.jboss.as.remoting.management.ManagementRemotingServices
org.jboss.as.server.operations.HttpManagementRemoveHandler
org.jboss.as.server.operations.NativeManagementRemoveHandler
org.wildfly.core.test.standalone.mgmt.HTTPSConnectionWithCLITestCase
org.wildfly.core.test.standalone.mgmt.HTTPSManagementInterfaceTestCase
org.jboss.as.remoting.logging.RemotingLogger
org.jboss.as.server.mgmt.HttpManagementResourceDefinition
org.jboss.as.host.controller.resources.NativeManagementResourceDefinition
org.jboss.as.host.controller.resources.HttpManagementResourceDefinition"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,Warnings about missing notification descriptions when an operation removes an extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
When I use migration operation the console log is filled with warning messages of type
If I do the sequence of operation
then I the log looks like","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,Inconsistent read-resource results with host scoped roles,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
Setting up host scoped roles as follows https://gist.github.com/heiko-braun/0dc810ed04db8739defd there are inconsistent results in the filtering.
When using a role which only selects the master there is no access-control response header showing the filtered resources, and the slave wrongly appears in the results:
When using a role that only selects the slave we get a proper access-control header","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-989,2015-09-19T03:11:07.000-05:00,Premature registration of reconnecting server,"{""server-group"" => {




        ""main-server-group"" => {""host"" => {""slave"" => {""main-three"" => ""WFLYHC0153: Channel closed""}
ServerInventoryImpl.reconnectServer is registering the ProxyController for the reconnecting server with the DomainModelControllerService.
The problem is that ProxyController is not yet in a state where it can forward requests to the server.
That won't happen until the server calls back with a DomainServerProtocol.SERVER_RECONNECT_REQUEST and the ServerToHostProtocolHandler.ServerReconnectRequestHandler handles it.
The effect is if a request for the server comes in during this window, it will fail.
Most significant logging is as follows:
The HC completes boot.
Then servers main-three and other-two register.
The HC sends a prepared response to a request.
This is the host rollout request to the slave from the DC with the response being the ops to invoke on the servers.
The HC reports sending a ""pre-prepare failed response"" to two requests.
These are the requests the DC has asked it to proxy to the servers.
The result of all this for the client is the following failure of a management op:
Failed operation:
Response:
The ""WFLYHC0153: Channel closed"" failure is what is produced when an attempt is made to invoke on a disconnected proxy controller.","org.jboss.as.host.controller.ServerInventoryImpl
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler"
FILE,WFCORE,WFCORE-1214,2015-12-11T23:17:45.000-06:00,Operation headers not propagated to domain servers when 'composite' op is used,"{blocking-timeout=5;rollback-on-runtime-failure=false}  
 {

[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3)     ""blocking-timeout"" => ""5"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""rollback-on-runtime-failure"" => ""false"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""caller-type"" => ""user"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""access-mechanism"" => ""NATIVE""

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3) }
When the user adds request headers to an op, they are not propagated to the servers during domain rollout if the 'composite' op is involved.
For example, if I add some stdout printing of what the headers are on the various processes and invoke this:
Then on a HC with two servers, this is logged:
Note the CLI 'deploy' is far from the only time the 'composite' op is used.
Among other places, the high level CLI 'batch' command in a domain involves use of 'composite'.","org.jboss.as.domain.controller.operations.coordination.DomainRolloutStepHandler
org.jboss.as.domain.controller.operations.coordination.OperationCoordinatorStepHandler"
FILE,WFCORE,WFCORE-1198,2015-12-09T09:30:00.000-06:00,CLI does not resolve multiple properties if one property is undefined,"{PROFILE-NAME}  {APP-VERSION}  {VAR}  add(auto-start=true, group=""${PROFILE-NAME}${APP-VERSION}-server-group"")
Multiple property substitution is working with EAP 6.4.3+, however, if a variable amongst the multiple variables is empty or has no value, then the subsequent property in the CLI command is not substituted.
For example :
and if I execute "".
Note APP-VERSION had no value, and so the subsequent SERVER-INSTANCE-NUMBER was not properly resolved","org.jboss.as.cli.parsing.test.PropertyReplacementTestCase
org.jboss.as.cli.parsing.ExpressionBaseState"
FILE,WFCORE,WFCORE-701,2015-05-19T15:06:17.000-05:00,Inconsistent domain server status reports between server-config resource and server resource,"attribute(name=status)




 {




    ""outcome"" => ""success"",




    ""result"" => ""FAILED""




}




  attribute(name=server-state)




 {




    ""outcome"" => ""success"",




    ""result"" => ""STOPPED""




}
When a managed server fails in some way, the server status reporting is inconsistent between the /host=<host>/server-config=<server> resources and the /host=<host>/server=<server> resource.
To reproduce, run domain.sh, find the pid of a server process, and kill -9 <thepid>.
Then with the CLI:",org.jboss.as.host.controller.ManagedServer
FILE,WFCORE,WFCORE-1572,2016-05-27T16:20:10.000-05:00,Suggestion list for argument value is not generated if the value contains a whitespace,"{rollout id=foo}
When an argument value contains a space (eg.
deploy /path/to/test-application.
war --all-server-groups --headers={rollout id=foo}) and if user hits tab after the whitespace, suggestions are generated based on the command, rather than the current argument's name.","org.jboss.as.cli.parsing.DefaultStateWithEndCharacter
org.jboss.as.cli.parsing.ParserUtil
org.jboss.as.cli.parsing.test.CommandTestCase"
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,Saved rollout-plan 'name' or 'id' attribute discrepancy,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
When using rollout plans for EAP deployment scenarios I can create my own named rollout-plan for ease of use.
I can then apply rollout command later on, referring with name of my own rollout plan that should be used.
When I create rollout-plan, I use command like:
see --name attribute given to name my rollout plan
When I then refer to it I use following command:
see id attribute given to rollout header operation","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,Better check of names of existing resources when adding '{local|remote-destination-outbound-socket-binding',"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
Lets have some /socket-binding-group=standard-sockets/socket-binding with particular name.
Then when I create some /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding or /socket-binding-group=standard-sockets/local-destination-outbound-socket-binding using same name as of already existing socket-binding resource, add operation is successful but when I perform server reload, it crashes as it is not able to parse configuration.
Start EAP and log to CLI create your own socket-binding resource and {remote|local}-destination-outbound-socket-binding resource with same names and perform reload
server crashes with following stacktrace in console log:","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1635,2016-07-05T07:04:51.000-05:00,Write attribute on a new deployment scanner fails in batch,"add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)




 
 
 add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)
Creating a new deployment-scanner and altering it's attribute fails if done in single batch.
Running the commands without batch or running batch on CLI embed-server works fine.",org.jboss.as.server.deployment.scanner.AbstractWriteAttributeHandler
FILE,WFCORE,WFCORE-1590,2016-06-12T14:18:43.000-05:00,Default parameter length validating ignores setMinSize(0),"static final SimpleAttributeDefinition REPLACEMENT = new SimpleAttributeDefinitionBuilder(ElytronDescriptionConstants.REPLACEMENT, ModelType.STRING, false)




        .setAllowExpression(true)




        .setMinSize(0)




        .setFlags(AttributeAccess.Flag.RESTART_RESOURCE_SERVICES)




        .build();






 
 add(pattern=""@ELYTRON.ORG"", replacement="""", replace-all=true)
With the following attribute definition: -
The following error is reported if an empty string is used as a parameter: -","org.jboss.as.controller.operations.validation.BytesValidator
org.jboss.as.controller.SimpleAttributeDefinitionUnitTestCase
org.jboss.as.controller.test.WriteAttributeOperationTestCase
org.jboss.as.controller.AbstractAttributeDefinitionBuilder
org.jboss.as.controller.AttributeDefinition"
FILE,WFCORE,WFCORE-1718,2016-08-16T09:49:11.000-05:00,Handlers within Audit Logger are not removed properly when Audit Logger is removed,"remove()
  remove()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0158: Operation handler failed: java.lang.NullPointerException"",




    ""rolled-back"" => true




}






   
 attribute(name=level,value=DEBUG)
If Audit Logger is removed, destination handlers (i.e. its child nodes) are not removed properly.
They are not present in the config file.
They seem to be not removed ""internally"" though.
1. It is not possible to remove referenced File/Syslog handlers.
If user tries to remove them the NullPointerException is given as a result.
Try following commands:
Their output is:
2. AuditLog continues to send auditable events to previously referenced File/Syslog handlers.
Create auditable event (e.g. /subsystem=logging/logger=com.arjuna:write-attribute(name=level,value=DEBUG))
See log in the file (WILDFLY_HOME/standalone/data/audit-log.log)
See log in the syslog (/var/log/messages)","org.jboss.as.domain.management.audit.AuditLogLoggerResourceDefinition
org.jboss.as.domain.management.audit.AccessAuditResourceDefinition
org.jboss.as.domain.management.audit.AuditLogHandlerReferenceResourceDefinition"
FILE,WFCORE,WFCORE-1765,2016-09-05T16:22:02.000-05:00,unclear NullPointerException if the deployment-scanner element is removed from the configuration,"{xml}
         {xml}
If the deployment scanner element is removed from the configuration of the standalone server a NullPointerException is logged which is unclear and difficult to find as the stack does not show any hint.
Log message:",org.jboss.as.controller.ParallelBootOperationStepHandler
FILE,WFCORE,WFCORE-1793,2016-09-14T08:08:21.000-05:00,add-content operation fails to overwrite existing content with overwrite=true set when passing content by file path,"{""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




  {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}
Upon overwriting content in managed exploded deployments on wildfly-core, the following errors are produced:","org.jboss.as.server.controller.resources.DeploymentAttributes
org.jboss.as.server.deployment.ExplodedDeploymentAddContentHandler"
FILE,WFCORE,WFCORE-1864,2016-10-13T09:12:31.000-05:00,Whitespaces are not removed from dependencies in module add command,"{{
...
    <dependencies>
        <module name=""org.a""/>
        <module name="" org.b ""/>
    </dependencies>
...
}}
Running module add --name=foo.bar --resources=foo.jar --dependencies=[org.a, org.b ] will result in following dependencies in module.xml","org.jboss.as.cli.handlers.module.ASModuleHandler
org.jboss.as.test.integration.management.cli.ModuleTestCase"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,Tab completion suggest writing attribute which has access type metric and is not writable,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
CLI tab completion suggests attributes that are not writable and their access-type is metric
Example
On attempt to write metric attribute, for example message-count, non writable error is printed","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,"Value of parameters ""restart-required"" for fixed-*port attributes does not match reality for socket-binding and *-destination-outbound-socket-binding in CLI","description(recursive=true)
If you tries to change such attributes you are informed that reload is necessary.","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
FILE,WFCORE,WFCORE-1959,2016-11-08T16:28:30.000-06:00,Deploying an empty managed exploded deployment to server group in domain fails,"{empty=true} 
 add()
Deploying an empty exploded deployment created on domain controller fails with the following:",org.jboss.as.domain.controller.operations.coordination.ServerOperationResolver
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,[resources] UnifiedTree should ensure file/folder exists,"getStat()
When the UnifiedTree finds a new file from the file system, it assumes that if the file is not an existing file, then it is a folder.
This is not always true, because for different reasons a file returned by java.io.File.list/listFiles may not actually exist (our CoreFileSystemLibrary#getStat() returns 0).
This problem appears to the user when executing refresh operations.
At the first moment, the file is found in the file system and assumed to be a folder, and a corresponding resource is created in the workspace.
At the second refresh, the folder corresponding to that resource is not found in the file system, and then it is removed from the workspace.
And so on.","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
