Dataset,System,Bug ID,Creation Date,Title,Description,Ground Truth
CLASS,tika-1.3,TIKA-1070,2013-01-31T05:43:33.000-06:00,StackOverflow error in org.apache.tika.sax.ToXMLContentHandler$ElementInfo.getPrefix(ToXMLContentHandler.java:58),"new ElementInfo(currentElement, namespaces);
The error occurs when parsing big ""XLS"" files and is caused by the ElementInfo stored in ""currentElement"".
start current elment start new element start time
use existing element as parent element
finish element traverse parents cause StackOverFlowError
For my understanding: something like:
currentElement = currentElement.parent;
in the endElement method solves the issue!
Best",tika-core.src.main.java.org.apache.tika.sax.ToXMLContentHandler
CLASS,tika-1.3,TIKA-1152,2013-07-23T08:45:11.000-05:00,Process loops infinitely on parsing of a CHM file,"{code}
 
    
     
    
    
    
    
    
    
    
    
 {code}
By parsing [the attachment CHM file|^eventcombmt.chm] (MS Microsoft Help Files), Java process stuck.
{code}
Thread[main,5,main]
org.apache.tika.parser.chm.lzx.ChmLzxBlock.extractContent(ChmLzxBlock.java:203)
org.apache.tika.parser.chm.lzx.ChmLzxBlock.<init>(ChmLzxBlock.java:77)
org.apache.tika.parser.chm.core.ChmExtractor.extractChmEntry(ChmExtractor.java:338)
org.apache.tika.parser.chm.CHMDocumentInformation.getContent(CHMDocumentInformation.java:72)
org.apache.tika.parser.chm.CHMDocumentInformation.getText(CHMDocumentInformation.java:141)
org.apache.tika.parser.chm.CHM2XHTML.process(CHM2XHTML.java:34)
org.apache.tika.parser.chm.ChmParser.parse(ChmParser.java:51)
org.apache.tika.parser.ParserDecorator.parse(ParserDecorator.java:91)
org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
org.apache.tika.parser.AbstractParser.parse(AbstractParser.java:53)
com.polyspot.document.converter.DocumentConverter.realizeConversion(DocumentConverter.java:192)
...
{code}",tika-parsers.src.main.java.org.apache.tika.parser.chm.lzx.ChmLzxBlock
FILE,DATAMONGO,DATAMONGO-467,2012-06-24T08:58:49.000-05:00,"String @id field is not mapped to ObjectId when using QueryDSL "".id"" path","@Document 
 @Id String id;






 
 QUser.id.eq(""4f43b6a384aea4e77d403709"")
Using this entity (@Document) definition with a String as the declared ID:
User.class
...
@Id String id;
and the following query
QUser.id.eq(""4f43b6a384aea4e77d403709"")
not translate String 4f43b6a384aea4e77d403709 to ObjectId(""4f43b6a384aea4e77d403709"") look at mongoDb query do in mongo shell",org.springframework.data.mongodb.repository.support.SpringDataMongodbSerializerUnitTests
FILE,DATAMONGO,DATAMONGO-505,2012-08-14T03:07:56.000-05:00,Conversion of associations doesn't work for collection values,"class Entity {









  Long id;




  @DBRef




  Property property;




}









 class Property {




  Long id;




}









 interface EntityRepository extends Repository<Entity, Long> {









  Entity findByPropertyIn(Property... property);




}






  findByProperty()
Assume you have the following scenario:
class Entity {
Long id;
@DBRef
Property property;
}
class Property {
Long id;
}
interface EntityRepository extends Repository<Entity, Long> {
Entity findByPropertyIn(Property... property);
}
not translate given array into collection
treat value create DBRef","org.springframework.data.mongodb.repository.query.ConvertingParameterAccessor
org.springframework.data.mongodb.repository.query.ConvertingParameterAccessorUnitTests"
FILE,DATAMONGO,DATAMONGO-523,2012-09-01T03:39:51.000-05:00,@TypeAlias annotation not used with AbstractMongoConfiguration,"@TypeAlias      @Document  @TypeAlias
When using the AbstractMongoConfiguration without any further modifications regarding the converter (afterMappingMongoConverterCreation) the @TypeAlias annotation is not used when writing the _class property.
use SimpleTypeInformationMapper
The documentation suggests that you just annotate your @Document classes with the @TypeAlias annotations and everything should be fine.",org.springframework.data.mongodb.core.convert.MappingMongoConverterUnitTests
FILE,DATAMONGO,DATAMONGO-585,2012-12-01T08:28:43.000-06:00,Exception during authentication in multithreaded access,"class which implements Runnable.  
Those
Bug & further details are here:
http://forum.springsource.org/showthread.php?132878-can-t-authenticate-twice-on-same-database
Specifically what can be used for a test case is use a ThreadPoolExecutor, set a minpool and maxpool value of 5, add 25 objects to be executed.
Those objects are a class which implements Runnable.
Those objects each then perform a bunch of inserts into mongoDB using a DocumentDao which calls mongoOperations.insert.
thanks",org.springframework.data.mongodb.core.MongoDbUtils
FILE,DATAMONGO,DATAMONGO-629,2013-03-22T04:08:25.000-05:00,Different results when using count and find with the same criteria with 'id' field,"Query q = query 
    
  
 
 
 {




		""id"" : /zzz/




	} 
 
 
 
 
 
  
  
 
 
 {




		""count"" : ""test"",




		""query"" : {




			""_id"" : /zzz/




		}




	 
 
 
 
 
 
     find()     count()
Assume we have following query:
Query q = query(where('id').
regex('zzz'))
{ ""ts"" : ISODate(""2013-03-22T10:00:51.685Z""),
""op"" : ""query"",
""ns"" : ""test.test"",
""query"" : {
""id"" : /zzz/
},
""nscanned"" : 1,
""responseLength"" : 20,
""millis"" : 0,
""client"" : ""127.0.0.1"",
""user"" : """"
}
use in count )
{
""ts"" : ISODate(""2013-03-22T10:00:36.299Z""),
""op"" : ""command"",
""ns"" : ""test.
$cmd"",
""command"" : {
""count"" : ""test"",
""query"" : {
""_id"" : /zzz/
}
},
""ntoreturn"" : 1,
""responseLength"" : 48,
""millis"" : 0,
""client"" : ""127.0.0.1"",
""user"" : """"
}
have records with field id retrieve from db
give bad results use _ id field
In org.springframework.data.mongodb.core.convert.QueryMapper the method determineKey, for some reason treats id and _id as the same field.
not use QueryMapper not use count()","org.springframework.data.mongodb.core.mapping.MongoMappingContext
org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-571,2012-11-09T08:00:10.000-06:00,Spring Data for MongoDb doesn't save null values when @Version is added to domain class,"Scenario 
 CrudRepository.findOne()  
 @Version 
 CrudRepository.save()  
 @Version
Scenario:
1. Domain class is loaded from mongodb using CrudRepository.findOne() method.
2. The loaded instances any non id nor @Version annotated field is set to null.
3. The loaded instance is saved to same mongodb using CrudRepository.save() method.
4. The field that has been set to null doesnt write to database, its unchanged.
Important: The problem doesnt occur when @Version annotation is not used in the domain class definition.","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.query.Update"
FILE,DATAMONGO,DATAMONGO-392,2012-02-07T04:28:15.000-06:00,Updating an object does not write type information for objects to be updated,"MappingMongoConverter.writeInternal(...)   addCustomTypeIfNecessary(...)     convertToMongoType(...)   removeTypeInfoRecursively(...)
I'm using quite complex domain model, that consist of instantiable domain classes as well as of abstract ones.
use 1.0.0
read from database store m5 version with object store type information with object
That worked perfectly for me till my upgrade to 1.0.0.
break application break RELEASE version save objects without type information read back to java model
call addCustomTypeIfNecessary(...) in turn call MappingMongoConverter.writeInternal(...) method in turn put type information into DBObject put addCustomTypeIfNecessary(...) into DBObject
call removeTypeInfoRecursively(...) during execution save under _ class key
I had to comment out this call in order to
The first point is that there is a contradiction: why to save type information to DBObject if it is later removed by other method?","org.springframework.data.mongodb.core.MongoTemplateTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-717,2013-07-10T11:13:46.000-05:00,Application context is not properly distributed to persistent entities,"@Override




	protected <T> BasicMongoPersistentEntity<T> createPersistentEntity(TypeInformation<T> typeInformation) {









		BasicMongoPersistentEntity<T> entity = new BasicMongoPersistentEntity<T>(typeInformation);









		if (context != null) {




			entity.setApplicationContext(context);




		}









		return entity;




	}









	




	 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




	}






 
  
 @Override




	public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {









		this.context = applicationContext;




		super.setApplicationContext(applicationContext);




                // Send the application context to ALL the PersistentEntities, not just ones created after this point




               for (BasicMongoPersistentEntity entity : getPersistentEntities()) {




                   entity.setApplicationContext(applicationContext);




               }




	}






      testMultiTenantSave()  
   initialize()    
 
 @Bean




	public MongoMappingContext mongoMappingContext() throws ClassNotFoundException {









		MongoMappingContext mappingContext = new MongoMappingContext();




		mappingContext.setInitialEntitySet(getInitialEntitySet());




		mappingContext.setSimpleTypeHolder(customConversions().getSimpleTypeHolder());




		mappingContext.initialize(); // <----









		return mappingContext;




	}
not distribute application context set to persistent entities
Current code:
MongoMappingContext.java
@Override
protected <T> BasicMongoPersistentEntity<T> createPersistentEntity(TypeInformation<T> typeInformation) {
BasicMongoPersistentEntity<T> entity = new BasicMongoPersistentEntity<T>(typeInformation);
if (context !
= null) {
entity.setApplicationContext(context);
}
return entity;
}
@Override
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
this.context = applicationContext;
super.setApplicationContext(applicationContext);
}
Possible fix:
MongoMappingContext.java
@Override
public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
this.context = applicationContext;
super.setApplicationContext(applicationContext);
// Send the application context to ALL the PersistentEntities, not just ones created after this point
for (BasicMongoPersistentEntity entity : getPersistentEntities()) {
entity.setApplicationContext(applicationContext);
}
}
See referenced URL PoC, no_indexes branch, test: MultiTenantTest::testMultiTenantSave() for example.
UPDATE:
AbstractMongoConfiguration is calling initialize() on MongoMappingContext before it returns the object.
AbstractMongoConfiguration
@Bean
public MongoMappingContext mongoMappingContext() throws ClassNotFoundException {
MongoMappingContext mappingContext = new MongoMappingContext();
mappingContext.setInitialEntitySet(getInitialEntitySet());
mappingContext.setSimpleTypeHolder(customConversions().
getSimpleTypeHolder());
mappingContext.initialize(); // <----
return mappingContext;
}",org.springframework.data.mongodb.config.AbstractMongoConfigurationUnitTests
FILE,DATAMONGO,DATAMONGO-721,2013-07-11T11:36:06.000-05:00,Polymorphic attribute type not persisted on update operations,"@Document
public class ParentClass {
   private List<ChildClass> list;
}
    @Document   
        
  
 mongoTemplate.updateFirst(Query.query(criteria), 
  new Update().push(""list"", child));
We found a problem with Spring Data for Mongo DB.
Here is our situation: we have an entity which have an attribute which is a list of another kind of entity, like the code below.
@Document public class ParentClass { private List<ChildClass> list;
} the ChildClass is annotated with @Document too, but we want to store it's content as an embed document of ParentClass.
When using MongoTemplate class with code such as below, the _class attribute is not inserted on the embedded document, so, if one of the items of the list attribute is a subclass of ChildClass, and ChildClass is an abstract class, we begin to face instantiation problems.
Here is one example of usage of MongoTemplate in which we found a problem.
mongoTemplate.updateFirst(Query.query(criteria), new Update().
push(""list"", child));
not add _ class attribute to embedded document",org.springframework.data.mongodb.core.convert.QueryMapper
FILE,DATAMONGO,DATAMONGO-602,2013-01-30T02:22:53.000-06:00,Querying with $in operator on the id field of type BigInteger returns zero results,"List<BigInteger> profileIds = findProfileIds();




Predicate predicate = QProfileDocument.profileDocument.id.in(profileIds);




Iterable<ProfileDocument> profiles = profileRepository.findAll(predicate);
There is a problem when trying to query for documents with id field is in a given list of BigInteger values.
Id field is mapped as a BigInteger.
For example, these lines will work only if given id list contains only one item.
List<BigInteger> profileIds = findProfileIds();
Predicate predicate = QProfileDocument.profileDocument.id.in(profileIds);
Iterable<ProfileDocument> profiles = profileRepository.findAll(predicate);
The underlying MongodbQuery is different if there is only one item in profileIds.
{ ""_id"" : { ""$in"" : [ ""25069473312490162649510603609"" , ""25045916045544535958655878835""]}}
{ ""_id"" : { ""$oid"" : ""5100fb776c67e7e092be6b59""}}
return results in first case return results in first case work Iterable with item work Iterable in second return Iterable with item return Iterable in second
As you can see, the problem is with the representation of the BigInteger.
using decimal format will not work with MongoDB.",org.springframework.data.mongodb.core.MongoTemplateTests
FILE,DATAMONGO,DATAMONGO-805,2013-12-02T06:34:36.000-06:00,Excluding DBRef field in a query causes a MappingException,"Query query = new Query(Criteria.where(""parentField"").is(""test""));
        query.fields().exclude(""children"");
        ParentClass parentClass = mongoOperations.findOne(query, ParentClass.class);
Excluding a field in a query where the field is a DBRef as below throws a MappingException.
Query query = new Query(Criteria.where(""parentField"").
is(""test""));
query.fields().
exclude(""children"");
ParentClass parentClass = mongoOperations.findOne(query, ParentClass.class);
org.springframework.data.mapping.model.MappingException: No mapping metadata found for class java.lang.Integer
at org.springframework.data.mongodb.core.convert.MappingMongoConverter.createDBRef(MappingMongoConverter.java:729)
at com.digitalshadows.collation.persistence.impl.CollectionNameProvidedMongoConverter.createDBRef(CollectionNameProvidedMongoConverter.java:28)
at org.springframework.data.mongodb.core.convert.MappingMongoConverter.toDBRef(MappingMongoConverter.java:288)
at org.springframework.data.mongodb.core.convert.QueryMapper.convertAssociation(QueryMapper.java:273)
at org.springframework.data.mongodb.core.convert.QueryMapper.getMappedValue(QueryMapper.java:204)
at org.springframework.data.mongodb.core.convert.QueryMapper.getMappedObject(QueryMapper.java:113)
at org.springframework.data.mongodb.core.MongoTemplate.doFindOne(MongoTemplate.java:1439)
at org.springframework.data.mongodb.core.MongoTemplate.findOne(MongoTemplate.java:489)
at org.springframework.data.mongodb.core.MongoTemplate.findOne(MongoTemplate.java:484)
at ExcludeDBRefFieldTest.testExcludeChildren(ExcludeDBRefFieldTest.java:28)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:74)
at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:83)
at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:72)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:231)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:88)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:174)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
I've attached a simple test case that throws the MappingException.
The workaround for this I can currently see is to use include for all the other fields instead.","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.mapping.MappingTests
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-897,2014-04-01T04:38:51.000-05:00,FindAndUpdate broken when using @DbRef and interface as target,"MongoTemplate.findAndModify(...)   @DbRef  @DbRef
NullPointerException is thrown when using MongoTemplate.findAndModify(...) with @DbRef and interface as @DbRef target.
See attached project for more details.
Probably regression issue since same test is passing with Spring Data Commons 1.6.3.
RELEASE and Spring Data MongoDB 1.3.3.
RELEASE.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.QueryMapper"
FILE,DATAMONGO,DATAMONGO-892,2014-03-28T09:08:03.000-05:00,<mongo:mapping-converter> can't be configured as nested bean definition,"parserContext.isNested()
The sample config which worked in the 1.1.1 version, but doesn't work now:
<beans:bean id=""messageStore"" class=""org.springframework.integration.mongodb.store.ConfigurableMongoDbMessageStore"">
<beans:constructor-arg ref=""mongoDbFactory""/>
<beans:constructor-arg>
<mongo:mapping-converter>
<mongo:custom-converters>
<mongo:converter>
<beans:bean class=""org.springframework.integration.mongodb.store.ConfigurableMongoDbMessageGroupStoreTests$MessageReadConverter""/>
</mongo:converter>
</mongo:custom-converters>
</mongo:mapping-converter>
</beans:constructor-arg>
<beans:constructor-arg value=""testConfigurableMongoDbMessageStore""/>
</beans:bean>
not check if returns null not check if parserContext.isNested() BeanDefinition","org.springframework.data.mongodb.config.MappingMongoConverterParser
org.springframework.data.mongodb.config.MappingMongoConverterParserIntegrationTests"
FILE,DATAMONGO,DATAMONGO-647,2013-04-09T17:29:02.000-05:00,"Using ""OrderBy"" in ""query by method name"" ignores the @Field annotation for field alias.","@Field(""sr"")
 
 List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
I created a method using the ""query by method name"" approach:
Inside my Answer object, I have a field called ""Score"" that is annotated with
@Field(""sr"")
int Score
List<Answer> findByQuestionIdOrderByScoreDesc(String questionId)
When the query is run, the database attempts to sort the results by ""score"" rather than my ""sr"" field name.",org.springframework.data.mongodb.core.convert.QueryMapperUnitTests
FILE,DATAMONGO,DATAMONGO-745,2013-09-03T02:15:08.000-05:00,@Query($in) and Pageable in result Page total = 0,"@Query(""{'snapshotId' : ?0 ,'defects.id':{ $in:?1}}"")
Page<Test> findBySnapshotIdAndDefects(ObjectId snapshotId, List<Integer> defectIds, Pageable pageable) 
 {5225a5ece4b0a01629fce9c6={ ""_id"" : 
{ ""$oid"" : ""5225a5ece4b0a01629fce9c6""}
Hi If I used MongoRepository and anotation Quary and Pageable in response I get true result but getTotalElements == 0
@Query(""{'snapshotId' : ?
0 ,'defects.id':{ $in:?
1}}"")
Page<Test> findBySnapshotIdAndDefects(ObjectId snapshotId, List<Integer> defectIds, Pageable pageable);
Base Struct
{5225a5ece4b0a01629fce9c6={ ""_id"" :
{ ""$oid"" : ""5225a5ece4b0a01629fce9c6""}
, ""snapshotId"" :
{ ""$oid"" : ""5225a5ece4b0a01629fce9c5""}
, ""defects"" : [
{ ""_id"" : 1 }
]}","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.AbstractPersonRepositoryIntegrationTests
org.springframework.data.mongodb.repository.PersonRepository"
FILE,DATAMONGO,DATAMONGO-938,2014-05-21T06:09:48.000-05:00,Exception when creating geo within Criteria using MapReduce,"Criteria.where(""location"")  within(new Box(lowerLeft, upperRight));
I am getting an IllegalArgumentException when I try to query a MongoDB collection using a Criteria.within and a Box.
Criteria.where(""location"").
within(new Box(lowerLeft, upperRight));
java.lang.IllegalArgumentException: can't serialize class org.springframework.data.mongodb.core.query.GeoCommand","org.springframework.data.mongodb.core.mapreduce.MapReduceTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-952,2014-06-10T22:45:47.000-05:00,@Query annotation does not work with only field restrictions,"@Query 
 @Query(fields = ""{ 'email' : 1 }"")




User findByEmail(String email)






  @Query
If you are using repository based queries and try to use @Query annotation to limit the fetched fields, it has zero effect.
For example repository query:
@Query(fields = ""{ 'email' : 1 }"")
User findByEmail(String email)
return fields of User have effect
If you are using @Query with value attribute to define the query, then the fields limitation is applied though.",org.springframework.data.mongodb.repository.query.PartTreeMongoQuery
FILE,DATAMONGO,DATAMONGO-987,2014-07-14T12:01:52.000-05:00,Problem with lazy loading in @DBRef when getting data using MongoTemplate,"@Document 
 @Document




class Parent {




     @Id




     private String id;




     private String name;




     @DBref(lazy=true)




     private Child child;









    // getters and setters ommited




}






 
 @Document




class Child {




      @Id




       private String id;




       private String name;




      //getters and setters ommited




}






 
 Parent parent = new Parent();




parent.setName(""Daddy"");




mongoTemplate.save(parent); //ok, it is persisted like we expected.




// Than we try to load this same entity from the database




Criteria criteria = Criteria.where(""_id"").is(parent.getId());




Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);




// The child attribute should be null, right?




assertNull(persisted.getChild()); // it fails
The situation is simple: if we reference on an entity class another entity (both annotated with @Document) called Parent and Child.
Here is the code:
@Document
class Parent {
@Id
private String id;
private String name;
@DBref(lazy=true)
private Child child;
// getters and setters ommited
}
and the Child class
@Document
class Child {
@Id
private String id;
private String name;
//getters and setters ommited
}
Parent parent = new Parent();
parent.setName(""Daddy"");
// Than we try to load this same entity from the database
Criteria criteria = Criteria.where(""_id"").
is(parent.getId());
Parent persisted = mongoTemplate.findOne(new Query(criteria), Parent.class);
The null attribute is actually an enhanced class generated by CGLib.
It should not be.
This brings a lot of problems when you, by accident, persist the same entity.
attach project with junit test reproduce project with junit test","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.DbRefMappingMongoConverterUnitTests"
FILE,DATAMONGO,DATAMONGO-1068,2014-10-08T19:43:32.000-05:00,elemMatch of Class Criteria fails to build special cirteria,"public class Room {




		private String name;




		private List<Date> occupied;




	}






 
 {




		occupied : {




			$not : {




				$elemMatch : {




					$gte : start,




					$lte : end




				}




			}




		}




	}






 
 Criteria c1 = new Criteria().gte(start).lte(end);




	Criteria c = Criteria.where(""occupied"").not().elemMatch(c1);






 
 {




	occupied : {




		$not : {




			$elemMatch : {




			}




		}




	}




}






  elemMatch(Criteria)
There is an entity like this:
public class Room {
private String name;
private List<Date> occupied;
}
I need to issue a criteria to fetch all documents in which none of elements of occupied falls into a specified date range.
The query JSON is:
{
occupied : {
$not : {
$elemMatch : {
$gte : start,
$lte : end
}
}
}
}
Then, I tried on
Criteria c1 = new Criteria().
gte(start).
lte(end);
Criteria c = Criteria.where(""occupied"").
not().
elemMatch(c1);
{
occupied : {
$not : {
$elemMatch : {
}
}
}
}
empty map by invoking invoke elemMatch(Criteria) not assign key
But really no key I can assign to it.","org.springframework.data.mongodb.core.query.CriteriaTests
org.springframework.data.mongodb.core.query.Criteria"
FILE,DATAMONGO,DATAMONGO-1078,2014-10-28T02:23:26.000-05:00,@Query annotated repository query fails to map complex Id structure.,"@Query(""{'_id': {$in: ?0}}"")




List<User> findByUserIds(Collection<MyUserId> userIds) 
 {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}
convert complex object
Therefore annotated queries like:
@Query(""{'_id': {$in: ?
0}}"")
List<User> findByUserIds(Collection<MyUserId> userIds);
{_id:  {$in: [ {_class:""com.sampleuser.MyUserId"", userId:""...."", sampleId:""....""}, ...","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1088,2014-11-07T03:08:58.000-06:00,"@Query $in does not remove ""_class"" property on collection of embedded objects","@Query(value = ""{ embedded : { $in : ?0} }"")




	List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c)
Following method on repository
@Query(value = ""{ embedded : { $in : ?
0} }"")
List<Foo> findByEmbeddedIn2(Collection<EmbeddedObject> c);
generate incorrect query
{ ""embedded"" : { ""$in"" : [ {  ""_class"" : ""demo.EmbeddedObject"" , ""s"" : ""hello""}]}}
{ ""embedded"" : { ""$in"" : [ { ""s"" : ""hello""}]}}
I attached test project demonstrating this bug.
This bug is related to https://jira.spring.io/browse/DATAMONGO-893","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1123,2014-12-17T09:39:36.000-06:00,"geoNear, does not return all matching elements, it returns only a max of 100 documents","public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {




   final NearQuery nearQuery = NearQuery.near(p).maxDistance(distance);




   log.info(""{}"",nearQuery.toDBObject());




   return mongoTemplate.geoNear(nearQuery, MyObject.class);




}






   
 {@link GeoResults}   {@link NearQuery}
Aloha,
I have the following query:
public GeoResults<MyObject> findByTypeAndLocationNear(MyObjectType type, Point p, Distance distance) {
final NearQuery nearQuery = NearQuery.near(p).
maxDistance(distance);
log.info(""{}"",nearQuery.toDBObject());
return mongoTemplate.geoNear(nearQuery, MyObject.class);
}
The geoNear method is documented like this:
Returns {@link GeoResults} for all entities matching the given {@link NearQuery}.
expect matching documents
restrict result
That should be stated in the method.
And another method having a pageable should be added.
What do you think?",org.springframework.data.mongodb.core.MongoOperations
FILE,DATAMONGO,DATAMONGO-1126,2014-12-21T06:03:21.000-06:00,Repository keyword query findByInId with pageable not returning correctly,"getTotalElements()   getTotalPages()  
 @Document




public class Item {









    @Id




    private String id;




    private String type;




}












 public interface ItemRepository extends MongoRepository<Item, String> {









    Page<Item> findByIdIn(Collection ids, Pageable pageable);




    Page<Item> findByTypeIn(Collection types, Pageable pageable);




}












 @RunWith(SpringJUnit4ClassRunner.class)




@ContextConfiguration(classes = {MongoDbConfig.class})




@TransactionConfiguration(defaultRollback = false)




public class TestPageableIdIn {









    @Autowired




    private ItemRepository itemRepository;




    




    private List<String> allIds = new LinkedList<>();









    @Before




    public void setUp() {




        itemRepository.deleteAll();




        String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};









        // 10 items per type




        for (String type : types) {




            for (int i = 0; i < 10; i++) {




                String id = UUID.randomUUID().toString();




                allIds.add(id);




                itemRepository.save(new Item(id, type));




            }




        }




    }









    @Test




    public void testPageableIdIn() {




        




        Pageable pageable = new PageRequest(0, 5);




        




        // expect 5 Items returned, total of 10 Items(SWORDS) in 2 Pages




        Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(10, results.getTotalElements());




        Assert.assertEquals(2, results.getTotalPages());




        




        // expect 5 Items returned, total of 30 Items in 6 Pages




        results = itemRepository.findByIdIn(allIds, pageable);




        Assert.assertEquals(5, results.getContent().size());




        Assert.assertEquals(30, results.getTotalElements()); // this is returning 0




        Assert.assertEquals(6, results.getTotalPages());     // this is returning 0




    }




}
I've been trying to use the In-keyword with identifiers and making the query pageable.
Also when you try to get any other page than 0, no results return.
I've tried using In with another member other than id and it works as expected.
Below is a strip down example I used for testing;
I've created 3 types and 10 items per those types, results in a total of 30 items.
@Document
public class Item {
@Id
private String id;
private String type;
}
public interface ItemRepository extends MongoRepository<Item, String> {
Page<Item> findByIdIn(Collection ids, Pageable pageable);
Page<Item> findByTypeIn(Collection types, Pageable pageable);
}
@RunWith(SpringJUnit4ClassRunner.class)
@ContextConfiguration(classes = {MongoDbConfig.class})
@TransactionConfiguration(defaultRollback = false)
public class TestPageableIdIn {
@Autowired
private ItemRepository itemRepository;
private List<String> allIds = new LinkedList<>();
@Before
public void setUp() {
itemRepository.deleteAll();
String[] types = {""SWORD"", ""SHIELD"", ""ARMOUR""};
// 10 items per type
for (String type : types) {
for (int i = 0; i < 10; i++) {
String id = UUID.randomUUID().
toString();
allIds.add(id);
itemRepository.save(new Item(id, type));
}
}
}
@Test
public void testPageableIdIn() {
Pageable pageable = new PageRequest(0, 5);
Page<Item> results = itemRepository.findByTypeIn(Arrays.asList(""SWORD""), pageable);
Assert.assertEquals(5, results.getContent().
size());
Assert.assertEquals(10, results.getTotalElements());
Assert.assertEquals(2, results.getTotalPages());
results = itemRepository.findByIdIn(allIds, pageable);
Assert.assertEquals(5, results.getContent().
size());
return //
return //
}
}","org.springframework.data.mongodb.repository.Person
org.springframework.data.mongodb.repository.query.AbstractMongoQueryUnitTests
org.springframework.data.mongodb.core.MongoOperations
org.springframework.data.mongodb.core.MongoTemplate
org.springframework.data.mongodb.repository.query.AbstractMongoQuery"
FILE,DATAMONGO,DATAMONGO-1202,2015-04-14T02:36:40.000-05:00,Indexed annotation problems under generics,"@Indexed
There is a problem with the @Indexed annotation with the model classes.
not create index use with generics use in conjunction work under simple scenarios
I provide a github project with two scenarios:
Employer: Very simple reflexive relation that works nicely (EmployerTest)
Customer: A little more complex scenario using GenericCustomer as a base class to allow having many different kinds of customers.
This scenario fails at creating the index (CustomerTest).
This test runs without a Mongo server because it uses embedmongo-spring (https://github.com/jirutka/embedmongo-spring) at a random port each time the test runs.
run Application create indexes put data in customer collection","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreator
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexCreatorIntegrationTests
org.springframework.data.mongodb.core.index.IndexResolver"
FILE,DATAMONGO,DATAMONGO-1250,2015-07-03T21:07:44.000-05:00,Custom converter implementation not used in updates,"@Document 
 
 
 @Document




public class MyPersistantObject  
 public Allocation allocation;




     public BigDecimal value;









     
 private final String code;









         Allocation(String code) {




            this.code = code;




        }









         public static Converter<Allocation, String> writer() {




            return new Converter<Allocation, String>() {




                public String convert(Allocation allocation) {




                    return allocation.getCode();




                }




            };




        }









         public static Converter<String, Allocation> reader() {




            return new Converter<String, Allocation>() {




                public Allocation convert(String source) {




                    return Allocation.getByCode(source);




                }




            };




        }









         public static Allocation getByCode(String code)  
 return AVAILABLE;




                 
 return ALLOCATED;




             
 throw new IllegalArgumentException(""Unable to get Allocation from: "" + code);




         
 public String getCode() {




            return code;




        }




     
 @Bean




    public CustomConversions customConversions() {




        return new CustomConversions(Arrays.asList(




                MyPersistantObject.Allocation.reader(),




                MyPersistantObject.Allocation.writer()




        ));




    }






 
 @Test




    public void testConversion() {




        Update update;




        Query query;




        MyPersistantObject returned;




        MyPersistantObject myPersistantObject = new MyPersistantObject();




        myPersistantObject.allocation = AVAILABLE;




        myPersistantObject.value = new BigDecimal(1234567);









        mongoTemplate.save(myPersistantObject);









        // Check it was saved correctly - first with invalid allocation to confirm conversion in query




        query = query(where(""allocation"").is(ALLOCATED));




        assertThat(mongoTemplate.findOne(query, MyPersistantObject.class), is(nullValue()));









        // Check it was saved correctly - now with valid allocation to confirm conversion in query




        query = query(where(""allocation"").is(AVAILABLE));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(AVAILABLE));




        assertThat(returned.value.longValue(), is(1234567L));









        try {




            // Update allocation from constant - will fail




            update = update(""allocation"", ALLOCATED);




            mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        } catch (Exception e) {




            System.err.println(""failed to convert allocation: java.lang.IllegalArgumentException: can't serialize class converter_test.MyPersistantObject$Allocation"");




        }









        // Update allocation from string value - succeeds




        update = update(""allocation"", ALLOCATED.getCode());




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check allocation update




        query = query(where(""allocation"").is(ALLOCATED));




        returned = mongoTemplate.findOne(query, MyPersistantObject.class);




        assertThat(returned.allocation, is(ALLOCATED));









        // Update value only - will fail: Caused by: java.lang.IllegalArgumentException: Unable to get MyPersistantObject.Allocation from: 54321




        // Tries to use MyPersistantObject.Allocation converter to String




        update = update(""value"", new BigDecimal(54321));




        mongoTemplate.updateMulti(query, update, MyPersistantObject.class);




        // Check value update




        returned = mongoTemplate.findAll(MyPersistantObject.class).get(0);




        assertThat(returned.value.longValue(), is(54321L));




    }
There does seem to be an issue with the use of customer converters when used in mongoTemplate.update* via an Update object.
I have a custom (de)serialiser for an enumerated type, and it works perfectly when saving and loading a @Document annotated POJO.
build Query execute Query
However when used in an Update, it is either ignored, or called in situations where it shouldn't.
Please clone https://github.com/patrickherrera/converter_test.git for a full test application.
In brief there is a POJO and for the purposes of the test it has a static enum with the desired converters:
@Document
public class MyPersistantObject {
public Allocation allocation;
public BigDecimal value;
public enum Allocation {
AVAILABLE(""V""),
ALLOCATED(""A"");
private final String code;
Allocation(String code) {
this.code = code;
}
public static Converter<Allocation, String> writer() {
return new Converter<Allocation, String>() {
public String convert(Allocation allocation) {
return allocation.getCode();
}
};
}
public static Converter<String, Allocation> reader() {
return new Converter<String, Allocation>() {
public Allocation convert(String source) {
return Allocation.getByCode(source);
}
};
}
public static Allocation getByCode(String code) {
switch (code) {
case ""V"":
return AVAILABLE;
case ""A"":
return ALLOCATED;
}
throw new IllegalArgumentException(""Unable to get Allocation from: "" + code);
}
public String getCode() {
return code;
}
}
}
It simply converts back and forward using a short code rather than the full Enum name.
These are registered in the Spring Boot application entry point:
@Bean
public CustomConversions customConversions() {
return new CustomConversions(Arrays.asList(
MyPersistantObject.Allocation.reader(),
MyPersistantObject.Allocation.writer()
));
}
There is a unit test that drives a few scenarios:
@Test
public void testConversion() {
Update update;
Query query;
MyPersistantObject returned;
MyPersistantObject myPersistantObject = new MyPersistantObject();
myPersistantObject.allocation = AVAILABLE;
myPersistantObject.value = new BigDecimal(1234567);
mongoTemplate.save(myPersistantObject);
// Check it was saved correctly - first with invalid allocation to confirm conversion in query
query = query(where(""allocation"").
is(ALLOCATED));
assertThat(mongoTemplate.findOne(query, MyPersistantObject.class), is(nullValue()));
// Check it was saved correctly - now with valid allocation to confirm conversion in query
query = query(where(""allocation"").
is(AVAILABLE));
returned = mongoTemplate.findOne(query, MyPersistantObject.class);
assertThat(returned.allocation, is(AVAILABLE));
assertThat(returned.value.longValue(), is(1234567L));
try {
// Update allocation from constant - will fail
update = update(""allocation"", ALLOCATED);
mongoTemplate.updateMulti(query, update, MyPersistantObject.class);
} catch (Exception e) {
System.err.println(""failed to convert allocation: java.lang.IllegalArgumentException: can't serialize class converter_test.
MyPersistantObject$Allocation"");
}
// Update allocation from string value - succeeds
update = update(""allocation"", ALLOCATED.getCode());
mongoTemplate.updateMulti(query, update, MyPersistantObject.class);
// Check allocation update
query = query(where(""allocation"").
is(ALLOCATED));
returned = mongoTemplate.findOne(query, MyPersistantObject.class);
assertThat(returned.allocation, is(ALLOCATED));
// Update value only - will fail: Caused by: java.lang.IllegalArgumentException: Unable to get MyPersistantObject.Allocation from: 54321
// Tries to use MyPersistantObject.Allocation converter to String
update = update(""value"", new BigDecimal(54321));
mongoTemplate.updateMulti(query, update, MyPersistantObject.class);
// Check value update
returned = mongoTemplate.findAll(MyPersistantObject.class).
get(0);
assertThat(returned.value.longValue(), is(54321L));
}
Hopefully that makes sense.
Firstly it saves and queries for the object to demonstrate that the converters are called correctly on the document.
I have confirmed that the document in the database correctly stores the Enum code rather than the name.
appear by use use in Query builder
come to update throw exception to effect
If I change it to use the code (a String), it works and we confirm that by Querying it back from the DB.
convert from Enum not call customer converter in situation not call customer converter for converting
Next I try and update the other value in the Document.
try numeric String into Allocation enum convert numeric String into Allocation enum convert BigDecimal to String fail Allocation enum of course
I tried to debug the code and it seems that there is an overloaded method in CustomConversions: getCustomWriteTarget that takes one or two arguments, the second being a requestedTargetType.
not use Allocation converter call in MappingMongoConverter
use first converter handle input type handle first converter seem without type information be in case
It is my custom one which is picked up first but can't actually handle it.
Please advise if there is something I am missing, as I can't find a workaround either - I have resorted to the Mongo Driver itself to do the update.","org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests
org.springframework.data.mongodb.core.convert.UpdateMapper"
FILE,DATAMONGO,DATAMONGO-1263,2015-07-30T09:03:41.000-05:00,Missing indexes in associations involving generic types,"class Book  
 class AbstractProduct  
 class ProductWrapper    
 class Catalog
involve generic types not infer type information at startup time result in missing indexes
Please, see https://github.com/agustisanchez/SpringDataMongoDBBug, for code samples.
Given:
class Book with index on ""ISBN"" attribute super class AbstractProduct with index on ""name"" attribute class ProductWrapper holding attribute ""content"" of generic type ""T extends AbstractProduct""
When defining a class Catalog with a list of ""wrapped"" books:
List<ProductWrapper<Book>> books2 = new ArrayList<>
infer type infromation from ProductWrapper class definition inherit from AbstractProduct create index name inside catalog define on Book class not create Book class as Spring data Mongo
define wrapper class as ProductWrapper<T> create indexes on Catalog.books2.content.","org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolver
org.springframework.data.mongodb.core.index.MongoPersistentEntityIndexResolverUnitTests"
FILE,DATAMONGO,DATAMONGO-1360,2016-01-16T07:47:34.000-06:00,Cannot query with JSR310,"query.addCriteria(where(""createdDate"").lte(LocalDateTime.now()));
I have a MongoDb document I successfully store using Spring Data MongoDb.
It looks like this:
{
""_id"" : ""1"",
""_class"" : ""SomeClass"",
""createdDate"" : ISODate(""2016-01-16T07:05:45.656Z""),
""lastUpdate"" : ISODate(""2016-01-16T07:05:45.656Z"")
}
When I create a custom Criteria query that looks like this:
query.addCriteria(where(""createdDate"").
lte(LocalDateTime.now()));
{ ""createdDate"" : { ""$lte"" : { $java : 2016-01-16T14:36:50.656 } } }
fail with message
java.lang.IllegalArgumentException: can't serialize class java.time.LocalDateTime at org.bson.BasicBSONEncoder.
_putObjectField(BasicBSONEncoder.java:299)
use java.util.Date in query persist document with java.time.LocalDateTime object
{ ""createdDate"" : { ""$lte"" : { ""$date"" : ""2016-01-16T07:35:19.985Z""}}}
I'm hoping there is a way to not have to convert my LocalDateTime objects to Date objects for querying.
Please advise.
Cheers,
Bjorn","org.springframework.data.mongodb.core.Venue
org.springframework.data.mongodb.core.geo.AbstractGeoSpatialTests
org.springframework.data.mongodb.core.MongoTemplate"
FILE,DATAMONGO,DATAMONGO-1438,2016-05-26T14:01:14.000-05:00,I get a warning in my logs since switched to Spring Data MongoDB Hopper-SR1 Release Train in Spring Boot 1.3.5,"@Document
When I start my Spring Boot 1.3.5 application with no custom conversions and with Spring Data MongoDB Release Train Hopper-SR1 I get following warning in my logs:
Registering converter from class java.lang.Number to class java.lang.Number as writing converter although it doesn't convert to a Mongo supported type!
You might wanna check you annotation setup at the converter implementation.
With the in Spring Boot 1.3.5 integrated version the warning is not exists.
.
I have alle my Domain classes they are saved in MongoDB annotated with @Document (see DATAMONGO-1413)","org.springframework.data.mongodb.core.convert.MongoConvertersUnitTests
org.springframework.data.mongodb.core.convert.MongoConverters"
FILE,DATAMONGO,DATAMONGO-1406,2016-04-04T18:59:49.000-05:00,Query mapper does not use @Field field name when querying nested fields in combination with nested keywords,";






@Document(collection = ""Computer"")




public class Computer




{




   @Id




   private String _id;









   private String batchId;









  @Field(""stat"")




   private String status;









   @Field(""disp"")




   private List<Monitor> displays;









   //setters and getters




}









public class Monitor {




   @Field(""res"")




   private String resolution;









  // setters/getters




}






   
 protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,




			CursorPreparer preparer, DbObjectCallback<T> objectCallback)









 DBObject mappedQuery = queryMapper.getMappedObject(query, entity);






  @Field   
  
  
 
  
  @Field
we have a document class;
@Document(collection = ""Computer"")
public class Computer
{
@Id
private String _id;
private String batchId;
@Field(""stat"")
private String status;
@Field(""disp"")
private List<Monitor> displays;
//setters and getters
}
public class Monitor {
@Field(""res"")
private String resolution;
// setters/getters
}
In MongoTemplate.java, the call to :
protected <S, T> List<T> doFind(String collectionName, DBObject query, DBObject fields, Class<S> entityClass,
CursorPreparer preparer, DbObjectCallback<T> objectCallback)
DBObject mappedQuery = queryMapper.getMappedObject(query, entity);
resolve to stat
Note the queries in the inner list, are setup as elemMatch.
The query submitted to mongo after getMappedObject is called:
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
not get data call resolution
Note: The query input to getMappedObject is:
{ ""$and"" : [ { ""status"" : ""A""} , { ""displays"" : { ""$elemMatch"" : { ""$and"" : [ { ""resolution"" : { ""$ne"" :  null }} , { ""resolution"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
convert to value
{ ""$and"" : [ { ""stat"" : ""A""} , { ""disp"" : { ""$elemMatch"" : { ""$and"" : [ { ""res"" : { ""$ne"" :  null }} , { ""res"" : { ""$ne"" : """"}}]}}}] , ""batchId"" : ""5d0f1c53-92a2-48cb-8c84-1061769962c1""}
operate queries on fields","org.springframework.data.mongodb.core.convert.QueryMapper
org.springframework.data.mongodb.core.convert.QueryMapperUnitTests"
FILE,DATAMONGO,DATAMONGO-1486,2016-09-07T16:46:53.000-05:00,Changes to MappingMongoConverter Result in Class Cast Exception,"Map<Integer, Map<Platform, String>> descriptions = new HashMap<>();






 
 public void setAlternateDescriptionMap(int compositeId, Map<Integer, Map<Platform, String>> alternateDescriptionsMap) {




		Query query = new Query();




		query.addCriteria(Criteria.where(""_id"").is(compositeId));




		Update update = new Update();




		update.set(""alternateDescriptionMap"", alternateDescriptionsMap);









		coreMongoTemplate.updateFirst(query, update, ""product"");




	}






   
    
 
 MappingMongoConverter.convertMongoType()
I am upgrading our software to use Spring Boot 1.4 + Spring 4.3.
As part of this upgrade, we are also using Spring Data Mongo 1.9.2.
RELEASE.
I am assuming that is 1.9.2 (Hopper SR2)?
We have a situation where we have a model object that looks like the following:
Map<Integer, Map<Platform, String>> descriptions = new HashMap<>();
Where ""Platform is Enum"", although this is not the core issue.
When we execute the following:
public void setAlternateDescriptionMap(int compositeId, Map<Integer, Map<Platform, String>> alternateDescriptionsMap) {
Query query = new Query();
query.addCriteria(Criteria.where(""_id"").
is(compositeId));
Update update = new Update();
update.set(""alternateDescriptionMap"", alternateDescriptionsMap);
coreMongoTemplate.updateFirst(query, update, ""product"");
}
get following exception
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
at com.mongodb.DBObjectCodec.encodeMap(DBObjectCodec.java:222)
at com.mongodb.DBObjectCodec.writeValue(DBObjectCodec.java:199)
at com.mongodb.DBObjectCodec.encodeMap(DBObjectCodec.java:223)
at com.mongodb.DBObjectCodec.writeValue(DBObjectCodec.java:199)
at com.mongodb.DBObjectCodec.encode(DBObjectCodec.java:131)
at com.mongodb.DBObjectCodec.encode(DBObjectCodec.java:62)
at org.bson.codecs.BsonDocumentWrapperCodec.encode(BsonDocumentWrapperCodec.java:63)
at org.bson.codecs.BsonDocumentWrapperCodec.encode(BsonDocumentWrapperCodec.java:29)
at com.mongodb.connection.RequestMessage.addDocument(RequestMessage.java:253)
at com.mongodb.connection.RequestMessage.addDocument(RequestMessage.java:205)
at com.mongodb.connection.UpdateMessage.encodeMessageBodyWithMetadata(UpdateMessage.java:80)
at com.mongodb.connection.RequestMessage.encodeWithMetadata(RequestMessage.java:160)
at com.mongodb.connection.WriteProtocol.execute(WriteProtocol.java:89)
at com.mongodb.connection.UpdateProtocol.execute(UpdateProtocol.java:67)
at com.mongodb.connection.UpdateProtocol.execute(UpdateProtocol.java:42)
at com.mongodb.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:168)
at com.mongodb.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:289)
at com.mongodb.connection.DefaultServerConnection.update(DefaultServerConnection.java:88)
at com.mongodb.operation.UpdateOperation.executeProtocol(UpdateOperation.java:66)
at com.mongodb.operation.BaseWriteOperation$1.call(BaseWriteOperation.java:144)
at com.mongodb.operation.BaseWriteOperation$1.call(BaseWriteOperation.java:134)
at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:232)
at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:223)
at com.mongodb.operation.BaseWriteOperation.execute(BaseWriteOperation.java:134)
at com.mongodb.operation.BaseWriteOperation.execute(BaseWriteOperation.java:61)
at com.mongodb.Mongo.execute(Mongo.java:827)
at com.mongodb.Mongo$2.execute(Mongo.java:810)
at com.mongodb.DBCollection.executeWriteOperation(DBCollection.java:333)
at com.mongodb.DBCollection.updateImpl(DBCollection.java:495)
at com.mongodb.DBCollection.update(DBCollection.java:455)
at com.mongodb.DBCollection.update(DBCollection.java:432)
at org.springframework.data.mongodb.core.MongoTemplate$12.doInCollection(MongoTemplate.java:1153)
at org.springframework.data.mongodb.core.MongoTemplate$12.doInCollection(MongoTemplate.java:1132)
at org.springframework.data.mongodb.core.MongoTemplate.execute(MongoTemplate.java:462)
at org.springframework.data.mongodb.core.MongoTemplate.doUpdate(MongoTemplate.java:1132)
at org.springframework.data.mongodb.core.MongoTemplate.updateFirst(MongoTemplate.java:1110)
at com.build.dao.product.ProductStorageDaoImpl.setAlternateDescriptionMap(ProductStorageDaoImpl.java:1170)
at com.build.dao.product.ProductStorageDaoImpl$$FastClassBySpringCGLIB$$4e03147e.
invoke(<generated>)
at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:720)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:136)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655)
at com.build.dao.product.ProductStorageDaoImpl$$EnhancerBySpringCGLIB$$8602f8b4.
setAlternateDescriptionMap(<generated>)
at com.build.dao.product.ProductStorageDaoIT.testSaveAlternateDescriptionsToCacheAndFetch(ProductStorageDaoIT.java:335)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:75)
at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:86)
at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:84)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:252)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:94)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:70)
at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:191)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:678)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)
We do not get this exception in spring-data-mongodb-1.9.1-RELEASE.
After a lot of debugging, It appears to be related to the code in MappingMongoConverter.convertMongoType().
It looks like the Issue DATAMONGO-1423 may have introduced this issue:
https://github.com/spring-projects/spring-data-mongodb/commit/0e60630393980cf2bb4634c8a9c1a5a50407c471
I am going to work on just overriding this default method with a custom mapper.
I suspect this code will also break in other cases where the key is mapped into anything other than a string.
Let me know if you need any further input.","org.springframework.data.mongodb.core.convert.MappingMongoConverter
org.springframework.data.mongodb.core.convert.UpdateMapperUnitTests"
CLASS,derby-10.7.1.1,DERBY-4835,2010-10-06T11:05:13.000-05:00,Trigger plan does not recompile with upgrade from 10.5.3.0 to 10.6.1.0 causing  java.lang.NoSuchMethodError,"tidlggls(blt_number,create_date,update_date,propagation_date,glossary_status,
     time_stamp,min_max_size )
    
      
 
  
 tidlrblt(BLT,BLT_SIZE,MIN_MAX_SIZE)  
 
     
  
   GeneratedMe
thod;    
  
  
 if (fromVersion.majorVersionNumber >= DataDictionary.DD_VERSION_DERBY_10_5)
				bootingDictionary.updateMetadataSPSes(tc);
			else
				bootingDictionary.clearSPSPlans();

  clearSPSPlans()
cause following exception not recompile to 10.6.1.0 not recompile on upgrade fire trigger after upgrade fire first time after upgrade
ATABASE = wombat), (DRDAID = null), Failed Statement is: INSERT INTO tidlggls(blt_number,create_date,update_date,propagation_date,glossary_status,
     time_stamp,min_max_size )
 VALUES ( (select max(blt_number) from tidlrblt), CURRENT_DATE,
CURRENT_DATE, CURRENT_DATE, '00' , CURRENT_TIMESTAMP, (select min_max_size from tidlrblt where blt_number = (select max(blt_number) from tidlrblt)))
java.lang.NoSuchMethodError: org/apache/derby/iapi/sql/execute/ResultSetFactory.getProjectRestrictResultSet(Lorg/apache/derby/iapi/sql/execute/NoPutResultSet;Lorg/apache/derby/iapi/services/loader/GeneratedMethod;Lorg/apache/derby/iapi/services/loader/GeneratedMethod;ILorg/apache/derby/iapi/services/loader/GeneratedMethod;IZZDD)Lorg/apache/derby/iapi/sql/execute/NoPutResultSet;
at org.apache.derby.exe.acf81e0010x012bx823cxd0d3x00000026c4a00.g0(Unknown Source)
at org.apache.derby.exe.acf81e0010x012bx823cxd0d3x00000026c4a00.execute(Unknown Source)
at org.apache.derby.impl.sql.GenericActivationHolder.execute(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.executeSubStatement(Unknown Source)
at org.apache.derby.impl.sql.execute.GenericTriggerExecutor.executeSPS(Unknown Source)
at org.apache.derby.impl.sql.execute.StatementTriggerExecutor.fireTrigger(Unknown Source)
at org.apache.derby.impl.sql.execute.TriggerEventActivator.notifyEvent(Unknown Source)
at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
at org.apache.derby.impl.tools.ij.ij.executeImmediate(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.doCatch(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.runScriptGuts(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.go(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.go(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.mainCore(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.main(Unknown Source)
at org.apache.derby.tools.ij.main(Unknown Source)
Cleanup action completed
To reproduce, run the attached script 10_5_3_work.sql with the 10.5.3.0  release and then connect with 10.6.1.0 and insert into the table with the trigger:
connect 'jdbc:derby:wombat;upgrade=true';
 
INSERT INTO tidlrblt(BLT,BLT_SIZE,MIN_MAX_SIZE) VALUES('Mamatha Testing2', 15, 20);
ERROR XJ001: Java exception: 'org/apache/derby/iapi/sql/execute/ResultSetFactory
.
getProjectRestrictResultSet(Lorg/apache/derby/iapi/sql/execute/NoPutResultSet;L
org/apache/derby/iapi/services/loader/GeneratedMethod;Lorg/apache/derby/iapi/ser
vices/loader/GeneratedMethod;ILorg/apache/derby/iapi/services/loader/GeneratedMe
thod;IZZDD)Lorg/apache/derby/iapi/sql/execute/NoPutResultSet;: java.lang.NoSuchM
ethodError'.
I think this may be related to the DERBY-1107 change in handleMinorRevisionChange which has the code:
if (fromVersion.majorVersionNumber >= DataDictionary.DD_VERSION_DERBY_10_5)
				bootingDictionary.updateMetadataSPSes(tc);
			else
				bootingDictionary.clearSPSPlans();
Likely, clearSPSPlans() should not be in the else clause but rather executed unconditionally.
To work around the issue, after connecting with 10.6.1, drop and recreate the trigger as in workaround.sql","org.apache.derby.impl.sql.catalog.DD_Version
org.apache.derbyTesting.functionTests.tests.upgradeTests.BasicSetup"
CLASS,derby-10.7.1.1,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:
1 testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)java.sql.SQLException: DERBY SQL error: SQLCODE: -1, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U
at org.apache.derby.client.am.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:96)
at org.apache.derby.client.am.SqlException.getSQLException(SqlException.java:358)
at org.apache.derby.jdbc.ClientDriver.connect(ClientDriver.java:149)
at java.sql.DriverManager.getConnection(DriverManager.java:322)
at java.sql.DriverManager.getConnection(DriverManager.java:273)
at org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest.testBoundaries(InternationalConnectTest.java:111)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:109)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
Caused by: org.apache.derby.client.am.SqlException: DERBY SQL error: SQLCODE: -1, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U
at org.apache.derby.client.am.Connection.completeSqlca(Connection.java:2117)
at org.apache.derby.client.net.NetConnectionReply.parseRdbAccessFailed(NetConnectionReply.java:541)
at org.apache.derby.client.net.NetConnectionReply.parseAccessRdbError(NetConnectionReply.java:434)
at org.apache.derby.client.net.NetConnectionReply.parseACCRDBreply(NetConnectionReply.java:297)
at org.apache.derby.client.net.NetConnectionReply.readAccessDatabase(NetConnectionReply.java:121)
at org.apache.derby.client.net.NetConnection.readSecurityCheckAndAccessRdb(NetConnection.java:846)
at org.apache.derby.client.net.NetConnection.flowSecurityCheckAndAccessRdb(NetConnection.java:769)
at org.apache.derby.client.net.NetConnection.flowUSRIDONLconnect(NetConnection.java:601)
at org.apache.derby.client.net.NetConnection.flowConnect(NetConnection.java:408)
at org.apache.derby.client.net.NetConnection.<init>(NetConnection.java:218)
at org.apache.derby.client.net.NetConnection40.<init>(NetConnection40.java:77)
at org.apache.derby.client.net.ClientJDBCObjectFactoryImpl40.newNetConnection(ClientJDBCObjectFactoryImpl40.java:269)
at org.apache.derby.jdbc.ClientDriver.connect(ClientDriver.java:140)
... 35 more
This is after the latest check in for DERBY-4836 (revision 1028035).
I'll attach derby.log.",org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.7.1.1,DERBY-4889,2010-11-05T20:06:56.000-05:00,Different byte to boolean conversion on embedded and client,"PreparedStatement ps = c.prepareStatement(""values cast(? as boolean)"");
        ps.setByte(1, (byte) 32);
        ResultSet rs = ps.executeQuery();
        rs.next();
        System.out.println(rs.getBoolean(1));

 If setByte()   setInt()
The following code prints ""true"" with the embedded driver and ""false"" with the client driver:
PreparedStatement ps = c.prepareStatement(""values cast(?
as boolean)"");
        ps.setByte(1, (byte) 32);
        ResultSet rs = ps.executeQuery();
        rs.next();
        System.out.println(rs.getBoolean(1));
If setByte() is replaced with setInt(), they both print ""true"".","org.apache.derbyTesting.functionTests.tests.jdbcapi.ParameterMappingTest
org.apache.derby.impl.drda.DRDAConnThread"
CLASS,derby-10.7.1.1,DERBY-4892,2010-11-06T04:14:51.000-05:00,Unsafe use of BigDecimal constructors,"test_06_casts(org.apache.derbyTesting.functionTests.tests.lang.UDTTest)
We have some code that's supposed to work on Java 1.4, but that uses BigDecimal constructors that were not added until Java 5.
The problematic constructors are the ones that take a single int or long.
The constructors are used in the following classes:
org.apache.derby.client.am.Cursor
org.apache.derbyTesting.functionTests.tests.lang.Price
org.apache.derbyTesting.system.oe.client.Submitter
All of the classes are compiled against ${java14compile.classpath}, so one would expect the build to fail when java14compile.classpath pointed to proper Java 1.4 libraries.
However, there is a constructor with a double parameter in Java 1.4, and the compiler picks that constructor if it cannot find the ones for int and long.
If that happens, the compiled byte-code works on Java 1.4 and newer, and everything is fine.
not use Java libraries
This can easily happen if you build without a customized ant.properties, and PropertySetter ends up building java14compile.classpath based on the auto-detected java15compile.classpath.
In that case, the compiler finds the int and long variants of the constructor, even when building against java14compile.classpath.
use Java constructors fail at run-time execute on Java JVM
To reproduce, build Derby without ant.properties on a system where PropertySetter doesn't find JDK 1.4.
Verify with -DprintCompilerProperties=true that java14compile.classpath is built up of jar files from a Java 5 or Java 6 directory.
Then run org.apache.derbyTesting.functionTests.tests.lang.UDTTest using a Java 1.4 JVM.
see errors of kind
1 test_06_casts(org.apache.derbyTesting.functionTests.tests.lang.UDTTest)java.lang.NoSuchMethodError: java.math.BigDecimal.
<init>(I)V
at org.apache.derbyTesting.functionTests.tests.lang.Price.makePrice(Price.java:49)
at org.apache.derbyTesting.functionTests.tests.lang.UDTTest.test_06_casts(UDTTest.java:501)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:109)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
The problem in client.am.Cursor can be seen if you follow the same procedure as above, and instead of UDTTest run ParameterMappingTest with the patch for DERBY-4891 that enables testing of booleans.","org.apache.derbyTesting.functionTests.tests.lang.Price
org.apache.derby.client.am.Cursor
org.apache.derbyTesting.system.oe.client.Submitter"
CLASS,pig-0.11.1,PIG-2767,2012-06-25T09:11:20.000-05:00,Pig creates wrong schema after dereferencing nested tuple fields,"PigStorage()  
  
   ;
DESCRIBE dereferenced;

   nested_tuple.f3;
DESCRIBE uses_dereferenced;

  {f1: int, nested_tuple: (f2: int,
f3: int)}  {f1: int, f2: int}
The following script fails:
data = LOAD 'test_data.
txt' USING PigStorage() AS (f1: int, f2: int, f3:
int, f4: int);
nested = FOREACH data GENERATE f1, (f2, f3, f4) AS nested_tuple;
dereferenced = FOREACH nested GENERATE f1, nested_tuple.
(f2, f3);
DESCRIBE dereferenced;
uses_dereferenced = FOREACH dereferenced GENERATE nested_tuple.
f3;
DESCRIBE uses_dereferenced;
(1 (2,3))
(5 (6,7))
...
This is not just a problem with DESCRIBE.
Invalid field projection.
Projected field [nested_tuple] does not exist in
schema: f1:int,f2:int.","src.org.apache.pig.newplan.logical.expression.DereferenceExpression
test.org.apache.pig.test.TestPigServer"
CLASS,pig-0.11.1,PIG-2828,2012-07-19T05:03:16.000-05:00,Handle nulls in DataType.compare,"Object field1 = o1.get(fieldNum);
                Object field2 = o2.get(fieldNum);
                if (!typeFound) {
                    datatype = DataType.findType(field1);
                    typeFound = true;
                }
                return DataType.compare(field1, field2, datatype, datatype);
While using TOP, and if the DataBag contains null value to compare, it will generate the following exception:
Caused by: java.lang.NullPointerException
at org.apache.pig.data.DataType.compare(DataType.java:427)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:97)
at org.apache.pig.builtin.TOP$TupleComparator.compare(TOP.java:1)
at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:649)
at java.util.PriorityQueue.siftUp(PriorityQueue.java:627)
at java.util.PriorityQueue.offer(PriorityQueue.java:329)
at java.util.PriorityQueue.add(PriorityQueue.java:306)
at org.apache.pig.builtin.TOP.updateTop(TOP.java:141)
at org.apache.pig.builtin.TOP.exec(TOP.java:116)
code: (TOP.java, starts with line 91)
Object field1 = o1.get(fieldNum);
Object field2 = o2.get(fieldNum);
if (! typeFound) { datatype = DataType.findType(field1);
typeFound = true;
} return DataType.compare(field1, field2, datatype, datatype);","src.org.apache.pig.data.DataType
src.org.apache.pig.builtin.TOP
test.org.apache.pig.test.TestNull"
CLASS,pig-0.11.1,PIG-3114,2013-01-03T19:49:42.000-06:00,Duplicated macro name error when using pigunit,"{code:title=test.pig|borderStyle=solid}
    {
    $C = ORDER $QUERY BY total DESC, $A;
}  
  
     AS total;

queries_ordered = my_macro_1(queries_count, query);

    
   ;
{code}
I'm using PigUnit to test a pig script within which a macro is defined.
get parsing error with pigunit
So I tried very basic pig script with macro and getting similar error.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing.
<line 9> null.
Reason: Duplicated macro name 'my_macro_1'
at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1607)
at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1546)
at org.apache.pig.PigServer.registerQuery(PigServer.java:516)
at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:988)
at org.apache.pig.pigunit.pig.GruntParser.processPig(GruntParser.java:61)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:412)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:194)
at org.apache.pig.pigunit.pig.PigServer.registerScript(PigServer.java:56)
at org.apache.pig.pigunit.PigTest.registerScript(PigTest.java:160)
at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:231)
at org.apache.pig.pigunit.PigTest.assertOutput(PigTest.java:261)
at FirstPigTest.MyPigTest.testTop2Queries(MyPigTest.java:32)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at junit.framework.TestCase.runTest(TestCase.java:176)
at junit.framework.TestCase.runBare(TestCase.java:141)
at junit.framework.TestResult$1.protect(TestResult.java:122)
at junit.framework.TestResult.runProtected(TestResult.java:142)
at junit.framework.TestResult.run(TestResult.java:125)
at junit.framework.TestCase.run(TestCase.java:129)
at junit.framework.TestSuite.runTest(TestSuite.java:255)
at junit.framework.TestSuite.run(TestSuite.java:250)
at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: Failed to parse: <line 9> null.
Reason: Duplicated macro name 'my_macro_1'
at org.apache.pig.parser.QueryParserDriver.makeMacroDef(QueryParserDriver.java:406)
at org.apache.pig.parser.QueryParserDriver.expandMacro(QueryParserDriver.java:277)
at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:178)
at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1599)
... 30 more
Pig script which is failing :
{code:title=test.pig|borderStyle=solid}
DEFINE my_macro_1 (QUERY, A) RETURNS C {
$C = ORDER $QUERY BY total DESC, $A;
} ;
data =  LOAD 'input' AS (query:CHARARRAY);
queries_group = GROUP data BY query;
queries_count = FOREACH queries_group GENERATE group AS query, COUNT(data) AS total;
queries_ordered = my_macro_1(queries_count, query);
queries_limit = LIMIT queries_ordered 2;
STORE queries_limit INTO 'output';
{code}
If I remove macro pigunit works fine.
Even just defining macro without using it results in parsing error.","src.org.apache.pig.PigServer
test.org.apache.pig.test.pigunit.TestPigTest
test.org.apache.pig.pigunit.PigTest
test.org.apache.pig.pigunit.pig.PigServer"
CLASS,pig-0.11.1,PIG-3267,2013-04-03T16:14:30.000-05:00,HCatStorer fail in limit query,"{code}
 
  
  
     ;
{code}

 
 {code}
  {code}
The following query fail:
{code}
data = LOAD 'student.txt' as (name:chararray, age:int, gpa:double);
data_limited = limit data 10;
samples = foreach data_limited generate age as number;
store samples into 'samples' using org.apache.hcatalog.pig.HCatStorer('part_dt=20130101T010000T36');
{code}
launch second job happen before launching
{code}
Message: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:8020/user/hive/warehouse/samples/part_dt=20130101T010000T36 already exists
at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
at org.apache.hcatalog.mapreduce.FileOutputFormatContainer.checkOutputSpecs(FileOutputFormatContainer.java:135)
at org.apache.hcatalog.mapreduce.HCatBaseOutputFormat.checkOutputSpecs(HCatBaseOutputFormat.java:72)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecsHelper(PigOutputFormat.java:207)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat.checkOutputSpecs(PigOutputFormat.java:188)
at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:887)
at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)
at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)
at org.apache.hadoop.mapred.jobcontrol.Job.submit(Job.java:378)
at org.apache.hadoop.mapred.jobcontrol.JobControl.startReadyJobs(JobControl.java:247)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.pig.backend.hadoop20.PigJobControl.mainLoopAction(PigJobControl.java:157)
at org.apache.pig.backend.hadoop20.PigJobControl.run(PigJobControl.java:134)
at java.lang.Thread.run(Thread.java:680)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:257)
{code}","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POStore
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.11.1,PIG-3292,2013-04-24T03:06:41.000-05:00,Logical plan invalid state: duplicate uid in schema during self-join to get cross product,"{code}
 
  
   {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}

 
 {code}
   
 {code}

 
 {code}
 
  
   {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}

 dump b;
{code}
Hi.
Looks like PIG-3020
but works in a different way.
Our pig version is: 
Apache Pig version 0.10.0-cdh4.2.0 (rexported) 
compiled Feb 15 2013, 12:20:54
Accoring to release note, PIG-3020 is included into CDH 4.2 dist
http://archive.cloudera.com/cdh4/cdh/4/pig-0.10.0-cdh4.2.0.CHANGES.txt
The problem:
We want to do self join to get cross-product
{code}
a = load '/input' as (key, x);
a_group = group a by key;
b = foreach a_group {
  y = a.x;
  pair = cross a.x, y;
  generate flatten(pair);
}
dump b;
{code}
Here is workaround :)
{code}
a = load '/input' as (key, x:int);
a_group = group a by key;
b = foreach a_group {
  y = foreach a generate -(-x);
  pair = cross a.x, y;
  generate flatten(pair);
}
dump b;
{code}","test.org.apache.pig.test.TestEvalPipelineLocal
src.org.apache.pig.newplan.logical.relational.LOCross"
CLASS,pig-0.11.1,PIG-3310,2013-05-03T02:59:57.000-05:00,"ImplicitSplitInserter does not generate new uids for nested schema fields, leading to miscomputations","{code}
     
    
        
        
    
           as shop;

EXPLAIN K;
DUMP K;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}
 
        
      
  
 {code}
                  
              
              
              
              
              
 {code}

 
 {code}
                   
  
  
 {code}

     
 LOSplitOutput.getSchema()
Hi,
Consider the following example
{code}
inp = LOAD '$INPUT' AS (memberId:long, shopId:long, score:int);
tuplified = FOREACH inp GENERATE (memberId, shopId) AS tuplify, score;
D1 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
D2 = FOREACH tuplified GENERATE tuplify.memberId as memberId, tuplify.shopId as shopId, score AS score;
J = JOIN D1 By shopId, D2 by shopId;
K = FOREACH J GENERATE D1::memberId AS member_id1, D2::memberId AS member_id2, D1::shopId as shop;
EXPLAIN K;
DUMP K;
{code}
It is a bit weird written like that, but it provides a minimal reproduction case (in the real case, the ""tuplified"" phase came from a multi-key grouping).
On input data:
{code}
1       1001    101
1       1002    103
1       1003    102
1       1004    102
2       1005    101
2       1003    101
2       1002    123
3       1042    101
3       1005    101
3       1002    133
{code}
give wrongful output
In the initial case, there was a FILTER (member_id1 < member_id2) after K, and computation failed because of PushUpFilter optimization mistakenly moving the LOFilter operation before the join, at a place where it tried to work on a tuple and failed.
My understanding of the issue is that when the ImplicitSplitInserter creates the LOSplitOutputs, it will correctly reset the schema, and the LOSplitOutput will regenerate uids for the fields of D1 and D2 ... but will not do that on the tuple members.
The logical plan after the ImplicitSplitINserter will look like (simplified)
{code}
|---D1: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[127]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#127:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[127]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
|---D2: (Name: LOForEach Schema: memberId#124:long,shopId#125:long)ColumnPrune:InputUids=[130]ColumnPrune:OutputUids=[125, 124]
|---tuplified: (Name: LOSplitOutput Schema: tuplify#130:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[130]
|---tuplified: (Name: LOSplit Schema: tuplify#123:tuple(memberId#124:long,shopId#125:long))ColumnPrune:InputUids=[123]ColumnPrune:OutputUids=[123]
{code}
tuplified correctly gets a new uid (127 and 130) but the members of the tuple don't.
When they get reprojected, both branches have the same uid and the join looks like:
{code}
|---J: (Name: LOJoin(HASH) Schema: D1::memberId#124:long,D1::shopId#125:long,D2::memberId#139:long,D2::shopId#132:long)ColumnPrune:InputUids=[125, 124, 132]ColumnPrune:OutputUids=[125, 124, 132]
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 0 Column: 1)
|   |
|   shopId:(Name: Project Type: long Uid: 125 Input: 1 Column: 1)
{code}
If for example instead of reprojecting ""memberId"", we project ""memberId+0"", a new node is created, and ultimately the two branches of the join will correctly get separate uids.
My understanding is that LOSplitOutput.getSchema() should recurse on nested schema fields.
However, I only have a light understanding of all of the logical plan handling, so I may be completely wrong.
Attached is a draft of patch and a test reproducing the issue.
Unfortunately, I haven't been able to run all unit tests with the ""fix"" (I have some weird hangs)
I'd be happy if you could indicate if that looks like completely the wrong way to fix the issue.",src.org.apache.pig.newplan.logical.relational.LOSplitOutput
CLASS,pig-0.11.1,PIG-3316,2013-05-08T14:01:36.000-05:00,Pig failed to interpret DateTime values in some special cases,";
dump A;
For the query
A = load 'date.txt' as ( f1:int, f2:datetime );
dump A;
with input data
1 1970-01-01
2 1970-01
generate following output
interpret day month part as time zone","test.org.apache.pig.test.TestDefaultDateTimeZone
src.org.apache.pig.builtin.ToDate"
CLASS,pig-0.11.1,PIG-3329,2013-05-16T22:44:41.000-05:00,RANK operator failed when working with SPLIT,"RANK b;
dump d;
input.txt:
1 2 3
4 5 6
7 8 9
script:
a = load 'input.txt' using PigStorage(' ') as (a:int, b:int, c:int);
SPLIT a into b if a > 0, c if a > 5;
d = RANK b;
dump d;
fail with error message
java.lang.RuntimeException: Unable to read counter pig.counters.counter_4929375455335572575_-1
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.addRank(PORank.java:161)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PORank.getNext(PORank.java:134)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:308)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSplit.getNext(POSplit.java:214)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:283)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:278)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:157)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:673)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:324)
at org.apache.hadoop.mapred.Child$4.run(Child.java:275)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1340)
at org.apache.hadoop.mapred.Child.main(Child.java:269)","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.11.1,PIG-3379,2013-07-16T13:37:26.000-05:00,Alias reuse in nested foreach causes PIG script to fail,"{code:title=temp.pig}
       
      
    
    {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);

  DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);

  GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
        
  GENERATE timeStamp;
describe A;
{code}
 
 {code}
   
    
 {code}
The following script fails:
{code:title=temp.pig}
Events = LOAD 'x' AS (eventTime:long, deviceId:chararray, eventName:chararray);
Events = FOREACH Events GENERATE eventTime, deviceId, eventName;
EventsPerMinute = GROUP Events BY (eventTime / 60000);
EventsPerMinute = FOREACH EventsPerMinute {
  DistinctDevices = DISTINCT Events.deviceId;
  nbDevices = SIZE(DistinctDevices);
DistinctDevices = FILTER Events BY eventName == 'xuaHeartBeat';
  nbDevicesWatching = SIZE(DistinctDevices);
GENERATE $0*60000 as timeStamp, nbDevices as nbDevices, nbDevicesWatching as nbDevicesWatching;
}
EventsPerMinute = FILTER EventsPerMinute BY timeStamp >= 0  AND timeStamp < 100000;
A = FOREACH EventsPerMinute GENERATE timeStamp;
describe A;
{code}
With the error:
{code}
2013-07-16 11:31:20,450 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1025: 
<file /home/xzhang/Documents/temp.pig, line 14, column 37> Invalid field projection.
Projected field [timeStamp] does not exist in schema: deviceId:chararray.
{code}
Using distinct alias name for the 2nd ""DistinctDevices"" fixes the problem.
As an observation, removing the last filter statement also fixes the problem.","src.org.apache.pig.parser.LogicalPlanBuilder
src.org.apache.pig.PigServer
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.expression.ProjectExpression"
CLASS,pig-0.11.1,PIG-3510,2013-10-09T18:02:45.000-05:00,New filter extractor fails with more than one filter statement,";
{code}
{code:title=one filter}
      ;
{code}
This is a regression from PIG-3461 - rewrite of partition filter optimizer.
Here is an example that demonstrates the problem:
{code:title=two filters}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001);
c = FILTER b BY (event_id == 419 OR event_id == 418);
{code}
{code:title=one filter}
b = FILTER a BY (dateint >= 20130901 AND dateint <= 20131001) AND (event_id == 419 OR event_id == 418);
{code}
Both dateint and event_id are partition columns.
push down whole expression for filter case push down for filter case",src.org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer
CLASS,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
the problem is in the reduce method itself: on line 60 ( return input.getCentroid(); )
similar to line 81.
java.lang.NullPointerException
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)
at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133)
at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)
it happens every time the REDUCE_STREAMING_KMEANS is set to true.",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer
CLASS,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
I'm not sure if I'm doing something wrong here, or if ClusterDumper does
not support my (fairly simple) use case
I had a repository of 500K documents, for which I generated the input
vectors and a dictionary using some custom code (not seq2sparse etc).
I hashed the features with max size 5M (because I didn't know how many
features were in the dataset and wanted to minimize collisions).
The kmeans ran fine and generate sensible looking results, but when I tried
to run ClusterDumper I got the following error:
#bash> bin/mahout clusterdump -dt sequencefile -d
completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*
-i test-kmeans/clusters-19 -b 10 -n 10 -sp 10 -o ~/test-kmeans-out
Running on hadoop, using /usr/bin/hadoop and HADOOP_CONF_DIR=
MAHOUT-JOB: /opt/mahout-distribution-0.7/mahout-examples-0.7-job.jar
13/05/17 08:26:41 INFO common.AbstractJob: Command line arguments:
{--dictionary=[completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*],
--dictionaryType=[sequencefile],
--distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure],
--endPhase=[2147483647], --input=[test-kmeans/clusters-19],
--numWords=[10], --output=[/usr/share/tomcat6/test-kmeans-out],
--outputFormat=[TEXT], --samplePoints=[10], --startPhase=[0],
--substring=[10], --tempDir=[temp]}
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 698948
at
org.apache.mahout.clustering.AbstractCluster.formatVector(AbstractCluster.java:350)
at
org.apache.mahout.clustering.AbstractCluster.asFormatString(AbstractCluster.java:306)
at
org.apache.mahout.utils.clustering.ClusterDumperWriter.write(ClusterDumperWriter.java:54)
at
org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:169)
at
org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:156)
at
org.apache.mahout.utils.clustering.ClusterDumper.printClusters(ClusterDumper.java:187)
at
org.apache.mahout.utils.clustering.ClusterDumper.run(ClusterDumper.java:153)
(...)
The error is when it tries to access the dictionary for the feature with
index 698948
Looking at the dictionary loading code (
http://grepcode.com/file/repo1.maven.org/maven2/org.apache.mahout/mahout-integration/0.7/org/apache/mahout/utils/vectors/VectorHelper.java#VectorHelper.loadTermDictionary%28java.io.File%29
-  checked 0.8 and it hasn't changed)
It looks like the dictionary array is sized for the number of unique
keywords, not the highest index:
OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
After I ran my custom dictionary/feature generation code I discovered I
only had 517,327 unique features, therefore it is not surprising it would
die on an index >= 517327 (though I don't understand why it didn't die when trying to load the dictionary file)
Is there any reason why the VectorHelper code should not create a
dictionary array that has size the highest index read from the dictionary
sequence file (which can be easily calculated during the preceding loop)?
Or am I misunderstanding something?
reduce hash size not know number of features run job
Alex Piggott
IKANOW",integration.src.main.java.org.apache.mahout.utils.vectors.VectorHelper
CLASS,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}

 {Code}

  StreamingKMeansThread.call()

 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }

    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
{Code}
java.lang.IllegalArgumentException: Must have nonzero number of training and test vectors.
Asked for %.1f %% of %d vectors for test [10.000000149011612, 0]
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:120)
at org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176)
at org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)
{Code}
The issue is caused by the following code in StreamingKMeansThread.call()
{Code}
Iterator<Centroid> datapointsIterator = datapoints.iterator();
if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) { estimatePoints.add(datapointsIterator.next());
} estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
}
StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
while (datapointsIterator.hasNext()) { clusterer.cluster(datapointsIterator.next());
}
{Code}
The code is using the same iterator twice, and it fails on the second use for obvious reasons.",core.src.main.java.org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread
CLASS,zookeeper-3.4.5,ZOOKEEPER-1535,2012-08-14T18:56:40.000-05:00,ZK Shell/Cli re-executes last command on exit,"{{ctrl+d}}   {{ls}}   {{ctrl+d}}   {{ls}}  
 {noformat}
 
 {noformat}
In the ZK 3.4.3 release's version of zkCli.sh, the last command that was executed is *re*-executed when you {{ctrl+d}} out of the shell.
In the snippet below, {{ls}} is executed, and then {{ctrl+d}} is triggered (inserted below to illustrate), the output from {{ls}} appears again, due to the command being re-run.
{noformat}
[zk: zookeeper.example.com:2181(CONNECTED) 0] ls /blah
[foo]
[zk: zookeeper.example.com:2181(CONNECTED) 1] <ctrl+d> [foo]
$
{noformat}",src.java.main.org.apache.zookeeper.ZooKeeperMain
CLASS,zookeeper-3.4.5,ZOOKEEPER-1619,2013-01-11T09:57:16.000-06:00,Allow spaces in URL,"{code}
 
 {code}

 
 {code}
 
 {code}
not allow spaces in url
This format will work.
{code}
10.10.1.1:2181,10.10.1.2:2181/usergrid
{code}
This format will not (notice the spaces around the comma)
{code}
10.10.1.1:2181 , 10.10.1.2:2181/usergrid
{code}",src.java.main.org.apache.zookeeper.client.ConnectStringParser
CLASS,zookeeper-3.4.5,ZOOKEEPER-1700,2013-05-07T19:43:31.000-05:00,FLETest consistently failing - setLastSeenQuorumVerifier seems to be hanging,"{noformat}
   
  
    
    
          
      
    
    
  
  
  
  
      
  
  
     
      
      
    
    
 {noformat}
I'm consistently seeing a failure on my laptop when running the FLETest ""testJoin"" test.
see following log from test
add few log messages to setLastSeenQuorumVerifier add few log messages around call turn on debug
Note: I've applied ZOOKEEPER-1324 to trunk code and then run this test but that doesn't seem to help.
Also note that this test is passing consistently when run against branch-3.4.
{noformat}
2013-05-07 17:35:57,859 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Follower@65] - FOLLOWING - LEADER ELECTION TOOK - 16
2013-05-07 17:35:57,859 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@436] - LEADING - LEADER ELECTION TOOK - 17
2013-05-07 17:35:57,863 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:FileTxnSnapLog@270] - Snapshotting: 0x0 to /home/phunt/dev/zookeeper-trunk/build/test/tmp/test3690487600947307322.junit.dir/version-2/snapshot.0
2013-05-07 17:35:57,873 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@269] - Follower sid: 0 : info : 0.0.0.0:11222:11223:participant;0.0.0.0:11221
2013-05-07 17:35:57,878 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@328] - Synchronizing with Follower sid: 0 maxCommittedLog=0x0 minCommittedLog=0x0 peerLastZxid=0x0
2013-05-07 17:35:57,878 [myid:] - DEBUG [LearnerHandler-/127.0.0.1:34262:LearnerHandler@395] - committedLog is empty but leader and follower are in sync, zxid=0x0
2013-05-07 17:35:57,878 [myid:] - INFO  [LearnerHandler-/127.0.0.1:34262:LearnerHandler@404] - Sending DIFF
2013-05-07 17:35:57,879 [myid:] - DEBUG [LearnerHandler-/127.0.0.1:34262:LearnerHandler@411] - Sending NEWLEADER message to 0
2013-05-07 17:35:57,880 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@331] - Getting a diff from the leader 0x0
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@457] - Learner received NEWLEADER message
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@460] - NEWLEADER calling configfromstring
2013-05-07 17:35:57,885 [myid:] - INFO  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:Learner@462] - NEWLEADER setting quorum verifier
2013-05-07 17:35:57,886 [myid:] - WARN  [QuorumPeer[myid=0]/0:0:0:0:0:0:0:0:11221:QuorumPeer@1218] - setLastSeenQuorumVerifier called with stale config 0. Current version: 0
2013-05-07 17:36:01,880 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@585] - Shutting down
2013-05-07 17:36:01,881 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:Leader@591] - Shutdown called
java.lang.Exception: shutdown Leader! reason: Waiting for a quorum of followers, only synced with sids: [ [1] ]
at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:591)
at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:487)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:949)
2013-05-07 17:36:01,881 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:ZooKeeperServer@398] - shutting down
2013-05-07 17:36:01,881 [myid:] - INFO  [LearnerCnxAcceptor-0.0.0.0/0.0.0.0:11225:Leader$LearnerCnxAcceptor@398] - exception while shutting down acceptor: java.net.SocketException: Socket closed
2013-05-07 17:36:01,882 [myid:] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@979] - PeerState set to LOOKING
2013-05-07 17:36:01,882 [myid:] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@863] - LOOKING
2013-05-07 17:36:01,883 [myid:] - DEBUG [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:11224:QuorumPeer@792] - Initializing leader election protocol...
{noformat}",src.java.test.org.apache.zookeeper.test.FLETest
CLASS,zookeeper-3.4.5,ZOOKEEPER-1781,2013-10-03T20:19:27.000-05:00,ZooKeeper Server fails if snapCount is set to 1,"int randRoll = r.nextInt(snapCount/2);
{code}
If snapCount is set to 1, ZooKeeper Server can start but it fails with the below error:
2013-10-02 18:09:07,600 [myid:1] - ERROR [SyncThread:1:SyncRequestProcessor@151] - Severe unrecoverable error, exiting java.lang.IllegalArgumentException: n must be positive
at java.util.Random.nextInt(Random.java:300)
at org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:93)
{code:title=org.apache.zookeeper.server.SyncRequestProcessor.java|borderStyle=solid}
91             // we do this in an attempt to ensure that not all ofthe servers
92             // in the ensemble take a snapshot at the same time
93             int randRoll = r.nextInt(snapCount/2);
{code}
I think this supposition is not bad because snapCount = 1 is not realistic setting...
But, it may be better to mention this restriction in documentation or add a validation in the source code.",src.java.main.org.apache.zookeeper.server.ZooKeeperServer
METHOD,bookkeeper-4.1.0,BOOKKEEPER-387,2012-09-04T04:27:35.000-05:00,BookKeeper Upgrade is not working.,"{code}
     
 {code}
I am trying to upgrade BK from 4.1.0 to 4.2.0, but it will log as ""Directory is current, no need to upgrade? even then it will continue and fail.
{code}
2012-09-03 17:25:12,468 - ERROR - [main:FileSystemUpgrade@229] - Error moving upgraded directories into place /home/BK4.1/bookkeeper1/ledger/upgradeTmp.2433718456734190 -> /home/BK4.1/bookkeeper1/ledger/current org.apache.commons.io.FileExistsException: Destination '/home/BK4.1/bookkeeper1/ledger/current' already exists
at org.apache.commons.io.FileUtils.moveDirectory(FileUtils.java:2304)
at org.apache.bookkeeper.bookie.FileSystemUpgrade.upgrade(FileSystemUpgrade.java:225)
at org.apache.bookkeeper.bookie.FileSystemUpgrade.main(FileSystemUpgrade.java:367)
{code}",org.apache.bookkeeper.bookie.UpgradeTest:testCommandLine()
CLASS,argouml-0.22,3923,2006-02-07T13:17:48.000-06:00,Problem importing Poseidon activity diagrams from XMI,"Collection actionStates = getModel().getAllActionStates();
  Iterator iterActionState = actionStates.iterator();
iterActionState.hasNext(); 
 ActionStateFacade actionState =
(ActionStateFacade) iterActionState.next();
There is a bug in Beta 3 which prevents you using the activity diagram for AndroMDA.
Here is what I've done:
1) Import an XMI from Poseidon, which works well with AndroMDA (the
PiggyBank example).
Everything went fine.
2) If I add my activity diagram under the use case diagram I always get a new activity graph, so I have 2 activity graphs alltogether.
not add activity diagram under imported activity graph
see screenshot
See: http://argouml.tigris.org/servlets/ReadMsg?list=dev&msgNo=19267
http://argouml.tigris.org/servlets/GetAttachment?list=dev&msgId=770688&attachId=1
3) This code, which works with Poseidon, won't work with ArgoUML:
Collection actionStates = getModel().
getAllActionStates();
for (Iterator iterActionState = actionStates.iterator();
iterActionState.hasNext();) {
ActionStateFacade actionState =
(ActionStateFacade) iterActionState.next();
4) Importing the activity diagram from Poseidon works and the result can be processed by AndroMDA but if you are making the activity diagram from the beginning with ArgoUML, it won't work because of the error above
(nr.
3).
So, it seems that ArgoUML still has a problem with activity diagram...
Thanks,
Lofi.",org.argouml.persistence.XMIParser
CLASS,lucene-4.0,LUCENE-4461,2012-10-05T10:21:38.000-05:00,Multiple FacetRequest with the same path creates inconsistent results,"FacetSearchParams facetSearchParams = new FacetSearchParams();
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
Multiple FacetRequest are getting merged into one creating wrong results in this case:
FacetSearchParams facetSearchParams = new FacetSearchParams();
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
		facetSearchParams.addFacetRequest(new CountFacetRequest(new CategoryPath(""author""), 10));
Problem can be fixed by defining hashcode and equals in certain way that Lucene recognize we are talking about different requests.
Attached test case.",org.apache.lucene.facet.search.StandardFacetsAccumulator
CLASS,lucene-4.0,LUCENE-4561,2012-11-15T10:06:09.000-06:00,DWPT assert tripped again,"{noformat}
   
 {noformat}

 
 {noformat}
   
  
    
    
    
            
    
    
            
    
    
               {field=DFR I(ne)1} 
                                                                                                                                                                                                                                                                                                                          
 {noformat}
trip spooky DWPT ram assert in http://jenkins.sd-datasolutions.de/job/Lucene-Solr-4.x-Linux/2472/
It reproduces for me:
{noformat}
ant test  -Dtestcase=TestBagOfPositions -Dtests.method=test -Dtests.seed=730E05D38A0E4AFA -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=de_DE_PREEURO -Dtests.timezone=America/Metlakatla -Dtests.file.encoding=UTF-8
{noformat}
{noformat}
[junit4:junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBagOfPositions -Dtests.method=test -Dtests.seed=730E05D38A0E4AFA -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=de_DE_PREEURO -Dtests.timezone=America/Metlakatla -Dtests.file.encoding=UTF-8
[junit4:junit4] ERROR   63.2s J0 | TestBagOfPositions.test <<<
[junit4:junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2163, name=Thread-1670, state=RUNNABLE, group=TGRP-TestBagOfPositions]
[junit4:junit4]    > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte
[junit4:junit4]    > 	at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160)
[junit4:junit4]    > 	at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111)
[junit4:junit4]    > Throwable #2: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2164, name=Thread-1671, state=RUNNABLE, group=TGRP-TestBagOfPositions]
[junit4:junit4]    > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte
[junit4:junit4]    > 	at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160)
[junit4:junit4]    > 	at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111)
[junit4:junit4]    > Throwable #3: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=2166, name=Thread-1673, state=RUNNABLE, group=TGRP-TestBagOfPositions]
[junit4:junit4]    > Caused by: java.lang.AssertionError: actual mem: 33763152 byte, expected mem: 33755888 byte, flush mem: 33610208, active mem: 152944, pending DWPT: 0, flushing DWPT: 2, blocked DWPT: 0, peakDelta mem: 67152 byte
[junit4:junit4]    > 	at __randomizedtesting.SeedInfo.seed([730E05D38A0E4AFA]:0)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.assertMemory(DocumentsWriterFlushControl.java:120)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriterFlushControl.doAfterDocument(DocumentsWriterFlushControl.java:187)
[junit4:junit4]    > 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:384)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1451)
[junit4:junit4]    > 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1126)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:201)
[junit4:junit4]    > 	at org.apache.lucene.index.RandomIndexWriter.addDocument(RandomIndexWriter.java:160)
[junit4:junit4]    > 	at org.apache.lucene.index.TestBagOfPositions$1.run(TestBagOfPositions.java:111)
[junit4:junit4]   2> NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {field=DFR I(ne)1}, locale=de_DE_PREEURO, timezone=America/Metlakatla
[junit4:junit4]   2> NOTE: Linux 3.2.0-32-generic amd64/IBM Corporation 1.6.0 (64-bit)/cpus=8,threads=1,free=55064736,total=63793152
[junit4:junit4]   2> NOTE: All tests run in this JVM: [Nested1, TestIndexWriterWithThreads, TestLevenshteinAutomata, TestBasicOperations, TestStressNRT, TestIndexWriterExceptions, TestConjunctions, TestMultiLevelSkipList, TestConcurrentMergeScheduler, TestIndexWriterOnDiskFull, TestBytesRef, TestThreadedForceMerge, TestDocument, TestCopyBytes, TestIndexWriterNRTIsCurrent, TestScoreCachingWrappingScorer, TestScorerPerf, TestDocValuesTypeCompatibility, TestPrefixCodedTerms, TestRollingBuffer, TestPhrasePrefixQuery, Test4GBStoredFields, TestCustomSearcherSort, TestExplanations, TestIndexInput, TestMultiThreadTermVectors, TestTermInfosReaderIndex, TestSearchAfter, Test2BPositions, TestField, TestSimpleAttributeImpl, TestFlushByRamOrCountsPolicy, TestSimilarityBase, TestByteSlices, TestFlex, TestRecyclingByteBlockAllocator, TestCrash, Test2BTerms, TestSearcherManager, TestDeterminism, TestDemo, TestSpanExplanationsOfNonMatches, TestSubScorerFreqs, TestDocIdSet, TestFieldValueFilter, TestSpanMultiTermQueryWrapper, TestTermVectors, TestSurrogates, TestSimilarity2, Nested1, TestParallelReaderEmptyIndex, TestShardSearching, TestBlockPostingsFormat3, TestPagedBytes, TestCustomNorms, Before3, Before3, TestSegmentReader, TestMatchAllDocsQuery, TestTopDocsCollector, TestNoMergeScheduler, TestDeletionPolicy, Nested1, ThrowInUncaught, TestMultiTermConstantScore, TestPhraseQuery, TestGraphTokenizers, TestBitVector, TestPerFieldPostingsFormat2, TestSegmentTermEnum, TestVersion, TestNGramPhraseQuery, TestBackwardsCompatibility3x, TestRegexpRandom2, TestDirectoryReaderReopen, TestCompoundFile, TestBlockPostingsFormat, TestNRTThreads, TestPayloads, TestTransactions, TestTermRangeQuery, TestSmallFloat, TestFSTs, TestNorms, TestLookaheadTokenFilter, TestDuelingCodecs, TestAtomicUpdate, TestTermsEnum, TestMultiMMap, TestTimeLimitingCollector, TestTopDocsMerge, TestNRTManager, TestArrayUtil, TestBufferedIndexInput, TestIndexWriterForceMerge, TestIndexWriterCommit, TestWeakIdentityMap, TestTypePromotion, TestSimpleExplanations, TestStressIndexing, TestSnapshotDeletionPolicy, TestNRTReaderWithThreads, TestTieredMergePolicy, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestNumericUtils, TestMultiValuedNumericRangeQuery, TestCharTermAttributeImpl, TestRollingUpdates, TestPrefixInBooleanQuery, TestBytesRefHash, TestRamUsageEstimatorOnWildAnimals, TestFieldCacheRangeFilter, TestPayloadSpans, TestMixedCodecs, TestSegmentTermDocs, TestFieldCacheSanityChecker, TestDoc, TestMergeSchedulerExternal, TestOmitTf, TestDisjunctionMaxQuery, TestSimpleSearchEquivalence, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, Nested, TestPayloadNearQuery, TestFilteredQuery, TestPayloadExplanations, TestDocsAndPositions, TestTransactionRollback, TestCompiledAutomaton, TestSentinelIntSet, TestIndexableField, TestBooleanQuery, TestAutomatonQuery, TestComplexExplanationsOfNonMatches, TestRegexpQuery, TestDocCount, TestSearchForDuplicates, TestFilteredSearch, NestedSetupChain, NestedTeardownChain, Nested, Nested, TestDateFilter, TestSpansAdvanced, TestConstantScoreQuery, TestDateTools, TestSearch, TestReaderClosed, TestElevationComparator, TestAutomatonQueryUnicode, TestDateSort, TestBinaryDocument, TestSpanFirstQuery, TestPriorityQueue, TestDocBoost, TestMockCharFilter, TestIsCurrent, TestPrefixFilter, TestBitUtil, TestVersionComparator, TestCachingTokenFilter, TestTermdocPerf, TestTerm, TestLucene40PostingsFormat, TestAllFilesHaveCodecHeader, TestBagOfPositions]
{noformat}",org.apache.lucene.index.DocumentsWriterFlushControl
CLASS,jedit-4.3,1193683,2005-05-02T09:22:25.000-05:00,"folding bug, text is in a black hole","{\{\{ test
aaaa
bbbb
cccc
\}
Hi, when you have some folded text \(folding closed\)
\{\{\{ test aaaa bbbb cccc
\}\}\}
Close it you'll get
\{\{\{ test \[4 lines\]
Delete one \{
\{\{ test
It seems really dangerous isn't it ?
type \ { \
hide text",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1571752,2006-10-05T21:26:12.000-05:00,'Add Explicit Fold'  in PHP mode - wrong comments,"{

\} 
 {\{\{  --&gt;
function foo\(\) \{

\} //\}\}\}
jEdit version: 4.3pre7
Java version: 1.6.0-beta2
Before 'Add Explicit fold' the content of buffer looks like this \('X' means selection boundaries\):
&lt;? php
Xfunction foo\(\) \{
\}X
/\* :folding=explicit:\*/
?&gt;
&lt;? php
&lt;\!
--\{\{\{  --&gt;
function foo\(\) \{
\} //\}\}\}
/\* :folding=explicit:\*/
?&gt;
If is between '&lt;? php' and 'function' empty line, then it works OK.",org.gjt.sp.jedit.textarea.TextArea
CLASS,jedit-4.3,1599709,2006-11-20T13:17:56.000-06:00,NPE with JEditBuffer and new indenting,"lt;ENTER&gt;
The indenting refactoring introduced some NullPointerExceptions.
I've ""catched"" one in r8096, but as stated in the commit message, it's probably an error further up.
r8096 should be reverted.
The problem seems to be that ctx.rules.getModeName\(\) returns null, while it should return the main rule name.
To reproduce:
Save an empty buffer as test.php.
Type:
\---------------
&lt;?
php
$foo = '&lt;ENTER&gt;
\---------------
When pressing ENTER, after the ""'"", the following exception gets thrown:
java.lang.NullPointerException
at org.gjt.sp.jedit.buffer.JEditBuffer.getIdealIndentForLine\(JEditBuffer.java:991\)
at org.gjt.sp.jedit.buffer.JEditBuffer.indentLine\(JEditBuffer.java:907\)
at org.gjt.sp.jedit.textarea.TextArea.insertEnterAndIndent\(TextArea.java:4327\)
at sun.reflect.GeneratedMethodAccessor23.invoke\(Unknown Source\)
at sun.reflect.DelegatingMethodAccessorImpl.invoke\(Unknown Source\)
at java.lang.reflect.Method.invoke\(Unknown Source\)
at bsh.Reflect.invokeMethod\(Reflect.java:134\)
at bsh.Reflect.invokeObjectMethod\(Reflect.java:80\)
at bsh.Name.invokeMethod\(Name.java:858\)
at bsh.BSHMethodInvocation.eval\(BSHMethodInvocation.java:75\)
at bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:102\)
at bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:47\)
at bsh.BSHBlock.evalBlock\(BSHBlock.java:130\)
at bsh.BSHBlock.eval\(BSHBlock.java:80\)
at bsh.BshMethod.invokeImpl\(BshMethod.java:362\)
at bsh.BshMethod.invoke\(BshMethod.java:258\)
at bsh.BshMethod.invoke\(BshMethod.java:186\)
at org.gjt.sp.jedit.BeanShell.runCachedBlock\(BeanShell.java:509\)
at org.gjt.sp.jedit.BeanShellAction.invoke\(BeanShellAction.java:76\)
at org.gjt.sp.jedit.gui.InputHandler.invokeAction\(InputHandler.java:415\)
at org.gjt.sp.jedit.gui.InputHandler.invokeAction\(InputHandler.java:381\)
at org.gjt.sp.jedit.gui.DefaultInputHandler.handleKey\(DefaultInputHandler.java:373\)
at org.gjt.sp.jedit.input.AbstractInputHandler.processKeyEventKeyStrokeHandling\(AbstractInputHandler.java:116\)
at org.gjt.sp.jedit.gui.InputHandler.processKeyEvent\(InputHandler.java:184\)
at org.gjt.sp.jedit.textarea.TextArea.processKeyEvent\(TextArea.java:4572\)
at java.awt.Component.processEvent\(Unknown Source\)
at java.awt.Container.processEvent\(Unknown Source\)
at java.awt.Component.dispatchEventImpl\(Unknown Source\)
at java.awt.Container.dispatchEventImpl\(Unknown Source\)
at java.awt.Component.dispatchEvent\(Unknown Source\)
at java.awt.KeyboardFocusManager.redispatchEvent\(Unknown Source\)
at java.awt.DefaultKeyboardFocusManager.dispatchKeyEvent\(Unknown Source\)
at java.awt.DefaultKeyboardFocusManager.preDispatchKeyEvent\(Unknown Source\)
at java.awt.DefaultKeyboardFocusManager.typeAheadAssertions\(Unknown Source\)
at java.awt.DefaultKeyboardFocusManager.dispatchEvent\(Unknown Source\)
at java.awt.Component.dispatchEventImpl\(Unknown Source\)
at java.awt.Container.dispatchEventImpl\(Unknown Source\)
at java.awt.Window.dispatchEventImpl\(Unknown Source\)
at java.awt.Component.dispatchEvent\(Unknown Source\)
at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
at java.awt.EventDispatchThread.run\(Unknown Source\)",org.gjt.sp.jedit.buffer.JEditBuffer
CLASS,jedit-4.3,1600401,2006-11-21T13:16:31.000-06:00,StringIndexOutOfBoundsException in TokenMarker,"lt;init&gt; 
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
      
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
   {12,39\} 
    
     lt;init&gt; 
      
      
      
      
      
      
      
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt; 
      
      
      
      
      
      
    
    
  
    
    
    
      
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     lt;init&gt;
After pressing ENTER in a rather long line in a .
php file get following exception
resolve highlight issue
\[error\] Buffer: Exception while sending buffer event to org.gjt.sp.jedit.textarea.BufferHandler@1484a8a :
\[error\] Buffer: java.lang.StringIndexOutOfBoundsException: String index out of range: 103
\[error\] Buffer:  at java.lang.String.&lt;init&gt;\(Unknown Source\)
\[error\] Buffer:  at org.gjt.sp.jedit.syntax.TokenMarker.handleRule\(TokenMarker.java:343\)
\[error\] Buffer:  at org.gjt.sp.jedit.syntax.TokenMarker.markTokens\(TokenMarker.java:155\)
\[error\] Buffer:  at org.gjt.sp.jedit.buffer.JEditBuffer.markTokens\(JEditBuffer.java:1234\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.ChunkCache.lineToChunkList\(ChunkCache.java:771\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.ChunkCache.getLineSubregionCount\(ChunkCache.java:266\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.DisplayManager.updateScreenLineCount\(DisplayManager.java:661\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.BufferHandler.doDelayedUpdate\(BufferHandler.java:327\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.BufferHandler.transactionComplete\(BufferHandler.java:287\)
\[error\] Buffer:  at org.gjt.sp.jedit.buffer.JEditBuffer.fireTransactionComplete\(JEditBuffer.java:2173\)
\[error\] Buffer:  at org.gjt.sp.jedit.buffer.JEditBuffer.endCompoundEdit\(JEditBuffer.java:1966\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.TextArea.insertEnterAndIndent\(TextArea.java:4331\)
\[error\] Buffer:  at sun.reflect.NativeMethodAccessorImpl.invoke0\(Native Method\)
\[error\] Buffer:  at sun.reflect.NativeMethodAccessorImpl.invoke\(Unknown Source\)
\[error\] Buffer:  at sun.reflect.DelegatingMethodAccessorImpl.invoke\(Unknown Source\)
\[error\] Buffer:  at java.lang.reflect.Method.invoke\(Unknown Source\)
\[error\] Buffer:  at bsh.Reflect.invokeMethod\(Reflect.java:134\)
\[error\] Buffer:  at bsh.Reflect.invokeObjectMethod\(Reflect.java:80\)
\[error\] Buffer:  at bsh.Name.invokeMethod\(Name.java:858\)
\[error\] Buffer:  at bsh.BSHMethodInvocation.eval\(BSHMethodInvocation.java:75\)
\[error\] Buffer:  at bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:102\)
\[error\] Buffer:  at bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:47\)
\[error\] Buffer:  at bsh.BSHBlock.evalBlock\(BSHBlock.java:130\)
\[error\] Buffer:  at bsh.BSHBlock.eval\(BSHBlock.java:80\)
\[error\] Buffer:  at bsh.BshMethod.invokeImpl\(BshMethod.java:362\)
\[error\] Buffer:  at bsh.BshMethod.invoke\(BshMethod.java:258\)
\[error\] Buffer:  at bsh.BshMethod.invoke\(BshMethod.java:186\)
\[error\] Buffer:  at org.gjt.sp.jedit.BeanShell.runCachedBlock\(BeanShell.java:509\)
\[error\] Buffer:  at org.gjt.sp.jedit.BeanShellAction.invoke\(BeanShellAction.java:76\)
\[error\] Buffer:  at org.gjt.sp.jedit.gui.InputHandler.invokeAction\(InputHandler.java:415\)
\[error\] Buffer:  at org.gjt.sp.jedit.gui.InputHandler.invokeAction\(InputHandler.java:381\)
\[error\] Buffer:  at org.gjt.sp.jedit.gui.DefaultInputHandler.handleKey\(DefaultInputHandler.java:373\)
\[error\] Buffer:  at org.gjt.sp.jedit.input.AbstractInputHandler.processKeyEventKeyStrokeHandling\(AbstractInputHandler.java:116\)
\[error\] Buffer:  at org.gjt.sp.jedit.gui.InputHandler.processKeyEvent\(InputHandler.java:184\)
\[error\] Buffer:  at org.gjt.sp.jedit.textarea.TextArea.processKeyEvent\(TextArea.java:4572\)
\[error\] Buffer:  at java.awt.Component.processEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.Container.processEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.Component.dispatchEventImpl\(Unknown Source\)
\[error\] Buffer:  at java.awt.Container.dispatchEventImpl\(Unknown Source\)
\[error\] Buffer:  at java.awt.Component.dispatchEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.KeyboardFocusManager.redispatchEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.DefaultKeyboardFocusManager.dispatchKeyEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.DefaultKeyboardFocusManager.preDispatchKeyEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.DefaultKeyboardFocusManager.typeAheadAssertions\(Unknown Source\)
\[error\] Buffer:  at java.awt.DefaultKeyboardFocusManager.dispatchEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.Component.dispatchEventImpl\(Unknown Source\)
\[error\] Buffer:  at java.awt.Container.dispatchEventImpl\(Unknown Source\)
\[error\] Buffer:  at java.awt.Window.dispatchEventImpl\(Unknown Source\)
\[error\] Buffer:  at java.awt.Component.dispatchEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] Buffer:  at java.awt.EventDispatchThread.run\(Unknown Source\)
\[error\] ExtensionManager: Error repainting line range \{12,39\}:
\[error\] ExtensionManager: java.lang.StringIndexOutOfBoundsException: String index out of range: 103
\[error\] ExtensionManager:  at java.lang.String.&lt;init&gt;\(Unknown Source\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.syntax.TokenMarker.handleRule\(TokenMarker.java:343\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.syntax.TokenMarker.markTokens\(TokenMarker.java:155\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.buffer.JEditBuffer.markTokens\(JEditBuffer.java:1234\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.lineToChunkList\(ChunkCache.java:771\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.updateChunksUpTo\(ChunkCache.java:646\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.getLineInfo\(ChunkCache.java:255\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ExtensionManager.paintScreenLineRange\(ExtensionManager.java:102\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.TextAreaPainter.paint\(TextAreaPainter.java:726\)
\[error\] ExtensionManager:  at javax.swing.JComponent.paintToOffscreen\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.BufferStrategyPaintManager.paint\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paint\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.JComponent.\_paintImmediately\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.JComponent.paintImmediately\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.seqPaintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.SystemEventQueueUtilities$ComponentWorkRequest.run\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.event.InvocationEvent.dispatch\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.run\(Unknown Source\)
\[error\] ExtensionManager: Error repainting line range \{12,39\}:
\[error\] ExtensionManager: java.lang.StringIndexOutOfBoundsException: String index out of range: 103
\[error\] ExtensionManager:  at java.lang.String.&lt;init&gt;\(Unknown Source\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.syntax.TokenMarker.handleRule\(TokenMarker.java:343\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.syntax.TokenMarker.markTokens\(TokenMarker.java:155\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.buffer.JEditBuffer.markTokens\(JEditBuffer.java:1234\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.lineToChunkList\(ChunkCache.java:771\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.updateChunksUpTo\(ChunkCache.java:646\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ChunkCache.getLineInfo\(ChunkCache.java:255\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.ExtensionManager.paintScreenLineRange\(ExtensionManager.java:102\)
\[error\] ExtensionManager:  at org.gjt.sp.jedit.textarea.Gutter.paintComponent\(Gutter.java:131\)
\[error\] ExtensionManager:  at javax.swing.JComponent.paint\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.JComponent.paintToOffscreen\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.BufferStrategyPaintManager.paint\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paint\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.JComponent.\_paintImmediately\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.JComponent.paintImmediately\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.RepaintManager.seqPaintDirtyRegions\(Unknown Source\)
\[error\] ExtensionManager:  at javax.swing.SystemEventQueueUtilities$ComponentWorkRequest.run\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.event.InvocationEvent.dispatch\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] ExtensionManager:  at java.awt.EventDispatchThread.run\(Unknown Source\)
\[error\] AWT-EventQueue-0: Exception in thread ""AWT-EventQueue-0""
\[error\] AWT-EventQueue-0: java.lang.StringIndexOutOfBoundsException: String index out of range: 103
\[error\] AWT-EventQueue-0:  at java.lang.String.&lt;init&gt;\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.syntax.TokenMarker.handleRule\(TokenMarker.java:343\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.syntax.TokenMarker.markTokens\(TokenMarker.java:155\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.buffer.JEditBuffer.markTokens\(JEditBuffer.java:1234\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.lineToChunkList\(ChunkCache.java:771\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.updateChunksUpTo\(ChunkCache.java:646\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.getLineInfo\(ChunkCache.java:255\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.Gutter.paintLine\(Gutter.java:544\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.Gutter.paintComponent\(Gutter.java:137\)
\[error\] AWT-EventQueue-0:  at javax.swing.JComponent.paint\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.JComponent.paintToOffscreen\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.BufferStrategyPaintManager.paint\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.RepaintManager.paint\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.JComponent.\_paintImmediately\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.JComponent.paintImmediately\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.RepaintManager.paintDirtyRegions\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.RepaintManager.seqPaintDirtyRegions\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.SystemEventQueueUtilities$ComponentWorkRequest.run\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.event.InvocationEvent.dispatch\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.run\(Unknown Source\)
\[error\] AWT-EventQueue-0: Exception in thread ""AWT-EventQueue-0""
\[error\] AWT-EventQueue-0: java.lang.StringIndexOutOfBoundsException: String index out of range: 103
\[error\] AWT-EventQueue-0:  at java.lang.String.&lt;init&gt;\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.syntax.TokenMarker.handleRule\(TokenMarker.java:343\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.syntax.TokenMarker.markTokens\(TokenMarker.java:155\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.buffer.JEditBuffer.markTokens\(JEditBuffer.java:1234\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.lineToChunkList\(ChunkCache.java:771\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.updateChunksUpTo\(ChunkCache.java:646\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.ChunkCache.getLineInfo\(ChunkCache.java:255\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.TextArea.invalidateLine\(TextArea.java:1145\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.TextArea.blinkCaret\(TextArea.java:2118\)
\[error\] AWT-EventQueue-0:  at org.gjt.sp.jedit.textarea.TextArea$CaretBlinker.actionPerformed\(TextArea.java:5907\)
\[error\] AWT-EventQueue-0:  at javax.swing.Timer.fireActionPerformed\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at javax.swing.Timer$DoPostEvent.run\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.event.InvocationEvent.dispatch\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventQueue.dispatchEvent\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpOneEventForFilters\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEventsForFilter\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEventsForHierarchy\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.pumpEvents\(Unknown Source\)
\[error\] AWT-EventQueue-0:  at java.awt.EventDispatchThread.run\(Unknown Source\)
\[debug\] PHPSideKickParser: Requesting sidekick complete
\[debug\] WorkThread: Running in work thread: \[id=265,run=org.gjt.sp.jedit.bufferio.BufferAutosaveRequest\[Y.php \(X\\\)\]\]
\[debug\] DockableWindowManager: Loading dockables from jeditresource:/ErrorList.jar\!/dockables.xml
\[debug\] EditBus: DockableWindowUpdate\[what=ACTIVATED,dockable=error-list,source=org.gjt.sp.jedit.gui.DockableWindowManager\[,0,0,1600x1081,invalid,layout=org.gjt.sp.jedit.gui.DockableLayout,alignmentX=0.0,alignmentY=0.0,border=,flags=9,maximumSize=,minimumSize=,preferredSize=\]\]",org.gjt.sp.jedit.syntax.TokenMarker
CLASS,jedit-4.3,1658252,2007-02-12T17:48:03.000-06:00,C mode: incorrect bracket matching in multi-line defines,"{ \
code;                         \
more code;                    \
even more code;               \
\}
Try this define:
\#define LONG\_MULTI\_LINE\_DEFINE \{ \
code;                         \
more code;                    \
even more code;               \
\}","org.gjt.sp.jedit.syntax.ParserRule
org.gjt.sp.jedit.syntax.XModeHandler
org.gjt.sp.jedit.syntax.XModeHandler.TagDecl
org.gjt.sp.jedit.syntax.TokenMarker"
CLASS,jedit-4.3,1724940,2007-05-24T15:02:18.000-05:00,typing in multiple select,"lt;body&gt;
  lt;p&gt;
 
 the &lt;p&gt;  
 lt;body&gt;
  lt;d&gt;
If I highlight multiple selections of text in the text area and then begin typing, only the first character of what I type is inserted in the selected areas \(except for where the cursor ended up after making the selection\).
For example if I have the text:
&lt;body&gt;
&lt;p&gt;
Some Text
&lt;/p&gt;
&lt;/body&gt;
and I highlight both p's in the &lt;p&gt; tags and then type ""div"" I end up with:
&lt;body&gt;
&lt;d&gt;
Some Text
&lt;/div&gt;
&lt;/body&gt;
I've attached a screenshot.",org.gjt.sp.jedit.textarea.BufferHandler
CLASS,jedit-4.3,1999448,2008-08-23T10:28:24.000-05:00,Unnecesarry fold expantion when folded lines are edited,"{\{\{ hello

something

\}
While testing the patch \#1999448, a problem was found.
But the patch was applied in r13404 to avoid more
serious black hole bugs.
This problem has now became a
bug.
\(Quoted from Matthieu's comment for patch \#1999448\)
if I use explicit fold, with this buffer
\{\{\{ hello
something
\}\}\}
I remove one ""l"" from hello.","org.gjt.sp.jedit.textarea.BufferHandler
org.gjt.sp.jedit.textarea.DisplayManager
org.gjt.sp.jedit.textarea.TextArea"
CLASS,jedit-4.3,2129419,2008-09-25T23:53:11.000-05:00,NPE in EditPane.setBuffer when quitting jEdit,"lt;init&gt;
When trying to quit jEdit, I get the following Null-Pointer-Exception, which is probably related to some files being changed/deleted \(due to a ""cvs up"" in the background\).
Previously to trying to quit jEdit, the ""Files have changed.
Reload?""
java.lang.NullPointerException
at org.gjt.sp.jedit.EditPane.setBuffer\(EditPane.java:136\)
at org.gjt.sp.jedit.View.setBuffer\(View.java:1009\)
at org.gjt.sp.jedit.View.showBuffer\(View.java:1484\)
at org.gjt.sp.jedit.View.showBuffer\(View.java:1042\)
at org.gjt.sp.jedit.gui.CloseDialog$ListHandler.valueChanged\(CloseDialog.java:233\)
at javax.swing.JList.fireSelectionValueChanged\(JList.java:1765\)
at javax.swing.JList$ListSelectionHandler.valueChanged\(JList.java:1779\)
at javax.swing.DefaultListSelectionModel.fireValueChanged\(DefaultListSelectionModel.java:167\)
at javax.swing.DefaultListSelectionModel.fireValueChanged\(DefaultListSelectionModel.java:147\)
at javax.swing.DefaultListSelectionModel.fireValueChanged\(DefaultListSelectionModel.java:194\)
at javax.swing.DefaultListSelectionModel.changeSelection\(DefaultListSelectionModel.java:388\)
at javax.swing.DefaultListSelectionModel.changeSelection\(DefaultListSelectionModel.java:398\)
at javax.swing.DefaultListSelectionModel.setSelectionInterval\(DefaultListSelectionModel.java:442\)
at javax.swing.JList.setSelectedIndex\(JList.java:2179\)
at org.gjt.sp.jedit.gui.CloseDialog.&lt;init&gt;\(CloseDialog.java:96\)
at org.gjt.sp.jedit.jEdit.closeAllBuffers\(jEdit.java:1871\)
at org.gjt.sp.jedit.jEdit.exit\(jEdit.java:2621\)
at sun.reflect.NativeMethodAccessorImpl.invoke0\(Native Method\)
at sun.reflect.NativeMethodAccessorImpl.invoke\(NativeMethodAccessorImpl.java:39\)
at sun.reflect.DelegatingMethodAccessorImpl.invoke\(DelegatingMethodAccessorImpl.java:25\)
at java.lang.reflect.Method.invoke\(Method.java:597\)
at org.gjt.sp.jedit.bsh.Reflect.invokeMethod\(Reflect.java:134\)
at org.gjt.sp.jedit.bsh.Reflect.invokeStaticMethod\(Reflect.java:98\)
at org.gjt.sp.jedit.bsh.Name.invokeMethod\(Name.java:871\)
at org.gjt.sp.jedit.bsh.BSHMethodInvocation.eval\(BSHMethodInvocation.java:75\)
at org.gjt.sp.jedit.bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:102\)
at org.gjt.sp.jedit.bsh.BSHPrimaryExpression.eval\(BSHPrimaryExpression.java:47\)
at org.gjt.sp.jedit.bsh.BSHBlock.evalBlock\(BSHBlock.java:130\)
at org.gjt.sp.jedit.bsh.BSHBlock.eval\(BSHBlock.java:80\)
at org.gjt.sp.jedit.bsh.BshMethod.invokeImpl\(BshMethod.java:362\)
at org.gjt.sp.jedit.bsh.BshMethod.invoke\(BshMethod.java:258\)
at org.gjt.sp.jedit.bsh.BshMethod.invoke\(BshMethod.java:186\)
at org.gjt.sp.jedit.BeanShellFacade.runCachedBlock\(BeanShellFacade.java:225\)
at org.gjt.sp.jedit.BeanShell.runCachedBlock\(BeanShell.java:441\)
at org.gjt.sp.jedit.BeanShellAction.invoke\(BeanShellAction.java:73\)
at org.gjt.sp.jedit.gui.InputHandler.invokeAction\(InputHandler.java:352\)
at org.gjt.sp.jedit.jEdit$4.invokeAction\(jEdit.java:3080\)
at org.gjt.sp.jedit.jEdit$4.invokeAction\(jEdit.java:3062\)
at org.gjt.sp.jedit.EditAction$Wrapper.actionPerformed\(EditAction.java:220\)
at javax.swing.AbstractButton.fireActionPerformed\(AbstractButton.java:1995\)
at javax.swing.AbstractButton$Handler.actionPerformed\(AbstractButton.java:2318\)
at javax.swing.DefaultButtonModel.fireActionPerformed\(DefaultButtonModel.java:387\)
at javax.swing.DefaultButtonModel.setPressed\(DefaultButtonModel.java:242\)
at javax.swing.AbstractButton.doClick\(AbstractButton.java:357\)
at javax.swing.plaf.basic.BasicMenuItemUI.doClick\(BasicMenuItemUI.java:1220\)
at javax.swing.plaf.basic.BasicMenuItemUI$Handler.mouseReleased\(BasicMenuItemUI.java:1261\)
at java.awt.AWTEventMulticaster.mouseReleased\(AWTEventMulticaster.java:272\)
at java.awt.Component.processMouseEvent\(Component.java:6041\)
at javax.swing.JComponent.processMouseEvent\(JComponent.java:3265\)
at java.awt.Component.processEvent\(Component.java:5806\)
at java.awt.Container.processEvent\(Container.java:2058\)
at java.awt.Component.dispatchEventImpl\(Component.java:4413\)
at java.awt.Container.dispatchEventImpl\(Container.java:2116\)
at java.awt.Component.dispatchEvent\(Component.java:4243\)
at java.awt.LightweightDispatcher.retargetMouseEvent\(Container.java:4322\)
at java.awt.LightweightDispatcher.processMouseEvent\(Container.java:3986\)
at java.awt.LightweightDispatcher.dispatchEvent\(Container.java:3916\)
at java.awt.Container.dispatchEventImpl\(Container.java:2102\)
at java.awt.Window.dispatchEventImpl\(Window.java:2440\)
at java.awt.Component.dispatchEvent\(Component.java:4243\)
at java.awt.EventQueue.dispatchEvent\(EventQueue.java:599\)
at java.awt.EventDispatchThread.pumpOneEventForFilters\(EventDispatchThread.java:273\)
at java.awt.EventDispatchThread.pumpEventsForFilter\(EventDispatchThread.java:183\)
at java.awt.EventDispatchThread.pumpEventsForHierarchy\(EventDispatchThread.java:173\)
at java.awt.EventDispatchThread.pumpEvents\(EventDispatchThread.java:168\)
at java.awt.EventDispatchThread.pumpEvents\(EventDispatchThread.java:160\)
at java.awt.EventDispatchThread.run\(EventDispatchThread.java:121\)",org.gjt.sp.jedit.gui.CloseDialog.ListHandler
CLASS,jabref-2.6,1631548,2007-01-09T14:20:57.000-06:00,"""Open last edited DB at startup"" depends on the working dir","{HOME\}
Hi there,
depend on working directory
Example:
pc03:~/at-work/Bibliography $ jabref my\_documents.bib
pc03:~/at-work/Bibliography $ cd .
.
pc03:~/at-work $ jabref
open HOME \
I suggest that JabRef stores the absolute path for the ""Open last edited..."" feature, even if a relative path was provided.
Since this setting is stored in a machine dependent configuration file \(~/.
java/.
userPrefs/net/sf/jabref/prefs.
xml\), this should be okay.
Another, more sophisticated approach, would be to store the home directory, the relative path and the absolute path and try to open \(1\) the absolute, \(2\) the relative path to the home directory and \(3\) the relative path as provided when opening JabRef.
Current office tools \(MS and OO\) are doing it that fault tolerant way.
kind regards
Bernd
p.s. this bug is found in JabRef 2.1",net.sf.jabref.JabRefFrame
METHOD,apache-nutch-2.1,NUTCH-1393,2012-06-13T18:38:39.000-05:00,Display consistent usage of GeneratorJob with 1.X,"{code}
 
  
  
  
  
  
 {code}
pass generate argument to nutch script begin generating fetchlists
An example is below
{code} lewis@lewis:~/ASF/nutchgora/runtime/local$ .
/bin/nutch generate
GeneratorJob: Selecting best-scoring urls due for fetch.
GeneratorJob: starting
GeneratorJob: filtering: true
GeneratorJob: done
GeneratorJob: generated batch id: 1339628223-1694200031
{code}
get usage params generate batch willy","org.apache.nutch.crawl.GeneratorJob:generate(long, long, boolean, boolean)
org.apache.nutch.crawl.GeneratorJob:run(String[])"
CLASS,openjpa-2.0.1,OPENJPA-1787,2010-09-10T11:23:51.000-05:00,Bean validation fails merging a new entity,"EntityManager em = entityManagerFactory.createEntityManager();
        Person person = new Person();
        person.setName(""Oliver"");                               // Employee.name is annotated @NotNull 
        person = em.merge(person);
The bean validation is not working correctly
If you try to merge a new entity.
EntityManager em = entityManagerFactory.createEntityManager();
        Person person = new Person();
        person.setName(""Oliver"");                               // Employee.name is annotated @NotNull 
        person = em.merge(person);
get ConstraintValidationException","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.kernel.AttachStrategy
org.apache.openjpa.integration.validation.TestValidationGroups"
CLASS,openjpa-2.0.1,OPENJPA-1903,2010-12-06T13:05:34.000-06:00,Some queries only work the first time they are executed,"@Entity
@IdClass(MandantAndNameIdentity.class)
public class Website {
    @Id
    private String mandant;
   
    @Id
    private String name;
...
}

 @Entity
@IdClass(WebsiteProduktDatumIdentity.class)
public class Preis {
    @Id
    @ManyToOne(cascade = CascadeType.MERGE)
    private Website website;

    @Id
    @Basic
    private String datum;
...
}

 
 em.getTransaction().begin();

        Website website = em.merge(new Website(""Mandant"", ""Website""));

        em.merge(new Preis(website, DATUM));
       
        em.getTransaction().commit();

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website.name = :website "", Preis.class);
       q.setParameter(""website"", website.getName());

 
 TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website = :website "", Preis.class);
        q.setParameter(""website"", website);
return data
I have reduced it to the code as much as I could into an Eclipse project available at http://ubuntuone.com/p/S9n/
This happens with OpenJPA 2.0.1 as well as the daily snapshot from 2010-12-05 and an out-of-process Derby database.
Basically I have two Entities which both use multiple Ids to produce the Primary Key, ""Preis"" contains a foreign key on ""Website"":
@Entity
@IdClass(MandantAndNameIdentity.class)
public class Website {
    @Id
    private String mandant;
   
    @Id
    private String name;
...
}
@Entity
@IdClass(WebsiteProduktDatumIdentity.class)
public class Preis {
    @Id
    @ManyToOne(cascade = CascadeType.MERGE)
    private Website website;
@Id
    @Basic
    private String datum;
...
}
I use the following to set up a website and a Preis:
em.getTransaction().
begin();
Website website = em.merge(new Website(""Mandant"", ""Website""));
em.merge(new Preis(website, DATUM));
       
        em.getTransaction().
commit();
Afterwards, if I run the query as follows:
TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website.name = :website "", Preis.class);
       q.setParameter(""website"", website.getName());
work time
However if I put the query as
TypedQuery<Preis> q = em.createQuery(
                ""select m from Preis m "" +
                ""where m.website = :website "", Preis.class);
        q.setParameter(""website"", website);
work ONCE not return ONCE
See testcase DataAccessVerifyTest for details.
Discussion on the mailinglist seems to indicate that this is a bug.",org.apache.openjpa.jdbc.kernel.PreparedQueryImpl
CLASS,openjpa-2.0.1,OPENJPA-1912,2011-01-03T13:48:09.000-06:00,enhancer generates invalid code if fetch-groups is activated,"@Entity
public abstract class AbstractGroup {
   ...
    @Temporal(TemporalType.TIMESTAMP)
    @TrackChanges
    private Date applicationBegin;
 ...
}

 
 @Entity
public class Group extends AbstractGroup {
...
}

 
 public void writeExternal(ObjectOutput objectoutput)
        throws IOException
     
 pcWriteUnmanaged(objectoutput);
        if(pcStateManager != null)
        {
            if(pcStateManager.writeDetached(objectoutput))
                return;
        } else
        {
            objectoutput.writeObject(pcGetDetachedState());
            objectoutput.writeObject(null);
        }
        objectoutput.writeObject(applicationBegin);
        objectoutput.writeObject(applicationEnd);
        objectoutput.writeObject(applicationLocked);
        objectoutput.writeObject(approvalRequired);
If openjpa.DetachState =fetch-groups is used, the enhancer will add a 'implements Externalizable' + writeExternal + readExternal.
externalize private members of given superclass
get runtime Exception access fields
Example:
@Entity public abstract class AbstractGroup {
...
@Temporal(TemporalType.TIMESTAMP)
@TrackChanges private Date applicationBegin;
...
}
and
@Entity public class Group extends AbstractGroup {
...
}
public void writeExternal(ObjectOutput objectoutput)
throws IOException
{ pcWriteUnmanaged(objectoutput);
if(pcStateManager !
= null)
{ if(pcStateManager.writeDetached(objectoutput))
return;
} else
{ objectoutput.writeObject(pcGetDetachedState());
objectoutput.writeObject(null);
} objectoutput.writeObject(applicationBegin);
objectoutput.writeObject(applicationEnd);
objectoutput.writeObject(applicationLocked);
objectoutput.writeObject(approvalRequired);
...",org.apache.openjpa.enhance.PCEnhancer
CLASS,openjpa-2.0.1,OPENJPA-1918,2011-01-06T08:11:24.000-06:00,MetaDataRepository.preload() ignores class loader returned by PersistenceUnitInfo.getClassLoader(),"PersistenceUnitInfo.getClassLoader() 
 MetaDataRepository.preload()      
 PersistenceUnitInfo.getClassLoader()    
  
   PersistenceProvider.createContainerEntityManagerFactory()  MetaDatRepository.preload()
We are using openjpa inside an OSGi container together with
openjpa.MetaDataRepository"" value=""Preload=true""
We pass the appliation class loeader as part of our PersistenceUnitInfo implementation by returning it from PersistenceUnitInfo.getClassLoader().
use context class loader from PersistenceUnitInfo leade PersistenceUnitInfo to ClassNotFoundExpcetions mention at end
A fix might be quite easily establihed by appending the return value of PersistenceUnitInfo.getClassLoader() to the list of claas loaders participating in the MultiClassLoader set up in
  
  MetaDataRepository.java:310ff
In the meanwhile, we are additionally setting our classloader as context loader during the creation of the EntityManagerFactory by PersistenceProvider.createContainerEntityManagerFactory(), but a fix in MetaDatRepository.preload() is highly appreciated.
TIA for fixing this,
Wolfgang
Stack trace:
org.osgi.service.blueprint.container.ComponentDefinitionException: Error when instantiating bean entityManagerFactory of class null
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:233)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:726)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintRepository.createAll(BlueprintRepository.java:147)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.instantiateEagerComponents(BlueprintContainerImpl.java:624)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.doRun(BlueprintContainerImpl.java:315)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BlueprintContainerImpl.run(BlueprintContainerImpl.java:213)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)[:1.6.0_20]
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)[:1.6.0_20]
at java.util.concurrent.FutureTask.run(FutureTask.java:166)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)[:1.6.0_20]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)[:1.6.0_20]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)[:1.6.0_20]
at java.lang.Thread.run(Thread.java:636)[:1.6.0_20]
Caused by: <openjpa-2.0.1-r422266:989424 fatal user error> org.apache.openjpa.persistence.ArgumentException: Unexpected error during early loading of entity metadata during initialization. See nested stacktrace for details.
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:331)
at org.apache.openjpa.persistence.PersistenceProviderImpl.preloadMetaDataRepository(PersistenceProviderImpl.java:280)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:211)
at org.apache.openjpa.persistence.PersistenceProviderImpl.createContainerEntityManagerFactory(PersistenceProviderImpl.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.container.AbstractServiceReferenceRecipe$JdkProxyFactory$1.invoke(AbstractServiceReferenceRecipe.java:632)
at $Proxy67.createContainerEntityManagerFactory(Unknown Source)
at org.clazzes.util.jpa.provider.EntityManagerFactoryFactory.newEntityManagerFactory(EntityManagerFactoryFactory.java:108)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[:1.6.0_20]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)[:1.6.0_20]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[:1.6.0_20]
at java.lang.reflect.Method.invoke(Method.java:616)[:1.6.0_20]
at org.apache.aries.blueprint.utils.ReflectionUtils.invoke(ReflectionUtils.java:221)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.invoke(BeanRecipe.java:844)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
at org.apache.aries.blueprint.container.BeanRecipe.getInstance(BeanRecipe.java:231)[7:org.apache.aries.blueprint:0.3.0.incubating-SNAPSHOT]
... 15 more
Caused by: java.security.PrivilegedActionException: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_20]
at org.apache.openjpa.meta.MetaDataRepository.preload(MetaDataRepository.java:326)
... 32 more
Caused by: java.lang.ClassNotFoundException: org.clazzes.fancymail.server.entities.EMail
at org.apache.openjpa.lib.util.MultiClassLoader.findClass(MultiClassLoader.java:216)
at java.lang.ClassLoader.loadClass(ClassLoader.java:321)[:1.6.0_20]
at java.lang.ClassLoader.loadClass(ClassLoader.java:266)[:1.6.0_20]
at java.lang.Class.forName0(Native Method)[:1.6.0_20]
at java.lang.Class.forName(Class.java:264)[:1.6.0_20]
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:233)
at org.apache.openjpa.lib.util.J2DoPrivHelper$4.run(J2DoPrivHelper.java:231)
... 34 more","org.apache.openjpa.meta.FieldMetaData
org.apache.openjpa.meta.MetaDataRepository
org.apache.openjpa.persistence.detach.NoVersionEntity"
CLASS,openjpa-2.0.1,OPENJPA-1928,2011-01-20T17:43:52.000-06:00,Resolving factory method does not allow method overriding,"@Factory 
 @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}

 public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}

 
 valueOf(String)  
 valueOf(String)
take different parameters take method annotate with @Factory not override method with method
not take type not take same name
For example:
        @Persistent(optional = false)
	@Column(name = ""STATUS"")
	@Externalizer(""getName"")
	@Factory(""valueOf"")
	public OrderStatus getStatus() {
		return this.status;
	}
public class OrderStatus {
   public static OrderStatus valueOf(final int ordinal) {
        return valueOf(ordinal, OrderStatus.class);
    }
    
    public static OrderStatus valueOf(final String name) {
        return valueOf(name, OrderStatus.class);
    }
}
The provided patches fix this defect by applying the method invocation conversion rules from the Java Language Specification, 3rd Ed.
This means that widening primitive, boxing and unboxing conversions are all respected.",org.apache.openjpa.meta.FieldMetaData
CLASS,openjpa-2.0.1,OPENJPA-1986,2011-04-27T11:44:53.000-05:00,Extra queries being generated when cascading a persist,"@Entity
public class CascadePersistEntity implements Serializable {
    private static final long serialVersionUID = -8290604110046006897L;

    @Id
    long id;

    @OneToOne(cascade = CascadeType.ALL)
    CascadePersistEntity other;
...
}

 
 CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);
find scenario cascade persist to new entity generate extra queries while cascading
See the following example:
@Entity
public class CascadePersistEntity implements Serializable {
    private static final long serialVersionUID = -8290604110046006897L;
@Id
    long id;
@OneToOne(cascade = CascadeType.ALL)
    CascadePersistEntity other;
...
}
and the following scenario:
CascadePersistEntity cpe1 = new CascadePersistEntity(1);
CascadePersistEntity cpe2 = new CascadePersistEntity(2);
cpe1.setOther(cpe2);
em.persist(cpe1);
The extra select is what I'm going to get rid of with this JIRA.","org.apache.openjpa.kernel.BrokerImpl
org.apache.openjpa.conf.Compatibility
org.apache.openjpa.kernel.SingleFieldManager"
METHOD,lang,LANG-363,2007-10-23T07:12:48.000-05:00,"StringEscapeUtils.escapeJavaScript() method did not escape '/' into '\/', it will make IE render page uncorrectly","document.getElementById(""test"")   document.getElementById(""test"") 
  
 String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);
For example, document.getElementById(""test"").
make IE render page uncorrect
value = '<script>alert(\'aaa\');<\/script>';
Btw, Spring's JavascriptEscape behavor is correct.
Try  to run below codes, you will find the difference:
  String s = ""<script>alert('aaa');</script>"";
  String str = org.springframework.web.util.JavaScriptUtils.javaScriptEscape(s);
  System.out.println(""Spring JS Escape : ""+str);
  str = org.apache.commons.lang.StringEscapeUtils.escapeJavaScript(s);
  System.out.println(""Apache Common Lang JS Escape : ""+ str);","org.apache.commons.lang.StringEscapeUtils:escapeJavaStyleString(Writer, String, boolean)"
METHOD,lang,LANG-477,2009-01-09T10:05:53.000-06:00,ExtendedMessageFormat: OutOfMemory with custom format registry and a pattern containing single quotes,"{code:title=ExtendedMessageFormatTest.java|borderStyle=solid}

 private static Map<String, Object> formatRegistry = new HashMap<String, Object>();    
     static {
        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());
    }
    
     public static void main(String[] args) {
        ExtendedMessageFormat mf = new ExtendedMessageFormat(""it''s a {dummy} 'test'!"", formatRegistry);
        String formattedPattern = mf.format(new String[] {""great""});
        System.out.println(formattedPattern);
    }
 
 {code}

 
 {code:title=ExtendedMessageFormat.java|borderStyle=solid}
 
 if (escapingOn && c[start] == QUOTE) {
        return appendTo == null ? null : appendTo.append(QUOTE);
}

WORKING:
if (escapingOn && c[start] == QUOTE) {
        next(pos);
        return appendTo == null ? null : appendTo.append(QUOTE);
}
{code}
use ExtendedMessageFormat with custom format registry conatine single quotes
Example that will cause error:
{code:title=ExtendedMessageFormatTest.java|borderStyle=solid}
private static Map<String, Object> formatRegistry = new HashMap<String, Object>();    
    static {
        formatRegistry.put(DummyFormatFactory.DUMMY_FORMAT, new DummyFormatFactory());
    }
    
    public static void main(String[] args) {
        ExtendedMessageFormat mf = new ExtendedMessageFormat(""it''s a {dummy} 'test'!""
, formatRegistry);
        String formattedPattern = mf.format(new String[] {""great""});
        System.out.println(formattedPattern);
    }
}
{code}
The following change starting at line 421 on the 2.4 release seems to fix the problem:
{code:title=ExtendedMessageFormat.java|borderStyle=solid}
CURRENT (Broken):
if (escapingOn && c[start] == QUOTE) {
        return appendTo == null ?
null : appendTo.append(QUOTE);
}
WORKING:
if (escapingOn && c[start] == QUOTE) {
        next(pos);
        return appendTo == null ?
null : appendTo.append(QUOTE);
}
{code}","org.apache.commons.lang.text.ExtendedMessageFormat:appendQuotedString(String, ParsePosition, StringBuffer, boolean)"
METHOD,lang,LANG-480,2009-01-20T17:36:44.000-06:00,StringEscapeUtils.escapeHtml incorrectly converts unicode characters above U+00FFFF into 2 characters,"import org.apache.commons.lang.*;

public class J2 {
    public static void main(String[] args) throws Exception {
        // this is the utf8 representation of the character:
        // COUNTING ROD UNIT DIGIT THREE
        // in unicode
        // codepoint: U+1D362
        byte[] data = new byte[] { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 };

        //output is: &amp;#55348;&amp;#57186;
        // should be: &amp;#119650;
        System.out.println(""'"" + StringEscapeUtils.escapeHtml(new String(data, ""UTF8"")) + ""'"");
    }
}
represent characters as characters convert characters
The following test displays the problem quite nicely:
import org.apache.commons.lang.
*;
public class J2 {
    public static void main(String[] args) throws Exception {
        // this is the utf8 representation of the character:
        // COUNTING ROD UNIT DIGIT THREE
        // in unicode
        // codepoint: U+1D362
        byte[] data = new byte[] { (byte)0xF0, (byte)0x9D, (byte)0x8D, (byte)0xA2 };
//output is: &amp;#55348;&amp;#57186;
        // should be: &amp;#119650;
        System.out.println(""'"" + StringEscapeUtils.escapeHtml(new String(data, ""UTF8"")) + ""'"");
    }
}
Should be very quick to fix, feel free to drop me an email if you want a patch.","org.apache.commons.lang.Entities:escape(Writer, String)"
METHOD,lang,LANG-538,2009-10-16T16:47:39.000-05:00,DateFormatUtils.format does not correctly change Calendar TimeZone in certain situations,"Calenar.getTime()    
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";

    // more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)
    // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);


    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}

 
 {noformat}
   public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
    cal.getTime();

    FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
 {noformat}
not change Calendars fields construct Calendar object in certain ways
Calling Calenar.getTime() seems to fix this problem.
While this is probably a bug in the JDK, it would be nice if DateFormatUtils was smart enough to detect/resolve this problem.
For example, the following unit test fails:
{noformat}
  public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
// more commonly constructed with: cal = new GregorianCalendar(2009, 9, 16, 8, 42, 16)
    // for the unit test to work in any time zone, constructing with GMT-8 rather than default locale time zone
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
{noformat}
However, this unit test passes:
{noformat}
  public void testFormat_CalendarIsoMsZulu() {
    final String dateTime = ""2009-10-16T16:42:16.000Z"";
    GregorianCalendar cal = new GregorianCalendar(TimeZone.getTimeZone(""GMT-8""));
    cal.clear();
    cal.set(2009, 9, 16, 8, 42, 16);
    cal.getTime();
FastDateFormat format = FastDateFormat.getInstance(""yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"", TimeZone.getTimeZone(""GMT""));
    assertEquals(""dateTime"", dateTime, format.format(cal));
  }
{noformat}","org.apache.commons.lang3.time.FastDateFormat:format(Calendar, StringBuffer)"
METHOD,lang,LANG-552,2009-11-09T12:40:57.000-06:00,StringUtils replaceEach - Bug or Missing Documentation,"{code}
 import static org.junit.Assert.assertEquals;

import org.apache.commons.lang.StringUtils;
import org.junit.Test;


public class StringUtilsTest {

	@Test
	public void replaceEach(){
		String original = ""Hello World!"";
		String[] searchList = {""Hello"", ""World""};
		String[] replacementList = {""Greetings"", null};
		String result = StringUtils.replaceEach(original, searchList, replacementList);
		assertEquals(""Greetings !"", result);
		//perhaps this is ok as well
                //assertEquals(""Greetings World!"", result);
                //or even
		//assertEquals(""Greetings null!"", result);
	}

	
}
 {code}
The following Test Case for replaceEach fails with a null pointer exception.
The use case is that i will stuff Values into the replacementList of which I do not want to check whether they are null.
I admit the use case is not perfect, because it is unclear what happens on the replace.
I outlined three expectations in the test case, of course only one should be met.
If it is decided that none of them should be possible, I propose to update the documentation with what happens when null is passed as replacement string
{code} import static org.junit.Assert.assertEquals;
import org.apache.commons.lang.StringUtils;
import org.junit.Test;
public class StringUtilsTest {
@Test public void replaceEach(){
String original = ""Hello World!""
;
String[] searchList = {""Hello"", ""World""};
String[] replacementList = {""Greetings"", null};
String result = StringUtils.replaceEach(original, searchList, replacementList);
assertEquals(""Greetings !""
, result);
//perhaps this is ok as well
//assertEquals(""Greetings World!""
, result);
//or even
//assertEquals(""Greetings null!""
, result);
}
}
{code}","org.apache.commons.lang3.StringUtils:replaceEach(String, String[], String[], boolean, int)"
METHOD,lang,LANG-645,2010-08-20T14:11:08.000-05:00,FastDateFormat.format() outputs incorrect week of year because locale isn't respected,"format()     
  
 {code}
 import java.util.Calendar;
import java.util.Date;
import java.util.Locale;
import java.text.SimpleDateFormat;

import org.apache.commons.lang.time.FastDateFormat;

public class FastDateFormatWeekBugDemo {
    public static void main(String[] args) {
        Locale.setDefault(new Locale(""en"", ""US""));
        Locale locale = new Locale(""sv"", ""SE"");

        Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome
        cal.set(2010, 0, 1, 12, 0, 0);
        Date d = cal.getTime();
        System.out.println(""Target date: "" + d);

        FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
 {code}
  Locale.setDefault()
not respect locale send locale on creation output in year
use settings of system locale use settings for firstDayOfWeek use settings for minimalDaysInFirstWeek result firstDayOfWeek in incorrect week number result minimalDaysInFirstWeek in incorrect week number
Here is a simple test program to demonstrate the problem by comparing with SimpleDateFormat, which gets the week number right:
{code}
import java.util.Calendar;
import java.util.Date;
import java.util.Locale;
import java.text.SimpleDateFormat;
import org.apache.commons.lang.time.FastDateFormat;
public class FastDateFormatWeekBugDemo {
    public static void main(String[] args) {
        Locale.setDefault(new Locale(""en"", ""US""));
        Locale locale = new Locale(""sv"", ""SE"");
Calendar cal = Calendar.getInstance(); // setting locale here doesn't change outcome
        cal.set(2010, 0, 1, 12, 0, 0);
        Date d = cal.getTime();
        System.out.println(""Target date: "" + d);
FastDateFormat fdf = FastDateFormat.getInstance(""EEEE', week 'ww"", locale);
        SimpleDateFormat sdf = new SimpleDateFormat(""EEEE', week 'ww"", locale);
        System.out.println(""FastDateFormat:   "" + fdf.format(d)); // will output ""FastDateFormat:   fredag, week 01""
        System.out.println(""SimpleDateFormat: "" + sdf.format(d)); // will output ""SimpleDateFormat: fredag, week 53""
    }
}
{code}
If sv/SE is passed to Locale.setDefault() instead of en/US, both FastDateFormat and SimpleDateFormat output the correct week number.",org.apache.commons.lang3.time.FastDateFormat:format(Date)
METHOD,lang,LANG-662,2010-12-06T22:40:30.000-06:00,"org.apache.commons.lang3.math.Fraction does not reduce (Integer.MIN_VALUE, 2^k)","class Fraction    
    
 
  public void testReducedFactory_int_int()  
 
  f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2);
		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator());
		assertEquals(1, f.getDenominator());

	 public void testReduce()  
 
  f = Fraction.getFraction(Integer.MIN_VALUE, 2);
		result = f.reduce();
		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator());
		assertEquals(1, result.getDenominator());
{code}
The greatestCommonDivisor method in class Fraction does not find the gcd of Integer.MIN_VALUE and 2^k, and this case can be triggered by taking Integer.MIN_VALUE as the numerator.
Note that the case of taking Integer.MIN_VALUE as the denominator is handled explicitly in the getReducedFraction factory method.
{code:title=FractionTest.java|borderStyle=solid}
	// additional test cases
	public void testReducedFactory_int_int() {
		// ...
		f = Fraction.getReducedFraction(Integer.MIN_VALUE, 2);
		assertEquals(Integer.MIN_VALUE / 2, f.getNumerator());
		assertEquals(1, f.getDenominator());
public void testReduce() {
		// ...
		f = Fraction.getFraction(Integer.MIN_VALUE, 2);
		result = f.reduce();
		assertEquals(Integer.MIN_VALUE / 2, result.getNumerator());
		assertEquals(1, result.getDenominator());
{code}","org.apache.commons.lang3.math.Fraction:greatestCommonDivisor(int, int)"
METHOD,lang,LANG-710,2011-07-01T20:57:30.000-05:00,"StringIndexOutOfBoundsException when calling unescapeHtml4(""&#03"")","unescapeHtml4()
When calling unescapeHtml4() on the String ""&#03"" (or any String that contains these characters) an Exception is thrown:
Exception in thread ""main"" java.lang.StringIndexOutOfBoundsException: String index out of range: 4
at java.lang.String.charAt(String.java:686)
at org.apache.commons.lang3.text.translate.NumericEntityUnescaper.translate(NumericEntityUnescaper.java:49)
at org.apache.commons.lang3.text.translate.AggregateTranslator.translate(AggregateTranslator.java:53)
at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:88)
at org.apache.commons.lang3.text.translate.CharSequenceTranslator.translate(CharSequenceTranslator.java:60)
at org.apache.commons.lang3.StringEscapeUtils.unescapeHtml4(StringEscapeUtils.java:351)","org.apache.commons.lang3.text.translate.NumericEntityUnescaper:translate(CharSequence, int, Writer)"
METHOD,lang,LANG-788,2012-02-11T12:36:48.000-06:00,SerializationUtils throws ClassNotFoundException when cloning primitive classes,"{noformat}
 import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;


public class SerializationUtilsTest {

	
	@Test
	public void primitiveTypeClassSerialization(){
		Class<?> primitiveType = int.class;
		
		Class<?> clone = SerializationUtils.clone(primitiveType);
		assertEquals(primitiveType, clone);
	}
}
 {noformat} 

  
         
    
  
 {noformat}
         protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
            String name = desc.getName();
            try {
                return Class.forName(name, false, classLoader);
            } catch (ClassNotFoundException ex) {
            	try {
            	     return Class.forName(name, false, Thread.currentThread().getContextClassLoader());
            	} catch (Exception e) {
		     return super.resolveClass(desc);
		}
            }
        }
 {noformat}

   
 {noformat}
     protected Class<?> resolveClass(ObjectStreamClass desc)
	throws IOException, ClassNotFoundException
    {
	String name = desc.getName();
	try {
	    return Class.forName(name, false, latestUserDefinedLoader());
	} catch (ClassNotFoundException ex) {
	    Class cl = (Class) primClasses.get(name);
	    if (cl != null) {
		return cl;
	    } else {
		throw ex;
	    }
	}
    }
 {noformat}
contain reference to primitive class throw ClassNotFoundException clone object
{noformat} import org.apache.commons.lang3.SerializationUtils;
import org.junit.Test;
public class SerializationUtilsTest {
@Test public void primitiveTypeClassSerialization(){
Class<?> primitiveType = int.class;
Class<?> clone = SerializationUtils.clone(primitiveType);
assertEquals(primitiveType, clone);
}
}
{noformat}
The problem was already reported as a java bug http://bugs.sun.com/view_bug.do?bug_id=4171142 and ObjectInputStream is fixed since java version 1.4.
use ClassLoaderAwareObjectInputStream override resoleClass method without delegating override ClassLoaderAwareObjectInputStream without delegating delegate to super method
For example:
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc) throws IOException, ClassNotFoundException {
String name = desc.getName();
try { return Class.forName(name, false, classLoader);
} catch (ClassNotFoundException ex) { try { return Class.forName(name, false, Thread.currentThread().
getContextClassLoader());
} catch (Exception e) { return super.resolveClass(desc);
}
}
}
{noformat}
Here is the code in ObjectInputStream that fixed the java bug.
{noformat} protected Class<?> resolveClass(ObjectStreamClass desc)
throws IOException, ClassNotFoundException
{
String name = desc.getName();
try { return Class.forName(name, false, latestUserDefinedLoader());
} catch (ClassNotFoundException ex) {
Class cl = (Class) primClasses.get(name);
if (cl !
= null) { return cl;
} else { throw ex;
}
}
}
{noformat}","org.apache.commons.lang3.SerializationUtils:ClassLoaderAwareObjectInputStream(InputStream, ClassLoader)
org.apache.commons.lang3.SerializationUtils:resolveClass(ObjectStreamClass)"
METHOD,lang,LANG-832,2012-09-27T00:27:58.000-05:00,FastDateParser does not handle unterminated quotes correctly,"{IsNd}
not handle unterminated quotes as SimpleDateFormat
For example:
Format: 'd'd'
Date: d3
parse format
Pattern: d(\p{IsNd}++)",org.apache.commons.lang3.time.FastDateParser:init()
METHOD,lang,LANG-879,2013-03-18T21:46:29.000-05:00,"LocaleUtils test fails with new Locale ""ja_JP_JP_#u-ca-japanese"" of JDK7","import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;

import java.util.Locale;

import org.testng.annotations.Test;

import com.scispike.foundation.i18n.StringToLocaleConverter;

public class LocaleStringConverterTest {

	StringToLocaleConverter converter = new StringToLocaleConverter();

	public void testStringToLocale(Locale l) {
		String s = l.toString();

		assertThat(converter.convert(s), equalTo(l));
	}

	@Test
	public void testAllLocales() {

		Locale[] locales = Locale.getAvailableLocales();
		for (Locale l : locales) {
			testStringToLocale(l);
		}
	}
}


  
 import java.util.Locale;

import org.apache.commons.lang3.LocaleUtils;
import org.springframework.core.convert.converter.Converter;

public class StringToLocaleConverter implements Converter<String, Locale> {

	@Override
	public Locale convert(String source) {
		if (source == null) {
			return LocaleToStringConverter.DEFAULT;
		}
		return LocaleUtils.toLocale(source);
	}
}
The Test below fails with the following error on JDK7, but succeeds on JDK6:
testAllLocales
""java.lang.AssertionError:
Expected: <ja_JP_JP_#u-ca-japanese>
but: was <ja_JP_JP_#u-ca-japanese>
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
at com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testStringToLocale(LocaleStringConverterTest.java:20)
at com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testAllLocales(LocaleStringConverterTest.java:28)
at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:76)
at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:161)
at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:101)
at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:115)
at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
... Removed 25 stack frames
java.lang.AssertionError:
Expected: <ja_JP_JP_#u-ca-japanese>
but: was <ja_JP_JP_#u-ca-japanese>
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
at com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testStringToLocale(LocaleStringConverterTest.java:20)
at com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testAllLocales(LocaleStringConverterTest.java:28)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:80)
at org.testng.internal.Invoker.invokeMethod(Invoker.java:715)
at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:907)
at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1237)
at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)
at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)
at org.testng.TestRunner.privateRun(TestRunner.java:767)
at org.testng.TestRunner.run(TestRunner.java:617)
at org.testng.SuiteRunner.runTest(SuiteRunner.java:334)
at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)
at org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)
at org.testng.SuiteRunner.run(SuiteRunner.java:240)
at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:51)
at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:85)
at org.testng.TestNG.runSuitesSequentially(TestNG.java:1197)
at org.testng.TestNG.runSuitesLocally(TestNG.java:1122)
at org.testng.TestNG.run(TestNG.java:1030)
at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:76)
at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:161)
at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:101)
at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:115)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
""
org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testStringToLocale(LocaleStringConverterTest.java:20)
com.scispike.foundation.test.unit.i18n.LocaleStringConverterTest.testAllLocales(LocaleStringConverterTest.java:28)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:601)
org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:80)
org.testng.internal.Invoker.invokeMethod(Invoker.java:715)
org.testng.internal.Invoker.invokeTestMethod(Invoker.java:907)
org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1237)
org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)
org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)
org.testng.TestRunner.privateRun(TestRunner.java:767)
org.testng.TestRunner.run(TestRunner.java:617)
org.testng.SuiteRunner.runTest(SuiteRunner.java:334)
org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)
org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)
org.testng.SuiteRunner.run(SuiteRunner.java:240)
org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:51)
org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:85)
org.testng.TestNG.runSuitesSequentially(TestNG.java:1197)
org.testng.TestNG.runSuitesLocally(TestNG.java:1122)
org.testng.TestNG.run(TestNG.java:1030)
org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:76)
org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:161)
org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:101)
org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:115)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:601)
org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
========== Test
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.equalTo;
import java.util.Locale;
import org.testng.annotations.Test;
import com.scispike.foundation.i18n.StringToLocaleConverter;
public class LocaleStringConverterTest {
StringToLocaleConverter converter = new StringToLocaleConverter();
public void testStringToLocale(Locale l) {
		String s = l.toString();
assertThat(converter.convert(s), equalTo(l));
	}
@Test
	public void testAllLocales() {
Locale[] locales = Locale.getAvailableLocales();
		for (Locale l : locales) {
			testStringToLocale(l);
		}
	}
}
========== StringToLocaleConverter
import java.util.Locale;
import org.apache.commons.lang3.LocaleUtils;
import org.springframework.core.convert.converter.Converter;
public class StringToLocaleConverter implements Converter<String, Locale> {
@Override
	public Locale convert(String source) {
		if (source == null) {
			return LocaleToStringConverter.DEFAULT;
		}
		return LocaleUtils.toLocale(source);
	}
}",org.apache.commons.lang3.LocaleUtils:toLocale(String)
FILE,SWARM,SWARM-528,2016-06-22T02:53:46.000-05:00,swarm.http.port and swarm.port.offset do not work with @ArquillianResource URL baseURL,"@ArquillianResource 
  
 
 
 
 @ArquillianResource 
  
 
 
 
 @ArquillianResource
First Example
If you set the swarm port using either swarm.http.port or swarm.port.offset via arquillian.xml e.g.
<container qualifier=""wildfly-swarm"" default=""true"">
<configuration>
<property name=""javaVmArguments"">
-Dswarm.port.offset=1
</property>
</configuration>
</container>
start arquillian swarm container on specified port/offset
The problem is that if you use:
@ArquillianResource
private URL baseURL;
retrieve url return http
Second Example
If you set the port property in arquillian.xml
<container qualifier=""wildfly-swarm"" default=""true"">
<configuration>
<property name=""port"">8081</property>
</configuration>
</container>
start swarm container on 8080 and
@ArquillianResource
private URL baseURL;
Third Example
Attempting to combine the port property and the offset does not work either e.g.
<container qualifier=""wildfly-swarm"" default=""true"">
<configuration>
<property name=""javaVmArguments"">
-Dswarm.port.offset=1
</property>
<property name=""port"">8081</property>
</configuration>
</container>
start container
@ArquillianResource
private URL baseURL;
use swarm.http.port",org.wildfly.swarm.arquillian.resources.SwarmURLResourceProvider
FILE,SWARM,SWARM-486,2016-05-28T18:25:37.000-05:00,Can't load project-stages.yml on classpath with Arq,"classpath(src/main/resources)  
 
 
 container.withStageConfig(Paths.get(""/tmp"", ""external-project-stages.yml"").toUri().toURL())
not load yml with Arquillian tests not load yml on classpath(src/main/resources)
I attached the error log and the reproducer in 'Steps to Reproduce' section.
https://github.com/wildfly-swarm/wildfly-swarm-core/blob/1.0.0.CR3/container/api/src/main/java/org/wildfly/swarm/cli/CommandLine.java#L109 workaround
To load the yml explicitly like below.
container.withStageConfig(Paths.get(""/tmp"", ""external-project-stages.
yml"").
toUri().
toURL())",org.wildfly.swarm.container.ProjectStagesTest
FILE,SWARM,SWARM-863,2016-11-30T14:54:40.000-06:00,Version 2016.11.0 doesn't stop properly (with custom main class),"container = new Swarm(); // fractions being added here also




    container.start();




    container.deploy(...);






 
 container.stop();
We are using a custom main class whose main method reacts on a single argument: ""start"" or ""stop"".
Actually we are feeding that argument through Procrun (https://commons.apache.org/proper/commons-daemon/procrun.html).
Inside that main class we hold a field
private static org.wildfly.swarm.Swarm container
which we handle as following during startup:
container = new Swarm(); // fractions being added here also
container.start();
container.deploy(...);
The reaction on the ""stop"" signal is as easy as following:
container.stop();
stop Swarm service in version 2016.11.0 not shutdown Swarm container
do debug session find out debug session block JVM shutdown
With version 2016.10.0 everything works fine.
An example project can be found at https://github.com/seelenvirtuose/de.mwa.testing.wfs.
But I also have attached it as a zip.
de.mwa.testing.wfs-master.zip
Procrun can be downloaded at http://mirror.serversupportforum.de/apache//commons/daemon/binaries/windows/commons-daemon-1.0.15-bin-windows.zip
Steps to reproduce:
1) Clone the github project and start a ""mvn package"" to produce the uber-jar, which is located in the ""swarm"" sub-module of that project.
2) Copy that uber-jar and both procrun executables (""prunsrv.exe"" and ""prunmgr.exe"") into a test directory.
For 64-bit OS's use the file ""amd64/prunsrv.exe"" (inside that procrun zip file).
Rename the file ""prunsrv.exe"" to ""testing-wfs.exe"" and the file ""prunmgr.exe"" to ""testing-wfsw.exe"".
3) Install a Windows service by running the command ""testing-wfs.exe //IS"".
Now this service can be configured by running the ""testing-wfsw.exe"" (a Windows GUI Tool for that purpose).
4) Configure the service as following:
Tab Logging
Log path: <path-to-the-service-directory>\logs
Redirect Stdout: auto
Redirect Stderror: auto
Tab Java
Java Virtual Machine: Path to the ""jvm.dll"" of a JRE 8 (usually <path-to-jdk>\jre\bin\server\jvm.dll).Java Classpath: Full path to the uber-jar.Java Options: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=7777 (to enable remote debugging).
Tab Startup
Class: org.wildfly.swarm.bootstrap.Main
Method: main
Arguments: start
Mode: jvm
Tab Shutdown
Class: org.wildfly.swarm.bootstrap.Main
Method: main
Arguments: stop
Mode:jvm
5) Now start the service.
result //localhost:8080/hello "" in hello world response
have many threads
attach screenshot
7) Now stop the service.
spit out failure message after time hang in stopping attempt
show output
have many threads
see attached screenshot
Note, that I have other services, which show only two non-deamon threads after a shutdown attempt.
9) Killing the task ""testing-wfs.exe"" is the only way to stop the process completely.
Switching the Wildfly Swarm version to 2016.10.0 (in the POM of ""swarm"" module) makes it work great.
Starting and stopping run both smoothly.",org.wildfly.swarm.container.runtime.ServerBootstrapImpl
METHOD,derby-10.9.1.0,DERBY-5951,2012-10-16T10:33:53.000-05:00,Missing method exception raised when using Clobs with territory based collation,"db;create=true; 
   varchar( 32672 )  
  
  
  
 clobTable( a )   makeClob( 'a' )  
   varchar( 32672 )  
  
  
 clobTable( a )   makeClob( 'a' )  
     Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;   
   Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;     
  
 clobTable( a )   makeClob( 'a' )
use territory-based collation with clobs raise error invoke missing method
The following script shows this problem:
connect 'jdbc:derby:memory:db;create=true;collation=TERRITORY_BASED';
create function makeClob( contents varchar( 32672 ) ) returns clob
language java parameter style java no sql deterministic
external name 'org.apache.derbyTesting.functionTests.tests.lang.UserDefinedAggregatesTest.makeClob';
create table clobTable( a clob );
fail with java.lang.NoSuchMethodError exception
insert into clobTable( a ) values ( makeClob( 'a' ) );
connect 'jdbc:derby:memory:db1;create=true';
create function makeClob( contents varchar( 32672 ) ) returns clob
language java parameter style java no sql deterministic
external name 'org.apache.derbyTesting.functionTests.tests.lang.UserDefinedAggregatesTest.makeClob';
create table clobTable( a clob );
-- succeeds
insert into clobTable( a ) values ( makeClob( 'a' ) );
ERROR 38000: The exception 'java.lang.NoSuchMethodError: org.apache.derby.iapi.types.DataValueFactory.getClobDataValue(Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;I)Lorg/apache/derby/iapi/types/StringDataValue;' was thrown while evaluating an expression.
ERROR XJ001: Java exception: 'org.apache.derby.iapi.types.DataValueFactory.getClobDataValue(Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;I)Lorg/apache/derby/iapi/types/StringDataValue;: java.lang.NoSuchMethodError'.
...and here is the stack trace:
Tue Oct 16 08:27:23 PDT 2012 Thread[main,5,main] (XID = 172), (SESSIONID = 1), (DATABASE = memory:db), (DRDAID = null), Failed Statement is: -- fails with a java.lang.NoSuchMethodError exception
insert into clobTable( a ) values ( makeClob( 'a' ) )
ERROR 38000: The exception 'java.lang.NoSuchMethodError: org.apache.derby.iapi.types.DataValueFactory.getClobDataValue(Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;I)Lorg/apache/derby/iapi/types/StringDataValue;' was thrown while evaluating an expression.
at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
at org.apache.derby.iapi.error.StandardException.unexpectedUserException(Unknown Source)
at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
at org.apache.derby.impl.sql.execute.RowResultSet.getNextRowCore(Unknown Source)
at org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)
at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
at org.apache.derby.impl.tools.ij.ij.executeImmediate(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.doCatch(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.runScriptGuts(Unknown Source)
at org.apache.derby.impl.tools.ij.utilMain.go(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.go(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.mainCore(Unknown Source)
at org.apache.derby.impl.tools.ij.Main.main(Unknown Source)
at org.apache.derby.tools.ij.main(Unknown Source)
Caused by: java.lang.NoSuchMethodError: org.apache.derby.iapi.types.DataValueFactory.getClobDataValue(Ljava/sql/Clob;Lorg/apache/derby/iapi/types/StringDataValue;I)Lorg/apache/derby/iapi/types/StringDataValue;
at org.apache.derby.exe.ace50d80a4x013ax6a2fxb54bx00000467ed600.e0(Unknown Source)
... 17 more",org.apache.derby.iapi.types.CollatorSQLClob:getNewNull()
FILE,IO,IO-481,2015-06-19T18:19:48.000-05:00,org.apache.commons.io.FileUtils#waitFor waits too long,"public void testRealWallTime() 
{

        long start = System.currentTimeMillis();

        FileUtils.waitFor(new File(""""), 2);

        System.out.println(""elapsed = "" + (System.currentTimeMillis() - start));

    }
The timing algorithm is basically broken, since Thread.sleep is imprecise.
There is also a counter error in the looping code.
The following testcase will never run in less than 4 seconds on my machine public void testRealWallTime()
{
long start = System.currentTimeMillis();
FileUtils.waitFor(new File(""""), 2);
System.out.println(""elapsed = "" + (System.currentTimeMillis() - start));
}",org.apache.commons.io.FileUtils
FILE,eclipse-3.1,100807,2005-06-20T09:30:00.000-05:00,Source not found,"JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
  
 JavaModelManager.getZipFile(IPath) 
 
   
 JavaModelManager.closeZipFile(ZipFile) 
    
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile) 
  
 
 JavaModelManager.getZipFile(IPath) 
 
 
 JavaModelManager.closeZipFile(ZipFile)
(from bug 99526)
I tried 3.1RC3 and the fix worked but it did result in a new failure.
I turned on the debug flags via .
search options file for source file search following archives for source file
The source lookup path was set by using the
""Restore default"".
[reading    java/io/PrintStream.class]
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/rt.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/rt.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/rt.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/sunrsasign.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/sunrsasign.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/sunrsasign.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/jsse.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/jsse.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/jsse.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/jce.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/jce.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/jce.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/charsets.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/charsets.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/charsets.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/ext/sunjce_provider.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/sunjce_provider.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/sunjce_provider.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/ext/dnsns.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/dnsns.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/dnsns.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/ext/ldapsec.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/ldapsec.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/ldapsec.jar
Thread[Worker-17,5,main] OPENING class file Test.class [in <default> [in
/usr/local/jdk1.4.2/jre/lib/ext/localedata.jar [in test]]]
(Thread[Worker-17,5,main]) [JavaModelManager.getZipFile(IPath)] Creating
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/localedata.jar
(Thread[Worker-17,5,main]) [JavaModelManager.closeZipFile(ZipFile)] Closing
ZipFile on /usr/local/jdk1.4.2/jre/lib/ext/localedata.jar",org.eclipse.debug.internal.core.sourcelookup.containers.ContainerSourceContainer
FILE,eclipse-3.1,102427,2005-06-30T20:45:00.000-05:00,Cannot inspect/display static import methods,"public class Helper {
    public static int getValue() {...}
}
  
import static Helper.*;

public class Doer {
    public void doit() {
        int i = getValue();
    }
}
 
 getValue() 
 getValue()
Consider:
---
public class Helper {
public static int getValue() {...}
}
---
import static Helper.
*;
public class Doer {
public void doit() {
int i = getValue();
}
}
---
When debugging, if you select 'getValue()' in the method 'doit' and execute
display (or inspect) you get an error indicating that the method 'getValue()' is
not undefined for type Doer.",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,102778,2005-07-05T15:40:00.000-05:00,Scrapbook page doesn't work with enhanced for statement,"int[] tab = new int[] {1, 2, 3, 4, 5, 6, 7, 8, 9 };
int sum = 0;
for (int i : tab) {
	sum += i;
}
Using 3.1, create a new java project.
Add a new scrapbook page that contains this source:
int[] tab = new int[] {1, 2, 3, 4, 5, 6, 7, 8, 9 };
int sum = 0;
for (int i : tab) { sum += i;
} sum
get error about syntax error",org.eclipse.jdt.internal.eval.CodeSnippetParser
FILE,eclipse-3.1,103379,2005-07-11T15:37:00.000-05:00,[MPE] [EditorMgmt] An editor instance is being leaked each time an editor is open and closed,"dispose()
Driver: eclipse-SDK-3.1-win32 with eclipse-test-framework-3.1
open editor close editor
We have a testcase that can demostrate the problem.
The testcase is really simple.
It creates a new simple project and a new file.
It opens up the new file in the editor that comes with the testcase, then close the editor.
Repeat 500 times.
allocate size
So this String array can be GC-ed if the editor itself can be GC-ed.
If you run this testcase with -Xmx256M, you will run out of memory.
However, if you explicitly set the String array to null in the dispose() method of the editor, then the same testcase will not run out of memory.",org.eclipse.ui.operations.OperationHistoryActionHandler
FILE,eclipse-3.1,103918,2005-07-14T17:25:00.000-05:00,100% CPU load while creating dynamic proxy in rich client app,"public void start(BundleContext context) throws Exception {
  super.start(context);
  XmlBeanFactory bf = new XmlBeanFactory(
     new ClassPathResource(""/bug/beans.xml""));
  bf.getBean(""hang"");
}

  bf.getBean(""hang"")  
 bf.getBean()
I've tried to integrate my ecplipse-rcp application with springframework.
get % load need % load fall into infinit loop
my start method contains the following code:
public void start(BundleContext context) throws Exception {
  super.start(context);
  XmlBeanFactory bf = new XmlBeanFactory(
     new ClassPathResource(""/bug/beans.
xml""));
  bf.getBean(""hang"");
}
The same code
executed outside eclipse-rcp works well and without problem.
bf.getBean() tries to create a proxy class for given interface with standard JDK
dynamic proxy facility (no cglib or any other byte code manipulation takes place).
I'm not sure but I think that this may be caused by classloaders.
My environment: 
  Eclipse Version: 3.1.0
  Build id: I20050627-1435
  OS: Linux 2.6.12
  Java: Sun jdk1.5.0_04
I attach a sample project which causes the 100% CPU load and rich client hang.",org.eclipse.core.runtime.internal.adaptor.ContextFinder
FILE,eclipse-3.1,106492,2005-08-09T11:01:00.000-05:00,NPE on console during debug session,"name.equals(""IResourceTest.testDelete"")  
  
  
          
       
  
       
  
       
   testDelete()  
  
   
    
 
  
   
  
   
    
 
   runTest()  
   runBare()  
   protect()
Build: I20050808-2000
While debugging, I noticed the attached stack trace on my Java console (not in the log file).
see from stack occur during evaluation
I have a single breakpoint with this condition:
name.equals(""IResourceTest.testDelete"") && count==83
have effect
I was still able to ""Suspend"" the process, and it resulted in a debug view showing:
org.eclipse.core.launcher.Main at localhost:1250
Thread [main] (Suspended (breakpoint at line 69 in TestPerformer))
IResourceTest$6(TestPerformer).
performTestRecursiveLoop(Object[][], Object[], int) line: 69
<unknown receiving type>(TestPerformer).
performTestRecursiveLoop(Object[][],
Object[], int) line: 111
<unknown receiving type>(TestPerformer).
performTestRecursiveLoop(Object[][],
Object[], int) line: 111
<unknown receiving type>(TestPerformer).
performTest(Object[][]) line: 55
<unknown receiving type>(<unknown declaring type>).
testDelete() <unknown line number>
<unknown receiving type>(<unknown declaring type>).
invoke0(Method, Object,
Object[]) <unknown line number>
<unknown receiving type>(<unknown declaring type>).
invoke(Object, Object[])
<unknown line number>
<unknown receiving type>(<unknown declaring type>).
invoke(Method, Object,
Object[]) <unknown line number>
<unknown receiving type>(<unknown declaring type>).
invoke(Method, Object,
Object[]) <unknown line number>
<unknown receiving type>(<unknown declaring type>).
invoke(Object, Object[])
<unknown line number>
<unknown receiving type>(<unknown declaring type>).
runTest() <unknown line number>
<unknown receiving type>(<unknown declaring type>).
runBare() <unknown line number>
<unknown receiving type>(<unknown declaring type>).
protect() <unknown line number>
... etc ...
I had to shutdown Eclipse to remove this from the debug view.",org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
FILE,eclipse-3.1,108466,2005-08-31T09:39:00.000-05:00,Dups from (Eclipse)ClassLoader.getResources(String),"EclipseClassLoader.getResources(String)  
   
 getClass()  getClassLoader()  getResources(""file.txt"")
run runtime workspace duplicate results for files find in plugin jarfiles
Steps to reproduce
1. create plugin project
2. add a jar with ""file.txt"" entry to the project
3. add the jar to plugin runtime classpath using plugin manifest editor
4. update plugin classpath (make sure the jar got added to java build path)
5. add code that counts number of entries returned by getClass().
getClassLoader().
getResources(""file.txt"")
5. start eclipse application, see that getResources returns two entries
add jar on classpath
classpath entry",org.eclipse.pde.internal.core.ClasspathHelper
FILE,eclipse-3.1,110837,2005-09-27T13:16:00.000-05:00,javax.crypto.KeyAgreement.getInstance(String) throws exception in IDE,"KeyAgreement.getInstance(""DiffieHellman"")  
  
 
import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;

public class KeyAgreementProblem
{
    public static void main(String[] args) throws NoSuchAlgorithmException
    {
        KeyAgreement ka = KeyAgreement.getInstance(""DiffieHellman"");
        System.out.println(ka);
    }
}
 
 
  
  
 javax.crypto.KeyAgreement.getInstance(DashoA12275)
throw a.
run from Eclipse run from command line
(JDK 1.5.0_04 or 1.4.2_09)
Here's the code:
--- import java.security.NoSuchAlgorithmException;
import javax.crypto.KeyAgreement;
public class KeyAgreementProblem
{ public static void main(String[] args) throws NoSuchAlgorithmException
{
KeyAgreement ka = KeyAgreement.getInstance(""DiffieHellman"");
System.out.println(ka);
}
}
---
Running from the command line goes like this (command line then output below):
C:\...\workspace\sandbox>java -classpath bin KeyAgreementProblem javax.crypto.KeyAgreement@141d683
---
produce exception run with same JDK run with environment run in Eclipse
Exception in thread ""main"" java.security.NoSuchAlgorithmException: Algorithm
DiffieHellman not available at javax.crypto.KeyAgreement.getInstance(DashoA12275)
at KeyAgreementProblem.main(KeyAgreementProblem.java:8)","org.eclipse.jdt.launching.AbstractJavaLaunchConfigurationDelegate
org.eclipse.jdt.internal.launching.JRERuntimeClasspathEntryResolver
org.eclipse.jdt.internal.launching.StandardVMType"
FILE,eclipse-3.1,113455,2005-10-22T11:32:00.000-05:00,[Markers] Some error markers do not appear,"problemView.getCurrentMarkers()
I20051018-0800, GTK+ 2.6.8, KDE 3.4.1, X.org 6.8.2, Linux 2.6.13
+ When I saw Bug 113454, I started up Eclipse and synchronized with HEAD.
show up in Problems view
+ So, I opened the file referenced in the first compile error
(ResourceMappingMarkersTest), and it had several errors in it.
+ I clicked on one of the methods that was an error
(problemView.getCurrentMarkers()), and hit ""F3"".
not resolve ModelProvider in CompositeResourceMapping
+ I tried cleaning all projects, to see if that would motivate the errors to appear.
have hard time make major make blocker
The big problem, as I see it, is that this can lead to broken builds (as we've seen).
So I marked it as a blocker.","org.eclipse.ui.views.markers.internal.Util
org.eclipse.ui.views.markers.internal.MarkerView"
FILE,eclipse-3.1,133072,2006-03-23T16:58:00.000-06:00,"Cannot launch an ""Eclipse Application"" without the -ws argument","package Fred;

import javax.swing.JFrame;
import javax.swing.SwingUtilities;

import org.eclipse.core.runtime.IPlatformRunnable;

public class Main implements IPlatformRunnable {

       public Object run(Object args) throws Exception {
               SwingUtilities.invokeLater(new Runnable() {
                       public void run() {
                               new JFrame(""Fred"").setVisible(true);
                       }
               });
               synchronized(this)
               {
                       wait();
               }
               return IPlatformRunnable.EXIT_OK;
       }

}
When trying to launch an Eclipse Application that does NOT use SWT under OSX from 3.1 or 3.2M5, you get the following problems:
get dreaded 2006-03-23 run against 1.4.2_09
15:35:27.340 java[1053] Apple AWT Java VM was loaded on first thread
-- can't start AWT.""
get messages on startup get 1.5.0_06 on startup
2006-03-23 15:29:59.468 java[1047] [Java CocoaComponent compatibility
mode]: Enabled
2006-03-23 15:29:59.468 java[1047] [Java CocoaComponent compatibility
mode]: Setting timeout for SWT to 0.100000
appear under 1.5.0_06
Under 3.0 (i think) you used to be able to tell Eclipse not to force
any ""first-thread"" flags by not passing the -ws carbon flag to the
application.
However, it appears that we no longer have the ability to suppress that flag.
We hacked around this by removing the check for the ""-ws"" flag in the  org.eclipse.jdt.internal.launching.macosx.MacOSXVMInstall.java class and rebuilding that plugin.
But this is not a good all-around solution.
Better would be to be able to override and not pass the -ws flag at all if we wanted to per Launch configuration.
Here's the test class we used:
package Fred;
import javax.swing.JFrame;
import javax.swing.SwingUtilities;
import org.eclipse.core.runtime.IPlatformRunnable;
public class Main implements IPlatformRunnable {
public Object run(Object args) throws Exception {
               SwingUtilities.invokeLater(new Runnable() {
                       public void run() {
                               new JFrame(""Fred"").
setVisible(true);
                       }
               });
               synchronized(this)
               {
                       wait();
               }
               return IPlatformRunnable.EXIT_OK;
       }
}","org.eclipse.pde.internal.ui.IPDEUIConstants
org.eclipse.pde.internal.ui.launcher.LaunchAction"
FILE,eclipse-3.1,300054,2010-01-19T10:12:00.000-06:00,Unexpected 'Save Resource' dialog appears when copying changes from right to left,"public class Bug {
	void bar() {
		System.out.println();
	}
}
  System.out.println();
R3.5, R3.5.x and I20100112-0800.
1 start with new workspace
2 paste this into the Package Explorer:
public class Bug {
void bar() {
System.out.println();
}
}
4 delete ""System.out.println();"" and save
5 compare the current state with the previous one
6 double-click on 'bar' in the compare editor's upper pane
7 click 'Copy Current Change from Right to Left' button
NOTE: step 6 is crucial: it only happens when the compare editor is focused on a method.",org.eclipse.compare.internal.Utilities
FILE,eclipse-3.1,76472,2004-10-18T11:31:00.000-05:00,Duplicate entries in the constant pool for some methods,"public class X {
	public static void main(String[] args) {
		long[] tab = new long[] {};
		System.out.println(tab.clone());
		System.out.println(tab.clone());
	}
}

  clone()
Compile this example:
public class X { public static void main(String[] args) { long[] tab = new long[] {};
System.out.println(tab.clone());
System.out.println(tab.clone());
}
}
Disassemble it and you can see that the call to clone() creates two entries in the constant pool.","org.eclipse.jdt.internal.compiler.ast.BreakStatement
org.eclipse.jdt.internal.compiler.flow.FlowContext
org.eclipse.jdt.internal.compiler.flow.LoopingFlowContext"
FILE,eclipse-3.1,76534,2004-10-18T22:57:00.000-05:00,Can't perform evaluations inside inner class with constructor_ parameters,"createViewer(...)
disallow evaluations in inner classes take parameters in referenced constructor _ take parameters in inner classes take evaluations in referenced constructor _ take evaluations in inner classes
For example, see the CheckBoxTreeViewer that's created in BreakpointsView#createViewer(...).
Is there anything we can do to allow evaluations in these kinds of classes?",org.eclipse.jdt.internal.debug.eval.ast.engine.SourceBasedSourceGenerator
FILE,eclipse-3.1,77234,2004-10-28T15:41:00.000-05:00,Detail formatter doesn't see inherited method,"getTypeName() 
  
  
  
 getTypeName()   JavaExceptionBreakpoint

getTypeName()
1 Create a detail formatter for the type JavaExceptionBreakpoint.
2 Set the contents to ""getTypeName()""
3 Debug to a breakpoint with a JavaExceptionBreakpoint in the variables view.
I'm debugging RemoveBreakpointAction and I delete an exception breakpoint to see
this.
4 Select the JavaExceptionBreakpoint.
get following in details pane
Detail formatter error:
The method getTypeName() is undefined for the type JavaExceptionBreakpoint
getTypeName() is declared on JavaBreakpoint, which JavaExceptionBreakpoint 
extends.",org.eclipse.jdt.internal.debug.ui.JavaDetailFormattersManager
FILE,eclipse-3.1,77573,2004-11-03T04:43:00.000-06:00,[1.5][assist] Code assist does not propose static fields,"import static java.lang.Math
200411022000
Steps to reproduce:
- Write ""import static java.lang.Math."" in a cu
- Press Ctrl+Space","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.core.CompletionProposal
org.eclipse.jdt.core.CompletionRequestor"
FILE,eclipse-3.1,78245,2004-11-09T18:34:00.000-06:00,Breakpoints in enums not correctly created.,"public enum TestEnum {
  a;
  public static void main(String[] args) {
    System.out.println();   // <- add a breakpoint here
  }
}
add breakpoint in enum classes
work OK
public enum TestEnum { a;
public static void main(String[] args) {
System.out.println();   // <- add a breakpoint here
}
}
display breakpoint as null [ line display breakpoint in breakpoint view
not stop on breakpoint",org.eclipse.jdt.internal.debug.ui.actions.ValidBreakpointLocationLocator
FILE,eclipse-3.1,78315,2004-11-10T12:53:00.000-06:00,org.eclipes.team.ui plugin's startup code forces compare to be loaded,"Platform.getAdapterManager()  registerAdapters(factory, DiffNode.class);

   
 startup()
3.1 M3
I wrote tests that ensure plug-ins like Search and Compare aren't loaded when opening a Java editor.
load in start(BundleContext) method
Platform.getAdapterManager().
registerAdapters(factory, DiffNode.class);
cause compare plug-in
Test Case:
1. add startup() method to CompareUIPlugin
2. put a breakpoint there",org.eclipse.team.internal.ui.TeamUIPlugin
FILE,eclipse-3.1,78740,2004-11-16T10:57:00.000-06:00,IDOMType.getFlags() fails to represent interface flags correctly.,"becomeDetailed()   

package org.example.jdom;

import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;

public class Test implements IPlatformRunnable
{
  public Object run(Object object)
  {
    DOMFactory factory = new DOMFactory();
    IDOMCompilationUnit jCompilationUnit =
factory.createCompilationUnit(""package x; /** @model */ interface X  {}"", ""NAME"");
    IDOMType jType = (IDOMType)jCompilationUnit.getFirstChild().getNextNode(); 
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    jType.getComment();
    System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) != 0));
    return new Integer(0);
  }
}
This code demonstrates that calling getComment on an IDOMType will change the flags from correctly encoding the type as being interface to incorrectly encoding it (because during becomeDetailed() that information is lost):
package org.example.jdom;
import org.eclipse.core.runtime.IPlatformRunnable;
import org.eclipse.jdt.core.Flags;
import org.eclipse.jdt.core.jdom.DOMFactory;
import org.eclipse.jdt.core.jdom.IDOMCompilationUnit;
import org.eclipse.jdt.core.jdom.IDOMType;
public class Test implements IPlatformRunnable
{ public Object run(Object object)
{
DOMFactory factory = new DOMFactory();
IDOMCompilationUnit jCompilationUnit = factory.createCompilationUnit(""package x; /** @model */ interface X  {}"", ""NAME"");
IDOMType jType = (IDOMType)jCompilationUnit.getFirstChild().
getNextNode();
System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) !
= 0));
jType.getComment();
System.err.println("""" + ((jType.getFlags() & Flags.AccInterface) !
= 0));
return new Integer(0);
}
}
This bug completely breaks EMF's JavaEcoreBuilder, which is a blocking problem for our clients and hence we see this as a blocking problem.",org.eclipse.jdt.internal.compiler.DocumentElementParser
FILE,eclipse-3.1,79545,2004-11-26T05:58:00.000-06:00,Eclipse vs Sun JDK: different class files from the same source code,"public class CharIntTest
{
    /**
     * Eclipse value: "" ""
     * JDK value:     ""32""
     */
    public static String C = """" + +' ';
    /**
     * Eclipse value: ""32""
     * JDK value:     ""32""
     */
    public static String I = """" + +32;

    public static void main(String[] args)
    {
        System.out.println(C);
        System.out.println(I);
    }
}
Compile the source code below in Eclipse and run it.
Do the same using Sun JDK (1.4.1 or 1.5).
The results are different.
connect with +
PS.
I'm not sure if I picked the proper product (JDT)...
The source code:
public class CharIntTest
{
/**
* Eclipse value: "" ""
* JDK value:     ""32""
*/ public static String C = """" + +' ';
/**
* Eclipse value: ""32""
* JDK value:     ""32""
*/ public static String I = """" + +32;
public static void main(String[] args)
{
System.out.println(C);
System.out.println(I);
}
}","org.eclipse.jdt.internal.compiler.impl.Constant
org.eclipse.jdt.internal.compiler.ast.EqualExpression"
FILE,eclipse-3.1,79957,2004-12-02T00:47:00.000-06:00,[Viewers] NPE changing input usingTableViewer and virtual,"Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
 
 tv.setInput(model1);
I'm using the latest code for Table viewer with a private virtual manager class
in table viewer.
i've straight forward code ... 
<code>
Table table=new Table(shell,SWT.VIRTUAL);
TableViewer tv=new TableViewer(table);
tv.setContentProvider(new NetworkContentProvider());
tv.setLabelProvider(new NetworkLabelProvider());
tv.setInput(model);
.
.
.
in a selection event handler for a button, i've to reset the model input
.
.
tv.setInput(model1);
.
.
throw null pointer exception
the stack trace was
java.lang.NullPointerException
at org.eclipse.jface.viewers.TableViewer$1.handleEvent(TableViewer.java:103)
at org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:82)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:796)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:820)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:805)
at org.eclipse.swt.widgets.Table.wmNotifyChild(Table.java:3158)
at org.eclipse.swt.widgets.Control.WM_NOTIFY(Control.java:4040)
at org.eclipse.swt.widgets.Composite.WM_NOTIFY(Composite.java:722)
at org.eclipse.swt.widgets.Control.windowProc(Control.java:3025)
at org.eclipse.swt.widgets.Decorations.windowProc(Decorations.java:1400)
at org.eclipse.swt.widgets.Display.windowProc(Display.java:3349)
at org.eclipse.swt.internal.win32.OS.CallWindowProcW(Native Method)
at org.eclipse.swt.internal.win32.OS.CallWindowProc(OS.java:1403)
at org.eclipse.swt.widgets.Table.callWindowProc(Table.java:137)
at org.eclipse.swt.widgets.Control.windowProc(Control.java:3056)
at org.eclipse.swt.widgets.Display.windowProc(Display.java:3349)
at org.eclipse.swt.internal.win32.OS.DispatchMessageW(Native Method)
at org.eclipse.swt.internal.win32.OS.DispatchMessage(OS.java:1479)
at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:2440)
at jface.viewers.TestJfaceVirtual.main(TestJfaceVirtual.java:49)",org.eclipse.jface.viewers.TableViewer
FILE,eclipse-3.1,80672,2004-12-10T04:44:00.000-06:00,[1.5] Annotation change does not trigger recompilation,"package p;
@q.Ann
public class Use {
}
  
package q;
public @interface Ann {
}


 
 
package q;
import java.lang.annotation.*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
 
 @Ann
Build 20041207
Define 2 files:
p/Use.
java===================================
package p;
@q.
Ann
public class Use {
}
q/Ann.
java===================================
package q;
public @interface Ann {
}
Build - all is fine
Now incrementally change q/Ann.
java to:
package q;
import java.lang.annotation.
*;
@Target(ElementType.METHOD)
public @interface Ann {
}
 
Build - still fine though p/Use.",org.eclipse.jdt.internal.compiler.classfmt.ClassFileReader
FILE,eclipse-3.1,81045,2004-12-14T20:13:00.000-06:00,ClassNotLoadedException when trying to change a value,"public class Test {
	static class Inner {
	}
	public static void main(String[] args) {
		Inner inner= null;
		System.out.println(1);  //  <- breakpoint here
	}
}
public class Test { static class Inner {
} public static void main(String[] args) {
Inner inner= null;
System.out.println(1);  //  <- breakpoint here
}
}
Debug to the breakpoint.
Right-click on the 'inner' variable > change value ...","org.eclipse.jdt.internal.debug.ui.actions.JavaVariableValueEditor
org.eclipse.jdt.internal.debug.eval.ast.engine.ASTEvaluationEngine
org.eclipse.jdt.internal.debug.core.model.JDILocalVariable"
FILE,eclipse-3.1,82712,2005-01-12T15:54:00.000-06:00,[1.5] Code assist does not show method parameters from static imports,"import static java.lang.Math.*; 
 public class Test {

    void t() {
        abs(<CTRL+SPACE>);
    }
}
Test:
import static java.lang.Math.
*;,
public class Test {
void t() { abs(<CTRL+SPACE>);
}
}",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,83205,2005-01-19T11:25:00.000-06:00,[osgi] shutdown did not complete,"System.exit()  
  
    
  
  
    
   
  
 
  
   Object.wait()  
   
  
  
  
  
  
  
  
 
 it()  
    
 
 
 
 
  
 Object.wait()  
   
  Object.wait()
Accidentally shut down Eclipse while exporting a plug-in project as deployable feature was in progress.
Noticed that the plug-in export failed due to classloaders being closed for business (see below), which is expected.
Took a snapshot of the VM state (see below).
dispatch threads
Typing ""exit"" on the console caused the VM to exit.
Unable to reproduce.
!
SESSION 2005-01-19 11:11:22.132 -----------------------------------------------
eclipse.buildId=I20050118-1015 java.version=1.5.0 java.vendor=Sun Microsystems Inc.
BootLoader constants: OS=win32, ARCH=x86, WS=win32, NL=en_CA
Framework arguments:  -keyring d:\temp\.
keyring -showlocation
Command-line arguments:  -os win32 -ws win32 -arch x86 -keyring d:\temp\.
keyring
-consolelog -console -debug -showlocation
!
ENTRY org.eclipse.pde.build 0 1 2005-01-19 11:11:22.132
!
MESSAGE Some inter-plug-in dependencies have not been satisfied.java.lang.NoClassDefFoundError: org/eclipse/pde/internal/core/ifeature/IFeatureM odel at org.eclipse.pde.internal.ui.wizards.exports.FeatureExportJob.deleteBu ildFiles(FeatureExportJob.java:298)
at org.eclipse.pde.internal.ui.wizards.exports.PluginExportJob.doExports
(PluginExportJob.java:50)
at org.eclipse.pde.internal.ui.wizards.exports.FeatureExportJob.run(Feat ureExportJob.java:95)
at org.eclipse.core.internal.jobs.Worker.run(Worker.java:66)
osgi>
osgi>
osgi> osgi>
osgi> Full thread dump Java HotSpot(TM) Client VM (1.5.0-b64 mixed mode):
""DestroyJavaVM"" prio=5 tid=0x000361e8 nid=0xc84 waiting on condition [0x00000000
.
.0x0007fae8]
""OSGi Console"" prio=5 tid=0x19bd0e28 nid=0xa28 in Object.wait() [0x19fcf000.
.0x1
9fcfb68]
at java.lang.Object.wait(Native Method)
- waiting on <0x042db588> (a java.lang.Object)
at org.eclipse.osgi.framework.internal.core.FrameworkConsole.console(Fra meworkConsole.java:265)
- locked <0x042db588> (a java.lang.Object)
at org.eclipse.osgi.framework.internal.core.FrameworkConsole.console(Fra meworkConsole.java:236)
at org.eclipse.osgi.framework.internal.core.FrameworkConsole.run(Framewo rkConsole.java:207)
at java.lang.Thread.run(Thread.java:595)
""Framework Event Dispatcher"" daemon prio=5 tid=0x199d55a0 nid=0x878 in Object.wa it() [0x19f8f000.
.0x19f8fbe8]
at java.lang.Object.wait(Native Method)
at java.lang.Object.wait(Object.java:474)
at org.eclipse.osgi.framework.eventmgr.EventThread.getNextEvent(EventThr ead.java:162)
- locked <0x042db620> (a org.eclipse.osgi.framework.eventmgr.EventThread
)
at org.eclipse.osgi.framework.eventmgr.EventThread.run(EventThread.java:
100)
""Low Memory Detector"" daemon prio=5 tid=0x00a912f8 nid=0xe60 runnable [0x0000000
0. .0x00000000]
""CompilerThread0"" daemon prio=10 tid=0x00a8fec8 nid=0x2b8 waiting on condition [
0x00000000.
.0x198cf6c0]
""Signal Dispatcher"" daemon prio=10 tid=0x00a8f250 nid=0xd4c waiting on condition
[0x00000000.
.0x00000000]
""Finalizer"" daemon prio=9 tid=0x00a86670 nid=0xd9c in Object.wait() [0x1984f000.
.0x1984fa68]
at java.lang.Object.wait(Native Method)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)
- locked <0x042db808> (a java.lang.ref.ReferenceQueue$Lock)
at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)
at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
""Reference Handler"" daemon prio=10 tid=0x00a851e0 nid=0x478 in Object.wait() [0x
1980f000.
.0x1980fae8]
at java.lang.Object.wait(Native Method)
at java.lang.Object.wait(Object.java:474)
at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
- locked <0x042db888> (a java.lang.ref.Reference$Lock)
""VM Thread"" prio=10 tid=0x00a82810 nid=0x750 runnable
""VM Periodic Task Thread"" prio=10 tid=0x00a924d0 nid=0xd04 waiting on condition",org.eclipse.core.launcher.Main
FILE,eclipse-3.1,83383,2005-01-21T06:39:00.000-06:00,IllegalArgumentException in Signature.getParameterCount,"String signature= ""foo(+Ljava.lang.Comparable;)"";
Signature.getParameterCount(signature);
I20050118 + unreleased code (not showing in any build)
String signature= ""foo(+Ljava.lang.Comparable;)"";
Signature.getParameterCount(signature);
Background:
I copied code from CompletionRequestorWrapper to work with the new
CompletionRequestor API.
When completing a METHOD_REF proposal for a method that has a parameter with an open type bound, getParameterPackages I get an IAE:
The reason is the Util.scanTypeSignature does not handle bounded types.
-----------
I get this:
!
ENTRY org.eclipse.ui 4 0 2005-01-21 12:11:32.109
!
MESSAGE The command for the key you pressed failed
!
STACK 0 java.lang.IllegalArgumentException
at org.eclipse.jdt.internal.core.util.Util.scanTypeSignature(Util.java:2115)
at org.eclipse.jdt.core.Signature.getParameterCount(Signature.java:944)
at org.eclipse.jdt.core.Signature.getParameterTypes(Signature.java:1060)
at
org.eclipse.jdt.internal.ui.text.java.ResultCollector.getParameterPackages(ResultCollector.java:732)
at
org.eclipse.jdt.internal.ui.text.java.ResultCollector.internalAcceptMethod(ResultCollector.java:254)
at
org.eclipse.jdt.internal.ui.text.java.ResultCollector.accept(ResultCollector.java:654)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.findLocalMethods(CompletionEngine.java:2816)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.findIntefacesMethods(CompletionEngine.java:2551)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.findMethods(CompletionEngine.java:3270)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.findFieldsAndMethods(CompletionEngine.java:1903)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.complete(CompletionEngine.java:627)
at
org.eclipse.jdt.internal.codeassist.CompletionEngine.complete(CompletionEngine.java:1205)
at org.eclipse.jdt.internal.core.Openable.codeComplete(Openable.java:119)
at
org.eclipse.jdt.internal.core.CompilationUnit.codeComplete(CompilationUnit.java:286)
at
org.eclipse.jdt.internal.core.CompilationUnit.codeComplete(CompilationUnit.java:279)
at
org.eclipse.jdt.internal.ui.text.java.JavaCompletionProcessor.internalComputeCompletionProposals(JavaCompletionProcessor.java:363)
at
org.eclipse.jdt.internal.ui.text.java.JavaCompletionProcessor.computeCompletionProposals(JavaCompletionProcessor.java:334)
at
org.eclipse.jface.text.contentassist.ContentAssistant.computeCompletionProposals(ContentAssistant.java:1470)
at
org.eclipse.jface.text.contentassist.CompletionProposalPopup.computeProposals(CompletionProposalPopup.java:250)
at
org.eclipse.jface.text.contentassist.CompletionProposalPopup.access$7(CompletionProposalPopup.java:247)
at
org.eclipse.jface.text.contentassist.CompletionProposalPopup$9.run(CompletionProposalPopup.java:961)
at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:69)
at
org.eclipse.jface.text.contentassist.CompletionProposalPopup.incrementalComplete(CompletionProposalPopup.java:956)
at
org.eclipse.jface.text.contentassist.ContentAssistant.showPossibleCompletions(ContentAssistant.java:1318)
at
org.eclipse.jdt.internal.ui.javaeditor.CompilationUnitEditor$AdaptedSourceViewer.doOperation(CompilationUnitEditor.java:180)
at org.eclipse.ui.texteditor.ContentAssistAction$1.run(ContentAssistAction.java:82)
at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:69)
at org.eclipse.ui.texteditor.ContentAssistAction.run(ContentAssistAction.java:80)
at org.eclipse.jface.action.Action.runWithEvent(Action.java:989)
at org.eclipse.ui.commands.ActionHandler.execute(ActionHandler.java:188)
at org.eclipse.ui.internal.commands.Command.execute(Command.java:130)
at
org.eclipse.ui.internal.keys.WorkbenchKeyboard.executeCommand(WorkbenchKeyboard.java:445)
at org.eclipse.ui.internal.keys.WorkbenchKeyboard.press(WorkbenchKeyboard.java:724)
at
org.eclipse.ui.internal.keys.WorkbenchKeyboard.processKeyEvent(WorkbenchKeyboard.java:767)
at
org.eclipse.ui.internal.keys.WorkbenchKeyboard.filterKeySequenceBindings(WorkbenchKeyboard.java:536)
at
org.eclipse.ui.internal.keys.WorkbenchKeyboard.access$2(WorkbenchKeyboard.java:479)
at
org.eclipse.ui.internal.keys.WorkbenchKeyboard$1.handleEvent(WorkbenchKeyboard.java:221)
at org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:82)
at org.eclipse.swt.widgets.Display.filterEvent(Display.java:1086)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1001)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1026)
at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1011)
at org.eclipse.swt.widgets.Widget.sendKeyEvent(Widget.java:1038)
at org.eclipse.swt.widgets.Widget.gtk_key_press_event(Widget.java:602)
at org.eclipse.swt.widgets.Control.gtk_key_press_event(Control.java:1889)
at org.eclipse.swt.widgets.Composite.gtk_key_press_event(Composite.java:527)
at org.eclipse.swt.widgets.Widget.windowProc(Widget.java:1338)
at org.eclipse.swt.widgets.Display.windowProc(Display.java:3261)
at org.eclipse.swt.internal.gtk.OS.
_gtk_main_do_event(Native Method)
at org.eclipse.swt.internal.gtk.OS.gtk_main_do_event(OS.java:4642)
at org.eclipse.swt.widgets.Display.eventProc(Display.java:926)
at org.eclipse.swt.internal.gtk.OS.
_g_main_context_iteration(Native Method)
at org.eclipse.swt.internal.gtk.OS.g_main_context_iteration(OS.java:1104)
at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:2415)
at org.eclipse.ui.internal.Workbench.runEventLoop(Workbench.java:1575)
at org.eclipse.ui.internal.Workbench.runUI(Workbench.java:1541)
at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:287)
at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:144)
at org.eclipse.ui.internal.ide.IDEApplication.run(IDEApplication.java:102)
at
org.eclipse.core.internal.runtime.PlatformActivator$1.run(PlatformActivator.java:220)
at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:274)
at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:129)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:585)
at org.eclipse.core.launcher.Main.basicRun(Main.java:255)
at org.eclipse.core.launcher.Main.run(Main.java:811)
at org.eclipse.core.launcher.Main.main(Main.java:795)
The command for the key you pressed failed",org.eclipse.jdt.internal.core.util.Util
FILE,eclipse-3.1,83489,2005-01-22T17:33:00.000-06:00,[select] Code select returns IType instead of ITypeParameter on method parameters types,"class Test<T> {
  void foo(T t) {}
}
Using HEAD.
Consider following test case:
class Test<T> { void foo(T t) {}
}
When I select ""T"" in method declaration, selection engine returns an IType",org.eclipse.jdt.internal.codeassist.SelectionEngine
FILE,eclipse-3.1,83536,2005-01-24T10:02:00.000-06:00,"""Incompatible argument to function"" at vararg function","package t1;
public class Test {
    public static void main (String[] args) {
        new Test ().test (new byte[5]);
    }
    private void test (Object... params) {
    }
}

 
  
 new Test ()  new Object[] {new byte[5]}
Platform: Eclipse 3.1 M4
Code example:
package t1;
public class Test {
    public static void main (String[] args) {
        new Test ().
test (new byte[5]);
    }
    private void test (Object... params) {
    }
}
generate following error at runtime start from Eclipse start to function
The error is eliminated if the call to method test is expressed as:
new Test ().
test (new Object[] {new byte[5]});
Could it be an autoboxing issue?","org.eclipse.jdt.internal.compiler.ast.Statement
org.eclipse.ui.internal.WorkbenchWindow
org.eclipse.ui.internal.Workbench"
FILE,eclipse-3.1,84194,2005-02-01T18:05:00.000-06:00,[content assist] Code assist in import statements insert at the end,"import org.eclipse.core.runtime.*;
Build: I-20050201
Open any Java file that contains > 1 import statements.
Let's say the first import statement reads:
import org.eclipse.core.runtime.
*;
delete the '*;' from the end and try to use code assist to insert
IRunnableWithProgress for example.
You will see that upon pressing Enter to select, the text gets inserted several lines down under all the import statements.
be in random position","org.eclipse.jdt.internal.ui.text.java.JavaTypeCompletionProposal
org.eclipse.jdt.internal.ui.text.java.ExperimentalResultCollector"
FILE,eclipse-3.1,84724,2005-02-08T13:41:00.000-06:00,[1.5][search] fails to find call sites for varargs constructor_s,"public class Test {
    public void foo() {
        Cell c= new Cell("""", """"); // calls Cell.Cell(String...)
    }
}
 class Cell {
    public Cell(String... args) { }
}
find call to varargs constructor _
Simply highlight the constructor_'s name and invoke ""References"" -
> ""Workspace"" from the Java editor context menu; no occurrences will be found.
Bug manifests with integration build I2005-0202.
public class Test { public void foo() {
Cell c= new Cell("""", """"); // calls Cell.Cell(String...)
}
} class Cell { public Cell(String... args) { }
}",org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
FILE,eclipse-3.1,84770,2005-02-09T06:46:00.000-06:00,Formatter fails in specific case (.class in code),"public class FormatterTest {
  void doTest(
      ) {
     System.out.println(""("" + 
         Object.class + "")"");
  }
}
 
 toString()
Steps to reproduce:
1 Make a new class in default package:
-----BEGIN-----
public class FormatterTest {
void doTest(
) {
System.out.println(""("" +
Object.class + "")"");
}
}
-----END-----
2 Try to format it by Ctrl+Shift+F  - nothing happens
3 Now change the 'Object.class' to 'Object.class.toString()'
(You can also delete the last ' + "")""'
4 Try to format it by Ctrl+Shift+F  - everything is ok
have string operatation after keyword class",org.eclipse.jdt.internal.formatter.BinaryExpressionFragmentBuilder
FILE,eclipse-3.1,84944,2005-02-10T16:49:00.000-06:00,[1.5][builder] Parameterized return type is sometimes not visible.,"package parser;

public interface ValueParser<T> {
	T parse(final String string);
}
This is a very strange error because it is not always reproduceable.
I defined an interface with a parameterized return type:
--------------------------
package parser;
public interface ValueParser<T> {
	T parse(final String string);
}
--------------------------
clean error until next compile not occur in different implementation
I tried to create a minimal persion project which could be attached in bugzilla
but it doesn't seem to show the bug.
The error was shown in line 21 of ""test/BooleanParserTest.
java"".","org.eclipse.jdt.internal.compiler.lookup.BinaryTypeBinding
org.eclipse.jdt.internal.compiler.lookup.ParameterizedTypeBinding
org.eclipse.jdt.internal.compiler.lookup.WildcardBinding"
FILE,eclipse-3.1,85344,2005-02-15T17:36:00.000-06:00,Error evaluating logical structure value for Map in Java 5.0,"public class Test {
  public static void main(String[] args) {
    Map<String, Integer> map= new HashMap<String, Integer>();
    System.out.println();     // <-- breakpoint here
  }
}

  entrySet()
public class Test { public static void main(String[] args) {
Map<String, Integer> map= new HashMap<String, Integer>();
System.out.println();     // <-- breakpoint here
}
}
I get ""Error: The method entrySet() is undefined for the type Map__"" when I expand map in the variables view.",org.eclipse.jdt.internal.debug.eval.ast.engine.BinaryBasedSourceGenerator
FILE,eclipse-3.1,85397,2005-02-16T08:20:00.000-06:00,[1.5][enum] erroneous strictfp keyword on enum type produces error on constructor_,"strictfp enum Natural {
	ONE, TWO;
}

 
 strictfp enum Natural {
	ONE, TWO;
	
	private Natural() {
	}
}
I20050215-2300 (M5 test pass)
- have this code:
strictfp enum Natural {
ONE, TWO;
}
not allow strictfp on enum actual
- alternatively, have this code:
strictfp enum Natural {
ONE, TWO;
private Natural() {
}
}
report wrong modifier with type name show error for constructor _","org.eclipse.jdt.internal.compiler.lookup.SyntheticMethodBinding
org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyViewPart
org.eclipse.jdt.internal.compiler.lookup.MethodScope"
FILE,eclipse-3.1,85402,2005-02-16T08:50:00.000-06:00,[1.5][assist] NPE while trying to complete on empty annotation,"import e.Team;
   @Author(name={Team.DAVID, Team.JEROME})
    
  public class Test {
	@Author(name=Team.PHILIPPE) void foo() {}
	@Author int t;
  }
  
  import e.Team;
  public @interface Author {
	Team[] name() default Team.FREDERIC;
  }
  
  package e;
  public enum Team {
	PHILIPPE, DAVID, JEROME, FREDERIC;
  }

 
 ResultCollector.accept(CompletionProposal)
Using build 3.1 M5 candidate (I20040215-2300).
Having following test case:
Test.java:
import e.Team;
@Author(name={Team.DAVID, Team.JEROME})
@| // <-- Try to complete here: NPE
public class Test {
@Author(name=Team.PHILIPPE) void foo() {}
@Author int t;
}
Author.java:
import e.Team;
public @interface Author {
Team[] name() default Team.FREDERIC;
}
Team.java:
package e;
public enum Team {
PHILIPPE, DAVID, JEROME, FREDERIC;
}
If you try to complete at caret position, then you get an Error Excuting Command
dialog.
name for CompletionProposal","org.eclipse.jdt.internal.codeassist.complete.CompletionOnAnnotationOfType
org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.codeassist.complete.CompletionParser"
FILE,eclipse-3.1,85672,2005-02-17T05:53:00.000-06:00,[projection] Unfolding a folded region with no line delimiter on the last line selects too much,"package folding;

class Test {
    
}
I20050215-2300 (m5 test pass)
Have this code:
""package folding;
class Test {
}""  <-- no delimiter on last line
Put the caret right after the closing brace and fold the region.
Put the caret on the last line and unfold the type.
Not a regression - it is like this in 3.0",org.eclipse.jface.text.source.projection.ProjectionViewer
FILE,eclipse-3.1,85734,2005-02-17T12:28:00.000-06:00,Debug view flickers excessively,"Runtime.exec(...)
I20050217-0800, KDE 3.3.2, GTK+ 2.4.14, X.Org 6.8.0, Linux 2.6.10
In particular, I have set a
breakpoint on ""Runtime.exec(...)"" and started a debugging session on Eclipse.
I
wish I could show you the effect; it is most disturbing.
:)","org.eclipse.debug.internal.ui.views.RemoteTreeViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewer
org.eclipse.debug.internal.ui.views.launch.LaunchViewEventHandler
org.eclipse.debug.internal.ui.views.RemoteTreeContentManager"
FILE,eclipse-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
produce bad JPG images
I have only verified this with JPEG output.
Simple test case below loads
 PNG Files and Saves them as JPEG.
produce proper JPG images expected
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}","org.eclipse.ui.internal.WorkbenchIntroManager
org.eclipse.swt.internal.image.JPEGFileFormat"
FILE,eclipse-3.1,87171,2005-03-04T14:19:00.000-06:00,Find declaring node doesn't work for methods/fields using type parameters,"public class Inline<T> {
	void foo(T t) {
		System.out.println(t);
	}
}

 class Use {
	public static void main(String[] args) {
		Inline<String> i= null;
		i.foo(""Eclipse"");
	}
}

  i.foo(""Eclipse"");
 
 root.findDeclaringNode(methodBinding);
The code below is all in compilation unit Inline.java
public class Inline<T> {
	void foo(T t) {
		System.out.println(t);
	}
}
class Use {
	public static void main(String[] args) {
		Inline<String> i= null;
		i.foo(""Eclipse"");
	}
}
- take the method binding of the invocation foo in i.foo(""Eclipse"");
- take the root node representing the whole CU
- call root.findDeclaringNode(methodBinding);
observe: null is returned although the CU contains the corresponding declaration.
use type parameters happen for fields","org.eclipse.jdt.core.dom.CompilationUnit
org.eclipse.jdt.core.dom.DefaultBindingResolver"
FILE,eclipse-3.1,87569,2005-03-09T16:41:00.000-06:00,Infinte loop obtaining image when switching to Debug Perspective,"class which implements java.io.Serializable
I20050308-1510
Create a Java Project using Java 5 (not sure it matters)
Create a class which implements java.io.Serializable
Use the ""Add generated serial version ID"" quickfix
Switch to the Debug perspective
3XMTHREADINFO      ""main"" (TID:0x02A08A00, sys_thread_t:0x000356F4, state:CW,
native ID:0x000009F4) prio=6
4XESTACKTRACE          at java/lang/Throwable.printStackTrace(Throwable.java:241)
4XESTACKTRACE          at
org/eclipse/core/runtime/CoreException.printStackTrace(CoreException.java:94)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseLog.getStackTrace(EclipseLog.java:316)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseLog.writeStack(EclipseLog.java:372)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseLog.writeLog(EclipseLog.java:337)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseLog.log(EclipseLog.java:208)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/PlatformLogWriter.logging(PlatformLogWriter.java:35)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform$1.run(InternalPlatform.java:831)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform.run(InternalPlatform.java:1015)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform.log(InternalPlatform.java:834)
4XESTACKTRACE          at org/eclipse/core/internal/runtime/Log.log(Log.java:56)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DebugUIPlugin.log(DebugUIPlugin.java:497)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DebugUIPlugin.log(DebugUIPlugin.java:506)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImageKey(DefaultLabelProvider.java:133)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:57)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
...
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/DebugElementHelper.getImageDescriptor(DebugElementHelper.java:55)
4XESTACKTRACE          at
org/eclipse/debug/ui/DebugElementWorkbenchAdapter.getImageDescriptor(DebugElementWorkbenchAdapter.java:37)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DefaultLabelProvider.getImage(DefaultLabelProvider.java:61)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getDefaultImage(DelegatingModelPresentation.java:198)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/DelegatingModelPresentation.getImage(DelegatingModelPresentation.java:150)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/DebugViewInterimLabelProvider.getImage(DebugViewInterimLabelProvider.java:62)
4XESTACKTRACE          at
org/eclipse/jface/viewers/DecoratingLabelProvider.getImage(DecoratingLabelProvider.java:82)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/RemoteTreeViewer.doUpdateItem(RemoteTreeViewer.java:448)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer$UpdateItemSafeRunnable.run(AbstractTreeViewer.java:86)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform.run(InternalPlatform.java:1015)
4XESTACKTRACE          at org/eclipse/core/runtime/Platform.run(Platform.java:757)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.doUpdateItem(AbstractTreeViewer.java:490)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer$UpdateItemSafeRunnable.run(StructuredViewer.java:352)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform.run(InternalPlatform.java:1015)
4XESTACKTRACE          at org/eclipse/core/runtime/Platform.run(Platform.java:757)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer.updateItem(StructuredViewer.java:1655)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.updateChildren(AbstractTreeViewer.java:1621)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.internalRefreshStruct(AbstractTreeViewer.java:1109)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.internalRefresh(AbstractTreeViewer.java:1086)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.internalRefresh(AbstractTreeViewer.java:1047)
4XESTACKTRACE          at
org/eclipse/jface/viewers/AbstractTreeViewer.internalRefresh(AbstractTreeViewer.java:1034)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer$7.run(StructuredViewer.java:1172)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer.preservingSelection(StructuredViewer.java:1109)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer.refresh(StructuredViewer.java:1170)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/LaunchViewer.refresh(LaunchViewer.java:80)
4XESTACKTRACE          at
org/eclipse/jface/viewers/StructuredViewer.refresh(StructuredViewer.java:1129)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/AbstractDebugEventHandler.refresh(AbstractDebugEventHandler.java:255)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/AbstractDebugEventHandler.viewBecomesVisible(AbstractDebugEventHandler.java:348)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/AbstractDebugEventHandlerView.becomesVisible(AbstractDebugEventHandlerView.java:69)
4XESTACKTRACE          at
org/eclipse/debug/internal/ui/views/launch/LaunchView.becomesVisible(LaunchView.java:1061)
4XESTACKTRACE          at
org/eclipse/debug/ui/AbstractDebugView$DebugViewPartListener.partVisible(AbstractDebugView.java:162)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartListenerList2$7.run(PartListenerList2.java:168)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/InternalPlatform.run(InternalPlatform.java:1015)
4XESTACKTRACE          at org/eclipse/core/runtime/Platform.run(Platform.java:757)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartListenerList2.fireEvent(PartListenerList2.java:54)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartListenerList2.firePartVisible(PartListenerList2.java:166)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage$1.propertyChange(WorkbenchPage.java:179)
4XESTACKTRACE          at
org/eclipse/ui/internal/LayoutPart.setVisible(LayoutPart.java:305)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartPane.setVisible(PartPane.java:331)
4XESTACKTRACE          at
org/eclipse/ui/internal/ViewPane.setVisible(ViewPane.java:614)
4XESTACKTRACE          at
org/eclipse/ui/internal/presentations/PresentablePart.setVisible(PresentablePart.java:126)
4XESTACKTRACE          at
org/eclipse/ui/internal/presentations/newapi/PresentablePartFolder.select(PresentablePartFolder.java:266)
4XESTACKTRACE          at
org/eclipse/ui/internal/presentations/newapi/LeftToRightTabOrder.select(LeftToRightTabOrder.java:65)
4XESTACKTRACE          at
org/eclipse/ui/internal/presentations/newapi/TabbedStackPresentation.selectPart(TabbedStackPresentation.java:391)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartStack.refreshPresentationSelection(PartStack.java:1051)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartStack.createControl(PartStack.java:536)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartStack.createControl(PartStack.java:473)
4XESTACKTRACE          at
org/eclipse/ui/internal/PartSashContainer.createControl(PartSashContainer.java:485)
4XESTACKTRACE          at
org/eclipse/ui/internal/PerspectiveHelper.activate(PerspectiveHelper.java:230)
4XESTACKTRACE          at
org/eclipse/ui/internal/Perspective.onActivate(Perspective.java:773)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage.setPerspective(WorkbenchPage.java:2829)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage.busySetPerspective(WorkbenchPage.java:845)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage.access$10(WorkbenchPage.java:830)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage$13.run(WorkbenchPage.java:2980)
4XESTACKTRACE          at
org/eclipse/swt/custom/BusyIndicator.showWhile(BusyIndicator.java:69)
4XESTACKTRACE          at
org/eclipse/ui/internal/WorkbenchPage.setPerspective(WorkbenchPage.java:2978)
4XESTACKTRACE          at
org/eclipse/ui/internal/PerspectiveBarContributionItem.select(PerspectiveBarContributionItem.java:109)
4XESTACKTRACE          at
org/eclipse/ui/internal/PerspectiveBarManager$1.widgetSelected(PerspectiveBarManager.java:145)
4XESTACKTRACE          at
org/eclipse/swt/widgets/TypedListener.handleEvent(TypedListener.java:89)
4XESTACKTRACE          at
org/eclipse/swt/widgets/EventTable.sendEvent(EventTable.java:82)
4XESTACKTRACE          at org/eclipse/swt/widgets/Widget.sendEvent(Widget.java:842)
4XESTACKTRACE          at
org/eclipse/swt/widgets/Display.runDeferredEvents(Display.java:2894)
4XESTACKTRACE          at
org/eclipse/swt/widgets/Display.readAndDispatch(Display.java:2527)
4XESTACKTRACE          at
org/eclipse/ui/internal/PerspectiveBarManager.handleChevron(PerspectiveBarManager.java:161)
4XESTACKTRACE          at
org/eclipse/ui/internal/PerspectiveSwitcher$9.widgetSelected(PerspectiveSwitcher.java:766)
4XESTACKTRACE          at
org/eclipse/swt/widgets/TypedListener.handleEvent(TypedListener.java:89)
4XESTACKTRACE          at
org/eclipse/swt/widgets/EventTable.sendEvent(EventTable.java:82)
4XESTACKTRACE          at org/eclipse/swt/widgets/Widget.sendEvent(Widget.java:842)
4XESTACKTRACE          at
org/eclipse/swt/widgets/Display.runDeferredEvents(Display.java:2894)
4XESTACKTRACE          at
org/eclipse/swt/widgets/Display.readAndDispatch(Display.java:2527)
4XESTACKTRACE          at
org/eclipse/ui/internal/Workbench.runEventLoop(Workbench.java:1514)
4XESTACKTRACE          at
org/eclipse/ui/internal/Workbench.runUI(Workbench.java:1478)
4XESTACKTRACE          at
org/eclipse/ui/internal/Workbench.createAndRunWorkbench(Workbench.java:297)
4XESTACKTRACE          at
org/eclipse/ui/PlatformUI.createAndRunWorkbench(PlatformUI.java:143)
4XESTACKTRACE          at
org/eclipse/ui/internal/ide/IDEApplication.run(IDEApplication.java:103)
4XESTACKTRACE          at
org/eclipse/core/internal/runtime/PlatformActivator$1.run(PlatformActivator.java:228)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseStarter.run(EclipseStarter.java:338)
4XESTACKTRACE          at
org/eclipse/core/runtime/adaptor/EclipseStarter.run(EclipseStarter.java:151)
4XESTACKTRACE          at sun/reflect/NativeMethodAccessorImpl.invoke0(Native
Method)
4XESTACKTRACE          at
sun/reflect/NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:85)
4XESTACKTRACE          at
sun/reflect/NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:58)
4XESTACKTRACE          at
sun/reflect/DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:60)
4XESTACKTRACE          at java/lang/reflect/Method.invoke(Method.java:391)
4XESTACKTRACE          at
org/eclipse/core/launcher/Main.invokeFramework(Main.java:268)
4XESTACKTRACE          at org/eclipse/core/launcher/Main.basicRun(Main.java:260)
4XESTACKTRACE          at org/eclipse/core/launcher/Main.run(Main.java:887)
4XESTACKTRACE          at org/eclipse/core/launcher/Main.main(Main.java:871)",org.eclipse.debug.internal.ui.DefaultLabelProvider
FILE,eclipse-3.1,87665,2005-03-10T11:38:00.000-06:00,Clicking on x on performance page opens details with no errors,"testOpenJavaEditor1()
Take a look at:
http://fullmoon.rtp.raleigh.ibm.com/downloads/drops/M-3.0.2RC2-200502161722/performance/org.eclipse.jdt.text.php?
Scroll down to performance.OpenJavaEditorStressTest#testOpenJavaEditor1()""
Observe: red x for RHEL 3.0 Sun 1.4.2_06.
Click on the red x ==> details show up all green.
Looks like a bug regarding the handling of negative numbers.","org.eclipse.swt.printing.PrintDialog
org.eclipse.swt.widgets.MessageBox"
FILE,eclipse-3.1,89632,2005-03-30T13:10:00.000-06:00,Exception when trying to evaluate in Snippet Editor,"Collection<String> c = new ArrayList<String>();
        c.add(""a"");
        c.add(""b"");
        c.add(""c"");

        for (Iterator<String> i = c.iterator(); i.hasNext(); )
            if (i.next().length() == 4)
            {
                String x = i.next();
                System.out.println(x);
            }
        
        return c;

 
   
  run()
Testcase:
Collection<String> c = new ArrayList<String>();
c.add(""a"");
c.add(""b"");
c.add(""c"");
for (Iterator<String> i = c.iterator(); i.hasNext(); )
if (i.next().
length() == 4)
{
String x = i.next();
System.out.println(x);
} return c;
I added the testcase to the snippet editor.
I then did a ""Set Imports..."" to include java.util.
* to resolve collection and iterator.
Trying a ""Display"" or ""Inspect"" resulted in the following error in the console:
java.lang.VerifyError: arguments are not type compatible (class: CodeSnippet_2 method: run()V) at pc: 57
at java.lang.Class.verifyImpl(Native Method)
at java.lang.Class.verify(Class.java:254)
at java.lang.Class.initialize(Class.java:317)
at java.lang.Class.forNameImpl(Native Method)
at java.lang.Class.forName(Class.java:128)
at org.eclipse.jdt.internal.debug.ui.snippeteditor.ScrapbookMain1.eval
(ScrapbookMain1.java:20)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:46)
at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:611)
at org.eclipse.jdt.internal.debug.ui.snippeteditor.ScrapbookMain.evalLoop
(ScrapbookMain.java:54)
at org.eclipse.jdt.internal.debug.ui.snippeteditor.ScrapbookMain.main
(ScrapbookMain.java:35)",org.eclipse.jdt.internal.eval.CodeSnippetMessageSend
FILE,eclipse-3.1,90283,2005-04-05T08:56:00.000-05:00,[WorkbenchParts]IPartListener2#partInputChanged is not being sent,"partActivated(IWorkbenchPartReference ref)  
 ref.getId()  
 ref.getPart(true)  getSite()  
 ASTProvider.ActivationListener.isJavaEditor()
3.1 M6
We (ASTProvider) receive partActivated(IWorkbenchPartReference ref) where:
ref.getId() -> null ref.getPart(true).
getSite() -> correct id.
Test Case:
0. enable mark occurrences
1. enable search to reuse the editor (see Search preference page)
2. do a Java Search where you have matches in more than one file
3. select the first match
4. step through the matches until the second file gets opened
5. activate the editor by clicking on the tab
see null value put breakpoint
Might be related to bug 89374.",org.eclipse.ui.texteditor.AbstractTextEditor
FILE,eclipse-3.1,90289,2005-04-05T09:17:00.000-05:00,>1 debug worker thread calling IStackFrame.getVariables(),"IStackFrame.getVariables()
M6 driver
If I add a Suspsend VM breakpoint on the call to IStackFrame.getVariables() in 
my StackFrame object, I am seeing it hit 2 or more times.
show duplicates as result show duplicates of local variables","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.registers.RegistersView
org.eclipse.debug.internal.ui.views.registers.RegistersViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
FILE,eclipse-3.1,91098,2005-04-12T06:07:00.000-05:00,The Mark Occurrences feature does not mark all occurrences,"String a;
String[] b;
String[][] c;
The precise test case is the following:
String a;
String[] b;
String[][] c;
Put the cursor on String or String[].
Now put the cursor on String[][].
It
looks like there is a missing loop when removing square brackets ;o)
I use 3.1M6.
Best regards,
Cyril",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,91346,2005-04-13T16:43:00.000-05:00,available property reference not found for marking occurrences,"{buildDirectory}
<project name=""project"" default=""default"">
	<property name=""buildDirectory"" location=""C:\buildDirectory"" />
	<target name=""default"">
		<available property=""${buildDirectory}"" file=""available2.xml"" />
		<echo message=""${C:\buildDirectory2}""></echo>
	</target>
</project>
Cursor in property=""${buildDirectory}"" does not mark the property declaration 
occurrence",org.eclipse.ant.internal.ui.model.AntPropertyNode
FILE,eclipse-3.1,92236,2005-04-21T11:12:00.000-05:00,ConcurrentModificationException on shutdown,"org.eclipse.team.internal.core.TeamPlugin.stop()
Build: I20050419
find following exception in log
It occurred while I was shutting down my workspace last night:
org.osgi.framework.BundleException: Exception in org.eclipse.team.internal.core.TeamPlugin.stop() of bundle org.eclipse.team.core.
at
org.eclipse.osgi.framework.internal.core.BundleContextImpl.stop(BundleContextImpl.java:1061)
at
org.eclipse.osgi.framework.internal.core.BundleHost.stopWorker(BundleHost.java:402)
at
org.eclipse.osgi.framework.internal.core.AbstractBundle.stop(AbstractBundle.java:410)
at
org.eclipse.core.runtime.adaptor.BundleStopper.basicStopBundles(BundleStopper.java:73)
at
org.eclipse.core.runtime.adaptor.BundleStopper.stopBundles(BundleStopper.java:62)
at
org.eclipse.core.runtime.adaptor.EclipseAdaptor.frameworkStopping(EclipseAdaptor.java:695)
at org.eclipse.osgi.framework.internal.core.Framework.shutdown(Framework.java:502)
at
org.eclipse.osgi.framework.internal.core.SystemBundle$1.run(SystemBundle.java:171)
at java.lang.Thread.run(Thread.java:813)
Caused by: java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:930)
at java.util.HashMap$KeyIterator.next(HashMap.java:966)
at
org.eclipse.team.internal.core.ResourceVariantCache.shutdown(ResourceVariantCache.java:102)
at org.eclipse.team.internal.core.TeamPlugin.stop(TeamPlugin.java:81)
at
org.eclipse.osgi.framework.internal.core.BundleContextImpl$3.run(BundleContextImpl.java:1045)
at java.security.AccessController.doPrivileged(AccessController.java:202)
at
org.eclipse.osgi.framework.internal.core.BundleContextImpl.stop(BundleContextImpl.java:1041)
at
org.eclipse.osgi.framework.internal.core.BundleHost.stopWorker(BundleHost.java:402)
at
org.eclipse.osgi.framework.internal.core.AbstractBundle.stop(AbstractBundle.java:410)
at
org.eclipse.core.runtime.adaptor.BundleStopper.basicStopBundles(BundleStopper.java:73)
at
org.eclipse.core.runtime.adaptor.BundleStopper.stopBundles(BundleStopper.java:62)
at
org.eclipse.core.runtime.adaptor.EclipseAdaptor.frameworkStopping(EclipseAdaptor.java:695)
at org.eclipse.osgi.framework.internal.core.Framework.shutdown(Framework.java:502)
at
org.eclipse.osgi.framework.internal.core.SystemBundle$1.run(SystemBundle.java:171)
at java.lang.Thread.run(Thread.java:813)",org.eclipse.team.internal.core.ResourceVariantCache
FILE,eclipse-3.1,92451,2005-04-22T16:36:00.000-05:00,code assist failure: new+cast+arrays,"public class Test {
	public static void main(String[] args) {
		java.util.List elements = null;
		// code assist works on this line
		new Test(Test.toStrings((Test[])elements.toArray(new Test
[0])));
		//code assist fails on this line
	}
	public Test(Object object) {
	}
	public static Object toStrings(Test[] objects) {
		return null;
	}
}
I20050419
J2SE 5 (but also fails in JDK 1.4)
Code assist fails in the following (self-contained) class (see comments for line of error)
public class Test { public static void main(String[] args) { java.util.List elements = null;
// code assist works on this line new Test(Test.toStrings((Test[])elements.toArray(new Test
[0])));
//code assist fails on this line
} public Test(Object object) {
} public static Object toStrings(Test[] objects) { return null;
}
}",org.eclipse.jdt.internal.codeassist.complete.CompletionParser
FILE,eclipse-3.1,93249,2005-04-29T05:49:00.000-05:00,Code assist doesn't propose full method stub,"IRunnableWithProgress runnable= new IRunnableWithProgress() {
};

  
  
 public void run(org.eclipse.core.runtime.IProgressMonitor monitor) throws
InvocationTargetException, InterruptedException
- take revision 1.8 of BuildPathAction.
- in run method add the following
IRunnableWithProgress runnable= new IRunnableWithProgress() {
};
- inside the runnable type run<code assist> and select run
public void run(org.eclipse.core.runtime.IProgressMonitor monitor) throws
InvocationTargetException, InterruptedException","org.eclipse.jdt.internal.codeassist.CompletionEngine
org.eclipse.jdt.internal.ui.text.java.OverrideCompletionProposal"
FILE,eclipse-3.1,93727,2005-05-04T17:43:00.000-05:00,Code Formatter fails with Method Parameter Annotations,"import org.drools.semantics.annotation.DroolsParameter;

public class Test
{
  public Object passthrough( @DroolsParameter(""parameter"") Object parameter ) {
    return parameter;
  }
}
The eclipse code formatter doesn't seem to work when you have methods with
parameter annotations.
not see error in <Workspace> /
metadata/.
log.
Example:
import org.drools.semantics.annotation.DroolsParameter;
public class Test
{
  public Object passthrough( @DroolsParameter(""parameter"") Object parameter ) {
    return parameter;
  }
}",org.eclipse.jdt.internal.formatter.CodeFormatterVisitor
FILE,eclipse-3.1,94201,2005-05-09T17:08:00.000-05:00,Applet Contextual Launch Action broken,"public class MyApplet extends Applet {
	private static final long serialVersionUID = 1L;

	public void paint(Graphics graphics) {
		graphics.drawString(""Hello World"", 50, 100);
	}
}
Example Code:
public class MyApplet extends Applet {
	private static final long serialVersionUID = 1L;
public void paint(Graphics graphics) {
		graphics.drawString(""Hello World"", 50, 100);
	}
}
When run from the context Menu an error message is display ""No Applet Found""
Manually creating a launch config via the lcd works fine.",org.eclipse.jdt.internal.debug.ui.launcher.AppletLaunchConfigurationUtils
FILE,eclipse-3.1,94216,2005-05-09T20:04:00.000-05:00,Open type does not work for generic types,"interface IGeneric<T> {
}
 public class Generic<T> implements IGeneric<T> {
    public static void main(String[] args) {
        IGeneric<String> gen= new Generic<String>();
        System.out.println();  // <-- breakpoint here
    }
}
interface IGeneric<T> {
} public class Generic<T> implements IGeneric<T> { public static void main(String[] args) {
IGeneric<String> gen= new Generic<String>();
System.out.println();  // <-- breakpoint here
}
}
Try to do 'open declaring type' or 'open concrete type' for 'gen' at the breakpoint, nothing happens.","org.eclipse.jdt.internal.debug.ui.actions.OpenVariableDeclaredTypeAction
org.eclipse.jdt.internal.debug.ui.actions.OpenVariableConcreteTypeAction"
FILE,eclipse-3.1,94465,2005-05-10T14:33:00.000-05:00,Java Core Dump where modifying value in the Variables View.,"String [] elms= { ""abc"", ""cde"", ""xyz"" };
Test case:
String [] elms= { ""abc"", ""cde"", ""xyz"" };
I have a string array.
1. In the variables, expand the array, and then expand the first element, [0]
=""abc"".
2. Select the ""value=char[3]"" field.
RMC->Change Value.
3. In the Change Primitive Value dialog, type in a new string value.
4. Click ok and it will result in a java dump.
Got the following error in the console:
JVMDG217: Dump Handler is Processing Signal 11 - Please Wait.
JVMDG303: JVM Requesting Java core file
JVMDG304: Java core file written to D:\eclipse3.1\I20050509
\eclipse\workspace\YetAnotherProj\javacore.20050510.142923.2576.txt
JVMDG215: Dump Handler has Processed Exception Signal 11.
Runnign IBM JVM 1.4.2","org.eclipse.jdt.internal.debug.ui.JDIModelPresentation
org.eclipse.jdt.internal.debug.ui.actions.JavaObjectValueEditor
org.eclipse.jdt.internal.debug.ui.actions.ActionMessages"
FILE,eclipse-3.1,95096,2005-05-13T06:16:00.000-05:00,[5.0][content assist] Content assist popup disappears while completing the statically imported method name,"import static java.lang.Math
I20050513-0010
Steps to reproduce:
- Create a new Class ""Foo""
- Type ""import static java.lang.Math.""
- Press Ctrl+Space
- Type ""a""
constrain proposals to members","org.eclipse.jdt.internal.ui.text.java.JavaMethodCompletionProposal
org.eclipse.jdt.internal.ui.text.java.LazyJavaCompletionProposal"
FILE,eclipse-3.1,95152,2005-05-13T12:14:00.000-05:00,[search] F3 can't find synthetic constructor_,"InputReadJob readJob = new InputReadJob(streamsProxy);
Build: I20050513-0010
1) Add org.eclipse.debug.ui to the search path (i.e., by clicking ""Add to Java
Search"" in the plugins view.
2) Open type on ""ProcessConsole"" (class file with source attached)
3) Go to line 483:
InputReadJob readJob = new InputReadJob(streamsProxy);
4) Highlight the InputReadJob constructor_ and hit F3.
-> It opens a new class file editor, positioned at the top of the file.
5) The outline view in this editor has the constructor_
InputReadJob(ProcessConsole, IStreamsProxy).
click entry in outline view not jump to constructor _
not handle synthetic addition of enclosing class not handle synthetic addition by compiler
break kind of navigation break kind to corresponding constructor _","org.eclipse.ant.internal.ui.views.AntViewDropAdapter
org.eclipse.ant.internal.ui.launchConfigurations.AntLaunchShortcut
org.eclipse.ant.internal.ui.AntUtil
org.eclipse.jdt.internal.core.search.matching.ConstructorLocator
org.eclipse.jdt.internal.core.search.indexing.BinaryIndexer
org.eclipse.jdt.internal.core.index.DiskIndex
org.eclipse.jdt.internal.core.search.matching.ConstructorPattern"
FILE,eclipse-3.1,95505,2005-05-17T02:56:00.000-05:00,Can not use code completion,"{cursor}
Eclipse 3.1 M7
know type write type press ctrl + space show type
For example:
button.addActionListener(new {cursor})
But in M7 I have now look, what type should be used and write several first
letters?
Why?
Please return old behaviour, at least make this optional.",org.eclipse.jdt.internal.codeassist.CompletionEngine
FILE,eclipse-3.1,96440,2005-05-24T11:11:00.000-05:00,Tables laying out 3 times when trying to determine sizes,"table.getClientArea()
20050522
See Bug 93611
When we attempt to layout a table we are scaling columns with
table.getClientArea().
width.
In M6 this returned the width of the table - post
M6 it is returning a much smaller value and making all of our columns very small
in the Table Layout.
STEPS
1 Put a breakpoint in the JFace TableLayout class
2 Launch a self hosted workspace
3 Open Preferences-> Ant Runtime
4 You will see a client area size of about 81
5 Do the same in M6 - it will be about 320 or so.",org.eclipse.jface.preference.PreferencePage
FILE,eclipse-3.1,96489,2005-05-24T14:40:00.000-05:00,[Presentations] (regression) Standalone view without title has no border,"layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
build N20050523
- change the browser example's BrowserPerspectiveFactory to have the following instead of the regular addView layout.addStandaloneView(BrowserApp.BROWSER_VIEW_ID, false,
IPageLayout.RIGHT, .25f, IPageLayout.ID_EDITOR_AREA);
- run the example, and show the history view
have border
This is a regression from 3.0.2.","org.eclipse.ui.presentations.WorkbenchPresentationFactory
org.eclipse.ui.internal.presentations.defaultpresentation.EmptyTabFolder"
FILE,eclipse-3.1,96766,2005-05-26T07:47:00.000-05:00,Console hyperlinks broken by 3.1M7,"public class Tst {
    
    public static void main(String[] args) throws Exception {
        
        System.out.println(""Log: Tst.main(Tst.java:5) Some message"");
        System.out.println(""Log: Tst.main(Tst.java:6)"");
    }
}
In releases prior to 3.1M7, you could print log messages which acted as
hyperlinks in the console.
A useful feature for speedy development.
In 3.1M7 the
parsing of the line number has been changed to require a new line following the
final "")"".
Example:
public class Tst {
    
    public static void main(String[] args) throws Exception {
        
        System.out.println(""Log: Tst.main(Tst.java:5) Some message"");
        System.out.println(""Log: Tst.main(Tst.java:6)"");
    }
}
Run the above in Eclipse.
give hyperlink error with parse line number give hyperlink error from hyperlink work in console
Fix would be to return the parsing to just need a terminating "")"", rather than
"")\n"".
This was the functionality in 3.1M6 and before.",org.eclipse.jdt.internal.debug.ui.console.JavaStackTraceHyperlink
FILE,eclipse-3.1,96820,2005-05-26T12:27:00.000-05:00,JME during Source lookup,"enable()
N20050526-0010
I work in a full source workspace (ZRH plugins from HEAD, all other plugins imported as source).
I set a breakpoint in ContentAssistHandler#enable()
(in plugin org.eclipse.ui.workbench.texteditor), and started a run-time workbench.
After opening the Find/Replace dialog and enabling ""Regular Expressions"", I got a ""Source not found."" editor and the JME below.
Since I imported all projects as source, a do have a project org.eclipse.core.boot, but it is not a Java project.
Error 2005-05-26 17:50:36.347 Error logged from Debug Core:
Java Model Exception: Java Model Status [org.eclipse.core.boot does not exist] at
org.eclipse.jdt.internal.core.JavaElement.newNotPresentException(JavaElement.java:468)
at
org.eclipse.jdt.internal.core.JavaModelManager.getPerProjectInfoCheckExistence(JavaModelManager.java:1200)
at
org.eclipse.jdt.internal.core.JavaProject.getPerProjectInfo(JavaProject.java:1794)
at org.eclipse.jdt.internal.core.JavaProject.getRawClasspath(JavaProject.java:1851)
at org.eclipse.jdt.internal.core.JavaProject.getRawClasspath(JavaProject.java:1837)
at
org.eclipse.jdt.launching.sourcelookup.containers.JavaProjectSourceContainer.createSourceContainers(JavaProjectSourceContainer.java:92)
at
org.eclipse.debug.core.sourcelookup.containers.CompositeSourceContainer.getSourceContainers(CompositeSourceContainer.java:126)
at
org.eclipse.jdt.launching.sourcelookup.containers.JavaProjectSourceContainer.findSourceElements(JavaProjectSourceContainer.java:133)
at
org.eclipse.debug.core.sourcelookup.AbstractSourceLookupParticipant.findSourceElements(AbstractSourceLookupParticipant.java:60)
at
org.eclipse.debug.core.sourcelookup.AbstractSourceLookupDirector$SourceLookupQuery.run(AbstractSourceLookupDirector.java:126)
at
org.eclipse.core.internal.runtime.InternalPlatform.run(InternalPlatform.java:1038)
at org.eclipse.core.runtime.Platform.run(Platform.java:775)
at
org.eclipse.debug.core.sourcelookup.AbstractSourceLookupDirector.doSourceLookup(AbstractSourceLookupDirector.java:465)
at
org.eclipse.debug.core.sourcelookup.AbstractSourceLookupDirector.getSourceElement(AbstractSourceLookupDirector.java:715)
at
org.eclipse.debug.internal.ui.sourcelookup.SourceLookupFacility.lookup(SourceLookupFacility.java:138)
at org.eclipse.debug.ui.DebugUITools.lookupSource(DebugUITools.java:658)
at
org.eclipse.debug.internal.ui.views.launch.LaunchView$SourceLookupJob.run(LaunchView.java:176)
at org.eclipse.core.internal.jobs.Worker.run(Worker.java:67)",org.eclipse.jdt.internal.launching.JavaSourceLookupUtil
FILE,eclipse-3.1,97722,2005-05-31T16:41:00.000-05:00,Pref Page Ant/Runtime/Tasks/Add Task dialog problems,"@

Dialog
crop error message at bottom
@@
Dialog font used: Trebuchet MS, size 11",org.eclipse.ant.internal.ui.preferences.AddCustomDialog
FILE,eclipse-3.1,98147,2005-06-02T13:09:00.000-05:00,Variables View does not show all children if same instance is expanded twice,"package xy;
public class Try {
	String fName;
	int fID;
	
	public Try(String name, int id) {
		fName= name;
		fID= id;
	}
	
	public static void main(String[] args) {
		Try t= new Try(""Hello"", 5);
		callee(t, t);
	}
	
	static void callee(Try t1, Try t2) {
		boolean same= t1.equals(t2); //breakpoint here
	}
	
}
N20050602-0010
- Debug the class below with the breakpoint where indicated.
- Expand t1 in the Variables view -> expands fine and shows fID and fName.
- Expand t2 -> only child fID is shown
package xy;
public class Try {
String fName;
int fID;
public Try(String name, int id) { fName= name;
fID= id;
} public static void main(String[] args) {
Try t= new Try(""Hello"", 5);
callee(t, t);
} static void callee(Try t1, Try t2) { boolean same= t1.equals(t2); //breakpoint here
}
}",org.eclipse.debug.internal.ui.views.RemoteTreeViewer
FILE,eclipse-3.1,98621,2005-06-06T22:04:00.000-05:00,[refactoring] [rename] Rename Type hangs,"class I18L  
 public class I18N {

	protected static void loadMessages(Class clazz, String name) {
		...
	}
}

 
 public class Messages extends I18L {
  public static String unexpectedException;
  ...
  static {
    loadMessages(Messages.class, ""messages.properties"");
  }
}
I renamed a (foolishly misspelled) class I18L to I18N.
The class looked like this:
public class I18N {
protected static void loadMessages(Class clazz, String name) {
		...
	}
}
The class was extended in 10 subclasses in 8 projects, like this:
public class Messages extends I18L {
  public static String unexpectedException;
  ...
  static {
    loadMessages(Messages.class, ""messages.properties"");
  }
}
I clicked OK in the dialog.
After about 5 minutes, I clicked Cancel.
Clicking the window exit
box had no effect.
I didn't have a Java console window so couldn't get a thread
dump.
I finally killed Eclipse with the Task Manager (WinXP SP2).
When I restarted Eclipse, 7 of the references had been changed to I18N and 3 had
not.
Open Type (after re-indexing its database) still shows the non-existant
I18L type, though if you try to open it, the path cannot be found.","org.eclipse.core.internal.jobs.ImplicitJobs
org.eclipse.core.internal.jobs.ThreadJob"
FILE,eclipse-3.1,98740,2005-06-07T13:25:00.000-05:00,Container attempts to refresh children on project that is not open,"String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$ 
IProjectDescription description = ResourcesPlugin.getWorkspace
().loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().getRoot().getProject
(description.getName());
project.create(description, new NullProgressMonitor());

  project.open()  
 The members()  
 if (info.isSet(ICoreConstants.M_CHILDREN_UNKNOWN))
			workspace.refreshManager.refresh(this);
Take an existing simple project on disk and import the project into the workspace by performing a simple create with code like:
String folder = ""/temp"";//$NON-NLS-1$
String projName = ""project"";//$NON-NLS-1$
IProjectDescription description = ResourcesPlugin.getWorkspace
().
loadProjectDescription(projPath);
IProject project = ResourcesPlugin.getWorkspace().
getRoot().
getProject
(description.getName());
project.create(description, new NullProgressMonitor());
Do not open the project with the project.open() API.
This is the key to the issue.
Now create a project either by API or UI and open it.
Or simply switch to the
Java perspective.
start background refresh job for closed project stick in infinite loop
I believe the offending code is in the class org.eclipse.core.internal.resources.Container.
because the projects members are not known.
call members on iproject
If you override this method in Project and do not refresh for closed projects, the problem goes away.
Our particular use case is that we are loading existing Java projects on disk by performing a create, but never an open.
On the next UI gesture, we get refresh infinite loops, one for each closed project.
We want the projects in the workspace, so we create them but do not open them, as open is very expensive.
This worked fine in Eclipse 3.0.","org.eclipse.core.internal.resources.Container
org.eclipse.core.internal.resources.Resource"
FILE,eclipse-3.1,99282,2005-06-09T19:46:00.000-05:00,[1.5][compiler] Enum / Switch method is not initialized in a thread safe way,"package com.bea;

public class TestEnumSwitch {
	
	public static synchronized void foo() {} 

	public static final void main(String args[]) {
		
		final TestEnum e = TestEnum.A1999;
		
		Thread[] runners = new Thread[40];
		for (int i = 0; i < runners.length; i++) {
			runners[i] = new Thread(new Runnable() {
				public void run() {
					switch (e) {
					case A1:
						System.err.println(""1"");
						break;
					case A2:
						System.err.println(""2"");
						break;
					case A8:
						System.err.println(""8"");
						break;
					case A13:
						System.err.println(""13"");
						break;
					case A1999:
						System.err.println(""1999"");
						break;
					default:
						System.err.println(""default"");
						break;
					}
					
				}
			});
		}
		
		for (int i = 0; i < runners.length; i++) {
			runners[i].start();
		}
		
	}
	
	public enum TestEnum {
		A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
		A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
		A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
		A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
		A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
		A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
		A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
		A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
		A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
		A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
		A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
		A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
		A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
		A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
		A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
		A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
		A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
		A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
		A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
		A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
		A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
		A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
		A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
		A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
		A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
		A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
		A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
		A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
		A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
		A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
		A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
		A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
		A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
		A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
		A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
		A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
		A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
		A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
		A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
		A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
		A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
		A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
		A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
		A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
		A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
		A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
		A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
		A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
		A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
		A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
		A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
		A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
		A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
		A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
		A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
		A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
		A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
		A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
		A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
		A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
		A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
		A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
		A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
		A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
		A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
		A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
		A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
		A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
		A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
		A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
		A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
		A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
		A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
		A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
		A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
		A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
		A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
		A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
		A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
		A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
		A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
		A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
		A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
		A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
		A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
		A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
		A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
		A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
		A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
		A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
		A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
		A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
		A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
		A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
		A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
		A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
		A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
		A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
		A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
		A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
		A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
		A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
		A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
		A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
		A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
		A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
		A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
		A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
		A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
		A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
		A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
		A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
		A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
	    A1999,
		}
}
The synthetic method that initializes the enum/switch table is not thread safe,
that is why javac places the initialization in the static initializer of an
anonymous class.
For example, the following program should print ""1999"" 40 times
(once from each thread).
print default on machine
package com.bea;
public class TestEnumSwitch {
	
	public static synchronized void foo() {}
public static final void main(String args[]) {
final TestEnum e = TestEnum.A1999;
Thread[] runners = new Thread[40];
for (int i = 0; i < runners.length; i++) {
runners[i] = new Thread(new Runnable() {
public void run() {
switch (e) {
case A1:
System.err.println(""1"");
break;
case A2:
System.err.println(""2"");
break;
case A8:
System.err.println(""8"");
break;
case A13:
System.err.println(""13"");
break;
case A1999:
System.err.println(""1999"");
break;
default:
System.err.println(""default"");
break;
}
}
});
}
for (int i = 0; i < runners.length; i++) {
runners[i].start();
}
}
public enum TestEnum {
A0, A1, A2, A3, A4, A5, A6, A7, A8, A9,
A10, A11, A12, A13, A14, A15, A16, A17, A18, A19,
A20, A21, A22, A23, A24, A25, A26, A27, A28, A29,
A30, A31, A32, A33, A34, A35, A36, A37, A38, A39,
A40, A41, A42, A43, A44, A45, A46, A47, A48, A49,
A50, A51, A52, A53, A54, A55, A56, A57, A58, A59,
A60, A61, A62, A63, A64, A65, A66, A67, A68, A69,
A70, A71, A72, A73, A74, A75, A76, A77, A78, A79,
A80, A81, A82, A83, A84, A85, A86, A87, A88, A89,
A90, A91, A92, A93, A94, A95, A96, A97, A98, A99,
A100, A101, A102, A103, A104, A105, A106, A107, A108, A109,
A110, A111, A112, A113, A114, A115, A116, A117, A118, A119,
A120, A121, A122, A123, A124, A125, A126, A127, A128, A129,
A130, A131, A132, A133, A134, A135, A136, A137, A138, A139,
A140, A141, A142, A143, A144, A145, A146, A147, A148, A149,
A150, A151, A152, A153, A154, A155, A156, A157, A158, A159,
A160, A161, A162, A163, A164, A165, A166, A167, A168, A169,
A170, A171, A172, A173, A174, A175, A176, A177, A178, A179,
A180, A181, A182, A183, A184, A185, A186, A187, A188, A189,
A190, A191, A192, A193, A194, A195, A196, A197, A198, A199,
A200, A201, A202, A203, A204, A205, A206, A207, A208, A209,
A210, A211, A212, A213, A214, A215, A216, A217, A218, A219,
A220, A221, A222, A223, A224, A225, A226, A227, A228, A229,
A230, A231, A232, A233, A234, A235, A236, A237, A238, A239,
A240, A241, A242, A243, A244, A245, A246, A247, A248, A249,
A250, A251, A252, A253, A254, A255, A256, A257, A258, A259,
A260, A261, A262, A263, A264, A265, A266, A267, A268, A269,
A270, A271, A272, A273, A274, A275, A276, A277, A278, A279,
A280, A281, A282, A283, A284, A285, A286, A287, A288, A289,
A290, A291, A292, A293, A294, A295, A296, A297, A298, A299,
A300, A301, A302, A303, A304, A305, A306, A307, A308, A309,
A310, A311, A312, A313, A314, A315, A316, A317, A318, A319,
A320, A321, A322, A323, A324, A325, A326, A327, A328, A329,
A330, A331, A332, A333, A334, A335, A336, A337, A338, A339,
A340, A341, A342, A343, A344, A345, A346, A347, A348, A349,
A350, A351, A352, A353, A354, A355, A356, A357, A358, A359,
A360, A361, A362, A363, A364, A365, A366, A367, A368, A369,
A370, A371, A372, A373, A374, A375, A376, A377, A378, A379,
A380, A381, A382, A383, A384, A385, A386, A387, A388, A389,
A390, A391, A392, A393, A394, A395, A396, A397, A398, A399,
A400, A401, A402, A403, A404, A405, A406, A407, A408, A409,
A410, A411, A412, A413, A414, A415, A416, A417, A418, A419,
A420, A421, A422, A423, A424, A425, A426, A427, A428, A429,
A430, A431, A432, A433, A434, A435, A436, A437, A438, A439,
A440, A441, A442, A443, A444, A445, A446, A447, A448, A449,
A450, A451, A452, A453, A454, A455, A456, A457, A458, A459,
A460, A461, A462, A463, A464, A465, A466, A467, A468, A469,
A470, A471, A472, A473, A474, A475, A476, A477, A478, A479,
A480, A481, A482, A483, A484, A485, A486, A487, A488, A489,
A490, A491, A492, A493, A494, A495, A496, A497, A498, A499,
A500, A501, A502, A503, A504, A505, A506, A507, A508, A509,
A510, A511, A512, A513, A514, A515, A516, A517, A518, A519,
A520, A521, A522, A523, A524, A525, A526, A527, A528, A529,
A530, A531, A532, A533, A534, A535, A536, A537, A538, A539,
A540, A541, A542, A543, A544, A545, A546, A547, A548, A549,
A550, A551, A552, A553, A554, A555, A556, A557, A558, A559,
A560, A561, A562, A563, A564, A565, A566, A567, A568, A569,
A570, A571, A572, A573, A574, A575, A576, A577, A578, A579,
A580, A581, A582, A583, A584, A585, A586, A587, A588, A589,
A590, A591, A592, A593, A594, A595, A596, A597, A598, A599,
A600, A601, A602, A603, A604, A605, A606, A607, A608, A609,
A610, A611, A612, A613, A614, A615, A616, A617, A618, A619,
A620, A621, A622, A623, A624, A625, A626, A627, A628, A629,
A630, A631, A632, A633, A634, A635, A636, A637, A638, A639,
A640, A641, A642, A643, A644, A645, A646, A647, A648, A649,
A650, A651, A652, A653, A654, A655, A656, A657, A658, A659,
A660, A661, A662, A663, A664, A665, A666, A667, A668, A669,
A670, A671, A672, A673, A674, A675, A676, A677, A678, A679,
A680, A681, A682, A683, A684, A685, A686, A687, A688, A689,
A690, A691, A692, A693, A694, A695, A696, A697, A698, A699,
A700, A701, A702, A703, A704, A705, A706, A707, A708, A709,
A710, A711, A712, A713, A714, A715, A716, A717, A718, A719,
A720, A721, A722, A723, A724, A725, A726, A727, A728, A729,
A730, A731, A732, A733, A734, A735, A736, A737, A738, A739,
A740, A741, A742, A743, A744, A745, A746, A747, A748, A749,
A750, A751, A752, A753, A754, A755, A756, A757, A758, A759,
A760, A761, A762, A763, A764, A765, A766, A767, A768, A769,
A770, A771, A772, A773, A774, A775, A776, A777, A778, A779,
A780, A781, A782, A783, A784, A785, A786, A787, A788, A789,
A790, A791, A792, A793, A794, A795, A796, A797, A798, A799,
A800, A801, A802, A803, A804, A805, A806, A807, A808, A809,
A810, A811, A812, A813, A814, A815, A816, A817, A818, A819,
A820, A821, A822, A823, A824, A825, A826, A827, A828, A829,
A830, A831, A832, A833, A834, A835, A836, A837, A838, A839,
A840, A841, A842, A843, A844, A845, A846, A847, A848, A849,
A850, A851, A852, A853, A854, A855, A856, A857, A858, A859,
A860, A861, A862, A863, A864, A865, A866, A867, A868, A869,
A870, A871, A872, A873, A874, A875, A876, A877, A878, A879,
A880, A881, A882, A883, A884, A885, A886, A887, A888, A889,
A890, A891, A892, A893, A894, A895, A896, A897, A898, A899,
A900, A901, A902, A903, A904, A905, A906, A907, A908, A909,
A910, A911, A912, A913, A914, A915, A916, A917, A918, A919,
A920, A921, A922, A923, A924, A925, A926, A927, A928, A929,
A930, A931, A932, A933, A934, A935, A936, A937, A938, A939,
A940, A941, A942, A943, A944, A945, A946, A947, A948, A949,
A950, A951, A952, A953, A954, A955, A956, A957, A958, A959,
A960, A961, A962, A963, A964, A965, A966, A967, A968, A969,
A970, A971, A972, A973, A974, A975, A976, A977, A978, A979,
A980, A981, A982, A983, A984, A985, A986, A987, A988, A989,
A990, A991, A992, A993, A994, A995, A996, A997, A998, A999,
A1000, A1001, A1002, A1003, A1004, A1005, A1006, A1007, A1008, A1009,
A1010, A1011, A1012, A1013, A1014, A1015, A1016, A1017, A1018, A1019,
A1020, A1021, A1022, A1023, A1024, A1025, A1026, A1027, A1028, A1029,
A1030, A1031, A1032, A1033, A1034, A1035, A1036, A1037, A1038, A1039,
A1040, A1041, A1042, A1043, A1044, A1045, A1046, A1047, A1048, A1049,
A1050, A1051, A1052, A1053, A1054, A1055, A1056, A1057, A1058, A1059,
A1060, A1061, A1062, A1063, A1064, A1065, A1066, A1067, A1068, A1069,
A1070, A1071, A1072, A1073, A1074, A1075, A1076, A1077, A1078, A1079,
A1080, A1081, A1082, A1083, A1084, A1085, A1086, A1087, A1088, A1089,
A1090, A1091, A1092, A1093, A1094, A1095, A1096, A1097, A1098, A1099,
A1100, A1101, A1102, A1103, A1104, A1105, A1106, A1107, A1108, A1109,
A1110, A1111, A1112, A1113, A1114, A1115, A1116, A1117, A1118, A1119,
A1120, A1121, A1122, A1123, A1124, A1125, A1126, A1127, A1128, A1129,
A1999,
}
}","org.eclipse.jdt.internal.compiler.lookup.SourceTypeBinding
org.eclipse.jdt.internal.compiler.codegen.CodeStream"
FILE,eclipse-3.1,99355,2005-06-10T09:48:00.000-05:00,extract method trips up with generics and final variables,"package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      final Container<String> container = c.get();
      final String string = container.get();
      //to here
   }
}
 
 

package p;

class Container<T>
{
   private final T m_t;

   public Container(T t)
   {
      m_t = t;
   }

   T get()
   {
      return m_t;
   }
}

class GenericContainer
{
   private final Container<?> m_c;

   public GenericContainer(Container<?> c) 
   {
      m_c = c;
   }

   public Container<?> getC()
   {
      return m_c;
   }
}

public class A
{
   GenericContainer createContainer()
   {
      final Container<String> innerContainer = new Container<String>(""hello"");
      final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
      return new GenericContainer(outerContainer);
   }
   
   void method()
   {
      final GenericContainer createContainer = createContainer();
      @SuppressWarnings(""unchecked"")
      final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
      //extract method from here
      extractedMethod(c);
      //to here
   }

   private void extractedMethod(final final final Container<Container<String>> c)
   {
      final Container<String> container = c.get();
      final String string = container.get();
   }
}
if you extract method where indicated below.
declare paramater with many final modifiers
-------------------------------------
package p;
class Container<T>
{ private final T m_t;
public Container(T t)
{ m_t = t;
}
T get()
{ return m_t;
}
}
class GenericContainer
{ private final Container<?> m_c;
public GenericContainer(Container<?> c)
{ m_c = c;
}
public Container<?> getC()
{ return m_c;
}
}
public class A
{
GenericContainer createContainer()
{ final Container<String> innerContainer = new Container<String>(""hello"");
final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
return new GenericContainer(outerContainer);
} void method()
{ final GenericContainer createContainer = createContainer();
@SuppressWarnings(""unchecked"")
final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
//extract method from here final Container<String> container = c.get();
final String string = container.get();
//to here
}
}
----------------------------------------------- results in
-----------------------------------------------
package p;
class Container<T>
{ private final T m_t;
public Container(T t)
{ m_t = t;
}
T get()
{ return m_t;
}
}
class GenericContainer
{ private final Container<?> m_c;
public GenericContainer(Container<?> c)
{ m_c = c;
}
public Container<?> getC()
{ return m_c;
}
}
public class A
{
GenericContainer createContainer()
{ final Container<String> innerContainer = new Container<String>(""hello"");
final Container<Container<String>> outerContainer = new
Container<Container<String>>(innerContainer);
return new GenericContainer(outerContainer);
} void method()
{ final GenericContainer createContainer = createContainer();
@SuppressWarnings(""unchecked"")
final Container<Container<String>> c = (Container<Container<String>>)
createContainer.getC();
//extract method from here extractedMethod(c);
//to here
}
private void extractedMethod(final final final Container<Container<String>> c)
{ final Container<String> container = c.get();
final String string = container.get();
}
}
-----------------------------------------------------------
notice final modifiers in extractedMethod signature",org.eclipse.jdt.core.dom.ASTConverter
FILE,eclipse-3.1,99693,2005-06-13T11:29:00.000-05:00,Invalid stack frames during display,"private static void doGenerics() {
		List<Integer> list = new ArrayList<Integer>();
		for (int i = 0; i < 1000; i++) {
			int num = rand.nextInt(10000) + 1;
			list.add(num);
		}
		
		int max = 0;
//start eval
		for (Integer integer : list) { // BREAKPOINT HERE
			max = Math.max(max, integer);
		}
		System.out.println(max);
//end eval
	}
Debug the following method to a breakpoint:
private static void doGenerics() {
List<Integer> list = new ArrayList<Integer>();
for (int i = 0; i < 1000; i++) { int num = rand.nextInt(10000) + 1;
list.add(num);
} int max = 0;
//start eval for (Integer integer : list) { // BREAKPOINT HERE max = Math.max(max, integer);
}
System.out.println(max);
//end eval
}
Select everything between start eval and end eval comments and ctrl-shift-d.
watch Variables view see lot of stack frames
Can we be smarter about not requesting and/or cancelling requests when the current stack frame is not valid?","org.eclipse.debug.internal.ui.views.variables.VariablesViewEventHandler
org.eclipse.debug.internal.ui.views.expression.ExpressionViewEventHandler"
CLASS,openjpa-2.2.0,OPENJPA-2163,2012-03-27T15:56:55.000-05:00,Lifecycle event callback occurs more often than expect,"final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
 
 MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
 
 ((OpenJPAEntityManagerSPI)em).addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).addLifecycleListener(l2, null);
A problem was uncovered in a scenario where multiple EntityManager instances created from the same EntityManagerFactory, and each instance is initialized with a new instance of a LifecycleListener instance, i.e.
final EntityManager em = factory.createEntityManager();
final EntityManager em2 = factory.createEntityManager();
...
MyLifecycleListener l1 = new MyLifecycleListener();
MyLifecycleListener l2 = new MyLifecycleListener();
...
((OpenJPAEntityManagerSPI)em).
addLifecycleListener(l1, null);
((OpenJPAEntityManagerSPI)em2).
addLifecycleListener(l2, null);
occur for specific entity manager create under emf","openjpa-kernel.src.main.java.org.apache.openjpa.conf.OpenJPAConfigurationImpl
openjpa-persistence-jdbc.src.test.java.org.apache.openjpa.persistence.validation.TestValidationMode"
CLASS,openjpa-2.2.0,OPENJPA-2227,2012-07-09T14:24:05.000-05:00,OpenJPA doesn't find custom SequenceGenerators,"{code}
 @Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable  
 @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
 {code}

 
     JavaTypes.classForName()     Class.forName()
I'm trying to use a custom SequenceGenerator within an enterprise application using openJPA (providing by WebSphere).
define custom Sequence insert data into database
ExampleConfiguration:
{code}
@Entity
@SequenceGenerator(name=""MySequence"", sequenceName=""org.apache.openjpa.generator.UIDGenerator()"")
public class Customer implements Serializable {
    @Id
    @GeneratedValue(strategy=GenerationType.SEQUENCE, generator=""MySequence"")
    private long id;
{code}
produce stacktrace
instantiate custom sequence class
A very similar issue seems to be: OPENJPA-758.
occur after deploying work with JavaSE deploy into WAS
I think within the method SequenceMetaData.instantiate(Classloader envLoader) the JavaTypes.classForName() -method with parameter mustExist=false should be used instead of the pure Class.forName() call.
But I'm not sure about the Metadata-parameter needed for this method call.",openjpa-kernel.src.main.java.org.apache.openjpa.meta.SequenceMetaData
CLASS,openjpa-2.2.0,OPENJPA-2247,2012-08-03T10:29:58.000-05:00,JoinColumn annotation is ignored when mapping a unidirectional owned OneToOne that is in a SecondaryTable,"@Entity
@SecondaryTable(name = ""ParentSecondaryTable"", pkJoinColumns = 
    { @PrimaryKeyJoinColumn(name = ""idParent"", referencedColumnName = ""idParent"") })
public class Parent {

    @Id
    @GeneratedValue
    int idParent;

    String child_ref;

    @OneToOne
    @JoinColumn(name = ""CHILD_REF"", table = ""ParentSecondaryTable"", referencedColumnName = ""idChild"")
    PChild child;

}
ignore @JoinColumn
map unidirectional own OneToOne be OneToOne in SecondaryTable
This problem only exists when running with a persistence.xml that is set to 2.0 (version=""2.0"">).
For example:
@Entity
@SecondaryTable(name = ""ParentSecondaryTable"", pkJoinColumns = 
    { @PrimaryKeyJoinColumn(name = ""idParent"", referencedColumnName = ""idParent"") })
public class Parent {
@Id
    @GeneratedValue
    int idParent;
String child_ref;
@OneToOne
    @JoinColumn(name = ""CHILD_REF"", table = ""ParentSecondaryTable"", referencedColumnName = ""idChild"")
    PChild child;
}
look for fk",openjpa-jdbc.src.main.java.org.apache.openjpa.jdbc.meta.MappingRepository
CLASS,openjpa-2.2.0,OPENJPA-428,2007-11-01T12:39:40.000-05:00,"Bad error message regarding ""openjpa.Id""","@Id 
 @Id  @Id 
 @Id 
 @Entity
@Table(name=""TAX"", schema=""JPA_SC"")
public class Tax  {
	
	// Class variables  
	protected double taxamount;
 
	public Tax(){
		
	}
	
	public Tax(double taxamount){
		this.taxamount = taxamount;
	}
//plus getter and setter for taxamount

}
Hi all, this bug is to report a confusing and misplaced error message.
Problem is described below.
Feel free to request more info from me.
run project with OpenJPA get following error message
140  INFO   [http-0.0.0.0-8080-Processor23] openjpa.Runtime - Starting OpenJPA 1.0.0
380  INFO   [http-0.0.0.0-8080-Processor23] openjpa.jdbc.JDBC - Using dictionary class ""org.apache.openjpa.jdbc.sql.DB2Dictionary"".
20  WARN   [http-0.0.0.0-8080-Processor25] openjpa.Runtime - The property named ""openjpa.Id"" was not recognized and will be ignored, although the name closely matches a valid property called ""openjpa.Id"".
100  INFO   [http-0.0.0.0-8080-Processor25] openjpa.Runtime - Starting OpenJPA 1.0.0
300  INFO   [http-0.0.0.0-8080-Processor25] openjpa.jdbc.JDBC - Using dictionary class ""org.apache.openjpa.jdbc.sql.DB2Dictionary"".
I retyped all my @Id annotations to make sure there was no special character in one of them coming from copy&paste.
remove @Id annotation
Here is a sample of my class without @Id annotation:
@Entity
@Table(name=""TAX"", schema=""JPA_SC"")
public class Tax  {
	
	// Class variables  
	protected double taxamount;
 
	public Tax(){
		
	}
	
	public Tax(double taxamount){
		this.taxamount = taxamount;
	}
//plus getter and setter for taxamount
}
Regards,
Vitor Rodrigues","openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.Configurations
openjpa-lib.src.main.java.org.apache.openjpa.lib.conf.ConfigurationImpl"
METHOD,atunes-1.10.0,231,2008-10-04T18:31:26.000-05:00,Can't add image if repository was read by older app version,"public boolean isSupportsInternalImage()
How to reproduce:
1. Read repository by older version of aTunes
2. Update to latest SVN
3. Try to add an image (assuming supported file format)
Result:
Problem:
return false never set to true",net.sourceforge.atunes.kernel.modules.repository.audio.AudioFile:supportsInternalPicture()
CLASS,solr-4.4.0,SOLR-5295,2013-10-02T00:09:02.000-05:00,The createshard collection API creates maxShardsPerNode number of replicas if replicationFactor is not specified,"{quote}
 
  
  
  
 {quote}
As reported by Brett Hoerner on solr-user:
http://www.mail-archive.com/solr-user@lucene.apache.org/msg89545.html
{quote}
It seems that changes in 4.5 collection configuration now require users to set a maxShardsPerNode (or it defaults to 1).
Maybe this was the case before, but with the new CREATESHARD API it seems a very restrictive.
I've just created a very simple test collection on 3 machines where I set maxShardsPerNode at collection creation time to 1, and
I made 3 shards.
Everything is good.
Now I want a 4th shard, it seems impossible to create because the cluster
""knows"" I should only have 1 shard per node.
Yet my problem doesn't require more hardware, I just my new shard to exist on one of the existing servers.
So I try again -- I create a collection with 3 shards and set maxShardsPerNode to 1000 (just as a silly test).
Everything is good.
Now I add shard4 and it immediately tries to add 1000 replicas of shard4...
{quote}",solr.core.src.java.org.apache.solr.cloud.OverseerCollectionProcessor
CLASS,solr-4.4.0,SOLR-5296,2013-10-02T00:20:01.000-05:00,Creating a collection with implicit router adds shard ranges to each shard,"{quote}
 {quote}
create collection with implicit router add shard ranges to shard
Using the Example A from SolrCloud wiki:
bq.
http://localhost:8983/solr/admin/collections?action=CREATE&name=myimplicitcollection3&numShards=2&maxShardsPerNode=5&router.name=implicit&shards=s1,s2&replicationFactor=2
{quote}
""myimplicitcollection3"":{
""shards"":{
""s1"":{
""range"":""80000000-ffffffff"",
""state"":""active"",
""replicas"":{
""core_node1"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s1_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node3"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s1_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}},
""s2"":{
""range"":""0-7fffffff"",
""state"":""active"",
""replicas"":{
""core_node2"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:8983/solr"",
""core"":""myimplicitcollection3_s2_replica2"",
""node_name"":""192.168.1.5:8983_solr""},
""core_node4"":{
""state"":""active"",
""base_url"":""http://192.168.1.5:7574/solr"",
""core"":""myimplicitcollection3_s2_replica1"",
""node_name"":""192.168.1.5:7574_solr"",
""leader"":""true""}}}},
""maxShardsPerNode"":""5"",
""router"":{""name"":""implicit""},
""replicationFactor"":""2""}
{quote}
do right thing",solr.core.src.java.org.apache.solr.cloud.Overseer
FILE,AMQP,AMQP-190,2011-09-10T20:24:17.000-05:00,CachingConnectionFactory leaks channels when synchronized with a TransactionManager,"convertAndSend()
It seems that when I use RabbitTemplate, channelTransacted=true, to convertAndSend() a message to an exchange within the context of a synchronized TransactionManager (e.g. an active transaction on the current thread), the channel is never closed, hence new publishes will always get their ""own"", shiny, new channel (that is never closed or released to the channel pool) until Rabbit can't handle any more channels.
See Forum Reference for more info.
The problem is not observed on the consumer side (e.g. MessageListenerContainer).
Its observed on the publishing side, (e.g. RabbitTemplate).
It is observed both if I use the RabbitTemplate, natively... or if I use spring-integration and the <int-amqp:outbound-channel-adapter...> tag.
BTW, the observed ""channel leak"" goes away when I choose channelTransacted=false.
I will look to supply a simple recreate, if I can scrounge the time.","org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer
org.springframework.amqp.rabbit.core.RabbitTemplatePerformanceIntegrationTests
org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils
org.springframework.amqp.rabbit.connection.RabbitResourceHolder"
FILE,AMQP,AMQP-502,2015-06-19T03:02:33.000-05:00,Fanout binding is not created due to missing routing key,"@RabbitListener(




      bindings = @QueueBinding(




          value = @Queue(




              autoDelete = ""true""




          ),




          exchange = @Exchange(




              type = ""fanout"",




              value = ""mytest.broadcast"",




              autoDelete = ""true""




          ),




          key = ""#""




      )




  )




  public void processBroadcast(String data) {




    int i = 0;




  }






 
  
  
  
     
 
     
 
  
  
  
  
  
   {}   
     
 
     
 
     
 
  
     
 
     
 
     
 
     
 
     
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   {}   
  
     
 
      
   {}
Currently i am using spring-cloud-starter-bus-amqp which in terms references spring-amqp 1.4.3.
when i declare a rabbitlistener like this:
@RabbitListener(
bindings = @QueueBinding(
value = @Queue(
autoDelete = ""true""
),
exchange = @Exchange(
type = ""fanout"",
value = ""mytest.broadcast"",
autoDelete = ""true""
),
key = ""#""
)
)
public void processBroadcast(String data) {
int i = 0;
}
get error
If i try another exchange type, the binding works.
Edit: It also does not work if omit the key.
11:53:20.977 DEBUG   org.springframework.amqp.rabbit.core.RabbitAdmin - Initializing declarations
11:53:20.980  INFO   org.springframework.amqp.rabbit.core.RabbitAdmin - Auto-declaring a non-durable or auto-delete Exchange (mytest.broadcast) durable:false, auto-delete:true.
It will be deleted by the broker if it shuts down, and can be redeclared by closing and reopening the connection.
11:53:20.980  INFO   org.springframework.amqp.rabbit.core.RabbitAdmin - Auto-declaring a non-durable, auto-delete, or exclusive Queue (5df237ec-13d4-4aab-a0ff-772707bd7d03) durable:false, auto-delete:false, exclusive:true.
It will be redeclared if the broker stops and is restarted while the connection factory is alive, but all messages will be lost.
11:53:20.982 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Creating cached Rabbit Channel from AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:20.982 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Executing callback on RabbitMQ Channel: Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:20.982 DEBUG   org.springframework.amqp.rabbit.core.RabbitAdmin - declaring Exchange 'mytest.broadcast'
11:53:20.984 DEBUG   org.springframework.amqp.rabbit.core.RabbitAdmin - declaring Queue '5df237ec-13d4-4aab-a0ff-772707bd7d03'
11:53:20.989 DEBUG   org.springframework.amqp.rabbit.core.RabbitAdmin - Binding destination [5df237ec-13d4-4aab-a0ff-772707bd7d03 (QUEUE)] to exchange [mytest.broadcast] with routing key [null]
11:53:20.991 DEBUG    o.s.a.r.listener.SimpleMessageListenerContainer - Recovering consumer in 5000 ms.
11:53:20.991 DEBUG    o.s.a.r.listener.SimpleMessageListenerContainer - Starting Rabbit listener container.
11:53:20.992 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Starting consumer Consumer: tags=[{}], channel=null, acknowledgeMode=AUTO local queue size=0
11:53:20.998 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Started on queue '5df237ec-13d4-4aab-a0ff-772707bd7d03' with tag amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA: Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:20.998 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - ConsumeOK : Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:20.999 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:21.093  INFO             mytest.server.Server - Started Server in 16.957 seconds (JVM running for 18.151)
11:53:22.003 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:23.004 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:24.009 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:25.013 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:26.017 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:26.071  WARN    o.s.a.r.listener.SimpleMessageListenerContainer - Consumer raised exception, processing can restart if the connection factory supports it
org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalStateException: Invalid configuration: 'routingKey' must be non-null.
at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitTemplate.doExecute(RabbitTemplate.java:1124) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1101) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:1077) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitAdmin.initialize(RabbitAdmin.java:381) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitAdmin$11.onCreate(RabbitAdmin.java:323) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.CompositeConnectionListener.onCreate(CompositeConnectionListener.java:32) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.CachingConnectionFactory.createConnection(CachingConnectionFactory.java:446) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils$1.createConnection(ConnectionFactoryUtils.java:80) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.doGetTransactionalResourceHolder(ConnectionFactoryUtils.java:130) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.getTransactionalResourceHolder(ConnectionFactoryUtils.java:67) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.start(BlockingQueueConsumer.java:451) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1107) ~[spring-rabbit-1.5.0.M1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20-ea]
Caused by: java.lang.IllegalStateException: Invalid configuration: 'routingKey' must be non-null.
at com.rabbitmq.client.impl.AMQImpl$Queue$Bind.<init>(AMQImpl.java:1577) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.AMQP$Queue$Bind$Builder.build(AMQP.java:870) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueBind(ChannelN.java:918) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueBind(ChannelN.java:61) ~[amqp-client-3.5.1.jar:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_20-ea]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_20-ea]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_20-ea]
at java.lang.reflect.Method.invoke(Method.java:483) ~[na:1.8.0_20-ea]
at org.springframework.amqp.rabbit.connection.CachingConnectionFactory$CachedChannelInvocationHandler.invoke(CachingConnectionFactory.java:633) ~[spring-rabbit-1.5.0.M1.jar:na]
at com.sun.proxy.$Proxy0.queueBind(Unknown Source) ~[na:na]
at org.springframework.amqp.rabbit.core.RabbitAdmin.declareBindings(RabbitAdmin.java:480) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitAdmin.access$300(RabbitAdmin.java:54) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitAdmin$12.doInRabbit(RabbitAdmin.java:386) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.core.RabbitTemplate.doExecute(RabbitTemplate.java:1118) ~[spring-rabbit-1.5.0.M1.jar:na]
... 12 common frames omitted
11:53:26.071  INFO    o.s.a.r.listener.SimpleMessageListenerContainer - Restarting Consumer: tags=[{}], channel=null, acknowledgeMode=AUTO local queue size=0
11:53:26.071 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Closing Rabbit Channel: null
11:53:26.072 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Executing callback on RabbitMQ Channel: Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:26.072 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Publishing message on exchange [errors], routingKey = [org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.WARN]
11:53:26.072 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Starting consumer Consumer: tags=[{}], channel=null, acknowledgeMode=AUTO local queue size=0
11:53:26.075 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Creating cached Rabbit Channel from AMQChannel(amqp://stinger@10.0.10.34:5672/,2)
11:53:26.077 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Channel shutdown: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
11:53:26.079 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Detected closed channel on exception.
Re-initializing: null
11:53:26.080  WARN     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Failed to declare queue:493eb6d6-8340-44a8-b73f-ab93446407dc
11:53:26.081 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Executing callback on RabbitMQ Channel: Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:26.081 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Publishing message on exchange [errors], routingKey = [org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.WARN]
11:53:26.081  WARN     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Queue declaration failed; retries left=3
org.springframework.amqp.rabbit.listener.BlockingQueueConsumer$DeclarationException: Failed to declare queue(s):[493eb6d6-8340-44a8-b73f-ab93446407dc]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.attemptPassiveDeclarations(BlockingQueueConsumer.java:554) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.start(BlockingQueueConsumer.java:465) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1107) [spring-rabbit-1.5.0.M1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20-ea]
Caused by: java.io.IOException: null
at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:106) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:102) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:124) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueDeclarePassive(ChannelN.java:873) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueDeclarePassive(ChannelN.java:61) ~[amqp-client-3.5.1.jar:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_20-ea]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_20-ea]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_20-ea]
at java.lang.reflect.Method.invoke(Method.java:483) ~[na:1.8.0_20-ea]
at org.springframework.amqp.rabbit.connection.CachingConnectionFactory$CachedChannelInvocationHandler.invoke(CachingConnectionFactory.java:633) ~[spring-rabbit-1.5.0.M1.jar:na]
at com.sun.proxy.$Proxy0.queueDeclarePassive(Unknown Source) ~[na:na]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.attemptPassiveDeclarations(BlockingQueueConsumer.java:544) ~[spring-rabbit-1.5.0.M1.jar:na]
... 3 common frames omitted
Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:67) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:33) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:348) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.privateRpc(AMQChannel.java:221) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:118) ~[amqp-client-3.5.1.jar:na]
... 12 common frames omitted
Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
at com.rabbitmq.client.impl.ChannelN.asyncShutdown(ChannelN.java:478) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.processAsync(ChannelN.java:315) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.handleCompleteInboundCommand(AMQChannel.java:144) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.handleFrame(AMQChannel.java:91) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:552) ~[amqp-client-3.5.1.jar:na]
... 1 common frames omitted
11:53:26.081 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Executing callback on RabbitMQ Channel: Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:26.081 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Publishing message on exchange [errors], routingKey = [org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.WARN]
11:53:27.018 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:28.022 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:29.025 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:30.030 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:31.034 DEBUG     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Retrieving delivery for Consumer: tags=[{amq.ctag-Gai8Uo2Q0SYYJDbSopNtLA=5df237ec-13d4-4aab-a0ff-772707bd7d03}], channel=Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1), acknowledgeMode=AUTO local queue size=0
11:53:31.086 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Channel shutdown: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
11:53:31.086 DEBUG   o.s.a.rabbit.connection.CachingConnectionFactory - Detected closed channel on exception.
Re-initializing: null
11:53:31.088  WARN     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Failed to declare queue:493eb6d6-8340-44a8-b73f-ab93446407dc
11:53:31.089 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Executing callback on RabbitMQ Channel: Cached Rabbit Channel: AMQChannel(amqp://stinger@10.0.10.34:5672/,1)
11:53:31.089 DEBUG  o.springframework.amqp.rabbit.core.RabbitTemplate - Publishing message on exchange [errors], routingKey = [org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.WARN]
11:53:31.089  WARN     o.s.amqp.rabbit.listener.BlockingQueueConsumer - Queue declaration failed; retries left=2
org.springframework.amqp.rabbit.listener.BlockingQueueConsumer$DeclarationException: Failed to declare queue(s):[493eb6d6-8340-44a8-b73f-ab93446407dc]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.attemptPassiveDeclarations(BlockingQueueConsumer.java:554) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.start(BlockingQueueConsumer.java:465) ~[spring-rabbit-1.5.0.M1.jar:na]
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:1107) [spring-rabbit-1.5.0.M1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_20-ea]
Caused by: java.io.IOException: null
at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:106) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:102) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:124) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueDeclarePassive(ChannelN.java:873) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.queueDeclarePassive(ChannelN.java:61) ~[amqp-client-3.5.1.jar:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_20-ea]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_20-ea]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_20-ea]
at java.lang.reflect.Method.invoke(Method.java:483) ~[na:1.8.0_20-ea]
at org.springframework.amqp.rabbit.connection.CachingConnectionFactory$CachedChannelInvocationHandler.invoke(CachingConnectionFactory.java:633) ~[spring-rabbit-1.5.0.M1.jar:na]
at com.sun.proxy.$Proxy0.queueDeclarePassive(Unknown Source) ~[na:na]
at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.attemptPassiveDeclarations(BlockingQueueConsumer.java:544) ~[spring-rabbit-1.5.0.M1.jar:na]
... 3 common frames omitted
Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:67) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:33) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:348) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.privateRpc(AMQChannel.java:221) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:118) ~[amqp-client-3.5.1.jar:na]
... 12 common frames omitted
Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method<channel.close>(reply-code=404, reply-text=NOT_FOUND - no queue '493eb6d6-8340-44a8-b73f-ab93446407dc' in vhost '/', class-id=50, method-id=10)
at com.rabbitmq.client.impl.ChannelN.asyncShutdown(ChannelN.java:478) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.ChannelN.processAsync(ChannelN.java:315) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.handleCompleteInboundCommand(AMQChannel.java:144) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQChannel.handleFrame(AMQChannel.java:91) ~[amqp-client-3.5.1.jar:na]
at com.rabbitmq.client.impl.AMQConnection$MainLoop.run(AMQConnection.java:552) ~[amqp-client-3.5.1.jar:na]
... 1 common frames omitted","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-516,2015-08-06T01:25:34.000-05:00,"Setting autoDelete or exclusive to anything, including ""true"" in @Queue without a queue name results in them being disabled","@RabbitListener(bindings = @QueueBinding(




    value = @Queue(autoDelete = ""true"", exclusive = ""true""),




    exchange = @Exchange(value = ""myFanout"", type = ExchangeTypes.FANOUT, durable = ""true"")




))






   
 if (!StringUtils.hasText(queueName)) {




    queueName = UUID.randomUUID().toString();




    if (!StringUtils.hasText(bindingQueue.exclusive())) {




        exclusive = true;




    }




    if (!StringUtils.hasText(bindingQueue.autoDelete())) {




        autoDelete = true;




    }




}




else {




    exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());




    autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());




}






 
 String e = bindingQueue.exclusive();




if (!StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {




    exclusive = true




}
The following queue declaration will result in a queue being declared with auto delete and exclusive set to false:
@RabbitListener(bindings = @QueueBinding(
value = @Queue(autoDelete = ""true"", exclusive = ""true""),
exchange = @Exchange(value = ""myFanout"", type = ExchangeTypes.FANOUT, durable = ""true"")
))
due to the following code in RabbitListenerAnnotationBeanProcessor:
if (!
StringUtils.hasText(queueName)) {
queueName = UUID.randomUUID().
toString();
if (!
StringUtils.hasText(bindingQueue.exclusive())) {
exclusive = true;
}
if (!
StringUtils.hasText(bindingQueue.autoDelete())) {
autoDelete = true;
}
}
else {
exclusive = resolveExpressionAsBoolean(bindingQueue.exclusive());
autoDelete = resolveExpressionAsBoolean(bindingQueue.autoDelete());
}
use random name seem like good idea change to something
String e = bindingQueue.exclusive();
if (!
StringUtils.hasText(e) || resolveExpressionAsBoolean(e)) {
exclusive = true
}","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
FILE,AMQP,AMQP-653,2016-10-08T02:53:08.000-05:00,RabbitMessagingTemplate doesn't take advantage of RabbitTemplate's registered converters,"@Bean




Jackson2JsonMessageConverter jackson2JsonMessageConverter() {




	return new Jackson2JsonMessageConverter();




}
When using RabbitTemplate in a Spring Boot application, it's very easy to register a Spring AMQP Message Converter.
Just add this to your code:
@Bean
Jackson2JsonMessageConverter jackson2JsonMessageConverter() {
return new Jackson2JsonMessageConverter();
}
However, if you switch to RabbitMessagingTemplate, that bean no longer works, because RabbitMessagingTemplate doesn't offer to look up RabbitTemplate's converters, and instead relies on its own.
Looking inside Spring Boot, there doesn't appear to be any wiring that offers to hook up message converters either.","org.springframework.amqp.rabbit.core.RabbitMessagingTemplateTests
org.springframework.amqp.rabbit.core.RabbitMessagingTemplate"
FILE,AMQP,AMQP-656,2016-10-15T00:25:46.000-05:00,Unable to refer to the default exchange using @Argument within a @RabbitListener,"@Argument 
 @RabbitListener(bindings =




        @QueueBinding(




            value = @Queue(




                value = ""app.events.myEvent"",




                durable = ""true"",




                exclusive = ""false"",




                autoDelete = ""false"",




                arguments = {




                        @Argument(name=""x-dead-letter-exchange"", value = """"),




                        @Argument(name=""x-dead-letter-routing-key"", value=""app.dlq"")




                }),




            exchange = @Exchange(value=""amq.topic"", durable = ""true"", type = ""topic""),




            key=""event.app.myEvent.v1""




        ))






 
 @Bean




    public Queue appMyEventQueue() {




        return QueueBuilder.durable(""app.events.myEvent"")




            .withArgument(""x-dead-letter-exchange"", """")




            .withArgument(""x-dead-letter-routing-key"", deadLetterQueue().getName())




            .build();




    }
use @Argument annotations refer to default exchange
For example, you should be able to configure a queue to use the default exchange as part of the dead letter config similar to the following:
@RabbitListener(bindings =
@QueueBinding(
value = @Queue(
value = ""app.events.myEvent"",
durable = ""true"",
exclusive = ""false"",
autoDelete = ""false"",
arguments = {
@Argument(name=""x-dead-letter-exchange"", value = """"),
@Argument(name=""x-dead-letter-routing-key"", value=""app.dlq"")
}),
exchange = @Exchange(value=""amq.topic"", durable = ""true"", type = ""topic""),
key=""event.app.myEvent.v1""
))
not send empty string
I tried being creative with using things like SPEL that would evaluate to an empty string, but same result.
If I use bean configs I am able to get the configuration I want using something like the following, the issue is just with the annotation based config.
@Bean
public Queue appMyEventQueue() {
return QueueBuilder.durable(""app.events.myEvent"")
.
withArgument(""x-dead-letter-exchange"", """")
.
withArgument(""x-dead-letter-routing-key"", deadLetterQueue().
getName())
.
build();
}","org.springframework.amqp.rabbit.annotation.EnableRabbitIntegrationTests
org.springframework.amqp.rabbit.annotation.RabbitListenerAnnotationBeanPostProcessor"
METHOD,commons-math-3-3.0,MATH-718,2011-12-03T18:40:44.000-06:00,inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.,"{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}
return wrong value for large trials
Following code will be reproduce the problem.
{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).
inverseCumulativeProbability(0.5));}}
I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.
As the result the checkedCumulativeProbability method doesn't work as expected.","org.apache.commons.math3.util.ContinuedFraction:evaluate(double, double, int)"
METHOD,commons-math-3-3.0,MATH-841,2012-08-05T04:27:07.000-05:00,gcd speed up,"public void testApache(){
        Random rng=new Random(0);
        long checksum=0;
        long start=System.nanoTime();
        checksum+=gcd(0,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,0);
        checksum+=gcd(Integer.MAX_VALUE,rng.nextInt());
        for(int i=0;i<10000;i++) checksum+=gcd(rng.nextInt(),Integer.MAX_VALUE);
        checksum+=gcd(Integer.MAX_VALUE,Integer.MAX_VALUE);
        checksum+=gcd(Integer.MIN_VALUE,1<<30);
        checksum+=gcd(1<<30,1<<30);
        checksum+=gcd(3 * (1<<20),9 * (1<<15));
        for(int i=0;i<30000000;i++) checksum+=gcd(rng.nextInt(),rng.nextInt());
        long end=System.nanoTime();
        long tns=end-start;
        long tms=(tns+500000)/1000000;
        long ts=(tms+500)/1000;
        System.out.println(""exec time=""+ts+""s, (""+tms+""ms), checksum=""+checksum);
        assertEquals(9023314441L,checksum);
    }
use modulo operator
The following test code runs in 11s with current version and in 6s with the patch.
public void testApache(){
Random rng=new Random(0);
long checksum=0;
long start=System.nanoTime();
checksum+=gcd(0,Integer.MAX_VALUE);
checksum+=gcd(Integer.MAX_VALUE,0);
checksum+=gcd(Integer.MAX_VALUE,rng.nextInt());
for(int i=0;i<10000;i++) checksum+=gcd(rng.nextInt(),Integer.MAX_VALUE);
checksum+=gcd(Integer.MAX_VALUE,Integer.MAX_VALUE);
checksum+=gcd(Integer.MIN_VALUE,1<<30);
checksum+=gcd(1<<30,1<<30);
checksum+=gcd(3 * (1<<20),9 * (1<<15));
for(int i=0;i<30000000;i++) checksum+=gcd(rng.nextInt(),rng.nextInt());
long end=System.nanoTime();
long tns=end-start;
long tms=(tns+500000)/1000000;
long ts=(tms+500)/1000;
System.out.println(""exec time=""+ts+""s, (""+tms+""ms), checksum=""+checksum);
assertEquals(9023314441L,checksum);
}","org.apache.commons.math3.util.ArithmeticUtils:gcd(int, int)"
FILE,DATACMNS,DATACMNS-68,2011-08-26T08:05:09.000-05:00,NullPointerException in AbstractPersistentProperty::getComponentType(),"class TestClassSet extends TreeSet<Object> { }









 class TestClassComplex {




    private String id;




    private TestClassSet testClassSet;









    public String getId() {




        return id;




    }









    public TestClassSet getTestClassSet() {




        return testClassSet;




    }









    public void setTestClassSet(TestClassSet testClassSet) {




        this.testClassSet = testClassSet;




    }




}






 
 List<TestClassSet> o = mongoTemplate.findAll(TestClassSet.class);






 
 List<TestClassComplex> o = mongoTemplate.findAll(TestClassComplex.class);
class TestClassSet extends TreeSet<Object> { }
class TestClassComplex {
private String id;
private TestClassSet testClassSet;
public String getId() {
return id;
}
public TestClassSet getTestClassSet() {
return testClassSet;
}
public void setTestClassSet(TestClassSet testClassSet) {
this.testClassSet = testClassSet;
}
}
The following code appears to work fine:
List<TestClassSet> o = mongoTemplate.findAll(TestClassSet.class);
But this fails with the NPE below:
List<TestClassComplex> o = mongoTemplate.findAll(TestClassComplex.class);
java.lang.NullPointerException: null
at org.springframework.data.mapping.model.AbstractPersistentProperty.getComponentType(AbstractPersistentProperty.java:147) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.model.AbstractPersistentProperty.isComplexType(AbstractPersistentProperty.java:136) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.model.AbstractPersistentProperty.isEntity(AbstractPersistentProperty.java:143) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext.getNestedTypeToAdd(AbstractMappingContext.java:316) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext.access$100(AbstractMappingContext.java:65) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext$1.doWith(AbstractMappingContext.java:267) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.util.ReflectionUtils.doWithFields(ReflectionUtils.java:513) ~[spring-core-3.0.5.RELEASE.jar:3.0.5.RELEASE]
at org.springframework.data.mapping.context.AbstractMappingContext.addPersistentEntity(AbstractMappingContext.java:244) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext.getPersistentEntity(AbstractMappingContext.java:165) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext.getPersistentEntity(AbstractMappingContext.java:140) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mapping.context.AbstractMappingContext.getPersistentEntity(AbstractMappingContext.java:65) ~[spring-data-commons-core-1.2.0.BUILD-20110826.083456-44.jar:na]
at org.springframework.data.mongodb.core.MongoTemplate.determineCollectionName(MongoTemplate.java:1105) ~[spring-data-mongodb-1.0.0.BUILD-20110826.114729-388.jar:na]
at org.springframework.data.mongodb.core.MongoTemplate.findAll(MongoTemplate.java:786) ~[spring-data-mongodb-1.0.0.BUILD-20110826.114729-388.jar:na]
...",org.springframework.data.util.ClassTypeInformation
FILE,DATACMNS,DATACMNS-114,2011-12-19T03:21:41.000-06:00,Wrong custom implementation automatically detected,"AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...)  getImplementationClassName()
We have two repositories with a similar name suffix.
Both repositories have a custom interface and implementation, also ending with a similar name suffix.
When automatically scanning the repositories, and their custom implementation, the wrong custom implementation is wired to our repository bean.
result in following exception
Caused by: java.lang.IllegalArgumentException: No property find found for type class com.myproject.Contract
at org.springframework.data.repository.query.parser.Property.<init>(Property.java:76)
at org.springframework.data.repository.query.parser.Property.<init>(Property.java:97)
at org.springframework.data.repository.query.parser.Property.create(Property.java:312)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:326)
at org.springframework.data.repository.query.parser.Property.create(Property.java:292)
at org.springframework.data.repository.query.parser.Property.from(Property.java:251)
at org.springframework.data.repository.query.parser.Property.from(Property.java:232)
at org.springframework.data.repository.query.parser.Part.<init>(Part.java:48)
at org.springframework.data.repository.query.parser.PartTree$OrPart.<init>(PartTree.java:242)
at org.springframework.data.repository.query.parser.PartTree.buildTree(PartTree.java:101)
at org.springframework.data.repository.query.parser.PartTree.<init>(PartTree.java:77)
at org.springframework.data.jpa.repository.query.PartTreeJpaQuery.<init>(PartTreeJpaQuery.java:56)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:92)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateIfNotFoundQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:159)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$AbstractQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:71)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.<init>(RepositoryFactorySupport.java:303)
at org.springframework.data.repository.core.support.RepositoryFactorySupport.getRepository(RepositoryFactorySupport.java:157)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:120)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:39)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:142)
... 67 more
For example:
We have a repository named ContractRepository with a custom interface ContractRepositoryCustom and an implementation ContractRepositoryImpl, all defined inside the same package.
In another package we have a repository, for another entity type, named AnotherContractRepository with a custom interface AnotherContractRepositoryCustom and an implementation AnotherContractRepositoryImpl.
When starting the application context, the contractRepository bean is linked to our anotherContractRepositoryImpl rather than the contractRepositoryImpl.
This behavior seems to be operating system dependent, as it only occurs on our Linux CI server.
The cause of our problem can be found at AbstractRepositoryConfigDefinitionParser.detectCustomImplementation(...).",org.springframework.data.repository.config.AbstractRepositoryConfigDefinitionParser
FILE,DATACMNS,DATACMNS-157,2012-04-20T01:24:38.000-05:00,@Query in extending interface is not picked up correctly,"@Query 
 @NoRepositoryBean




public interface EntityRepository<T> extends JpaRepository<T, Long> {









	T findByDealer(Dealer dealer);




}









 public interface CarRepository extends EntityRepository<PersonalSiteVehicle> {









	@Override




	@Query(""select p from PersonalSiteVehicle p join p.detail d join d.enrichable e where e.dealer = ?1"")




	PersonalSiteVehicle findByDealer(Dealer dealer);




}






 
  @Query
I try to define an interface method in a super repository interface and 'implement' this in an extending interface with @Query.
This does not work.
Tested in the latest nightly build:
@NoRepositoryBean
public interface EntityRepository<T> extends JpaRepository<T, Long> {
T findByDealer(Dealer dealer);
}
public interface CarRepository extends EntityRepository<PersonalSiteVehicle> {
@Override
@Query(""select p from PersonalSiteVehicle p join p.detail d join d.enrichable e where e.dealer = ?
1"")
PersonalSiteVehicle findByDealer(Dealer dealer);
}
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'carRepository': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: Could not create query metamodel for method public abstract java.lang.Object nl.inmotiv.indi.repository.EntityRepository.findByDealer(nl.inmotiv.indi.domain.Dealer)!
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:149)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:102)
at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1442)
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:248)
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)
at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:848)
at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:790)
at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:707)
at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredFieldElement.inject(AutowiredAnnotationBeanPostProcessor.java:478)
... 32 more
Caused by: java.lang.IllegalArgumentException: Could not create query metamodel for method public abstract java.lang.Object nl.inmotiv.indi.repository.EntityRepository.findByDealer(nl.inmotiv.indi.domain.Dealer)!
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:95)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateIfNotFoundQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:164)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$AbstractQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:71)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.<init>(RepositoryFactorySupport.java:269)
at org.springframework.data.repository.core.support.RepositoryFactorySupport.getRepository(RepositoryFactorySupport.java:142)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:114)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:38)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:142)
... 40 more
Caused by: java.lang.IllegalArgumentException: No property dealer found for type class nl.inmotiv.indi.domain.PersonalSiteVehicle
at org.springframework.data.mapping.PropertyPath.<init>(PropertyPath.java:73)
at org.springframework.data.mapping.PropertyPath.<init>(PropertyPath.java:92)
at org.springframework.data.mapping.PropertyPath.create(PropertyPath.java:319)
at org.springframework.data.mapping.PropertyPath.create(PropertyPath.java:301)
at org.springframework.data.mapping.PropertyPath.from(PropertyPath.java:265)
at org.springframework.data.mapping.PropertyPath.from(PropertyPath.java:239)
at org.springframework.data.repository.query.parser.Part.<init>(Part.java:70)
at org.springframework.data.repository.query.parser.PartTree$OrPart.<init>(PartTree.java:180)
at org.springframework.data.repository.query.parser.PartTree$Predicate.buildTree(PartTree.java:260)
at org.springframework.data.repository.query.parser.PartTree$Predicate.<init>(PartTree.java:240)
at org.springframework.data.repository.query.parser.PartTree.<init>(PartTree.java:71)
at org.springframework.data.jpa.repository.query.PartTreeJpaQuery.<init>(PartTreeJpaQuery.java:57)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:93)
not use @Query annotation in sub interface","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-160,2012-04-21T08:47:35.000-05:00,Regression of Repository instances with only delete* methods,"public interface DeleteOnlyRepository<T, ID extends Serializable> extends Repository<T, ID>{









    public void delete(ID paramID);









    public void delete(T paramT);









    public void delete(Iterable<? extends T> paramIterable);









    public void deleteAll();









}
not create delete methods
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'treeEntityDeleteRepository': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: Could not create query metamodel for method public abstract void example.data.DeleteOnlyRepository.delete(java.lang.Object)!
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:149)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:102)
at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1441)
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:305)
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)
at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:585)
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:913)
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)
at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)
at example.data.RepositoryTest.testExample(RepositoryTest.java:10)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.lang.IllegalArgumentException: Could not create query metamodel for method public abstract void example.data.DeleteOnlyRepository.delete(java.lang.Object)!
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:95)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateIfNotFoundQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:164)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$AbstractQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:71)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.<init>(RepositoryFactorySupport.java:269)
at org.springframework.data.repository.core.support.RepositoryFactorySupport.getRepository(RepositoryFactorySupport.java:142)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:114)
at org.springframework.data.repository.core.support.RepositoryFactoryBeanSupport.getObject(RepositoryFactoryBeanSupport.java:38)
at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:142)
... 33 more
Caused by: java.lang.IllegalArgumentException: No property delete found for type class example.data.TreeEntity
at org.springframework.data.mapping.PropertyPath.<init>(PropertyPath.java:73)
at org.springframework.data.mapping.PropertyPath.<init>(PropertyPath.java:92)
at org.springframework.data.mapping.PropertyPath.create(PropertyPath.java:319)
at org.springframework.data.mapping.PropertyPath.create(PropertyPath.java:301)
at org.springframework.data.mapping.PropertyPath.from(PropertyPath.java:265)
at org.springframework.data.mapping.PropertyPath.from(PropertyPath.java:239)
at org.springframework.data.repository.query.parser.Part.<init>(Part.java:70)
at org.springframework.data.repository.query.parser.PartTree$OrPart.<init>(PartTree.java:180)
at org.springframework.data.repository.query.parser.PartTree$Predicate.buildTree(PartTree.java:260)
at org.springframework.data.repository.query.parser.PartTree$Predicate.<init>(PartTree.java:240)
at org.springframework.data.repository.query.parser.PartTree.<init>(PartTree.java:68)
at org.springframework.data.jpa.repository.query.PartTreeJpaQuery.<init>(PartTreeJpaQuery.java:57)
at org.springframework.data.jpa.repository.query.JpaQueryLookupStrategy$CreateQueryLookupStrategy.resolveQuery(JpaQueryLookupStrategy.java:93)
... 40 more
caused by a repository which extends:
public interface DeleteOnlyRepository<T, ID extends Serializable> extends Repository<T, ID>{
public void delete(ID paramID);
public void delete(T paramT);
public void delete(Iterable<? extends T> paramIterable);
public void deleteAll();
}
This appears to be a regression following the upgrade to Spring Data Commons 1.3.0 RC1, as it's not present when using Spring Data JPA 1.1.0.
RC1, only when using the build snapshots.
I realise that the bug is most likely in the data commons package, but I wasn't sure how to reproduce it without using the JPA component, so I'm reporting here for the mo - hope that's OK.
I attach a sample Maven project to reproduce the issue
The reason for wanting such a repository is to prevent clients from performing CRU operations on a child object without the use of the parent, but I do need to offer the ability to delete it.","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
FILE,DATACMNS,DATACMNS-176,2012-05-21T11:47:05.000-05:00,StackOverflowError when inserted object is a CGLIB proxy,"@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)
When trying to persist an object [to MongoDB (spring-data-mongodb v1.1.0.
M1)] that is in ""session"" scope and using a CGLIB proxy (ie: ""@Scope(value=""session"", proxyMode = ScopedProxyMode.TARGET_CLASS)"") I receive a StackOverflowError.
When removing the session scoping, it works correctly.java.lang.StackOverflowError
at java.util.HashMap$EntryIterator.<init>(HashMap.java:832)
at java.util.HashMap$EntryIterator.<init>(HashMap.java:832)
at java.util.HashMap.newEntryIterator(HashMap.java:846)
at java.util.HashMap$EntrySet.iterator(HashMap.java:950)
at java.util.AbstractMap.hashCode(AbstractMap.java:459)
at org.springframework.util.ObjectUtils.nullSafeHashCode(ObjectUtils.java:336)
at org.springframework.data.util.TypeDiscoverer.hashCode(TypeDiscoverer.java:365)
at org.springframework.data.util.ClassTypeInformation.hashCode(ClassTypeInformation.java:39)
at org.springframework.util.ObjectUtils.nullSafeHashCode(ObjectUtils.java:336)
at org.springframework.data.util.ParentTypeAwareTypeInformation.hashCode(ParentTypeAwareTypeInformation.java:79)
at org.springframework.util.ObjectUtils.nullSafeHashCode(ObjectUtils.java:336)
at org.springframework.data.util.ParentTypeAwareTypeInformation.hashCode(ParentTypeAwareTypeInformation.java:79)
at org.springframework.util.ObjectUtils.nullSafeHashCode(ObjectUtils.java:336)
at org.springframework.data.util.ParentTypeAwareTypeInformation.hashCode(ParentTypeAwareTypeInformation.java:79)
at org.springframework.util.ObjectUtils.nullSafeHashCode(ObjectUtils.java:336)
.... (Repeats)",org.springframework.data.util.ClassTypeInformation
FILE,DATACMNS,DATACMNS-233,2012-09-14T07:38:12.000-05:00,DomainClassConverter should gracefully return null for null sources or empty strings,"@javax.validation.constraints.NotNull  @javax.persistence.ManyToOne
I've noticed an important issue related to automatic web binding of String id to Domain class.
Imagine the use case where you have an Order domain class which has a ManyToOne reference to Customer.
When posting a new Order where Order.customer == """" then a converter exception is thrown:
Failed to convert property value of type java.lang.String to required type org.mycomp.domain.Customer for property customer; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @javax.
validation.constraints.NotNull @javax.
persistence.ManyToOne org.mycomp.domain.Customer for value '; nested exception is org.springframework.dao.InvalidDataAccessApiUsageException: The given id must not be null!
; nested exception is java.lang.IllegalArgumentException: The given id must not be null!
And note that for optional references this even might even cause a complete blocker?
This is the code I used:
<form:select path=""customer"">
<form:option value="""" label=""Select"" />
<form:options items=""${customers}"" itemValue=""id""></form:options>
</form:select>","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-257,2012-11-29T02:29:27.000-06:00,PropertyPath cannot deal with all uppercase fields,"@Id 
 class Foo{




  




  @Id




  private String UID;









  //code omitted




}
not execute MongoOperations.findOne method contain @Id field
Here an example:
class Foo{
@Id
private String UID;
//code omitted
}
Steps to reproduce:
1) create an entity like in example code snippet in MongoDb
2) try to perform find by id operation by calling MongoOperations.findOne
get exception get step
java.lang.IllegalArgumentException: No property uID found on com.xxxxxxxxxxxxx.TemplateDefinitionObject!
at org.springframework.data.mapping.context.AbstractMappingContext.getPersistentPropertyPath(AbstractMappingContext.java:225)
at org.springframework.data.mongodb.core.convert.QueryMapper.getPath(QueryMapper.java:202)
at org.springframework.data.mongodb.core.convert.QueryMapper.determineKey(QueryMapper.java:221)
at org.springframework.data.mongodb.core.convert.QueryMapper.getMappedObject(QueryMapper.java:87)
at org.springframework.data.mongodb.core.MongoTemplate.doFindOne(MongoTemplate.java:1307)
at org.springframework.data.mongodb.core.MongoTemplate.findById(MongoTemplate.java:516)
at org.springframework.data.mongodb.core.MongoTemplate.findById(MongoTemplate.java:509)
at org.springframework.data.mongodb.repository.support.SimpleMongoRepository.findOne(SimpleMongoRepository.java:99)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:616)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.executeMethodOn(RepositoryFactorySupport.java:334)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.invoke(RepositoryFactorySupport.java:319)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:172)
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:202)
at $Proxy37.findOne(Unknown Source)
at com.xxxxxxxx.ShortTest.testFoo(ShortTest.java:65)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:616)
at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:74)
at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:83)
at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:72)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:231)
at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:174)
at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)","org.springframework.data.mapping.PropertyPath
org.springframework.data.mapping.PropertyPathUnitTests"
FILE,DATACMNS,DATACMNS-509,2014-05-08T08:39:02.000-05:00,NullableWrapper Breaks JSON Conversion,"{




    final Set<Pos> allPos = posService.findAll();




    return ImmutableSortedSet.copyOf(allPos);




}






 
 {""name: ""pos1""}  {""name: ""pos2""} 
  
     {""name: ""pos1""}  {""name: ""pos2""}
address NullableWrapper fail since upgrade return with contents contain with NullableWrapper
I have a MVC method that is returning:
public Callable<Set<Pos>> get(.....) {
final Set<Pos> allPos = posService.findAll();
return ImmutableSortedSet.copyOf(allPos);
}
get in JSON format get of pos get on wire
[{""name: ""pos1""}, {""name: ""pos2""}]
get NullableWrapper with Spring data JPA get NullableWrapper with contents
[valueType: ""java.util.ArrayList"", value: [{""name: ""pos1""}, {""name: ""pos2""}]]","org.springframework.data.repository.core.support.DummyRepositoryFactory
org.springframework.data.repository.core.support.RepositoryFactorySupport
org.springframework.data.repository.core.support.RepositoryFactorySupportUnitTests"
FILE,DATACMNS,DATACMNS-511,2014-05-22T00:04:43.000-05:00,AbstractMappingContext.addPersistentEntity causes infinite loop,"public class User extends AbstractTenantUser<User, Role, Permission, Tenant> {




    ...




}




 public abstract class AbstractTenantUser<USER extends AbstractTenantUser<USER, ROLE, PERMISSION, TENANT>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>, TENANT extends AbstractTenant<USER>> extends AbstractUser<USER, ROLE, PERMISSION> implements TenantEntity<TENANT> {




    ...




}




 public abstract class AbstractUser<USER extends AbstractUser<USER, ROLE, PERMISSION>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AbstractPermission<USER extends AbstractUser<USER, ?, ?>> extends AuditingDateBaseEntity<USER> {




    ...




}




 public abstract class AuditingDateBaseEntity<USER extends AbstractUser<USER, ?, ?>> extends AbstractDateBaseEntity implements AuditingEntity<USER> {




    ...




}




 public abstract class AbstractDateBaseEntity extends AbstractBaseEntity implements DateEntity {




    ...




}




 public abstract class AbstractBaseEntity implements BaseEntity {




    ...




}
After updating from Codd SR2 to Dijkstra I could not run my tests.
After debugging the issue I found that the problem lies in AbstractMappingContext.addPersistentEntity.
This method is never called in Codd SR2 due to initialize not being triggered.
We use quite a few abstract MappedSuperclasses that have circular references and apparently this does not work.
An example:
public class User extends AbstractTenantUser<User, Role, Permission, Tenant> {
...
}
public abstract class AbstractTenantUser<USER extends AbstractTenantUser<USER, ROLE, PERMISSION, TENANT>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>, TENANT extends AbstractTenant<USER>> extends AbstractUser<USER, ROLE, PERMISSION> implements TenantEntity<TENANT> {
...
}
public abstract class AbstractUser<USER extends AbstractUser<USER, ROLE, PERMISSION>, ROLE extends AbstractRole<USER, PERMISSION>, PERMISSION extends AbstractPermission<USER>> extends AuditingDateBaseEntity<USER> {
...
}
public abstract class AbstractPermission<USER extends AbstractUser<USER, ?
, ?
>> extends AuditingDateBaseEntity<USER> {
...
}
public abstract class AuditingDateBaseEntity<USER extends AbstractUser<USER, ?
, ?
>> extends AbstractDateBaseEntity implements AuditingEntity<USER> {
...
}
public abstract class AbstractDateBaseEntity extends AbstractBaseEntity implements DateEntity {
...
}
public abstract class AbstractBaseEntity implements BaseEntity {
...
}
I hope this gives enough insight into the problem and hopefully you can fix this soon.",org.springframework.data.util.TypeVariableTypeInformation
FILE,DATACMNS,DATACMNS-616,2014-12-17T02:25:54.000-06:00,AnnotationRevisionMetadata can't access private fields,"@Entity




@RevisionEntity(ExtendedRevisionListener.class)




@Table(name = ""revinfo"")




public class ExtendedRevision implements Serializable  
 @Id




	@GeneratedValue




	@Column(name = ""REV"")




	@RevisionNumber




	private Integer id;









	 @RevisionTimestamp




	@Temporal(TemporalType.TIMESTAMP)




	@Column(name = ""REVTSTMP"", nullable = false)




	private Date date;









	 @Column(nullable = false, length = 15)




	private String username;









	 public Integer getId() {




		return id;




	}









	 public Date getDate() {




		return date;




	}









	 public String getUsername() {




		return username;




	}









	 public void setUsername(String username) {




		this.username = username;




	}
Trying to use a custom Envers revision class:
@Entity
@RevisionEntity(ExtendedRevisionListener.class)
@Table(name = ""revinfo"")
public class ExtendedRevision implements Serializable {
@Id
@GeneratedValue
@Column(name = ""REV"")
@RevisionNumber
private Integer id;
@RevisionTimestamp
@Temporal(TemporalType.TIMESTAMP)
@Column(name = ""REVTSTMP"", nullable = false)
private Date date;
@Column(nullable = false, length = 15)
private String username;
public Integer getId() {
return id;
}
public Date getDate() {
return date;
}
public String getUsername() {
return username;
}
public void setUsername(String username) {
this.username = username;
}
trigger error
java.lang.IllegalStateException: Could not access method: Class org.springframework.util.ReflectionUtils can not access a member of class ExtendedRevision with modifiers ""private""
at org.springframework.util.ReflectionUtils.handleReflectionException(ReflectionUtils.java:262)
at org.springframework.util.ReflectionUtils.getField(ReflectionUtils.java:132)
at org.springframework.data.util.AnnotationDetectionFieldCallback.getValue(AnnotationDetectionFieldCallback.java:82)
at org.springframework.data.history.AnnotationRevisionMetadata.<init>(AnnotationRevisionMetadata.java:54)
I assume the fields have to be made accessible from the field callback.",org.springframework.data.util.AnnotationDetectionFieldCallback
FILE,DATACMNS,DATACMNS-683,2015-04-13T05:31:25.000-05:00,Enabling Spring Data web support breaks @ModelAttribute binding in Spring MVC,"package be.vdab.web;









import org.springframework.context.annotation.ComponentScan;




import org.springframework.context.annotation.Configuration;




import org.springframework.data.web.config.EnableSpringDataWebSupport;




import org.springframework.web.servlet.config.annotation.EnableWebMvc;




import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;









// enkele imports




@Configuration




@EnableWebMvc




@EnableSpringDataWebSupport




@ComponentScan




public class CreateControllerBeans extends WebMvcConfigurerAdapter {




}






  






package be.vdab.web;









import org.springframework.stereotype.Controller;




import org.springframework.web.bind.annotation.ModelAttribute;




import org.springframework.web.bind.annotation.RequestMapping;




import org.springframework.web.bind.annotation.RequestMethod;




import org.springframework.web.servlet.ModelAndView;









import be.vdab.entities.Person;









@Controller




@RequestMapping(value = ""/"")




public class PersonController {




	private static final String TOEVOEGEN_VIEW = ""/WEB-INF/JSP/index.jsp"";














	@RequestMapping(method=RequestMethod.GET)




	ModelAndView get() {




		return new ModelAndView(TOEVOEGEN_VIEW).addObject(new Person());




	}




	




	@RequestMapping(method = RequestMethod.POST)




	String post(@ModelAttribute Person person) {




	  if (person == null) {




		  throw new IllegalArgumentException(""person IS NULL"");




	  }




	  return ""redirect:/"";




	}



















}






 
    
 
 
 
    
 @EnableSpringDataWebSupport   
 @ModelAttribute
Given following Java config class
package be.vdab.web;
import org.springframework.context.annotation.ComponentScan;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.web.config.EnableSpringDataWebSupport;
import org.springframework.web.servlet.config.annotation.EnableWebMvc;
import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;
// enkele imports
@Configuration
@EnableWebMvc
@EnableSpringDataWebSupport
@ComponentScan
public class CreateControllerBeans extends WebMvcConfigurerAdapter {
}
, following Controller class
package be.vdab.web;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.ModelAttribute;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.servlet.ModelAndView;
import be.vdab.entities.Person;
@Controller
@RequestMapping(value = ""/"")
public class PersonController {
private static final String TOEVOEGEN_VIEW = ""/WEB-INF/JSP/index.jsp"";
@RequestMapping(method=RequestMethod.GET)
ModelAndView get() {
return new ModelAndView(TOEVOEGEN_VIEW).
addObject(new Person());
}
@RequestMapping(method = RequestMethod.POST)
String post(@ModelAttribute Person person) {
if (person == null) {
throw new IllegalArgumentException(""person IS NULL"");
}
return ""redirect:/"";
}
}
and following JSP
<%@page contentType=""text/html"" pageEncoding=""UTF-8"" session=""false""%>
<%@taglib prefix=""form"" uri=""http://www.springframework.org/tags/form"" %>
<!doctype html>
<html lang=""nl"">
<head>
<title>Add person</title>
</head>
<body>
<form:form action="""" method=""post"" commandName=""person"">
<form:label path=""name"">Name:</form:label>
<form:input path=""name"" autofocus=""true""/>
<input type=""submit"">
</form:form>
</body>
</html>
throw InvalidArgumentException
Observation 1:
This worked up to and including spring-data-jpa 1.7.2.
RELEASE
Observation 2:
The bug disappears when @EnableSpringDataWebSupport is put in comment in CreateControllerBeans.java
Observation 3:
The bug disappears when @ModelAttribute is put in comment in PersonController.java
You can clone a project that shows the bug from https://github.com/desmethans/springDataJpaError.git","org.springframework.data.repository.support.DomainClassConverterUnitTests
org.springframework.data.repository.support.DomainClassConverter"
FILE,DATACMNS,DATACMNS-695,2015-05-13T09:08:15.000-05:00,Potential NullPointerException in AbstractMappingContext,"public class External{




 ..




 private Optional<Internal> field = new Optional<Internal>();




 ..




}
We found the reported issue upgrading Spring Data MongoDB library from 1.3.5.
RELEASE to 1.5.5.
RELEASE.
The issue is triggered querying a nested generic field qualified with a custom class (not primitive).
Following snippet shows the nested field we are trying to query:
public class External{
.
.
private Optional<Internal> field = new Optional<Internal>();
.
.
}
throw NullPointerException originate from AbstractMappingContext
It is a Spring Data Commons class and we noticed that the issue starts from version 1.7.2.
RELEASE of this library, just after commit 02046da.","org.springframework.data.mapping.context.AbstractMappingContext
org.springframework.data.mapping.context.AbstractMappingContextUnitTests"
FILE,DATACMNS,DATACMNS-943,2016-10-04T21:54:22.000-05:00,Redeclared save(Iterable) results in wrong method overload to be invoked eventually,"myRepository.save(Arrays.asList(new MyEntity(1, ""foo""), new MyEntity(2, ""bar"")));
I have upgrade my project from spring boot 1.3.1 to 1.4.1.
After the update the code seems to behave fine in windows with java 1.8.0_45 64bit
break with exception break in jenkins server
org.springframework.beans.NotReadablePropertyException: Invalid property 'id' of bean class [java.util.Arrays$ArrayList]: Could not find field for property during fallback access!
at org.springframework.data.util.DirectFieldAccessFallbackBeanWrapper.getPropertyValue(DirectFieldAccessFallbackBeanWrapper.java:56)
at org.springframework.data.jpa.repository.support.JpaMetamodelEntityInformation.getId(JpaMetamodelEntityInformation.java:149)
at org.springframework.data.repository.core.support.AbstractEntityInformation.isNew(AbstractEntityInformation.java:51)
at org.springframework.data.jpa.repository.support.JpaMetamodelEntityInformation.isNew(JpaMetamodelEntityInformation.java:225)
at org.springframework.data.jpa.repository.support.SimpleJpaRepository.save(SimpleJpaRepository.java:505)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:497)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.executeMethodOn(RepositoryFactorySupport.java:503)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.doInvoke(RepositoryFactorySupport.java:488)
at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.invoke(RepositoryFactorySupport.java:460)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:61)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99)
at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:281)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:136)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at
The invoking call is
myRepository.save(Arrays.asList(new MyEntity(1, ""foo""), new MyEntity(2, ""bar"")));
.
cause exception seem in linux with boot 1.4.1","org.springframework.data.repository.core.support.DefaultRepositoryInformationUnitTests
org.springframework.data.repository.core.support.DefaultRepositoryInformation"
CLASS,derby-10.9.1.0,DERBY-3024,2007-08-23T05:24:31.000-05:00,Validation of shared plans hurts scalability,"GenericPreparedStatement.upToDate()   BaseActivation.checkStatementValidity()
To investigate whether there was anything in the SQL execution layer that prevented scaling on a multi-CPU machine, I wrote a multi-threaded test which continuously executed ""VALUES 1"" using a PreparedStatement. I ran the test on a machine with 8 CPUs and expected the throughput to be proportional to the number of concurrent clients up to 8 clients (the same as the number of CPUs). However, the throughput only had a small increase from 1 to 2 clients, and adding more clients did not increase the throughput. Looking at the test in a profiler, it seems like the threads are spending a lot of time waiting to enter synchronization blocks in GenericPreparedStatement.upToDate() and BaseActivation.checkStatementValidity() (both of which are synchronized on the a GenericPreparedStatement object).
I then changed the test slightly, appending a comment with a unique thread id to the ""VALUES 1"" statement.
That means the threads still did the same work, but each thread got its own plan (GenericPreparedStatement object) since the statement cache didn't regard the SQL text strings as identical.
make change scale to concurrent threads","java.engine.org.apache.derby.impl.store.access.heap.HeapConglomerateFactory
java.engine.org.apache.derby.impl.store.raw.data.FileContainer
java.engine.org.apache.derby.impl.store.raw.data.RAFContainer
java.testing.org.apache.derbyTesting.functionTests.tests.lang.DBInJarTest
java.engine.org.apache.derby.impl.store.raw.data.TempRAFContainer
java.engine.org.apache.derby.impl.store.raw.data.InputStreamContainer
java.engine.org.apache.derby.impl.store.access.btree.index.B2IFactory"
CLASS,derby-10.9.1.0,DERBY-4647,2010-05-07T13:34:26.000-05:00,BaseTestCase.execJavaCmd() does not work with weme 6.2,"BaseTestCase.execJavaCmd()
spawn java process with BaseTestCase.execJavaCmd()
This issue came up in DERBY-4179.
After this issue is fixed, BootLockTest should be enabled for weme.
JVMJ9VM011W Unable to load jclfoun10_24: The specified module could not be foun
d.
JVMEXEX013E Internal VM error: Failed to create Java VM
JVMEXEX014I Run C:\cygwin\ibmsvn\ntsoftware\weme6.2\bin\j9.exe -help for usage
pick up j9 executable not pass on other settings
This is how my script invokes the test with j9.
It probably has a lot of legacy system properties not needed, but I suppose execJavaCmd should just pass along all system properties, but I don't know how it would get the bootclasspath.
Perhaps -Dbootcp was a way to pass it on in the old harness.
c:/cygwin/ibmsvn/ntsoftware/weme6.2/bin/j9 -jcl:foun11 -DderbyTesting.serverho
st=localhost -DderbyTesting.clienthost=localhost -Demma.active= -Xbootclasspath/
a:c:/cygwin/ibmsvn/ntsoftware/weme6.2/lib/jdbc.
jar -Dbootcp=c:/cygwin/ibmsvn/nts
oftware/weme6.2/lib/jdbc.
jar junit.textui.TestRunner org.apache.derbyTesting.fun
ctionTests.tests.store.BootLockTest
Otherwise, currently I think the method is only used in replication and network server, but am not sure.","java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockMinion
java.testing.org.apache.derbyTesting.functionTests.tests.store.BootLockTest"
CLASS,derby-10.9.1.0,DERBY-4873,2010-10-28T18:45:13.000-05:00,NullPointerException in testBoundaries with ibm jvm 1.6,"testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)
With the line skipping the testBoundaries fixture of the InternationalConnectTest commented out, I get the following stack when I run the test with ibm 1.6:
1 testBoundaries(org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest)java.sql.SQLException: DERBY SQL error: SQLCODE: -1, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U
at org.apache.derby.client.am.SQLExceptionFactory40.getSQLException(SQLExceptionFactory40.java:96)
at org.apache.derby.client.am.SqlException.getSQLException(SqlException.java:358)
at org.apache.derby.jdbc.ClientDriver.connect(ClientDriver.java:149)
at java.sql.DriverManager.getConnection(DriverManager.java:322)
at java.sql.DriverManager.getConnection(DriverManager.java:273)
at org.apache.derbyTesting.functionTests.tests.jdbcapi.InternationalConnectTest.testBoundaries(InternationalConnectTest.java:111)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:48)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:109)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:22)
at junit.extensions.TestSetup$1.protect(TestSetup.java:19)
at junit.extensions.TestSetup.run(TestSetup.java:23)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
Caused by: org.apache.derby.client.am.SqlException: DERBY SQL error: SQLCODE: -1, SQLSTATE: XJ001, SQLERRMC: java.lang.NullPointerExceptionXJ001.U
at org.apache.derby.client.am.Connection.completeSqlca(Connection.java:2117)
at org.apache.derby.client.net.NetConnectionReply.parseRdbAccessFailed(NetConnectionReply.java:541)
at org.apache.derby.client.net.NetConnectionReply.parseAccessRdbError(NetConnectionReply.java:434)
at org.apache.derby.client.net.NetConnectionReply.parseACCRDBreply(NetConnectionReply.java:297)
at org.apache.derby.client.net.NetConnectionReply.readAccessDatabase(NetConnectionReply.java:121)
at org.apache.derby.client.net.NetConnection.readSecurityCheckAndAccessRdb(NetConnection.java:846)
at org.apache.derby.client.net.NetConnection.flowSecurityCheckAndAccessRdb(NetConnection.java:769)
at org.apache.derby.client.net.NetConnection.flowUSRIDONLconnect(NetConnection.java:601)
at org.apache.derby.client.net.NetConnection.flowConnect(NetConnection.java:408)
at org.apache.derby.client.net.NetConnection.<init>(NetConnection.java:218)
at org.apache.derby.client.net.NetConnection40.<init>(NetConnection40.java:77)
at org.apache.derby.client.net.ClientJDBCObjectFactoryImpl40.newNetConnection(ClientJDBCObjectFactoryImpl40.java:269)
at org.apache.derby.jdbc.ClientDriver.connect(ClientDriver.java:140)
... 35 more
This is after the latest check in for DERBY-4836 (revision 1028035).
I'll attach derby.log.",java.engine.org.apache.derby.impl.store.raw.data.BaseDataFileFactory
CLASS,derby-10.9.1.0,DERBY-5407,2011-09-12T08:50:38.000-05:00,"When run across the network, dblook produces unusable DDL for VARCHAR FOR BIT DATA columns.","varchar( 20 )  
 
 
 VARCHAR ()
omit length specification for VARCHAR FOR BIT DATA columns report in private correspondence run across network
I can reproduce this problem as follows:
1 Bring up a server (here I am using port 8246).
2 Create a database with the following ij script:
connect 'jdbc:derby://localhost:8246/memory:db;create=true';
create table t( a varchar( 20 ) for bit data );
3 Now run dblook across the network:
java -org.apache.derby.tools.dblook -d ""jdbc:derby://localhost:8246/memory:db""
produce following DDL for table
CREATE TABLE ""APP"".
""T"" (""A"" VARCHAR () FOR BIT DATA);
use embedded database produce usable DDL include length specification for VARCHAR FOR BIT DATA column include usable DDL for VARCHAR FOR BIT DATA column","java.testing.org.apache.derbyTesting.functionTests.tests.lang.SystemCatalogTest
java.engine.org.apache.derby.catalog.types.BaseTypeIdImpl"
CLASS,derby-10.9.1.0,DERBY-5567,2012-01-05T07:35:04.000-06:00,AlterTableTest#testDropColumn fails: drop view cannot be performed due to dependency,"testDropColumn(org.apache.derbyTesting.functionTests.tests.lang.AlterTableTest)
Saw this when running suitesAll on 10.8.2.2:
1 testDropColumn(org.apache.derbyTesting.functionTests.tests.lang.AlterTableTest)java.sql.SQLException: Operation 'DROP VIEW' cannot be performed on object 'ATDC_VW_5A_1' because VIEW 'ATDC_VW_5A_2' is dependent on that object.
at org.apache.derby.client.am.SQLExceptionFactory40.getSQLException(Unknown Source)
at org.apache.derby.client.am.SqlException.getSQLException(Unknown Source)
at org.apache.derby.client.am.Statement.executeUpdate(Unknown Source)
at org.apache.derbyTesting.functionTests.tests.lang.AlterTableTest.testDropColumn(AlterTableTest.java:2465)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:113)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.extensions.TestSetup.run(TestSetup.java:25)
Caused by: org.apache.derby.client.am.SqlException: Operation 'DROP VIEW' cannot be performed on object 'ATDC_VW_5A_1' because VIEW 'ATDC_VW_5A_2' is dependent on that object.
at org.apache.derby.client.am.Statement.completeSqlca(Unknown Source)
at org.apache.derby.client.am.Statement.completeExecuteImmediate(Unknown Source)
at org.apache.derby.client.net.NetStatementReply.parseEXCSQLIMMreply(Unknown Source)
at org.apache.derby.client.net.NetStatementReply.readExecuteImmediate(Unknown Source)
at org.apache.derby.client.net.StatementReply.readExecuteImmediate(Unknown Source)
at org.apache.derby.client.net.NetStatement.readExecuteImmediate_(Unknown Source)
at org.apache.derby.client.am.Statement.readExecuteImmediate(Unknown Source)
at org.apache.derby.client.am.Statement.flowExecute(Unknown Source)
at org.apache.derby.client.am.Statement.executeUpdateX(Unknown Source)
... 55 more
see on error/failure see on console
Probably not related, I believe we have seen this before:
java.lang.Exception: DRDA_InvalidReplyTooShort.S:Invalid reply from network server: Insufficient data.
at org.apache.derby.impl.drda.NetworkServerControlImpl.consolePropertyMessageWork(Unknown Source)
at org.apache.derby.impl.drda.NetworkServerControlImpl.consolePropertyMessage(Unknown Source)
at org.apache.derby.impl.drda.NetworkServerControlImpl.fillReplyBuffer(Unknown Source)
at org.apache.derby.impl.drda.NetworkServerControlImpl.readResult(Unknown Source)
at org.apache.derby.impl.drda.NetworkServerControlImpl.pingWithNoOpen(Unknown Source)
at org.apache.derby.impl.drda.NetworkServerControlImpl.ping(Unknown Source)
at org.apache.derby.drda.NetworkServerControl.ping(Unknown Source)
at org.apache.derbyTesting.junit.NetworkServerTestSetup.pingForServerUp(NetworkServerTestSetup.java:567)
at org.apache.derbyTesting.functionTests.tests.derbynet.ServerPropertiesTest.canPingServer(ServerPropertiesTest.java:280)
at org.apache.derbyTesting.functionTests.tests.derbynet.ServerPropertiesTest.ttestSetPortPriority(ServerPropertiesTest.java:472)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at junit.framework.TestCase.runTest(TestCase.java:164)
at junit.framework.TestCase.runBare(TestCase.java:130)
at org.apache.derbyTesting.junit.BaseTestCase.runBare(BaseTestCase.java:113)
at junit.framework.TestResult$1.protect(TestResult.java:106)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.framework.TestResult.run(TestResult.java:109)
at junit.framework.TestCase.run(TestCase.java:120)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at org.apache.derbyTesting.junit.BaseTestSetup.run(BaseTestSetup.java:57)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at junit.extensions.TestDecorator.basicRun(TestDecorator.java:24)
at junit.extensions.TestSetup$1.protect(TestSetup.java:21)
at junit.framework.TestResult.runProtected(TestResult.java:124)
at junit.extensions.TestSetup.run(TestSetup.java:25)
at junit.framework.TestSuite.runTest(TestSuite.java:230)
at junit.framework.TestSuite.run(TestSuite.java:225)
at junit.framework.TestSuite.runTest(TestSuite.java:230)
at junit.framework.TestSuite.run(TestSuite.java:225)
at junit.framework.TestSuite.runTest(TestSuite.java:230)
at junit.framework.TestSuite.run(TestSuite.java:225)
at junit.framework.TestSuite.runTest(TestSuite.java:230)
at junit.framework.TestSuite.run(TestSuite.java:225)
at junit.textui.TestRunner.doRun(TestRunner.java:121)
at junit.textui.TestRunner.start(TestRunner.java:185)
at junit.textui.TestRunner.main(TestRunner.java:143)",java.engine.org.apache.derby.iapi.sql.dictionary.ViewDescriptor
CLASS,derby-10.9.1.0,DERBY-6053,2013-01-25T09:02:53.000-06:00,Client should use a prepared statement rather than regular statement for Connection.setTransactionIsolation,"client.am.Connection setTransactionIsolation()   setTransactionIsolation()   
 private Statement setTransactionIsolationStmt = null;
 
  
 createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
 
 private void setTransactionIsolationX(int level)
 
 setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);


 
   

import java.sql.*;
import java.net.*;
import java.io.*;
import org.apache.derby.drda.NetworkServerControl;

/**
 * Client template starts its own NetworkServer and runs some SQL against it.
 * The SQL or JDBC API calls can be modified to reproduce issues
 * 
 */public class SetTransactionIsolation {
    public static Statement s;
    
    public static void main(String[] args) throws Exception {
        try {
            // Load the driver. Not needed for network server.
            
            Class.forName(""org.apache.derby.jdbc.ClientDriver"");
            // Start Network Server
            startNetworkServer();
            // If connecting to a customer database. Change the URL
            Connection conn = DriverManager
                    .getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
            // clean up from a previous run
            s = conn.createStatement();
            try {
                s.executeUpdate(""DROP TABLE T"");
            } catch (SQLException se) {
                if (!se.getSQLState().equals(""42Y55""))
                    throw se;
            }

            for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);

	    }
            
            // rs.close();
            // ps.close();
            runtimeInfo();
            conn.close();
            // Shutdown the server
            shutdownServer();
        } catch (SQLException se) {
            while (se != null) {
                System.out.println(""SQLState="" + se.getSQLState()
                        + se.getMessage());
                se.printStackTrace();
                se = se.getNextException();
            }
        }
    }
    
    /**
     * starts the Network server
     * 
     */
    public static void startNetworkServer() throws SQLException {
        Exception failException = null;
        try {
            
            NetworkServerControl networkServer = new NetworkServerControl(
                    InetAddress.getByName(""localhost""), 1527);
            
            networkServer.start(new PrintWriter(System.out));
            
            // Wait for the network server to start
            boolean started = false;
            int retries = 10; // Max retries = max seconds to wait
            
            while (!started && retries > 0) {
                try {
                    // Sleep 1 second and then ping the network server
                    Thread.sleep(1000);
                    networkServer.ping();
                    
                    // If ping does not throw an exception the server has
                    // started
                    started = true;
                } catch (Exception e) {
                    retries--;
                    failException = e;
                }
                
            }
            
            // Check if we got a reply on ping
            if (!started) {
                throw failException;
            }
        } catch (Exception e) {
            SQLException se = new SQLException(""Error starting network  server"");
            se.initCause(failException);
            throw se;
        }
    }
    
    public static void shutdownServer() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        networkServer.shutdown();
    }
    
    public static void runtimeInfo() throws Exception {
        NetworkServerControl networkServer = new NetworkServerControl(
                InetAddress.getByName(""localhost""), 1527);
        System.out.println(networkServer.getRuntimeInfo());
    }
    
}
o.a.d.client.am.Connection setTransactionIsolation() uses a Statement which  it builds up each time for setTransactionIsolation()  is called.
private Statement setTransactionIsolationStmt = null;
...
setTransactionIsolationStmt =
                    createStatementX(java.sql.ResultSet.TYPE_FORWARD_ONLY,
                            java.sql.ResultSet.CONCUR_READ_ONLY,
                            holdability());
....
private void setTransactionIsolationX(int level)
...
            setTransactionIsolationStmt.executeUpdate(
                ""SET CURRENT ISOLATION = "" + levelString);
The program below shows repeated calls to setTransactionIsolation.
import java.sql.
*;
import java.net.
*;
import java.io.
*;
import org.apache.derby.drda.NetworkServerControl;
/**
* Client template starts its own NetworkServer and runs some SQL against it.
* The SQL or JDBC API calls can be modified to reproduce issues
*
*/public class SetTransactionIsolation {
public static Statement s;
public static void main(String[] args) throws Exception {
try {
// Load the driver.
Not needed for network server.
Class.forName(""org.apache.derby.jdbc.ClientDriver"");
// Start Network Server
startNetworkServer();
// If connecting to a customer database.
Change the URL
Connection conn = DriverManager
.
getConnection(""jdbc:derby://localhost:1527/wombat;create=true"");
// clean up from a previous run
s = conn.createStatement();
try {
s.executeUpdate(""DROP TABLE T"");
} catch (SQLException se) {
if (!
se.getSQLState().
equals(""42Y55""))
throw se;
}
for (int i = 0; i < 50000; i++) {
		conn.setTransactionIsolation(Connection.TRANSACTION_REPEATABLE_READ);
		conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
}
// rs.close();
// ps.close();
runtimeInfo();
conn.close();
// Shutdown the server
shutdownServer();
} catch (SQLException se) {
while (se !
= null) {
System.out.println(""SQLState="" + se.getSQLState()
+ se.getMessage());
se.printStackTrace();
se = se.getNextException();
}
}
}
/**
* starts the Network server
*
*/
public static void startNetworkServer() throws SQLException {
Exception failException = null;
try {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.start(new PrintWriter(System.out));
// Wait for the network server to start
boolean started = false;
int retries = 10; // Max retries = max seconds to wait
while (!
started && retries > 0) {
try {
// Sleep 1 second and then ping the network server
Thread.sleep(1000);
networkServer.ping();
// If ping does not throw an exception the server has
// started
started = true;
} catch (Exception e) {
retries--;
failException = e;
}
}
// Check if we got a reply on ping
if (!
started) {
throw failException;
}
} catch (Exception e) {
SQLException se = new SQLException(""Error starting network  server"");
se.initCause(failException);
throw se;
}
}
public static void shutdownServer() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
networkServer.shutdown();
}
public static void runtimeInfo() throws Exception {
NetworkServerControl networkServer = new NetworkServerControl(
InetAddress.getByName(""localhost""), 1527);
System.out.println(networkServer.getRuntimeInfo());
}
}",java.client.org.apache.derby.client.am.Connection
METHOD,time,18,2013-04-19T08:28:47.000-05:00,NPE in DateTimeZoneBuilder,"PrecalculatedZone.create()  ZoneInfoCompiler.verbose() 
    
 static {
 cVerbose.set(Boolean.FALSE);
 }
 
 public static boolean verbose() {
 return cVerbose.get();
 }
 
 public static boolean verbose(){
 Boolean verbose = cVerbose.get();
 return (verbose != null) ? verbose : false;
}
 
 @Test
 public void testDateTimeZoneBuilder() throws Exception {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ1"", true);
 Thread t = new Thread(new Runnable() {
 @Override
 public void run() {
 getTestDataTimeZoneBuilder().toDateTimeZone(""TestDTZ2"", true);
 }
 });
 t.start();
 t.join();
 }

  private DateTimeZoneBuilder getTestDataTimeZoneBuilder() {
 return new DateTimeZoneBuilder()
 .addCutover(1601, 'w', 1, 1, 1, false, 7200000)
 .setStandardOffset(3600000)
 .addRecurringSavings("""", 3600000, 1601, Integer.MAX_VALUE, 'w', 3, -1, 1, false, 7200000)
 .addRecurringSavings("""", 0, 1601, Integer.MAX_VALUE, 'w', 10, -1, 1, false, 10800000);
 }
go ok in first thread go recurring saving time in first thread build DateTimeZone with recurring saving time generate identifier in PrecalculatedZone.create()
generate NPE in ZoneInfoCompiler.verbose()
The cause is that the cVerbose ThreadLocal is incorrectly initialized in ZoneInfoCompiler:
``` java
 static {
 cVerbose.set(Boolean.FALSE);
 }
```
...will initialize cVerbose only for the first thread and not for the subsequent ones.
The NPE is caused by the autoboxing in:
``` java
 public static boolean verbose() {
 return cVerbose.get();
 }
```
A better approach could be to remove the initialization and test for null:
``` java
public static boolean verbose(){
 Boolean verbose = cVerbose.get();
 return (verbose !
= null) ?
verbose : false;
}
```
---
Here follows a test case:
``` java
 @Test
 public void testDateTimeZoneBuilder() throws Exception {
 getTestDataTimeZoneBuilder().
toDateTimeZone(""TestDTZ1"", true);
 Thread t = new Thread(new Runnable() {
 @Override
 public void run() {
 getTestDataTimeZoneBuilder().
toDateTimeZone(""TestDTZ2"", true);
 }
 });
 t.start();
 t.join();
 }
private DateTimeZoneBuilder getTestDataTimeZoneBuilder() {
 return new DateTimeZoneBuilder()
 .
addCutover(1601, 'w', 1, 1, 1, false, 7200000)
 .
setStandardOffset(3600000)
 .
addRecurringSavings("""", 3600000, 1601, Integer.MAX_VALUE, 'w', 3, -1, 1, false, 7200000)
 .
addRecurringSavings("""", 0, 1601, Integer.MAX_VALUE, 'w', 10, -1, 1, false, 10800000);
 }
```",org.joda.time.tz.ZoneInfoCompiler:<clinit0>
METHOD,time,21,2013-05-03T21:03:46.000-05:00,DateTimeFormat.parseInto sometimes miscalculates year (2.2),"public void testParseInto_monthDay_feb29_startOfYear() {
 DateTimeFormatter f = DateTimeFormat.forPattern(""M d"").withLocale(Locale.UK);
 MutableDateTime result = new MutableDateTime(2000, 1, 1, 0, 0, 0, 0, NEWYORK);
 assertEquals(4, f.parseInto(result, ""2 29"", 0));
 assertEquals(new MutableDateTime(2000, 2, 29, 0, 0, 0, 0, NEWYORK), result);
 }
There appears to be a bug in the fix to http://sourceforge.net/p/joda-time/bugs/148 (which I also reported).
The following code (which can be added to org.joda.time.format.TestDateTimeFormatter) breaks, because the input mutable date time's millis appear to be mishandled and the year for the parse is changed to 1999:
``` java
 public void testParseInto_monthDay_feb29_startOfYear() {
 DateTimeFormatter f = DateTimeFormat.forPattern(""M d"").
withLocale(Locale.UK);
 MutableDateTime result = new MutableDateTime(2000, 1, 1, 0, 0, 0, 0, NEWYORK);
 assertEquals(4, f.parseInto(result, ""2 29"", 0));
 assertEquals(new MutableDateTime(2000, 2, 29, 0, 0, 0, 0, NEWYORK), result);
 }
```","org.joda.time.format.DateTimeFormatter:parseInto(ReadWritableInstant, String, int)"
METHOD,time,77,2013-10-16T15:36:22.000-05:00,addDays(0) changes value of MutableDateTime,"final MutableDateTime mdt = new MutableDateTime(2011, 10, 30, 3, 0, 0, 0, DateTimeZone.forID(""Europe/Berlin""));
System.out.println(""Start date: "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(-1);
System.out.println(""addHours(-1): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addHours(0);
System.out.println(""addHours(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
mdt.addDays(0);
System.out.println(""addDays(0): "" + mdt + "" ("" + mdt.toInstant().getMillis() + "")"");
 
 
 addHours(-1)  
 addHours(0)  
 addDays(0)  
 
          
 addDays(0)
add amount of days add amount to mutable date time object change value of object change value upon DST transition
The code
``` java final MutableDateTime mdt = new MutableDateTime(2011, 10, 30, 3, 0, 0, 0, DateTimeZone.forID(""Europe/Berlin""));
System.out.println(""Start date: "" + mdt + "" ("" + mdt.toInstant().
getMillis() + "")"");
mdt.addHours(-1);
System.out.println(""addHours(-1): "" + mdt + "" ("" + mdt.toInstant().
getMillis() + "")"");
mdt.addHours(0);
System.out.println(""addHours(0): "" + mdt + "" ("" + mdt.toInstant().
getMillis() + "")"");
mdt.addDays(0);
System.out.println(""addDays(0): "" + mdt + "" ("" + mdt.toInstant().
getMillis() + "")"");
```
```
Start date: 2011-10-30T03:00:00.000+01:00 (1319940000000) //OK addHours(-1): 2011-10-30T02:00:00.000+01:00 (1319936400000) //OK addHours(0): 2011-10-30T02:00:00.000+01:00 (1319936400000) //OK, no change in time addDays(0): 2011-10-30T02:00:00.000+02:00 (1319932800000) //error, time has changed by 1 hour
```
show same problem
I have tested with version 2.3.
However, if I repeat the test with Joda 1.5.2, the invocation of addDays(0) does not change the date's value.","org.joda.time.MutableDateTime:add(DurationFieldType, int)
org.joda.time.MutableDateTime:addWeeks(int)
org.joda.time.MutableDateTime:addYears(int)
org.joda.time.MutableDateTime:addMonths(int)
org.joda.time.MutableDateTime:addMinutes(int)
org.joda.time.MutableDateTime:addHours(int)
org.joda.time.MutableDateTime:addWeekyears(int)
org.joda.time.MutableDateTime:addDays(int)
org.joda.time.MutableDateTime:addSeconds(int)
org.joda.time.MutableDateTime:addMillis(int)"
METHOD,time,79,2013-10-17T20:36:31.000-05:00,none standard PeriodType without year throws exception,"Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()})).normalizedStandard(PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()}));
return p.getMonths();
 
    
      
   
 withYearsRemoved() throws the  
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard()).normalizedStandard(PeriodType.standard());
return p.getMonths();
 
 Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard().withYearsRemoved()).normalizedStandard(PeriodType.standard().withYearsRemoved());
return p.getMonths();
Hi.
I tried to get a Period only for months and weeks with following code:
``` Java
Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()})).
normalizedStandard(PeriodType.forFields(new DurationFieldType[]{DurationFieldType.months(), DurationFieldType.weeks()}));
return p.getMonths();
```
throw following exception
```
10-17 14:35:50.999: E/AndroidRuntime(1350): java.lang.UnsupportedOperationException: Field is not supported
10-17 14:35:50.999: E/AndroidRuntime(1350): at org.joda.time.PeriodType.setIndexedField(PeriodType.java:690)
10-17 14:35:50.999: E/AndroidRuntime(1350): at org.joda.time.Period.withYears(Period.java:896) 10-17
14:35:50.999: E/AndroidRuntime(1350): at org.joda.time.Period.normalizedStandard(Period.java:1630)
```
throw same exception
this works:
``` Java
Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard()).
normalizedStandard(PeriodType.standard());
return p.getMonths();
```
this fails:
``` Java
Period p = new Period(new DateTime(startDate.getTime()), new DateTime(endDate.getTime()), PeriodType.standard().
withYearsRemoved()).
normalizedStandard(PeriodType.standard().
withYearsRemoved());
return p.getMonths();
```",org.joda.time.Period:normalizedStandard(PeriodType)
METHOD,time,88,2013-11-25T19:15:46.000-06:00,Constructing invalid Partials,"Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).with(hourOfDay(), 1);
assert(a == b);
 
 new Partial(new DateTimeFieldType[] { clockhourOfDay(), hourOfDay() }, new int[] { 1, 1}); // throws Types array must not contain duplicate
new Partial(clockhourOfDay(), 1).with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
 
 new Partial(clockhourOfDay(), 1)  with(hourOfDay(), 1)  isEqual(new Partial(hourOfDay() ,1).with(clockhourOfDay(), 1)) // throws objects must have matching field types
Partials can be constructed by invoking a constructor `Partial(DateTimeFieldType[], int[])` or by merging together a set of partials using `with`, each constructed by calling `Partial(DateTimeFieldType, int)`, e.g.:
``` java
Partial a = new Partial(new DateTimeFieldType[] { year(), hourOfDay() }, new int[] { 1, 1});
Partial b = new Partial(year(), 1).
with(hourOfDay(), 1);
assert(a == b);
```
However, the above doesn't work in all cases:
duplicate new Partial(clockhourOfDay(), 1)
with(hourOfDay(), 1); // #<Partial [clockhourOfDay=1, hourOfDay=1]>
```
Is that right?
There's also a related issue (probably stems from the fact that the Partial is invalid):
``` java
new Partial(clockhourOfDay(), 1).
with(hourOfDay(), 1).
isEqual(new Partial(hourOfDay() ,1).","org.joda.time.Partial:with(DateTimeFieldType, int)"
METHOD,time,93,2013-12-01T09:33:58.000-06:00,Partial.with fails with NPE,"new Partial(yearOfCentury(), 1)  with(weekyear(), 1);
// NullPointerException
// org.joda.time.Partial.with (Partial.java:447)
With the latest master:
``` java
new Partial(yearOfCentury(), 1).
with(weekyear(), 1);
// NullPointerException
// org.joda.time.Partial.with (Partial.java:447)
```
fail with yearOfCentury fail with year fail with yearOfEra
Probably because weekyear has a null range duration type.","org.joda.time.Partial:Partial(DateTimeFieldType[], int[], Chronology)
org.joda.time.field.UnsupportedDurationField:compareTo(DurationField)"
FILE,COMPRESS,COMPRESS-131,2011-06-03T16:25:45.000-05:00,ArrayOutOfBounds while decompressing bz2,"recvDecodingTables()
Decompressing a bz2 file generated by bzip2 utility throws an ArrayIndexOutOfBounds at method recvDecodingTables() line 469.
I believe it is related to encodings used to generate the original text file (the compressed file).",org.apache.commons.compress.compressors.BZip2TestCase
FILE,COMPRESS,COMPRESS-189,2012-06-26T21:30:39.000-05:00,ZipArchiveInputStream may read 0 bytes when reading from a nested Zip file,"ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
    for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
      ZipArchiveEntry entry = iterator.nextElement();
      InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
      ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
      ZipArchiveEntry innerEntry;
      while ((innerEntry = zipInput.getNextZipEntry()) != null){
        if (innerEntry.getName().endsWith(""XML""))
{

          //zipInput.read();

          System.out.println(IOUtils.toString(zipInput));

        }
      }
    }
When the following code is run an error ""Underlying input stream returned zero bytes"" is produced.
return bytes
This only happens the first time read is called, subsequent calls work as expected i.e. the following code actually works correctly with that line uncommented!
The zip file used to produce this behavious is available at http://wwmm.ch.cam.ac.uk/~dl387/test.ZIP
If this is not the correct way of processing a zip file of zip files please let me know.
iterate over entries look at master table
Is there anyway of instantiating a ZipFile from a zip file inside another zip file without first extracting the nested zip file?
ZipFile zipFile = new ZipFile(""C:/test.ZIP"");
for (Enumeration<ZipArchiveEntry> iterator = zipFile.getEntries(); iterator.hasMoreElements(); ) {
ZipArchiveEntry entry = iterator.nextElement();
InputStream is = new BufferedInputStream(zipFile.getInputStream(entry));
ZipArchiveInputStream zipInput = new ZipArchiveInputStream(is);
ZipArchiveEntry innerEntry;
while ((innerEntry = zipInput.getNextZipEntry()) !
= null){ if (innerEntry.getName().
endsWith(""XML""))
{
//zipInput.read();
System.out.println(IOUtils.toString(zipInput));
}
}
}","org.apache.commons.compress.archivers.zip.ZipArchiveInputStreamTest
org.apache.commons.compress.archivers.zip.ZipArchiveInputStream"
FILE,COMPRESS,COMPRESS-273,2014-04-11T04:13:32.000-05:00,NullPointerException when creation fields/entries from scratch,"org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
    org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();
The API has public default constructors for many data types.
result in NullPointerException
This also applies to some 1-argument constructors where two references should be set before get... is used later.
In the latter case, there must be public set methods for the missing data.
The attachment contains a number of similar test cases that show the same issue in a couple of classes.
An example:
org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField var0 = new org.apache.commons.compress.archivers.zip.UnicodeCommentExtraField();
org.apache.commons.compress.archivers.zip.ZipShort var1 = var0.getLocalFileDataLength();","org.apache.commons.compress.archivers.zip.AbstractUnicodeExtraField
org.apache.commons.compress.archivers.cpio.CpioArchiveEntry
org.apache.commons.compress.archivers.zip.ExtraFieldUtils
org.apache.commons.compress.archivers.zip.UnrecognizedExtraField"
FILE,COMPRESS,COMPRESS-278,2014-04-18T16:09:05.000-05:00,Incorrect handling of NUL username and group Tar.gz entries,"package TestBed;



import java.io.File;

import java.io.FileInputStream;

import java.io.FileNotFoundException;

import java.io.IOException;



import org.apache.commons.compress.archivers.tar.TarArchiveEntry;

import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;

import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;

import org.junit.Test;



/**

 * Unit test for simple App.

 */

public class AppTest

{



    @Test

    public void extractNoFileOwner()

    {

        TarArchiveInputStream tarInputStream = null;



        try

        {

            tarInputStream =

                new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(

                    ""/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz"" ) ) ) );

            TarArchiveEntry entry;

            while ( ( entry = tarInputStream.getNextTarEntry() ) != null )

            {

                System.out.println( entry.getName() );

                System.out.println(entry.getUserName()+""/""+entry.getGroupName());

            }



        }

        catch ( FileNotFoundException e )

        {

            e.printStackTrace();

        }

        catch ( IOException e )

        {

            e.printStackTrace();

        }

    }



}
contain entries have null
set as username and/or usergroup
get exception work with version
java.io.IOException: Error detected parsing the header
at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:249)
at TestBed.AppTest.extractNoFileOwner(AppTest.java:30)
Caused by: java.lang.IllegalArgumentException: Invalid byte 32 at offset 7 in '       {NUL}' len=8
at org.apache.commons.compress.archivers.tar.TarUtils.parseOctal(TarUtils.java:134)
at org.apache.commons.compress.archivers.tar.TarUtils.parseOctalOrBinary(TarUtils.java:173)
at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:953)
at org.apache.commons.compress.archivers.tar.TarArchiveEntry.parseTarHeader(TarArchiveEntry.java:940)
at org.apache.commons.compress.archivers.tar.TarArchiveEntry.<init>(TarArchiveEntry.java:324)
at org.apache.commons.compress.archivers.tar.TarArchiveInputStream.getNextTarEntry(TarArchiveInputStream.java:247)
... 27 more
This exception leads to my suspision that the regression was introduced with the fix for this ticket COMPRESS-262, which has a nearly identical exception provided.
Some test code you can run to verify it:
package TestBed;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
import org.junit.Test;
/**
* Unit test for simple App.
*/
public class AppTest
{
@Test
public void extractNoFileOwner()
{
TarArchiveInputStream tarInputStream = null;
try
{
tarInputStream =
new TarArchiveInputStream( new GzipCompressorInputStream( new FileInputStream( new File(
""/home/pknobel/redis-dist-2.8.3_1-linux.tar.gz"" ) ) ) );
TarArchiveEntry entry;
while ( ( entry = tarInputStream.getNextTarEntry() ) !
= null )
{
System.out.println( entry.getName() );
System.out.println(entry.getUserName()+""/""+entry.getGroupName());
}
}
catch ( FileNotFoundException e )
{
e.printStackTrace();
}
catch ( IOException e )
{
e.printStackTrace();
}
}
}
redis-dist-2.8.3_1/bin/
/
redis-dist-2.8.3_1/bin/redis-server
jenkins/jenkins
redis-dist-2.8.3_1/bin/redis-cli
jenkins/jenkins
reach null value entry
The archive is created using maven assembly plugin, and I tried the same with maven ant task.
Both generating an archive with not set username and groups for at least some entries.
You can download the archive from http://heli0s.darktech.org/redis/2.8.3_1/redis-dist-2.8.3_1-linux.tar.gz
If you run a tar -tvzf on the file you see this report:
drwxr-xr-x 0/0               0 2014-04-18 09:43 redis-dist-2.8.3_1-SNAPSHOT/bin/
-rwxr-xr-x pknobel/pknobel 3824588 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-cli
-rwxr-xr-x pknobel/pknobel 5217234 2014-01-02 14:58 redis-dist-2.8.3_1-SNAPSHOT/bin/redis-server
The user 0/0 probably indicates that it's not set although it's the root user id.
A correctly root user file would show up as root/root","org.apache.commons.compress.archivers.tar.TarUtilsTest
org.apache.commons.compress.archivers.tar.TarUtils"
FILE,COMPRESS,COMPRESS-309,2015-02-18T17:22:16.000-06:00,BZip2CompressorInputStream return value wrong when told to read to a full buffer.,"BZip2CompressorInputStream.read(buffer, offset, length)  
 @Test

	public void testApacheCommonsBZipUncompression () throws Exception {

		// Create a big random piece of data

		byte[] rawData = new byte[1048576];

		for (int i=0; i<rawData.length; ++i) {

			rawData[i] = (byte) Math.floor(Math.random()*256);

		}



		// Compress it

		ByteArrayOutputStream baos = new ByteArrayOutputStream();

		BZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);

		bzipOut.write(rawData);

		bzipOut.flush();

		bzipOut.close();

		baos.flush();

		baos.close();



		// Try to read it back in

		ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());

		BZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);

		byte[] buffer = new byte[1024];

		// Works fine

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		// Fails, returns -1 (indicating the stream is complete rather than that the buffer 

		// was full)

		Assert.assertEquals(0, bzipIn.read(buffer, 1024, 0));

		// But if you change the above expected value to -1, the following line still works

		Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));

		bzipIn.close();

	}
It seems like a pretty stupid thing to do - but I'm getting this when trying to use Kryo serialization (which is probably a bug on their part, too), so it does occur and has negative affects.
Here's a JUnit test that shows the problem specifically:
@Test
public void testApacheCommonsBZipUncompression () throws Exception {
// Create a big random piece of data
byte[] rawData = new byte[1048576];
for (int i=0; i<rawData.length; ++i) {
rawData[i] = (byte) Math.floor(Math.random()*256);
}
// Compress it
ByteArrayOutputStream baos = new ByteArrayOutputStream();
BZip2CompressorOutputStream bzipOut = new BZip2CompressorOutputStream(baos);
bzipOut.write(rawData);
bzipOut.flush();
bzipOut.close();
baos.flush();
baos.close();
// Try to read it back in
ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
BZip2CompressorInputStream bzipIn = new BZip2CompressorInputStream(bais);
byte[] buffer = new byte[1024];
// Works fine
Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));
return ( indicate stream
Assert.assertEquals(0, bzipIn.read(buffer, 1024, 0));
// But if you change the above expected value to -1, the following line still works
Assert.assertEquals(1024, bzipIn.read(buffer, 0, 1024));
bzipIn.close();
}",org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream
METHOD,mahout-0.8,MAHOUT-1301,2013-08-01T09:31:21.000-05:00,toString() method of SequentialAccessSparseVector has excess comma at the end,"SequentialAccessSparseVector toString()   toString()  
 {code:java}
 Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
  v.toString()  
 {code:java}
 {1:0.1,3:0.3}
 {code}
 
 {code:java}
 {1:0.1,3:0.3,}
 {code}
Realization of SequentialAccessSparseVector toString() method had changed in MAHOUT-1259 patch.
introduce new bug add at end
Example: 
Consider following sparse vector
{code:java}
Vector v = new SequentialAccessSparseVector(capacity);
v.set(1, 0.1);
v.set(3, 0.3);
{code}
In 0.7 v.toString() returns following string:
{code:java}
{1:0.1,3:0.3}
{code}
but in 0.8 it returns
{code:java}
{1:0.1,3:0.3,}
{code}
As you can see, there is extra comma at the end of the string.","org.apache.mahout.math.SequentialAccessSparseVector:toString()
org.apache.mahout.math.RandomAccessSparseVector:toString()"
METHOD,mahout-0.8,MAHOUT-1314,2013-08-18T09:07:48.000-05:00,StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true,"return input.getCentroid();  
 input.getCentroid()  clone();
when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.
the problem is in the reduce method itself: on line 60 ( return input.getCentroid(); )
it should be input.getCentroid().
clone();
similar to line 81.
full stack trace:
java.lang.NullPointerException
at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)
at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133)
at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)
it happens every time the REDUCE_STREAMING_KMEANS is set to true.","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:reduce(IntWritable, Iterable<CentroidWritable>, Context)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer:getBestCentroids(List<Centroid>, Configuration)"
METHOD,mahout-0.8,MAHOUT-1349,2013-11-01T07:59:17.000-05:00,Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?,"OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
  String [] dictionary = new String[dict.size()];
I'm not sure if I'm doing something wrong here, or if ClusterDumper does not support my (fairly simple) use case
I had a repository of 500K documents, for which I generated the input vectors and a dictionary using some custom code (not seq2sparse etc).
I hashed the features with max size 5M (because I didn't know how many features were in the dataset and wanted to minimize collisions).
The kmeans ran fine and generate sensible looking results, but when I tried to run ClusterDumper I got the following error:
#bash> bin/mahout clusterdump -dt sequencefile -d completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*
-i test-kmeans/clusters-19 -b 10 -n 10 -sp 10 -o ~/test-kmeans-out
Running on hadoop, using /usr/bin/hadoop and HADOOP_CONF_DIR=
MAHOUT-JOB: /opt/mahout-distribution-0.7/mahout-examples-0.7-job.jar
13/05/17 08:26:41 INFO common.AbstractJob: Command line arguments:
{--dictionary=[completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*],
--dictionaryType=[sequencefile],
--distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure],
--endPhase=[2147483647], --input=[test-kmeans/clusters-19],
--numWords=[10], --output=[/usr/share/tomcat6/test-kmeans-out],
--outputFormat=[TEXT], --samplePoints=[10], --startPhase=[0],
--substring=[10], --tempDir=[temp]}
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 698948 at
org.apache.mahout.clustering.AbstractCluster.formatVector(AbstractCluster.java:350)
at
org.apache.mahout.clustering.AbstractCluster.asFormatString(AbstractCluster.java:306)
at
org.apache.mahout.utils.clustering.ClusterDumperWriter.write(ClusterDumperWriter.java:54)
at
org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:169)
at
org.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:156)
at
org.apache.mahout.utils.clustering.ClusterDumper.printClusters(ClusterDumper.java:187)
at
org.apache.mahout.utils.clustering.ClusterDumper.run(ClusterDumper.java:153)
(...)
access dictionary for feature
Looking at the dictionary loading code ( http://grepcode.com/file/repo1.maven.org/maven2/org.apache.mahout/mahout-integration/0.7/org/apache/mahout/utils/vectors/VectorHelper.java#VectorHelper.loadTermDictionary%28java.io.File%29
-  checked 0.8 and it hasn't changed)
size dictionary array for number
OpenObjectIntHashMap dict = new OpenObjectIntHashMap();
//...
String [] dictionary = new String[dict.size()];
After I ran my custom dictionary/feature generation code I discovered I only had 517,327 unique features, therefore it is not surprising it would die on an index >= 517327 (though I don't understand why it didn't die when trying to load the dictionary file)
Is there any reason why the VectorHelper code should not create a dictionary array that has size the highest index read from the dictionary sequence file (which can be easily calculated during the preceding loop)?
Or am I misunderstanding something?
It worked fine when I reduced the hash size to be <= than the total number of features, but this is not desirable in general (for me) since I don't know the number of features before I run the job (and if I guess too high then ClusterDumper crashes)
Alex Piggott
IKANOW","org.apache.mahout.utils.vectors.VectorHelper:loadTermDictionary(Configuration, String)
org.apache.mahout.utils.vectors.VectorHelperTest:testJsonFormatting()"
METHOD,mahout-0.8,MAHOUT-1358,2013-11-18T01:58:22.000-06:00,StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true,"{Code}


 {Code}


  StreamingKMeansThread.call()


 {Code}
     Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }


    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error
{Code}
java.lang.IllegalArgumentException: Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test [10.000000149011612, 0]
at com.google.common.base.Preconditions.checkArgument(Preconditions.java:120)
at org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176)
at org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73)
at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)
{Code}
The issue is caused by the following code in StreamingKMeansThread.call()
{Code}
    Iterator<Centroid> datapointsIterator = datapoints.iterator();
    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {
      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);
      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {
        estimatePoints.add(datapointsIterator.next());
      }
      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());
    }
StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);
    while (datapointsIterator.hasNext()) {
      clusterer.cluster(datapointsIterator.next());
    }
{Code}
The code is using the same iterator twice, and it fails on the second use for obvious reasons.","org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Path, Configuration)
org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread:StreamingKMeansThread(Iterable<Centroid>, Configuration)"
FILE,swt-3.1,104150,2005-07-16T19:58:00.000-05:00,[Patch] Table cursor separated from table selection when clicking on grid lines or empty space,"table.getLinesVisible()  
 table.setLinesVisible(true)
SWT-win32, v3138 (3.1-final)
use table cursor have potential of table regions have kinds of table regions separate table cursor from table selection
Expected behaviour:
To reproduce the problem, use snippet 96 with an added table.setLinesVisible(true).",org.eclipse.swt.custom.TableCursor
FILE,swt-3.1,104545,2005-07-20T14:21:00.000-05:00,Make default size of empty composites smaller,"static final int DEFAULT_WIDTH	= 64;
 static final int DEFAULT_HEIGHT	= 64;
/* Default widths for widgets */ static final int DEFAULT_WIDTH	= 64;
static final int DEFAULT_HEIGHT	= 64;
I have run our tests (JFace, UI, RCP) and they run fine when the constants are 0.
Background: When you write an RCP app and enable the cool bar, the cool bar will initially be empty, but 64x64 pixels in size.
not see border of empty coolbar not see border on windows get big empty space at top confuse big empty space at top
See also Bug 70049, where the same problem occurs in an RCP application that starts off with no open perspective and thus no cool bar items.",org.eclipse.swt.widgets.CoolBar
FILE,swt-3.1,81264,2004-12-15T13:17:00.000-06:00,Table fails to setTopIndex after new items are added to the table,"public static void main(String[] args) {
		final Display display = new Display();
		Shell shell = new Shell(display);
		shell.setBounds(10,10,200,200);
		final Table table = new Table(shell, SWT.NONE);
		table.setBounds(10,10,100,100);
		for (int i = 0; i < 99; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}
		
		table.setTopIndex(20);

		shell.open();

		System.out.println(""top visible index: "" + table.getTopIndex());
		
		for (int i = 0; i < 5; i++) {
			new TableItem(table, SWT.NONE).setText(""item "" + i);
		}

		table.setTopIndex(40);
		System.out.println(""top visible index: "" + table.getTopIndex());
		
		while (!shell.isDisposed()) {
			if (!display.readAndDispatch()) display.sleep();
		}
		display.dispose();
	}

  
  
 setTopTable(40)  
  
 setTopIndex(40)
I am working on a table viewer that keeps track of the scroll bar and loads content into the table dynamically as the user scrolls to the end of the table.
Items could be added/removed from the table as the user scrolls.
To maintain the position of the table, I call setTopIndex at the end of the update.
I have created a small testcase to simulate the process.
Here's my testcase to demonstrate the problem:
public static void main(String[] args) { final Display display = new Display();
Shell shell = new Shell(display);
shell.setBounds(10,10,200,200);
final Table table = new Table(shell, SWT.NONE);
table.setBounds(10,10,100,100);
for (int i = 0; i < 99; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
} table.setTopIndex(20);
shell.open();
System.out.println(""top visible index: "" + table.getTopIndex());
for (int i = 0; i < 5; i++) { new TableItem(table, SWT.NONE).
setText(""item "" + i);
}
table.setTopIndex(40);
System.out.println(""top visible index: "" + table.getTopIndex());
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}
position to correct table item add new items to table
position table at item
add new table items to table have effect after adding
Expected Result:
If the last 5 items are added before the shell is opened, setTopIndex to 40 will also succeed.
The testcase works as expected on Windows.","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.List
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,84609,2005-02-07T13:35:00.000-06:00,TableColumn has NPE while calling pack()  on last column,"lvtTable.getColumn(0).pack();
lvtTable.getColumn(1).pack();
lvtTable.getColumn(2).pack();

   
 parent.getColumns()
Consider followed code, table has only 3 columns:
// refresh table on new data lvtTable.getColumn(0).
pack();
lvtTable.getColumn(1).
pack();
lvtTable.getColumn(2).
pack();
catch NPE in TableColumn return array with elements get on third call
My system is WinXP, Eclipse Version: 3.1.0, Build id: I20050202-0800, JDK 1.5.0.
1","org.eclipse.swt.widgets.TableColumn
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,86000,2005-02-21T14:47:00.000-06:00,ImageLoader Save - produces invalid JPEG images,"package com.ibm.test.image;

import org.eclipse.swt.*;
import org.eclipse.swt.graphics.*;

public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".png"";
			String fileout = dir+files[i]+"".jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}
produce bad JPG images
I have only verified this with JPEG output.
Simple test case below loads
 PNG Files and Saves them as JPEG.
produce proper JPG images expected
The attached Zip file contains
 only those files that did not save correctly to JPEG.
package com.ibm.test.image;
import org.eclipse.swt.
*;
import org.eclipse.swt.graphics.
*;
public class ImageLoaderTest {
		
	public static void main(String[] args) {
		ImageLoader loader;
		String dir=""c:\\image-problems\\"";
		String files[]={
				""s34i3p04"",
				""s34n3p04"",
				""s35i3p04"",
				""s35n3p04"",
				""s36i3p04"",
				""s36n3p04"",
				""s37i3p04"",
				""s37n3p04"",
				""s38i3p04"",
				""s38n3p04"",
				""s39i3p04"",
				""s39n3p04""
		};
		
		try {
			for (int i=0; i<files.length; i++) {
			String filein  = dir+files[i]+"".
png"";
			String fileout = dir+files[i]+"".
jpg"";
			
			loader = new ImageLoader();
			loader.load(filein);
			loader.save(fileout,SWT.IMAGE_JPEG);
			}
		} catch (SWTException e) {
		  e.printStackTrace();
		}
	}
}",org.eclipse.swt.internal.image.JPEGFileFormat
FILE,swt-3.1,87460,2005-03-08T21:22:00.000-06:00,StyledText: Caret location not updated when line style is used,"import org.eclipse.swt.*;
import org.eclipse.swt.custom.*;
import org.eclipse.swt.graphics.*;
import org.eclipse.swt.layout.*;
import org.eclipse.swt.widgets.*;

public class LineStyleCaretTest {
  public static void main(String[] args) {
    Display display = new Display();
    
    Shell shell = new Shell(display);
    shell.setLayout(new FillLayout());
    
    Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
      
    final StyledText text = new StyledText(shell, SWT.MULTI);
    text.setFont(font);
    text.setText(""Standard Widget Toolkit"");
    text.setCaretOffset(text.getText().length());
    
    text.addLineStyleListener(new LineStyleListener() {
      public void lineGetStyle(LineStyleEvent event) {
        StyleRange[] styles = new StyleRange[1];
        
        styles[0] = new StyleRange();
        styles[0].start  = 0;
        styles[0].length = text.getText().length();
        styles[0].fontStyle = SWT.BOLD;
        
        event.styles = styles;
      }
    });
    
    shell.setSize(300, 100);
    shell.open();
    
    while (!shell.isDisposed()) {
      if (!display.readAndDispatch()) {
        display.sleep();
      }
    }
    
    font.dispose();
    display.dispose();
  }
}
SWT-win32, v3124
In the snippet below, there is a StyledText with a line style listener.
In the line style listener, a bold font style is set, changing the width of the rendered text.
For an italic style, it does not look right either.
Might be a bug?
---
import org.eclipse.swt.
*;
import org.eclipse.swt.custom.
*;
import org.eclipse.swt.graphics.
*;
import org.eclipse.swt.layout.
*;
import org.eclipse.swt.widgets.
*;
public class LineStyleCaretTest { public static void main(String[] args) {
Display display = new Display();
Shell shell = new Shell(display);
shell.setLayout(new FillLayout());
Font font = new Font(display, ""Arial"", 12, SWT.NORMAL);
final StyledText text = new StyledText(shell, SWT.MULTI);
text.setFont(font);
text.setText(""Standard Widget Toolkit"");
text.setCaretOffset(text.getText().
length());
text.addLineStyleListener(new LineStyleListener() { public void lineGetStyle(LineStyleEvent event) {
StyleRange[] styles = new StyleRange[1];
styles[0] = new StyleRange();
styles[0].
start  = 0;
styles[0].
length = text.getText().
length();
styles[0].
fontStyle = SWT.BOLD;
event.styles = styles;
}
});
shell.setSize(300, 100);
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch()) { display.sleep();
}
} font.dispose();
display.dispose();
}
}",org.eclipse.swt.custom.StyledText
FILE,swt-3.1,87997,2005-03-14T19:21:00.000-06:00,TableEditor.dispose( ) causes NPE if linked Table is being disposed,"TableEdtior.dispose( )  
  
   

import org.eclipse.swt.custom.TableEditor;
import org.eclipse.swt.events.*;
import org.eclipse.swt.widgets.*;

public class Test
{
    public static void main( String[ ] args )
    {
        Shell shell = new Shell( );
        Table table = new Table( shell, 0 );
        new TableColumn( table, 0 );
        TableItem item = new TableItem( table, 0 );
        final TableEditor editor = new TableEditor( table );
        final Text text = new Text( table, 0 );
        editor.setEditor( text, item, 0 );
        item.addDisposeListener( new DisposeListener( ) {
            public void widgetDisposed( DisposeEvent e )
            {
                text.dispose( );
                editor.dispose( ); // Triggers a NPE
            }
        } );
        shell.dispose( );
    }
}
Found in 3.1 I20050308-0835.
TableEdtior.dispose( ) calls methods on it's owning Table to remove some
listeners from the table's columns.
be in process result in NPE
Specifically this prevents one from adding a dispose listener on the Table
or a TableItem and trying to dispose of the associated editor, as in the code
below.
set dispose listener on parent throw instead_of NPE
This leaves
no place to hook to trigger the disposal of the TableEditor.
import org.eclipse.swt.custom.TableEditor;
import org.eclipse.swt.events.
*;
import org.eclipse.swt.widgets.
*;
public class Test
{
    public static void main( String[ ] args )
    {
        Shell shell = new Shell( );
        Table table = new Table( shell, 0 );
        new TableColumn( table, 0 );
        TableItem item = new TableItem( table, 0 );
        final TableEditor editor = new TableEditor( table );
        final Text text = new Text( table, 0 );
        editor.setEditor( text, item, 0 );
        item.addDisposeListener( new DisposeListener( ) {
            public void widgetDisposed( DisposeEvent e )
            {
                text.dispose( );
                editor.dispose( ); // Triggers a NPE
            }
        } );
        shell.dispose( );
    }
}","org.eclipse.swt.widgets.Tree
org.eclipse.swt.widgets.Table"
FILE,swt-3.1,90258,2005-04-05T04:56:00.000-05:00,Table item not updated when item count == 1,"clearAll() 
 table.setItemCount(1);
table.clearAll();

 
 private void handleSetData(Event event) {

	TableItem item= (TableItem) event.item;
	int index= fProposalTable.indexOf(item);
	
	ICompletionProposal current= fFilteredProposals[index];
	
	item.setText(current.getDisplayString());
	item.setImage(current.getImage());
	item.setData(current);
}
I20050401 (M6)
I am using a Table with SWT.VIRTUAL.
set item count not receive SWT.SetData notification
Tried to reproduce using a modified version of Snippet151, but everything works as expected there.
Do you have any idea what could be going wrong?
display updated contents in variable view display updated contents of table.items[0] display updated contents after calling call clearAll()
My code looks like this:
----------------
Table table...
table.setItemCount(1);
table.clearAll();
...
private void handleSetData(Event event) {
TableItem item= (TableItem) event.item;
int index= fProposalTable.indexOf(item);
ICompletionProposal current= fFilteredProposals[index];
item.setText(current.getDisplayString());
item.setImage(current.getImage());
item.setData(current);
}
-----------------",org.eclipse.swt.widgets.Table
FILE,swt-3.1,93724,2005-05-04T17:35:00.000-05:00,Drag-and-drop creates signal names every time,"byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);	
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
Here is an example of some code in DragSource.java for GTK+:
byte[] buffer = Converter.wcsToMbcs(null, ""drag_data_get"", true);
OS.g_signal_connect(control.handle, buffer, DragGetData.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_end"", true);
OS.g_signal_connect(control.handle, buffer, DragEnd.getAddress(), 0);
buffer = Converter.wcsToMbcs(null, ""drag_data_delete"", true);
OS.g_signal_connect(control.handle, buffer, DragDataDelete.getAddress(), 0);
convert names for signals define signal names in OS.java","org.eclipse.swt.dnd.DropTarget
org.eclipse.swt.dnd.DragSource"
FILE,swt-3.1,97651,2005-05-31T14:43:00.000-05:00,tree insert mark cheese,"Tree.redraw() 
 public static void main(String[] args) {
	final Display display = new Display();
	final Shell shell = new Shell(display);
	shell.setBounds(10, 10, 300, 300);
	final Tree tree = new Tree(shell, SWT.NONE);
	tree.setBounds(10, 10, 200, 200);
	new TreeItem(tree, SWT.NONE).setText(""pre-root"");
	TreeItem root1 = new TreeItem(tree, SWT.NONE);
	root1.setText(""root"");
	TreeItem child = new TreeItem(root1, SWT.NONE);
	child.setText(""child"");
	Button button = new Button(shell, SWT.PUSH);
	button.setBounds(230,10,30,30);
	button.addSelectionListener(new SelectionAdapter() {
		public void widgetSelected(SelectionEvent e) {
			tree.redraw();
		}
	});
	root1.setExpanded(true);
	tree.setInsertMark(root1, false);
	shell.open();
	while (!shell.isDisposed()) {
		if (!display.readAndDispatch()) display.sleep();
	}
	display.dispose();
}
3.1RC1
- run the snippet below
- the insert line is set to be under the ""root"" item
- collapse the root item
except for pointy ends
- press the button to the right of the Table: this does a Tree.redraw(), and note that the insert line reappears, so I guess it never really meant to go away
expand root item copy insert mark to child item copy insert mark to initial location
public static void main(String[] args) { final Display display = new Display();
final Shell shell = new Shell(display);
shell.setBounds(10, 10, 300, 300);
final Tree tree = new Tree(shell, SWT.NONE);
tree.setBounds(10, 10, 200, 200);
new TreeItem(tree, SWT.NONE).
setText(""pre-root"");
TreeItem root1 = new TreeItem(tree, SWT.NONE);
root1.setText(""root"");
TreeItem child = new TreeItem(root1, SWT.NONE);
child.setText(""child"");
Button button = new Button(shell, SWT.PUSH);
button.setBounds(230,10,30,30);
button.addSelectionListener(new SelectionAdapter() { public void widgetSelected(SelectionEvent e) { tree.redraw();
}
});
root1.setExpanded(true);
tree.setInsertMark(root1, false);
shell.open();
while (! shell.isDisposed()) { if (! display.readAndDispatch()) display.sleep();
} display.dispose();
}","org.eclipse.swt.dnd.TreeDragUnderEffect
org.eclipse.swt.widgets.Tree"
FILE,CONFIGURATION,CONFIGURATION-214,2006-05-26T21:35:46.000-05:00,Adding an integer and getting it as a long causes an exception,"bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .getLong ( ""foo"" )
   
  PropertyConverter.toLong()
Try this in a BeanShell:
bsh % p = new org.apache.commons.configuration.PropertiesConfiguration();
bsh % p.setProperty(""foo"", 6);
bsh % p.getLong(""foo"");
// Error: // Uncaught Exception: Method Invocation p.getLong : at Line: 3 : in file: <unknown file> : p .
getLong ( ""foo"" )
Target exception: org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object org.apache.commons.configuration.ConversionException: 'foo' doesn't map to a Long object
at org.apache.commons.configuration.AbstractConfiguration.getLong(AbstractConfiguration.java:667)
treat as string
It is a very confusing behaviour, because if you save and reload the properties everything works fine (as now the integer is a string).","org.apache.commons.configuration.TestPropertyConverter
org.apache.commons.configuration.PropertyConverter
org.apache.commons.configuration.TestBaseConfiguration"
FILE,CONFIGURATION,CONFIGURATION-241,2006-12-02T00:03:48.000-06:00,clearProperty() does not generate events,"clearProperty() 
 ConfigurationFactory configurationFactory = new ConfigurationFactory();
   
 configurationFactory.setConfigurationURL(configFileURL);
Configuration configuration = ConfigurationFactory.getConfiguration();
configuration.addConfigurationListener(new ConfigurationListener() {
    public void configurationChanged(ConfigurationEvent e) 
{
        System.out.println(e.getPropertyName() + "": "" + e.getPropertyValue());
    }
});
System.out.println(configuration.getProperty(""name.first"")); // prints ""Mike""
 configuration.claerProperty(""name.first"")  ; // no output whatsoever
System.out.println(configuration.getProperty(""name.first"")); // prints ""null""
I am loading configuration information from multiple sources and have registered a listener with the resulting configuration object.
not receive clear property events
receive other events clear property
I've tried setting ""details"" to true, which had no effect.
Below is a watered down version of what I am doing (note, my configuration file simply pulls in a property file containing this property: name.first=Mike):
ConfigurationFactory configurationFactory = new ConfigurationFactory();
URL configFileURL = ... get the config file ...
configurationFactory.setConfigurationURL(configFileURL);
Configuration configuration = ConfigurationFactory.getConfiguration();
configuration.addConfigurationListener(new ConfigurationListener() { public void configurationChanged(ConfigurationEvent e)
{
System.out.println(e.getPropertyName() + "": "" + e.getPropertyValue());
}
});
System.out.println(configuration.getProperty(""name.first"")); // prints ""Mike"" configuration.claerProperty(""name.first"")); // no output whatsoever
System.out.println(configuration.getProperty(""name.first"")); // prints ""null""","org.apache.commons.configuration.TestCompositeConfiguration
org.apache.commons.configuration.CompositeConfiguration"
FILE,CONFIGURATION,CONFIGURATION-259,2007-03-28T08:47:56.000-05:00,ConfigurationFactory Merge is broken,"URL configURL = getClass().getResource(configFile);
ConfigurationFactory factory = new ConfigurationFactory();
factory.setConfigurationURL(configURL);
myConfig = factory.getConfiguration();
 
 
 DefaultConfigurationBuilder builder = new DefaultConfigurationBuilder();
            builder.setURL(configURL);
            myConfig = builder.getConfiguration();
I am trying to merge two Configuration using the ConfigurationFactory and the additional tag.
It turns out that subsequent operations on the merged data provide wrong results.
In particular, after creating a particular subset from a loaded configuration, the subset is empty.
Strangely enough, when using DefaultConfigurationBuilder to load exactly the same configurations this works properly.
So when initializing the configuration as follows, I get the following error:
URL configURL = getClass().
getResource(configFile);
ConfigurationFactory factory = new ConfigurationFactory();
factory.setConfigurationURL(configURL);
myConfig = factory.getConfiguration();
60043 java.util.NoSuchElementException: 'HvNr' doesn't map to an existing object
at org.apache.commons.configuration.AbstractConfiguration.getLong(AbstractConfiguration.java:743)
at de.awd.vertriebsportal.portal.person.TestConfiguration.main(TestConfiguration.java:84)
Exception in thread ""main""
But when initializing it like this everything works properly
DefaultConfigurationBuilder builder = new DefaultConfigurationBuilder();
builder.setURL(configURL);
myConfig = builder.getConfiguration();
60043
54564
I will attach full source code and xml files",org.apache.commons.configuration.ConfigurationFactory
FILE,CONFIGURATION,CONFIGURATION-332,2008-07-04T15:54:10.000-05:00,PropertiesConfiguration.save() doesn't persist properties added through a DataConfiguration,"public void testSaveWithDataConfiguration() throws ConfigurationException
{
    File file = new File(""target/testsave.properties"");
    if (file.exists()) {
        assertTrue(file.delete());
    }

    PropertiesConfiguration config = new PropertiesConfiguration(file);

    DataConfiguration dataConfig = new DataConfiguration(config);

    dataConfig.setProperty(""foo"", ""bar"");
    assertEquals(""bar"", config.getProperty(""foo""));
    config.save();

    // reload the file
    PropertiesConfiguration config2 = new PropertiesConfiguration(file);
    assertFalse(""empty configuration"", config2.isEmpty());
}
There is a regression in Commons Configuration with PropertiesConfiguration wrapped into a DataConfiguration.
add through DataConfiguration
Commons Configuration 1.4 wasn't affected by this issue.
The following test fails on the last assertion :
public void testSaveWithDataConfiguration() throws ConfigurationException
{
File file = new File(""target/testsave.
properties"");
if (file.exists()) { assertTrue(file.delete());
}
PropertiesConfiguration config = new PropertiesConfiguration(file);
DataConfiguration dataConfig = new DataConfiguration(config);
dataConfig.setProperty(""foo"", ""bar"");
assertEquals(""bar"", config.getProperty(""foo""));
config.save();
// reload the file
PropertiesConfiguration config2 = new PropertiesConfiguration(file);
assertFalse(""empty configuration"", config2.isEmpty());
}","org.apache.commons.configuration.TestPropertiesConfiguration
org.apache.commons.configuration.DataConfiguration"
FILE,CONFIGURATION,CONFIGURATION-347,2008-11-05T21:06:22.000-06:00,Iterating over the keys of a file-based configuration can cause a ConcurrentModificationException,"getKeys()
return iterator in getKeys() method
This behavior is very confusing because ConcurrentModificationExceptions are typically related to multi-threading access.
perform iteration access configuration access only instance","org.apache.commons.configuration.TestFileConfiguration
org.apache.commons.configuration.AbstractFileConfiguration"
FILE,CONFIGURATION,CONFIGURATION-408,2010-02-11T01:01:05.000-06:00,"When I save a URL as a property value, the forward slashes are getting escaped","public static void main(String[] args)
  {
    try
    {

      PropertiesConfiguration config = new PropertiesConfiguration();     

      File newProps = new File(""foo.properties"");

      config.setProperty(""foo"", ""http://www.google.com/"");     

      config.save(newProps);

      

    }
    catch (Exception e){}
  }
save URL as property value
ie:
foo = http:\/\/www.google.com\/
Example Code :
public static void main(String[] args)
{ try
{
PropertiesConfiguration config = new PropertiesConfiguration();
File newProps = new File(""foo.properties"");
config.setProperty(""foo"", ""http://www.google.com/"");
config.save(newProps);
} catch (Exception e){}
}",org.apache.commons.configuration.TestPropertiesConfiguration
FILE,CONFIGURATION,CONFIGURATION-481,2012-02-26T20:27:46.000-06:00,Variable interpolation across files broken in 1.7 & 1.8,"{myvar}  
 
 
 
 combinedConfig.getConfiguration(""test"")  configurationAt(""products/product[@name='abc']"", true)  getString(""desc"")

  {myvar}
With Commons Configuration 1.6, I was able to declare a variable in a properties file, and then reference it in a XML file using the ${myvar} syntax.
For example:
global.properties:
myvar=abc
test.xml:
<products>
<product name=""abc"">
<desc>${myvar}-product</desc>
</product>
</products>
config.xml:
<properties fileName=""global.properties""/>
<xml fileName=""test.xml"" config-name=""test"">
<expressionEngine config-class=""org.apache.commons.configuration.tree.xpath.XPathExpressionEngine""/>
</xml>
When I try to retrieve the value, like so:
combinedConfig.getConfiguration(""test"").
configurationAt(""products/product[@name='abc']"", true).
getString(""desc"")
get myvar } instead_of abc-product
This was working in Commons Configuration 1.6, but seems to be broken in 1.7 and 1.8.","org.apache.commons.configuration.DefaultConfigurationBuilder
org.apache.commons.configuration.interpol.ConfigurationInterpolator
org.apache.commons.configuration.TestDefaultConfigurationBuilder"
CLASS,hibernate-3.5.0b2,HHH-4617,2009-11-28T11:42:08.000-06:00,Using materialized blobs with Postgresql causes error,"@Lob
I have entity with byte[] property annotated as @Lob and lazy fetch type, when table is createad the created column is of type oid, but when the column is read in application, the Hibernate reads the OID value instead of bytes under given oid.
It's behavior like to read / write bytea.
create oid column","org.hibernate.type.CharacterArrayClobType
org.hibernate.type.MaterializedClobType
org.hibernate.type.PrimitiveCharacterArrayClobType
org.hibernate.type.WrappedMaterializedBlobType
org.hibernate.type.MaterializedBlobType
org.hibernate.test.lob.MaterializedBlobTest
org.hibernate.type.BlobType
org.hibernate.type.ClobType
org.hibernate.test.lob.ClobLocatorTest
org.hibernate.dialect.Dialect
org.hibernate.cfg.annotations.SimpleValueBinder
org.hibernate.dialect.PostgreSQLDialect
org.hibernate.Hibernate"
CLASS,hibernate-3.5.0b2,HHH-5042,2010-03-26T05:06:09.000-05:00,TableGenerator does not increment hibernate_sequences.next_hi_value anymore after having exhausted the current lo-range,"class MultipleHiLoPerTableGenerator 
 IntegralDataTypeHolder value;
 
 int lo;

 
  
  
 IntegralDataTypeHolder hiVal = (IntegralDataTypeHolder) doWorkInNewTransaction( session );

   
  
 varchar(255) 
     varchar(255)
This bug is new in 3.5
In version 3.5 class MultipleHiLoPerTableGenerator.java was modified introducing a new increment variable
IntegralDataTypeHolder value;
along with int lo;
if ( lo > maxLo ) {
IntegralDataTypeHolder hiVal = (IntegralDataTypeHolder) doWorkInNewTransaction( session );
deliver numbers without update hibernate_sequences
duplicate keys insert new objects on concerning table
Please see attached testcase.
IMPORTANT ADVICE TO RUN THE TESTCASE:
as the testcase uses 2 sessionfactories hibernate.hbm2ddl.auto=create cannot be used!!
Schema has to be exported separately and the testcase must run without hbm2ddl.auto property!
Here the schema for HSQLDB:
create table A (oid bigint not null, name varchar(255), version integer not null, primary key (oid), unique (name))
create table hibernate_sequences ( sequence_name varchar(255),  sequence_next_hi_value integer )","org.hibernate.id.SequenceHiLoGenerator
org.hibernate.id.enhanced.OptimizerFactory
org.hibernate.id.SequenceGenerator
org.hibernate.id.MultipleHiLoPerTableGenerator"
METHOD,openjpa-2.0.1,OPENJPA-1627,2010-04-12T05:21:13.000-05:00,ORderBy with @ElementJoinColumn and EmbeddedId uses wrong columns in SQL,"@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id._processDate ASC, _id._tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;



      
 @EmbeddedId
	private TransactionId _id;
	
	 @Column(name = ""mtrancde"")
	private int _transactionCode;
	
	 @Column(name = ""mamount"")
	private BigDecimal _amount;
	
	 @Column(name = ""mdesc"")
	private String _description;
	


	 @Column(name = ""mactdate"")
	private Date _actualDate;
	
	 @Column(name = ""mbranch"")
	private int _branch;



   
 @Embeddable
public class TransactionId  
 @Column(name = ""maccno"")
	private String _accountNumber;
	
	 @Column(name = ""mprocdate"")
	private Date _processDate;
	
	 @Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
Typical bank example, Account with Transactions.
It is a legacy db so Transaction has compound key - represented by TransactionId class.
be for columns map in transaction entity NOT
So the Account class has the following fragment....
@OneToMany(fetch = FetchType.LAZY, cascade = {CascadeType.PERSIST})
	@ElementJoinColumn(name=""maccno"", referencedColumnName=""maccno"")
	@OrderBy(value = ""_id.
_processDate ASC, _id.
_tranSequenceNumber ASC"")
	private LinkedList<Transaction> _transactions;
_processDate and _tranSequenceNumber are defined in the TransactionId class.
Transaction has the following fragment....
@EmbeddedId
	private TransactionId _id;
	
	@Column(name = ""mtrancde"")
	private int _transactionCode;
	
	@Column(name = ""mamount"")
	private BigDecimal _amount;
	
	@Column(name = ""mdesc"")
	private String _description;
@Column(name = ""mactdate"")
	private Date _actualDate;
	
	@Column(name = ""mbranch"")
	private int _branch;
And TransactionId defines the primary key columns....
@Embeddable
public class TransactionId {
	
	@Column(name = ""maccno"")
	private String _accountNumber;
	
	@Column(name = ""mprocdate"")
	private Date _processDate;
	
	@Column(name = ""mtranseqno"")
	private int _tranSequenceNumber;
do on columns map in transaction
executing prepstmnt 23188098 SELECT t0.maccno, t0.mprocdate, t0.mtranseqno, t0.mactdate, t0.mamount, t0.mbranch, t0.mchqcash, t0.mdesc,
 t0.mtmnlno, t0.mtrancde, t0.mtrnfeed 
FROM transaction t0 
WHERE t0.maccno = ?
ORDER BY t0.mamount ASC, t0.mbranch ASC [params=(String) 000734123]
ORDER BY t0.mprocdate ASC, t0.mtranseqno ASC [params=(String) 000734123]
Thanks
Michael","org.apache.openjpa.jdbc.meta.JDBCRelatedFieldOrder:order(Select, ClassMapping, Joins)"
METHOD,openjpa-2.0.1,OPENJPA-1784,2010-09-08T08:31:29.000-05:00,Map value updates not flushed,"@Embeddable
public class LocalizedString {


    private String language;
    private String string;


    // getters and setters omitted
}


 


 @Entity
public class MultilingualString {


    @Id
    private long id;


    @ElementCollection(fetch=FetchType.EAGER)
    private Map<String, LocalizedString> map = new HashMap<String, LocalizedString>();
}



 
   ;
    em.getTransaction().begin();
    m.getMap().get(""en"").setString(""foo"");
     em.merge(m)
     em.getTransaction()  commit();
   
 
   ;
    em.getTransaction().begin();
     m.getMap()  put(""en"")  new LocalizedString(""en"", ""foo"") 
 em.merge(m)
     em.getTransaction()  commit();


 
 hashCode()   equals()   equal()
I have an entity with a map element collection where the map value is an Embeddable.
@Embeddable
public class LocalizedString {
private String language;
    private String string;
// getters and setters omitted
}
@Entity
public class MultilingualString {
@Id
    private long id;
@ElementCollection(fetch=FetchType.EAGER)
    private Map<String, LocalizedString> map = new HashMap<String, LocalizedString>();
}
Given a persistent instance m of my entity, I update a member of a given map value and then merge the modified entity:
EntityManager em = ...;
    em.getTransaction().
begin();
    m.getMap().
get(""en"").
setString(""foo"");
    em.merge(m)
    em.getTransaction().
not save state change of map not save state change to database
not trigger SQL UPDATE see with DEBUG logging
To force the update, I have to put a new value into the map instead of just changing the existing one.
EntityManager em = ...;
    em.getTransaction().
begin();
    m.getMap().
put(""en""), new LocalizedString(""en"", ""foo""));
    em.merge(m)
    em.getTransaction().
commit();
see expected UPDATE after change
My Embeddable does have hashCode() and equals() implemented such that the changed map is not equal() to the former version in either case.
This looks like a bug in the dirty-checking logic in OpenJPA.","org.apache.openjpa.util.ProxyMaps:beforePut(ProxyMap, Object, Object)"
METHOD,openjpa-2.0.1,OPENJPA-526,2008-02-27T13:28:05.000-06:00,Insert text more than 4K bytes to Clob column causes SQLException: Exhausted Resultset,"public class Exam 
 @Lob 
 @Column(name = ""text"", nullable = false)  
 private String text;
 
 With nullable = false
Here's the persistence class:
public class Exam... {
    @Lob
    @Column(name = ""text"", nullable = false) ***** NOTE: set nullable = true will fix the problem but it leads to bug OPENJPA-525 *****
    private String text;
}
Here are the differences with nullable = true:
INSERT INTO exam (id, text) VALUES (?
, ?)
[params=(long) 1, (Clob) oracle.sql.CLOB@d402dd]
SELECT t0.text FROM exam t0 WHERE t0.id = ?
FOR UPDATE [params=(long) 1]
With nullable = false:
INSERT INTO exam (id, text) VALUES (?
, ?)
[params=(long) 1, (Reader) java.io.StringReader@1603522]
SELECT t0.text FROM exam t0 WHERE t0.id = ?
FOR UPDATE [params=(long) 1] [code=1400, state=23000]
Here's the full stack trace:
[2008-02-27 10:43:51,232][main][org.apache.openjpa.lib.log.Log4JLogFactory$LogAdapter:72][DEBUG] <t 11050211, conn 32112901> executing prepstmnt 15029693 INSERT INTO exam (id, last_updated_by, comments, sustained_on, text, version, course_id, professor_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?) [params=(long) 11, (String) test, (null) null, (Date) 2008-02-27, (Reader) java.io.StringReader@da9ea4, (int) 1, (long) 1, (long) 8]
[2008-02-27 10:43:51,248][main][org.apache.openjpa.lib.log.Log4JLogFactory$LogAdapter:72][DEBUG] <t 11050211, conn 32112901> [16 ms] spent
[2008-02-27 10:43:51,248][main][org.apache.openjpa.lib.log.Log4JLogFactory$LogAdapter:72][DEBUG] <t 11050211, conn 32112901> executing prepstmnt 24422114 SELECT t0.text FROM exam t0 WHERE t0.id = ? FOR UPDATE [params=(long) 11]
[2008-02-27 10:43:51,279][main][org.apache.openjpa.lib.log.Log4JLogFactory$LogAdapter:72][DEBUG] <t 11050211, conn 32112901> [31 ms] spent
[2008-02-27 10:43:51,279][main][org.apache.openjpa.lib.log.Log4JLogFactory$LogAdapter:76][DEBUG] An exception occurred while ending the transaction.  This exception will be re-thrown.
<openjpa-1.0.2-r420667:627158 fatal store error> org.apache.openjpa.util.StoreException: The transaction has been rolled back.  See the nested exceptions for details on the errors that occurred.
at org.apache.openjpa.kernel.BrokerImpl.newFlushException(BrokerImpl.java:2108)
at org.apache.openjpa.kernel.BrokerImpl.flush(BrokerImpl.java:1955)
at org.apache.openjpa.kernel.BrokerImpl.flushSafe(BrokerImpl.java:1853)
at org.apache.openjpa.kernel.BrokerImpl.beforeCompletion(BrokerImpl.java:1771)
at org.apache.openjpa.kernel.LocalManagedRuntime.commit(LocalManagedRuntime.java:81)
at org.apache.openjpa.kernel.BrokerImpl.commit(BrokerImpl.java:1293)
at org.apache.openjpa.kernel.DelegatingBroker.commit(DelegatingBroker.java:861)
at org.apache.openjpa.persistence.EntityManagerImpl.commit(EntityManagerImpl.java:408)
at org.springframework.orm.jpa.JpaTransactionManager.doCommit(JpaTransactionManager.java:438)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:662)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:632)
at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:319)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:631)
at com.intellapps.university.service.impl.ServiceDahImpl$$EnhancerByCGLIB$$81ecf35d.insertExam(<generated>)
at com.intellapps.university.service.impl.ServiceImpl.insertExam(ServiceImpl.java:98)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:585)
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:301)
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
at $Proxy17.insertExam(Unknown Source)
at com.intellapps.university.app.Main.testInsertExamWithLongText(Main.java:103)
at com.intellapps.university.app.Main.main(Main.java:203)
Caused by: <openjpa-1.0.2-r420667:627158 nonfatal store error> org.apache.openjpa.util.StoreException: Exhausted Resultset
at org.apache.openjpa.jdbc.sql.DBDictionary.newStoreException(DBDictionary.java:3946)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:97)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:83)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:59)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:96)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:72)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager.flush(JDBCStoreManager.java:514)
at org.apache.openjpa.kernel.DelegatingStoreManager.flush(DelegatingStoreManager.java:130)
... 29 more
Caused by: java.sql.SQLException: Exhausted Resultset
at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:146)
at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:208)
at oracle.jdbc.driver.ScrollableResultSet.getOracleObject(ScrollableResultSet.java:510)
at oracle.jdbc.driver.ScrollableResultSet.getCLOB(ScrollableResultSet.java:1446)
at oracle.jdbc.driver.UpdatableResultSet.getCLOB(UpdatableResultSet.java:1639)
at oracle.jdbc.driver.UpdatableResultSet.getClob(UpdatableResultSet.java:982)
at org.apache.commons.dbcp.DelegatingResultSet.getClob(DelegatingResultSet.java:515)
at org.apache.openjpa.lib.jdbc.DelegatingResultSet.getClob(DelegatingResultSet.java:576)
at org.apache.openjpa.jdbc.meta.strats.MaxEmbeddedClobFieldStrategy.putData(MaxEmbeddedClobFieldStrategy.java:69)
at org.apache.openjpa.jdbc.meta.strats.MaxEmbeddedLobFieldStrategy.customUpdate(MaxEmbeddedLobFieldStrategy.java:162)
at org.apache.openjpa.jdbc.meta.strats.MaxEmbeddedLobFieldStrategy.customInsert(MaxEmbeddedLobFieldStrategy.java:140)
at org.apache.openjpa.jdbc.meta.FieldMapping.customInsert(FieldMapping.java:684)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager$CustomMapping.execute(AbstractUpdateManager.java:358)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:94)
... 32 more
NestedThrowables:
<openjpa-1.0.2-r420667:627158 nonfatal store error> org.apache.openjpa.util.ReferentialIntegrityException: ORA-01400: cannot insert NULL into (""TEST"".""EXAM"".""TEXT"")
{prepstmnt 15029693 INSERT INTO exam (id, last_updated_by, comments, sustained_on, text, version, course_id, professor_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?) [params=(long) 11, (String) test, (null) null, (Date) 2008-02-27, (Reader) java.io.StringReader@da9ea4, (int) 1, (long) 1, (long) 8]} [code=1400, state=23000]
FailedObject: com.intellapps.university.core.model.Exam@1417690
at org.apache.openjpa.jdbc.sql.DBDictionary.newStoreException(DBDictionary.java:3944)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:97)
at org.apache.openjpa.jdbc.sql.SQLExceptions.getStore(SQLExceptions.java:67)
at org.apache.openjpa.jdbc.kernel.PreparedStatementManagerImpl.flushInternal(PreparedStatementManagerImpl.java:108)
at org.apache.openjpa.jdbc.kernel.PreparedStatementManagerImpl.flush(PreparedStatementManagerImpl.java:73)
at org.apache.openjpa.jdbc.kernel.OperationOrderUpdateManager.flushPrimaryRow(OperationOrderUpdateManager.java:203)
at org.apache.openjpa.jdbc.kernel.OperationOrderUpdateManager.flush(OperationOrderUpdateManager.java:89)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:89)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:72)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager.flush(JDBCStoreManager.java:514)
at org.apache.openjpa.kernel.DelegatingStoreManager.flush(DelegatingStoreManager.java:130)
at org.apache.openjpa.kernel.BrokerImpl.flush(BrokerImpl.java:1955)
at org.apache.openjpa.kernel.BrokerImpl.flushSafe(BrokerImpl.java:1853)
at org.apache.openjpa.kernel.BrokerImpl.beforeCompletion(BrokerImpl.java:1771)
at org.apache.openjpa.kernel.LocalManagedRuntime.commit(LocalManagedRuntime.java:81)
at org.apache.openjpa.kernel.BrokerImpl.commit(BrokerImpl.java:1293)
at org.apache.openjpa.kernel.DelegatingBroker.commit(DelegatingBroker.java:861)
at org.apache.openjpa.persistence.EntityManagerImpl.commit(EntityManagerImpl.java:408)
at org.springframework.orm.jpa.JpaTransactionManager.doCommit(JpaTransactionManager.java:438)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:662)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:632)
at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:319)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:631)
at com.intellapps.university.service.impl.ServiceDahImpl$$EnhancerByCGLIB$$81ecf35d.insertExam(<generated>)
at com.intellapps.university.service.impl.ServiceImpl.insertExam(ServiceImpl.java:98)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:585)
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:301)
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
at $Proxy17.insertExam(Unknown Source)
at com.intellapps.university.app.Main.testInsertExamWithLongText(Main.java:103)
at com.intellapps.university.app.Main.main(Main.java:203)
Caused by: org.apache.openjpa.lib.jdbc.ReportingSQLException: ORA-01400: cannot insert NULL into (""TEST"".""EXAM"".""TEXT"")
{prepstmnt 15029693 INSERT INTO exam (id, last_updated_by, comments, sustained_on, text, version, course_id, professor_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?) [params=(long) 11, (String) test, (null) null, (Date) 2008-02-27, (Reader) java.io.StringReader@da9ea4, (int) 1, (long) 1, (long) 8]} [code=1400, state=23000]
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.wrap(LoggingConnectionDecorator.java:192)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator.access$800(LoggingConnectionDecorator.java:57)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeUpdate(LoggingConnectionDecorator.java:858)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:269)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeUpdate(JDBCStoreManager.java:1363)
at org.apache.openjpa.jdbc.kernel.PreparedStatementManagerImpl.flushInternal(PreparedStatementManagerImpl.java:97)
... 36 more
NestedThrowables:
java.sql.SQLException: ORA-01400: cannot insert NULL into (""TEST"".""EXAM"".""TEXT"")
at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:331)
at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:288)
at oracle.jdbc.driver.T4C8Oall.receive(T4C8Oall.java:745)
at oracle.jdbc.driver.T4CPreparedStatement.doOall8(T4CPreparedStatement.java:216)
at oracle.jdbc.driver.T4CPreparedStatement.executeForRows(T4CPreparedStatement.java:966)
at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1170)
at oracle.jdbc.driver.OraclePreparedStatement.executeInternal(OraclePreparedStatement.java:3339)
at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:3423)
at org.apache.commons.dbcp.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:102)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:269)
at org.apache.openjpa.lib.jdbc.LoggingConnectionDecorator$LoggingConnection$LoggingPreparedStatement.executeUpdate(LoggingConnectionDecorator.java:856)
at org.apache.openjpa.lib.jdbc.DelegatingPreparedStatement.executeUpdate(DelegatingPreparedStatement.java:269)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager$CancelPreparedStatement.executeUpdate(JDBCStoreManager.java:1363)
at org.apache.openjpa.jdbc.kernel.PreparedStatementManagerImpl.flushInternal(PreparedStatementManagerImpl.java:97)
at org.apache.openjpa.jdbc.kernel.PreparedStatementManagerImpl.flush(PreparedStatementManagerImpl.java:73)
at org.apache.openjpa.jdbc.kernel.OperationOrderUpdateManager.flushPrimaryRow(OperationOrderUpdateManager.java:203)
at org.apache.openjpa.jdbc.kernel.OperationOrderUpdateManager.flush(OperationOrderUpdateManager.java:89)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:89)
at org.apache.openjpa.jdbc.kernel.AbstractUpdateManager.flush(AbstractUpdateManager.java:72)
at org.apache.openjpa.jdbc.kernel.JDBCStoreManager.flush(JDBCStoreManager.java:514)
at org.apache.openjpa.kernel.DelegatingStoreManager.flush(DelegatingStoreManager.java:130)
at org.apache.openjpa.kernel.BrokerImpl.flush(BrokerImpl.java:1955)
at org.apache.openjpa.kernel.BrokerImpl.flushSafe(BrokerImpl.java:1853)
at org.apache.openjpa.kernel.BrokerImpl.beforeCompletion(BrokerImpl.java:1771)
at org.apache.openjpa.kernel.LocalManagedRuntime.commit(LocalManagedRuntime.java:81)
at org.apache.openjpa.kernel.BrokerImpl.commit(BrokerImpl.java:1293)
at org.apache.openjpa.kernel.DelegatingBroker.commit(DelegatingBroker.java:861)
at org.apache.openjpa.persistence.EntityManagerImpl.commit(EntityManagerImpl.java:408)
at org.springframework.orm.jpa.JpaTransactionManager.doCommit(JpaTransactionManager.java:438)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:662)
at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:632)
at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:319)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.Cglib2AopProxy$DynamicAdvisedInterceptor.intercept(Cglib2AopProxy.java:631)
at com.intellapps.university.service.impl.ServiceDahImpl$$EnhancerByCGLIB$$81ecf35d.insertExam(<generated>)
at com.intellapps.university.service.impl.ServiceImpl.insertExam(ServiceImpl.java:98)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:585)
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:301)
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:149)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:106)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
at $Proxy17.insertExam(Unknown Source)
at com.intellapps.university.app.Main.testInsertExamWithLongText(Main.java:103)
at com.intellapps.university.app.Main.main(Main.java:203)","org.apache.openjpa.persistence.kernel.common.apps.Lobs:getId()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:getLob()
org.apache.openjpa.persistence.kernel.common.apps.Lobs:Lobs(String, int)
org.apache.openjpa.persistence.kernel.common.apps.Lobs:setLob(String)
org.apache.openjpa.jdbc.sql.OracleDictionary:setNull(PreparedStatement, int, int, Column)"
METHOD,adempiere-3.1.0,1240,2008-05-16T03:03:55.000-05:00,Posting not balanced when is producing more than 1 produc,"Production Quantity= 2
when is necesary serialize, then the line must be put 1 each line.
in this case the production  cant  apply.
Step reproduce
1. En Gardenworld , Production windows , create a row in the tab Production Header for  ""Production 2 Patio  set"".
2. in the tab Production Plan create a row for the Patio Furniture Set product, Production Quantity= 2
3. Then click on  ""Create/post Production"" button in the Production header tab, this create the production line.
verify in the production line tab.
4. the first line is necesary divide in two for give a serial each one.
then in the Patio Furniture Set product set in movement quantity 1 and give a serial  in the attribute set instance field, then create other row similar but the other serial
5. then click on  ""Create/post Production"" button in the Production header tab
6. click on ""Not Postet"" Button, then there the botton label is changed to ""Dont Balanced""
Regards,
Layda Salas - globalqss http://globalqss.com",org.compiere.acct.Doc_Production:createFacts(MAcctSchema)
CLASS,pig-0.8.0,PIG-1188,2010-01-14T13:32:46.000-06:00,Padding nulls to the input tuple according to input schema,"{code}
  as (a0, a1);
dump a;
{code}
 
 {code}
 
 {code}
 
 {code}
 
 {code}

 
 {code}
 
 {code}
determine number of fields
Here is one example:
Pig script:
{code}
a = load '1.
txt' as (a0, a1);
dump a;
{code}
Input file:
{code}
1       2
1       2       3
1
{code}","test.org.apache.pig.test.TestMergeForEachOptimization
src.org.apache.pig.newplan.logical.rules.TypeCastInserter
test.org.apache.pig.test.TestNewPlanLogicalOptimizer
test.org.apache.pig.test.TestNewPlanFilterRule
test.org.apache.pig.test.TestNewPlanPushDownForeachFlatten
test.org.apache.pig.test.TestEvalPipeline2
test.org.apache.pig.test.TestMultiQueryCompiler
test.org.apache.pig.test.TestPartitionFilterPushDown
test.org.apache.pig.test.TestNewPlanFilterAboveForeach"
CLASS,pig-0.8.0,PIG-1277,2010-03-05T13:02:03.000-06:00,Pig should give error message when cogroup on tuple keys of different inner type,"UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}

 
 {code}
 
  
 by (c0, c1);
dump e;
{code}

 
 {code}
 
 {code}

 
 {code}
 
 {code}

 
 {code}
  {(1,1)}  {(1,1)} 
 {code}

 
 {code}
  {(1,1)}  {} 
 {}  {(1,1)} 
 {code}
treat as different keys cogroup on tuple
This is confusing.
Here is one example:
UDF:
{code}
public class MapGenerate extends EvalFunc<Map> {
    @Override
    public Map exec(Tuple input) throws IOException {
        // TODO Auto-generated method stub
        Map m = new HashMap();
        m.put(""key"", new Integer(input.size()));
        return m;
    }
    
    @Override
    public Schema outputSchema(Schema input) {
        return new Schema(new Schema.FieldSchema(null, DataType.MAP));
    }
}
{code}
Pig script: 
{code}
a = load '1.
txt' as (a0);
b = foreach a generate a0, MapGenerate(*) as m:map[];
c = foreach b generate a0, m#'key' as key;
d = load '2.
txt' as (c0, c1);
e = cogroup c by (a0, key), d by (c0, c1);
dump e;
{code}
1 txt
{code}
1
{code}
2 txt
{code}
1 1
{code}","src.org.apache.pig.impl.io.NullableBytesWritable
test.org.apache.pig.test.TestPackage
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigBytesRawComparator
src.org.apache.pig.backend.hadoop.HDataType
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMultiQueryPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestSecondarySort
src.org.apache.pig.newplan.logical.relational.LOUnion"
CLASS,pig-0.8.0,PIG-1771,2010-12-16T14:45:37.000-06:00,"New logical plan: Merge schema fail if LoadFunc.getSchema return different schema with ""Load...AS""","{code}
 
 BinStorage() 
         tuple()  ;
dump auxData;
{code}
The following script fail:
{code}
a = load '1.
txt' as (a0:chararray, a1:chararray, a3, a4:map[]);
store a into '1.
bin' using BinStorage();
auxData = LOAD '1.
bin' USING BinStorage('Utf8StorageConverter') AS (cookieId:chararray, type:chararray, record:tuple(), state:map[]);
dump auxData;
{code}
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2246: Error merging schema record#-1:tuple{} and null#-1:bytearray
at org.apache.pig.newplan.logical.relational.LogicalSchema.merge(LogicalSchema.java:337)
at org.apache.pig.newplan.logical.relational.LOLoad.getSchema(LOLoad.java:103)
at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:59)
at org.apache.pig.newplan.logical.relational.LOLoad.accept(LOLoad.java:159)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:261)
... 12 more","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogicalSchema"
CLASS,pig-0.8.0,PIG-1776,2010-12-17T16:28:09.000-06:00,"changing statement corresponding to alias after explain , then doing dump gives incorrect result","{code}
 
  
 {code}
{code}
grunt> a = load '/tmp/t2.
txt' as (str:chararray, num1:int, alph : chararray);
grunt> dump a;
(ABC,1,a)
(ABC,1,b)
(ABC,1,a)
(ABC,2,b)
(DEF,1,d)
(XYZ,1,x)
grunt> c = foreach b  generate group.str, group.
$1, COUNT(a.alph) ;          
grunt> dump c; -- gives correct results
(ABC,1,3)
(ABC,2,1)
(DEF,1,1)
(XYZ,1,1)
/* but dumping c after following steps gives incorrect results */
grunt> c = foreach b  generate group.
$0 , (CHARARRAY)group.
$1;                                                                                 
grunt> explain c;
...
...
grunt> c = foreach b  generate group.str, group.
$1, COUNT(a.alph) ;
grunt> dump c;             
(ABC,1,0)
(ABC,2,0)
(DEF,1,0)
(XYZ,1,0)
{code}","src.org.apache.pig.PigServer
src.org.apache.pig.newplan.logical.relational.LOLoad
test.org.apache.pig.test.TestUDFContext"
CLASS,pig-0.8.0,PIG-1808,2011-01-17T08:50:48.000-06:00,Error message in 0.8 not much helpful as compared to 0.7,"null;
DUMP D;
A = LOAD 'i1' ;
B = LOAD 'i2' ;
C = JOIN A by $92 left outer,B by $92  ;
D =  filter C by $100 is null;
DUMP D;
The below script fails both in 0.7 and 0.8 since A requires a valid schema to be defined.
-----------------------------
ERROR 2000: Error processing rule PushUpFilter.
Try -t PushUpFilter org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
....
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
....
Caused by: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2000: Error processing rule PushUpFilter. Try -t PushUpFilter
....
Caused by: java.lang.NullPointerException
at org.apache.pig.newplan.logical.rules.PushUpFilter$PushUpFilterTransformer.hasAll(PushUpFilter.java:308)
at org.apache.pig.newplan.logical.rules.PushUpFilter$PushUpFilterTransformer.check(PushUpFilter.java:141)
at org.apache.pig.newplan.optimizer.PlanOptimizer.optimize(PlanOptimizer.java:108)
... 13 more
----------------------------- org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias D
....
....
Caused by: org.apache.pig.backend.hadoop.executionengine.physicalLayer.LogicalToPhysicalTranslatorException:
ERROR 1109: Input (B) on which outer join is desired should have a valid schema","test.org.apache.pig.test.TestPushUpFilter
src.org.apache.pig.newplan.logical.rules.PushUpFilter"
CLASS,pig-0.8.0,PIG-1812,2011-01-19T20:06:36.000-06:00,Problem with DID_NOT_FIND_LOAD_ONLY_MAP_PLAN,"{t:(id:chararray, wht:float)} 
    
 flatten(cat_bag.id)    
    
 {
        I = order M by ts;
        J = order B by ts;
        generate flatten(group) as (pkg:chararray, cat_id:chararray), J.ts as tsorig, I.ts as tsmap;
}
Hi,
I have the following input files:
pkg.txt
a       3       {(123,1.0),(236,2.0)} a       3       {(236,1.0)}
model.txt
a       123     2       0.33 a       236     2       0.5
A = load 'pkg.txt' using PigStorage('\t') as (pkg:chararray, ts:int, cat_bag:{t:(id:chararray, wht:float)});
M = load 'model.txt' using PigStorage('\t') as (pkg:chararray, cat_id:chararray, ts:int, score:double);
B = foreach A generate ts, pkg, flatten(cat_bag.
id) as (cat_id:chararray);
B = distinct B;
H1 = cogroup M by (pkg, cat_id) inner, B by (pkg, cat_id);
H2 = foreach H1 {
I = order M by ts;
J = order B by ts;
generate flatten(group) as (pkg:chararray, cat_id:chararray), J.ts as tsorig, I.ts as tsmap;
}
dump H2;
run script get warning about encountered Warning DID_NOT_FIND_LOAD_ONLY_MAP_PLAN time get warning about pig error log
Pig Stack Trace
---------------
ERROR 2043: Unexpected error during execution.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias H2
at org.apache.pig.PigServer.openIterator(PigServer.java:764)
at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:612)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:165)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
at org.apache.pig.Main.run(Main.java:500)
at org.apache.pig.Main.main(Main.java:107)
Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias H2
at org.apache.pig.PigServer.storeEx(PigServer.java:888)
at org.apache.pig.PigServer.store(PigServer.java:826)
at org.apache.pig.PigServer.openIterator(PigServer.java:738)
... 7 more
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2043: Unexpected error during execution.
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:403)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1208)
at org.apache.pig.PigServer.storeEx(PigServer.java:884)
... 9 more
Caused by: java.lang.ClassCastException: org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLoad cannot be cast to org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizer.visitMROp(SecondaryKeyOptimizer.java:352)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:246)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceOper.visit(MapReduceOper.java:41)
at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:69)
at org.apache.pig.impl.plan.DepthFirstWalker.depthFirst(DepthFirstWalker.java:71)
at org.apache.pig.impl.plan.DepthFirstWalker.walk(DepthFirstWalker.java:52)
at org.apache.pig.impl.plan.PlanVisitor.visit(PlanVisitor.java:51)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.compile(MapReduceLauncher.java:498)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:117)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.execute(HExecutionEngine.java:378)
... 11 more
But, when I removed the DISTINCT statement before COGROUP, i.e. ""B = distinct B;""  this script can run smoothly.
try other reducer side operations like ORDER trigger above error
This is really very confusing.","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.LimitAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.KeyTypeDiscoveryVisitor
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler.RearrangeAdjuster
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1813,2011-01-20T10:25:01.000-06:00,Pig 0.8 throws ERROR 1075 while trying to refer a map in the result of  eval udf.Works with 0.7,"flatten(org.myudf.GETFIRST(value))  
 PigStorage()
register myudf.jar;
A = load 'input' MyZippedStorage('\u0001') as ($inputSchema);
B = foreach A generate id , value  ;
C = foreach B generate id , org.myudf.ExplodeHashList( (chararray)value, '\u0002', '\u0004', '\u0003') as value;
D = FILTER C by value is not null;
E = foreach D generate id , flatten(org.myudf.GETFIRST(value)) as hop;
F = foreach E generate id , hop#'rmli' as rmli:bytearray ;
store F into 'output.bz2' using PigStorage();
The above script fails when run with Pig 0.8 but runs fine with Pig 0.7 or if pig.usenewlogicalplan=false.
org.apache.pig.backend.executionengine.ExecException: ERROR 1075: Received a bytearray from the UDF. Cannot determine how to convert the bytearray to map.
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POCast.getNext(POCast.java:952)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.processInput(POMapLookUp.java:87)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:98)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POMapLookUp.getNext(POMapLookUp.java:117)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:346)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:236)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:638)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:314)
at org.apache.hadoop.mapred.Child$4.run(Child.java:217)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1062)
at org.apache.hadoop.mapred.Child.main(Child.java:211)","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LOGenerate"
CLASS,pig-0.8.0,PIG-1831,2011-01-28T04:02:31.000-06:00,Indeterministic behavior in local mode due to static variable PigMapReduce.sJobConf,"PigStorage()
The below script when run in local mode gives me a different output.
It looks like in local mode I have to store a relation obtained through streaming in order to use it afterwards.
For example consider the below script :
DEFINE MySTREAMUDF `test.sh`;
A  = LOAD 'myinput' USING PigStorage() AS (myId:chararray, data2, data3,data4 );
B = STREAM A THROUGH MySTREAMUDF AS (wId:chararray, num:int);
--STORE B into 'output.B';
C = JOIN B by wId LEFT OUTER, A by myId;
D = FOREACH C GENERATE B::wId,B::num,data4 ;
D = STREAM D THROUGH MySTREAMUDF AS (f1:chararray,f2:int);
--STORE D into 'output.D';
E = foreach B GENERATE wId,num;
F = DISTINCT E;
G = GROUP F ALL;
H = FOREACH G GENERATE COUNT_STAR(F) as TotalCount;
I = CROSS D,H;
STORE I  into 'output.I';
test.sh
---------
#/bin/bash
cut -f1,3
And input is 
abcd    label1  11      feature1
acbd    label2  22      feature2
adbc    label3  33      feature3
store relation get result
get empty output","src.org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeCogroup
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPackage
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POPartitionRearrange
src.org.apache.pig.builtin.Distinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.PODistinct
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POJoinPackage
src.org.apache.pig.data.InternalSortedBag
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POMergeJoin
src.org.apache.pig.impl.builtin.DefaultIndexableLoader
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce.Map
src.org.apache.pig.impl.io.FileLocalizer
test.org.apache.pig.test.TestFinish
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.partitioners.SkewedPartitioner
src.org.apache.pig.backend.hadoop.streaming.HadoopExecutableManager
src.org.apache.pig.data.InternalCachedBag
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce
test.org.apache.pig.test.TestPruneColumn
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCombinerPackage
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase
test.org.apache.pig.test.TestFRJoin
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POSort
src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POCollectedGroup
test.org.apache.pig.test.utils.FILTERFROMFILE
src.org.apache.pig.data.InternalDistinctBag"
CLASS,pig-0.8.0,PIG-1856,2011-02-15T17:26:16.000-06:00,Custom jar is not packaged with the new job created by LimitAdjuster,"{code}
 
  
 {code}
The script:
{code}
A = load 'data' as (s, m);
B = order A by s parallel 2;
C = limit B 20;
store C into 'output' using org.apache.pig.piggybank.storage.PigStorageSchema('\t')
{code}
where piggybank jar is in the classpath.
not ship piggybank jar with additional job not ship piggybank jar to backend
The workaround is to explicitly register the piggybank jar in the script.","src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler
test.org.apache.pig.test.TestMRCompiler"
CLASS,pig-0.8.0,PIG-1858,2011-02-17T02:27:48.000-06:00,UDF in nested plan results frontend exception,"{code}
 
 PigStorage()  
 {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).(count,sum);
        generate Const.sum as sum;
        } 
   ;
{code}
The below is my script :
{code}
register myanotherudf.jar;
A = load 'myinput' using PigStorage() as ( date:chararray,bcookie:chararray,count:int,avg:double,pvs:int);
B = foreach A generate (int)(avg / 100.0) * 100   as avg, pvs;
C = group B by ( avg );
D = foreach C {
        Pvs = order B by pvs;
        Const = org.vivek.MyAnotherUDF(Pvs.pvs).
(count,sum);
        generate Const.sum as sum;
        };
store D into 'out_D';
{code}
The script is failing during compilation of the plan.
The usage of the udf inside the foreach is causing the problem.
The udf implements algebraic and the 
output schema is also defined.
ERROR 2042: Error in new logical plan.
Try -Dpig.usenewlogicalplan=false.
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:309)
at org.apache.pig.PigServer.compilePp(PigServer.java:1364)
at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1206)
at org.apache.pig.PigServer.execute(PigServer.java:1200)
at org.apache.pig.PigServer.access$100(PigServer.java:128)
at org.apache.pig.PigServer$Graph.execute(PigServer.java:1527)
at org.apache.pig.PigServer.executeBatchEx(PigServer.java:372)
at org.apache.pig.PigServer.executeBatch(PigServer.java:339)
at org.apache.pig.tools.grunt.GruntParser.executeBatch(GruntParser.java:112)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:169)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:141)
at org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:90)
at org.apache.pig.Main.run(Main.java:500)
at org.apache.pig.Main.main(Main.java:107)
Caused by: java.lang.NullPointerException
at org.apache.pig.newplan.ReverseDependencyOrderWalker.walk(ReverseDependencyOrderWalker.java:70)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:105)
at org.apache.pig.newplan.logical.relational.LOGenerate.accept(LOGenerate.java:229)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.logical.optimizer.SchemaResetter.visit(SchemaResetter.java:94)
at org.apache.pig.newplan.logical.relational.LOForEach.accept(LOForEach.java:71)
at org.apache.pig.newplan.DependencyOrderWalker.walk(DependencyOrderWalker.java:75)
at org.apache.pig.newplan.PlanVisitor.visit(PlanVisitor.java:50)
at org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.compile(HExecutionEngine.java:261)
... 13 more
When i trun off new logical plan the script executes successfully.
The issue is observed in both 0.8 and 0.9",test.org.apache.pig.test.TestEvalPipeline2
CLASS,pig-0.8.0,PIG-1866,2011-02-23T14:01:13.000-06:00,Dereference a bag within a tuple does not work,"{code}
     
 t.b1;
dump b;
{code}
The following script does not work (both in new and old logical plan):
{code}
a = load '1.
txt' as (t : tuple(i: int, b1: bag { b_tuple : tuple ( b_str: chararray) }));
b = foreach a generate t.b1;
dump b;
{code}
1 txt:
(1 {(one),(two)})
java.lang.ClassCastException: org.apache.pig.data.BinSedesTuple cannot be cast to org.apache.pig.data.DataBag
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:482)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.processInputBag(POProject.java:480)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:197)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
java.lang.NullPointerException
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.consumeInputBag(POProject.java:246)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject.getNext(POProject.java:200)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:339)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNext(POForEach.java:291)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:237)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
generate t. i work = foreach generate t.b1 refer to bag","src.org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POProject
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler"
CLASS,pig-0.8.0,PIG-1868,2011-02-24T00:42:05.000-06:00,New logical plan fails when I have complex data types from udf,"{code}
 
 {
 Tuples = order B1 by ts;
 generate Tuples;
} 
   { t: ( previous, current, next ) } 
 as id;
dump C3;
{code}

 
 {code}
 
 {code}

  on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The new logical plan fails when I have complex data types returning from my eval function.
The below is my script :
{code}
register myudf.jar;   
B1 = load 'myinput' as (id:chararray,ts:int,url:chararray);
B2 = group B1 by id;
B = foreach B2 {
 Tuples = order B1 by ts;
 generate Tuples;
};
C1 = foreach B generate TransformToMyDataType(Tuples,-1,0,1) as seq: { t: ( previous, current, next ) };
C2 = foreach C1 generate FLATTEN(seq);
C3 = foreach C2 generate  current.id as id;
dump C3;
{code}
match uid for project code } fail on c3
The below is the describe on C1 ;
{code}
C1: {seq: {t: (previous: (id: chararray,ts: int,url: chararray),current: (id: chararray,ts: int,url: chararray),next: (id: chararray,ts: int,url: chararray))}}
{code}
The script works if I turn off new logical plan or use Pig 0.7.","src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-1892,2011-03-10T02:44:12.000-06:00,Bug in new logical plan : No output generated even though there are valid records,"Maploader()
I have the below script which provides me no output even though there are valid records in relation B which is used for the left out join.
A0 = load 'input' using Maploader()  as ( map1, map2, map3 );
A = filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) );
B0 = filter A by (map1#'type' == 'c');
B = filter B0 by ( map2#'info'#'s' matches 'aaaa|bbb|cccc');
C =  filter A by (map1#'type' == 'p');
D = join B by map2#'params'#'query' LEFT OUTER , C by map2#'params'#'query';
store D into 'output';
This is a bug with the newlogical plan.
From the plan i can see that  map1#'type'  and map2#'info'#'s' is not marked as RequiredKeys ,
but where as all the fields reffered in the firts filter statement is marked as required.
For the script to work I have to turn off the coloumn prune optimizer by -t ColumnMapKeyPrune or rearrange the script such that;
B0 = filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'c') );
C =  filter A0 by ( (map2#'params'#'prop' == 464)   and (map2#'params'#'query' is not null) and (map1#'type' == 'p') );","test.org.apache.pig.test.TestPruneColumn.PigStorageWithTrace
src.org.apache.pig.newplan.logical.rules.MapKeysPruneHelper
test.org.apache.pig.test.TestPruneColumn"
CLASS,pig-0.8.0,PIG-1893,2011-03-10T20:43:13.000-06:00,Pig report input size -1 for empty input file,"{code}
 
 by b0;
dump c;
{code}
In the following script:
{code} a = load '1.txt' as (a0, a1);
b = load '2.txt' as (b0, b1);
c = join a by a0, b by b0;
dump c;
{code}
read records read 1.txt
In WebUI, we can see we only have one MultiInputCounters: ""Input records from _0_2.txt"".","src.org.apache.pig.tools.pigstats.JobStats
test.org.apache.pig.test.TestPigRunner"
CLASS,pig-0.8.0,PIG-1912,2011-03-16T16:11:46.000-05:00,non-deterministic output when a file is loaded multiple times,"while (( i < 10 ));  
  
 {results[*]}

 
  
  
  
 
 
  
  
 @operasolutions.com
(360)
have small demonstration script
I will paste the files below this message, and I can also email the tarball to anybody who would like it; I wanted to just upload the tarball but I don't see a way to do that.
get LOADed with things not choose correct columns occur between loads perform FOREACH GENERATE on x
not see failures
-- FILES FOR REPLICATING THE PROBLEM
-- I will paste the name of the file as a comment, with the content of the file beneath it.
-- I will put the contents of the following files:
- 1) The Pig scripts (main.pig, calc_x_W.
pig, calc_x_Y.
pig, and load_raw_data.
pig)
- 2) The input data file (data.csv)
- 3) The correct output file (correct_output.
csv)
- 4) The shell script that runs the pig files and compares their output to what it should be
- 5) README
-- main.pig
RUN calc_x_W.
pig;
RUN calc_x_Y.
pig;
STORE x_W INTO 'output/W' USING PigStorage(',');
STORE x_Y INTO 'output/Y' USING PigStorage(',');  -- this is wrong sometimes
-- calc_x_W.
pig
RUN load_raw_data.
pig;
x_W = FOREACH raw_data GENERATE x, w;
-- calc_x_Y.
pig
RUN load_raw_data.
pig;
x_Y = FOREACH raw_data GENERATE x, y;
-- load_raw_data.
pig
raw_data = LOAD 'data.csv' USING PigStorage(',')
AS (
x,
y,
w
);
-- data.csv
x1,CORRECT  ANSWER,21148.59
x2,CORRECT  OUTPUT,27219.98
x3,RIGHT    ANSWER,10818.15
csv
x1,CORRECT  ANSWER
x2,CORRECT  OUTPUT
x3,RIGHT    ANSWER
-- testmany.sh
typeset -a results
i=0
while (( i < 10 )); do
rm -rf output/*
pig -x local -d WARN -e ""set debug off;run main.pig"" || break
diff correct_output.
csv output/Y/part-m-00000 && echo good
results[$i]=$?
i=$((i+1))
done;
echo ${results[*]}
-- README
This directory is intended to show a non-deterministic bug in pig.
run on same input
The scripts and dataset included in this directory demonstrate the
issue.
load file data.csv write to output directory
The root of the problem appears to be that there is an intermediate
LOAD of data.csv, after some relations have already been defined.
The following things will make the error stop:
* commenting out ""STORE x_W INTO 'output/W' USING PigStorage(',');"" in main.pig
* making a copy of data.csv called data2.csv, and a file load_daw_data2.
pig
that loads data2.csv and having having calc_x_W.
pig use that instead.
It's possible that this isn't a bug and I'm just mis-using Pig;
if that is the case I would greatly appreciate hearing about it.
I believe this issue was also discussed here:
http://mail-archives.apache.org/mod_mbox/pig-user/201102.mbox/%3CAANLkTi=2ZtkVGJevKLYSSzSH--KCcX38+Xaw2d2STNiS@mail.gmail.com%3E
I have a shell script testmany.sh which runs my script multiple times
and reports for which runs the output agrreed with the file correct_output.
csv.
IMPORTANT NOTE: We have run this code on 4 different laptops, all running
pig 0.8.0.
On one laptop (the one I'm using) the output of this script
was highly non-deterministic, generally giving both the wrong and the right
output several times each during 10 runs.
Another laptop consistently got
the wrong output up until the 28th run, when it finally gave the right output.
The other two computer never actually observed the wrong output.
We suspect
this is likely a race condition.
Thanks!
USAGE
$ cd pigbug
$ bash testmany.sh
$ # the last line of output will be a sequence of 0s and 1s, with 1
$ # meaning that there was disagreement between the output and
$ # correct_output.
csv
Field Cady
field.cady@gmail.com
fcady@operasolutions.com
(360)621-4810","src.org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.relational.LogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.relational.LOLoad"
CLASS,pig-0.8.0,PIG-1963,2011-04-04T17:18:24.000-05:00,"in nested foreach, accumutive udf taking input from order-by does not get results in order","{code}
 
 explain d;
dump d;
{code}
This happens only when secondary sort is not being used for the order-by.
For example -
{code}
a1 = load 'fruits.txt' as (f1:int,f2);
a2 = load 'fruits.txt' as (f1:int,f2);
b = cogroup a1 by f1, a2 by f1;
d = foreach b {
   sort1 = order a1 by f2;
   sort2 = order a2 by f2; -- secondary sort not getting used here, MYCONCATBAG gets results in wrong order
   generate group, MYCONCATBAG(sort1.f1), MYCONCATBAG(sort2.f2);
}
-- explain d;
dump d;
{code}","test.org.apache.pig.test.TestAccumulator
src.org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.AccumulatorOptimizer"
CLASS,pig-0.8.0,PIG-1979,2011-04-08T02:24:01.000-05:00,New logical plan failing with ERROR 2229: Couldn't find matching uid -1,"{code}
 
  
    
    
      
     PigStorage() 
  
  
  
   PigStorage() ;
{code}

   
  
    
 {code}

 import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;

public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}

  @Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().newTuple(1);
      tp2.set(0, (inputTuple.get(0).toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}

 {code}
The below is my script 
{code}
register myudf.jar;
c01 = LOAD 'input'  USING org.test.MyTableLoader('');
c02 = FILTER c01  BY result == 'OK'  AND formatted IS NOT NULL  AND formatted !
= '' ;
c03 = FOREACH c02 GENERATE url, formatted, FLATTEN(usage);
c04 = FOREACH c03 GENERATE usage::domain AS domain, url, formatted;
doc_001 = FOREACH c04 GENERATE domain,url, FLATTEN(MyExtractor(formatted)) AS category;
doc_004_1 = GROUP doc_001 BY (domain,url);
doc_005 = FOREACH doc_004_1 GENERATE group.domain as domain, group.url as url, doc_001.
category as category;
STORE doc_005 INTO 'out_final' USING PigStorage();
review1 = FOREACH c04 GENERATE domain,url, MyExtractor(formatted) AS rev;
review2 = FILTER review1 BY SIZE(rev)>0;
joinresult = JOIN review2 by (domain,url), doc_005 by (domain,url);
finalresult = FOREACH joinresult GENERATE  doc_005::category;
STORE finalresult INTO 'out_final' using PigStorage();
{code}
build plan fail in building fail while applying apply for logical optimization rule
ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2229: Couldn't find matching uid -1 for project (Name: Project Type: bytearray Uid: 106 Input: 0 Column: 5)
The problem is happening when I try to include doc_005::category in the projection for relation finalresult.
This is field is orginated from the udf org.vivek.udfs.MyExtractor (source given below).
{code}
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.
*;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
public class MyExtractor extends EvalFunc<DataBag>
{
  @Override
	public Schema outputSchema(Schema arg0) {
	  try {
			return Schema.generateNestedSchema(DataType.BAG, DataType.CHARARRAY);
		} catch (FrontendException e) {
			System.err.println(""Error while generating schema. ""
+e);
			return new Schema(new FieldSchema(null, DataType.BAG));
		}
	}
@Override
  public DataBag exec(Tuple inputTuple)
    throws IOException
  {
    try {
      Tuple tp2 = TupleFactory.getInstance().
newTuple(1);
      tp2.set(0, (inputTuple.get(0).
toString()+inputTuple.hashCode()));
      DataBag retBag = BagFactory.getInstance().
newDefaultBag();
      retBag.add(tp2);
      return retBag;
    }
    catch (Exception e) {
      throw new IOException("" Caught exception"", e);
    }
  }
}
{code}
The script goes through fine if I disable AddForEach rule by -t AddForEach","test.org.apache.pig.test.TestEvalPipeline2
src.org.apache.pig.newplan.logical.expression.DereferenceExpression"
CLASS,pig-0.8.0,PIG-1993,2011-04-12T19:47:41.000-05:00,PigStorageSchema throw NPE with ColumnPruning,"{code}
 
  
  
 GENERATE a1;
dump b;
{code}
The following script fail:
{code} a = load '1.txt' as (a0:int, a1:int, a2:int);
store a into 'temp' using org.apache.pig.piggybank.storage.PigStorageSchema();
exec a = LOAD 'temp' using org.apache.pig.piggybank.storage.PigStorageSchema();
b = FOREACH a GENERATE a1;
dump b;
{code}
java.lang.ArrayIndexOutOfBoundsException: 2
at org.apache.pig.piggybank.storage.PigStorageSchema.getNext(PigStorageSchema.java:94)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)","contrib.piggybank.java.src.test.java.org.apache.pig.piggybank.test.TestPigStorageSchema
contrib.piggybank.java.src.main.java.org.apache.pig.piggybank.storage.PigStorageSchema"
CLASS,pig-0.8.0,PIG-730,2009-03-24T14:36:45.000-05:00,"problem combining schema from a union of several LOAD expressions, with a nested bag inside the schema.","flatten(outlinks.target);
  flatten(outlinks.target);
grunt> a = load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)});
grunt> b = union (load 'foo' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)})), (load 'bar' using BinStorage as (url:chararray,outlinks:{t:(target:chararray,text:chararray)}));
grunt> c = foreach a generate flatten(outlinks.target);
grunt> d = foreach b generate flatten(outlinks.target);
give error
---> Turns out using outlinks.t.target (instead of outlinks.target) works for D but not for C.
2009-03-24 13:15:05,376 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1000: Error during parsing.
Invalid alias: target in {t: (target: chararray,text: chararray)}
Details at logfile: /echo/olston/data/pig_1237925683748.
log
grunt> quit
$ cat pig_1237925683748.log
ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1000: Error during parsing. Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.PigServer.parseQuery(PigServer.java:317)
at org.apache.pig.PigServer.registerQuery(PigServer.java:276)
at org.apache.pig.tools.grunt.GruntParser.processPig(GruntParser.java:529)
at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:280)
at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:99)
at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:69)
at org.apache.pig.Main.main(Main.java:321)
Caused by: org.apache.pig.impl.logicalLayer.parser.ParseException: Invalid alias: target in {t: (target: chararray,text: chararray)}
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AliasFieldOrSpec(QueryParser.java:6042)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ColOrSpec(QueryParser.java:5898)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BracketedSimpleProj(QueryParser.java:5423)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseEvalSpec(QueryParser.java:4100)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.UnaryExpr(QueryParser.java:3967)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.CastExpr(QueryParser.java:3920)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.MultiplicativeExpr(QueryParser.java:3829)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.AdditiveExpr(QueryParser.java:3755)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.InfixExpr(QueryParser.java:3721)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItem(QueryParser.java:3617)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.FlattenedGenerateItemList(QueryParser.java:3557)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.GenerateStatement(QueryParser.java:3514)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.NestedBlock(QueryParser.java:2985)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.ForEachClause(QueryParser.java:2395)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.BaseExpr(QueryParser.java:1028)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Expr(QueryParser.java:804)
at org.apache.pig.impl.logicalLayer.parser.QueryParser.Parse(QueryParser.java:595)
at org.apache.pig.impl.logicalLayer.LogicalPlanBuilder.parse(LogicalPlanBuilder.java:60)
at org.apache.pig.PigServer.parseQuery(PigServer.java:310)
... 6 more","src.org.apache.pig.impl.logicalLayer.schema.Schema
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LogicalSchema
test.org.apache.pig.test.TestSchema"
CLASS,pig-0.8.0,PIG-767,2009-04-15T23:43:29.000-05:00,Schema reported from DESCRIBE and actual schema of inner bags are different.,"BinStorage()  
 DESCRIBE urlContents;
DUMP urlContents;

     BY url;
DESCRIBE urlContentsG;

     urlContents.pg;

DESCRIBE urlContentsF;
DUMP urlContentsF;


 
   {url: chararray,pg: chararray}
   {group: chararray,urlContents: {url: chararray,pg: chararray}}
   {group: chararray,pg: {pg: chararray}}

      
 
    
   {group: chararray,urlContents: {t1:(url: chararray,pg: chararray)}}

  {chararray}   {(chararray)}
The following script:
urlContents = LOAD 'inputdir' USING BinStorage() AS (url:bytearray, pg:bytearray);
-- describe and dump are in-sync
DESCRIBE urlContents;
DUMP urlContents;
urlContentsG = GROUP urlContents BY url;
DESCRIBE urlContentsG;
urlContentsF = FOREACH urlContentsG GENERATE group,urlContents.pg;
DESCRIBE urlContentsF;
DUMP urlContentsF;
print for DESCRIBE commands
urlContents: {url: chararray,pg: chararray}
urlContentsG: {group: chararray,urlContents: {url: chararray,pg: chararray}}
urlContentsF: {group: chararray,pg: {pg: chararray}}
be against section schemas be for Complex data types
This may sound like a technicality, but it isn't.
For instance, a UDF that assumes an inner bag of {chararray} will not work with {(chararray)}.","test.org.apache.pig.test.TestNewPlanLogToPhyTranslationVisitor
src.org.apache.pig.newplan.logical.expression.DereferenceExpression
src.org.apache.pig.newplan.logical.relational.LOInnerLoad
src.org.apache.pig.newplan.logical.rules.DuplicateForEachColumnRewrite
test.org.apache.pig.test.TestLogicalPlanMigrationVisitor
src.org.apache.pig.newplan.logical.relational.LOCogroup
test.org.apache.pig.test.TestSchema
src.org.apache.pig.newplan.logical.relational.LOGenerate"
METHOD,math,MATH-1021,2013-08-10T00:00:22.000-05:00,HypergeometricDistribution.sample suffers from integer overflow,"HypergeometricDistribution.sample()  
 {code}
 import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
 {code}

  HypergeometricDistribution.getNumericalMean()  
 {code}
 return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
 
 {code}
 return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
Hi, I have an application which broke when ported from commons math 2.2 to 3.2.
It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.
{code}
import org.apache.commons.math3.distribution.HypergeometricDistribution;
public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
{code}
In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing
{code}
return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
it could do:
{code}
return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
This seemed to fix it, based on a quick test.",org.apache.commons.math3.distribution.HypergeometricDistribution:getNumericalMean()
METHOD,math,MATH-280,2009-07-06T21:26:57.000-05:00,bug in inverseCumulativeProbability() for Normal Distribution,"public class NormalDistributionImpl extends AbstractContinuousDistribution 


  
 public abstract class AbstractContinuousDistribution


 
 DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);

 
 normal.inverseCumulativeProbability(0.977249868051820);
* @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $
*/
public class NormalDistributionImpl extends AbstractContinuousDistribution
* @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $
*/
public abstract class AbstractContinuousDistribution
This code:
DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);
give exception
normal.inverseCumulativeProbability(0.977249868051820); works fine
These also give errors:
0 9986501019683698 (should return 3.0000...)
0 9999683287581673 (should return 4.0000...)
org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0
at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)
at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)","org.apache.commons.math.analysis.solvers.UnivariateRealSolverUtils:bracket(UnivariateRealFunction, double, double, double, int)"
METHOD,math,MATH-305,2009-10-22T06:35:08.000-05:00,NPE in  KMeansPlusPlusClusterer unittest,"package org.fao.fisheries.chronicles.calcuation.cluster;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.Arrays;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;

public class ClusterAnalysisTest {


	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());

	}

}
run unittest face NPE
java.lang.NullPointerException
at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)
This is the unittest:
package org.fao.fisheries.chronicles.calcuation.cluster;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;
import java.util.Arrays;
import java.util.List;
import java.util.Random;
import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;
public class ClusterAnalysisTest {
@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());
}
}","org.apache.commons.math.util.MathUtils:distance(int[], int[])"
METHOD,math,MATH-318,2009-11-06T15:09:36.000-06:00,wrong result in eigen decomposition,"{code}
     public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }
 {code}
The following case computed by Fortran Lapack fails with version 2.0
{code}
    public void testMathpbx02() {
double[] mainTridiagonal = {
7484.860960227216, 18405.28129035345, 13855.225609560746,
10016.708722343366, 559.8117399576674, 6750.190788301587,
71.21428769782159
};
double[] secondaryTridiagonal = {
-4175.088570476366,1975.7955858241994,5193.178422374075,
1995.286659169179,75.34535882933804,-234.0808002076056
};
// the reference values have been computed using routine DSTEMR
// from the fortran library LAPACK version 3.2.1
double[] refEigenValues = {
20654.744890306974412,16828.208208485466457,
6893.155912634994820,6757.083016675340332,
5887.799885688558788,64.309089923240379,
57.992628792736340
};
RealVector[] refEigenVectors = {
new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
};
// the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);
double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].
dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].
add(decomposition.getEigenvector(i)).
getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].
subtract(decomposition.getEigenvector(i)).
getNorm(), 1.0e-5);
            }
        }
}
{code}","org.apache.commons.math.linear.EigenDecompositionImpl:flipIfWarranted(int, int)"
METHOD,math,MATH-358,2010-03-24T17:25:37.000-05:00,ODE integrator goes past specified end of integration range,"{code}
   public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

 {code}
End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
{code}
public void testMissedEvent() throws IntegratorException, DerivativeException {
final double t0 = 1878250320.0000029;
final double t =  1878250379.9999986;
FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
public int getDimension() {
return 1;
}
public void computeDerivatives(double t, double[] y, double[] yDot)
throws DerivativeException {
yDot[0] = y[0] * 1.0e-6;
}
};
DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
1 0e-10, 1.0e-10);
double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }
{code}","org.apache.commons.math.ode.nonstiff.EmbeddedRungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])
org.apache.commons.math.ode.nonstiff.RungeKuttaIntegrator:integrate(FirstOrderDifferentialEquations, double, double[], double, double[])"
METHOD,math,MATH-369,2010-05-03T15:48:27.000-05:00,"BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException","new BisectionSolver()  solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);
Method
BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)
invokes
BisectionSolver.solve(double min, double max)
which throws NullPointerException, as member variable
UnivariateRealSolverImpl.f
is null.
Instead the method:
BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)
should be called.
Steps to reproduce:
invoke:
new BisectionSolver().
solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);","org.apache.commons.math.analysis.solvers.BisectionSolver:solve(UnivariateRealFunction, double, double, double)"
METHOD,math,MATH-482,2011-01-17T19:52:10.000-06:00,"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f","FastMath.max(50.0f, -50.0f)  
 testMinMaxFloat()
FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.
This is because the wrong variable is returned.
have bug not detect bug","org.apache.commons.math.util.FastMath:max(float, float)"
METHOD,math,MATH-567,2011-05-05T17:49:00.000-05:00,"class Dfp toDouble method return -inf whan Dfp value is 0 ""zero""","toDouble()  
 toDouble()  
 import org.apache.commons.math.dfp.DfpField;


 
 
 
 public static void main(String[] args)  
 DfpField field = new DfpField(100);
		    getZero()   field.getZero()  toDouble() 
   newDfp(0.0)  
 field.newDfp(0.0)  toDouble() 
 toDouble()
I found a bug in the toDouble() method of the Dfp class.
return negative infini
This is because the double value returned has an exposant equal to 0xFFF and a significand is equal to 0.
In the IEEE754 this is a -inf.
To be equal to zero, the exposant and the significand must be equal to zero.
A simple test case is :
---------------------------------------------- import org.apache.commons.math.dfp.DfpField;
public class test {
/**
* @param args
*/ public static void main(String[] args) {
DfpField field = new DfpField(100);
System.out.println(""toDouble value of getZero() =""+field.getZero().
toDouble()+
"" toDouble value of newDfp(0.0) =""+ field.newDfp(0.0).
toDouble());
}
}
May be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?","org.apache.commons.math.dfp.Dfp:Dfp(DfpField, double)
org.apache.commons.math.dfp.Dfp:toDouble()"
METHOD,math,MATH-60,2006-05-14T04:20:21.000-05:00,"[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result","Fraction parse(String source, 
ParsePostion pos)  class ProperFractionFormat  
 ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);

//Test 1 : fail 
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
 public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

 
 parse(String, ParsePosition)
Hello,
I find illogical returned result from function ""Fraction parse(String source,
ParsePostion pos)"" (in class ProperFractionFormat of the Fraction Package) of the Commons Math library.
Please see the following code segment for more details:
""
ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);
//Test 1 : fail public void testParseNegative(){
String source = ""-1 -2 / 3"";
ParsePosition pos = new ParsePosition(0);
Fraction actual = properFormat.parse(source, pos);
assertNull(actual);
}
// Test2: success public void testParseNegative(){
String source = ""-1 -2 / 3"";
ParsePosition pos = new ParsePosition(0);
Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3 assertEquals(1, source.getNumerator());
assertEquals(3, source.getDenominator());
}
""
Note: Similarly, when I passed in the following inputs:
input 2: (source = 1 2 / -3, pos = 0)
input 3: ( source =  -1 -2 / 3, pos = 0)
return Fraction for inputs
parse numberator/ denominator properly incase input string provide invalid numerator/denominator
Thank you!","org.apache.commons.math.fraction.ProperFractionFormat:parse(String, ParsePosition)"
METHOD,math,MATH-631,2011-07-23T23:48:27.000-05:00,"""RegulaFalsiSolver"" failure","{code}
 @Test
public void testBug() {
    final UnivariateRealFunction f = new UnivariateRealFunction() {
            @Override
            public double value(double x) {
                return Math.exp(x) - Math.pow(Math.PI, 3.0);
            }
        };

    UnivariateRealSolver solver = new RegulaFalsiSolver();
    double root = solver.solve(100, f, 1, 10);
}
 {code}
 
 {noformat}
 
 {noformat}
The following unit test:
{code}
@Test
public void testBug() {
    final UnivariateRealFunction f = new UnivariateRealFunction() {
            @Override
            public double value(double x) {
                return Math.exp(x) - Math.pow(Math.PI, 3.0);
            }
        };
UnivariateRealSolver solver = new RegulaFalsiSolver();
    double root = solver.solve(100, f, 1, 10);
}
{code}
fail with { noformat }
use pegasussolver find answer after evaluations",org.apache.commons.math.analysis.solvers.BaseSecantSolver:doSolve()
METHOD,math,MATH-645,2011-08-13T16:18:48.000-05:00,MathRuntimeException with simple ebeMultiply on OpenMapRealVector,"{code:java}
 import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}
 {code}
 
 {noformat}
  
 {noformat}
The following piece of code
{code:java} import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;
public class DemoBugOpenMapRealVector { public static void main(String[] args) { final RealVector u = new OpenMapRealVector(3, 1E-6);
u.setEntry(0, 1.)
;
u.setEntry(1, 0.)
;
u.setEntry(2, 2.)
;
final RealVector v = new OpenMapRealVector(3, 1E-6);
v.setEntry(0, 0.)
;
v.setEntry(1, 3.)
;
v.setEntry(2, 0.)
;
System.out.println(u);
System.out.println(v);
System.out.println(u.ebeMultiply(v));
}
}
raise exception
{noformat} org.apache.commons.math.linear.OpenMapRealVector@7170a9b6
Exception in thread ""main"" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating
at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)
at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)
at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)
at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)
at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)
{noformat}","org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(double[])
org.apache.commons.math.linear.OpenMapRealVector:ebeMultiply(RealVector)
org.apache.commons.math.linear.OpenMapRealVector:ebeDivide(RealVector)"
METHOD,math,MATH-691,2011-10-16T17:18:34.000-05:00,Statistics.setVarianceImpl makes getStandardDeviation produce NaN,"new Variance(true/false)    
 {code:java}
 int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use ""population variance""
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);
{code}

 
 {code:java}
   double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
{code}
invoke SummaryStatistics.setVarianceImpl(new Variance(true/false) makes produce nan for invoking
The code to reproduce it:
{code:java}
int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use ""population variance""
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);
{code}
A workaround suggested by Mikkel is:
{code:java}
  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
{code}",org.apache.commons.math.stat.descriptive.SummaryStatistics:addValue(double)
METHOD,math,MATH-713,2011-11-25T16:35:02.000-06:00,Negative value with restrictNonNegative,"SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);

 
 x = 1; y = -1;
Problem: commons-math-2.2 SimplexSolver.
assign negative value with coefficient assign variable with coefficient
Function
1 * x + 1 * y + 0
Constraints:
1 * x + 0 * y = 1
Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.",org.apache.commons.math.optimization.linear.SimplexTableau:getSolution()
METHOD,math,MATH-836,2012-07-31T17:04:25.000-05:00,"Fraction(double, int) constructor_ strange behaviour","public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }
take double value take int maximal denominator approximate fraction
When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:
return positive fraction
Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value
reduce Fraction
Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.
I have, as of yet, not found a solution.
The constructor_ looks like this:
public Fraction(double value, int maxDenominator)
throws FractionConversionException
{ this(value, 0, maxDenominator, 100);
}
Increasing the 100 value (max iterations) does not fix the problem for all cases.
Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest.
The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly.
A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.
This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html).
Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.
* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that ""since fractions are always in lowest terms, numerators and can be compared directly for equality"", so it seems like this is the intention.","org.apache.commons.math3.fraction.Fraction:Fraction(double, double, int, int)"
METHOD,math,MATH-929,2013-01-15T11:45:28.000-06:00,MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd,"{code}
 Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);
{code}
To reproduce:
{code}
Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).
density(new double[]{0}), 1e-15);
{code}",org.apache.commons.math3.distribution.MultivariateNormalDistribution:density(double[])
METHOD,math,MATH-942,2013-03-09T15:05:04.000-06:00,DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type,"Array.newInstance(singletons.get(0).getClass(), sampleSize)   
 singleons.get(0) 
 {{DiscreteDistribution.sample()}}  
 {code}
 List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).sample(1);
{code}
Creating an array with {{Array.newInstance(singletons.get(0).
getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky.
be of type t1
return object be object
To reproduce:
{code}
List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).
sample(1);
{code}
Attaching a patch.",org.apache.commons.math3.distribution.DiscreteDistribution:sample(int)
METHOD,math,MATH-949,2013-03-15T18:11:56.000-05:00,LevenbergMarquardtOptimizer reports 0 iterations,"LevenbergMarquardtOptimizer.getIterations()     BaseOptimizer.incrementEvaluationsCount()

 
 {noformat}
     @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }

 {noformat}
not report correct number of iterations
A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()
I've put a test case below.
{noformat}
    @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();
// action
otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
new Weight(new double[] { 1 }), new InitialGuess(
new double[] { 3 }), new ModelFunction(
new MultivariateVectorFunction() {
@Override
public double[] value(double[] point)
throws IllegalArgumentException {
return new double[] { FastMath.pow(point[0], 4) };
}
}), new ModelFunctionJacobian(
new MultivariateMatrixFunction() {
@Override
public double[][] value(double[] point)
throws IllegalArgumentException {
return new double[][] { { 0.25 * FastMath.pow(
point[0], 3) } };
}
}));
// verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }
{noformat}","org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.GaussNewtonOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.SimplexOptimizer:doOptimize()
org.apache.commons.math3.optim.BaseOptimizer:BaseOptimizer(ConvergenceChecker<PAIR>)
org.apache.commons.math3.optim.nonlinear.scalar.noderiv.PowellOptimizer:doOptimize()
org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer:doOptimize()"
FILE,WFCORE,WFCORE-267,2014-11-19T19:47:31.000-06:00,CLI prints output twice if using cli client jar,"INFO: {




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}




{




    ""outcome"" => ""success"",




    ""result"" => [




        ""core-service"",




        ""deployment"",




        ""deployment-overlay"",




        ""extension"",




        ""interface"",




        ""path"",




        ""socket-binding-group"",




        ""subsystem"",




        ""system-property""




    ]




}
If you are using the CLI client jar, all output is printed twice.
print log messages to standard
look something
[standalone@localhost:9999 /] :read-children-types
Nov 19, 2014 8:57:19 AM org.jboss.as.cli.impl.CommandContextImpl printLine
INFO: {
""outcome"" => ""success"",
""result"" => [
""core-service"",
""deployment"",
""deployment-overlay"",
""extension"",
""interface"",
""path"",
""socket-binding-group"",
""subsystem"",
""system-property""
]
}
{
""outcome"" => ""success"",
""result"" => [
""core-service"",
""deployment"",
""deployment-overlay"",
""extension"",
""interface"",
""path"",
""socket-binding-group"",
""subsystem"",
""system-property""
]
}",org.jboss.as.cli.CommandLineMain
FILE,WFCORE,WFCORE-495,2015-01-12T08:48:29.000-06:00,"WFLY won't startup due to ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""","file(standalone.xml)  file(standalone.xml)
not startup to wflyctl0212
If you firstly deploy a .
war application by CLI command, then deploy 2nd time same .
war application by copying it to /depolyment directory.
After server restart, it shows:
15:26:26,913 INFO  [org.jboss.as] (MSC service thread 1-8) WFLYSRV0049: WildFly Full 9.0.0.Alpha2-SNAPSHOT (WildFly Core 1.0.0.
Alpha15) starting
15:26:27,133 ERROR [org.jboss.as.controller.management-operation] (Controller Boot Thread) WFLYCTL0013: Operation (""add"") failed - address: ([(""deployment"" => ""xxx.war"")]) - failure description: ""WFLYCTL0212: Duplicate resource [(\""deployment\"" => \""xxx.war\"")]""
15:26:27,136 FATAL [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0056: Server boot has failed in an unrecoverable manner; exiting.
See previous messages for details.
15:26:27,138 INFO  [org.jboss.as.server] (Thread-2) WFLYSRV0220: Server shutdown has been requested.
15:26:27,153 INFO  [org.jboss.as] (MSC service thread 1-1) WFLYSRV0050: WildFly Full 9.0.0.Alpha2-SNAPSHOT (WildFly Core 1.0.0.
Alpha15) stopped in 4ms
steps to reproduce:
1. start wildfly and deploy a .
war application by CLI.
2. copy same .
war application to /deployment directory
3. restart wildfly to see the error message.
do full-replace-deployment operation not remove full-replace-deployment operation from configuration file(standalone.xml)
have xxx.war in standalone/data/content configuration file(standalone.xml)
cause duplicate resource error",org.jboss.as.server.deployment.DeploymentFullReplaceHandler
FILE,WFCORE,WFCORE-604,2015-03-18T09:19:35.000-05:00,"After failed to deploy, remain deployment information in JBOSS_HOME/{standalone|domaine}/data/content directory","{standalone|domaine} 
 {standalone|domaine}  
 {standalone|domaine} 
 {standalone|domaine} 
 {standalone|domaine}
Description of problem:
===
- Please see following reproduce steps.
How reproducible:
===
Steps to Reproduce:
1. Fail to deploy application via jboss-cli
2. Find deployment info in JBOSS_HOME/{standalone|domaine}/data/content, but there are no standalone.xml in <deployments> tag.
3. Fix deployment and success to deploy.
4. Find ""new"" deployment info in JBOSS_HOME/{standalone|domaine}/data/content, and the old deployment info will be still there.
- I know that as we changed application in step-3, its hash value was changed.
And then, old info is remained in JBOSS_HOME/{standalone|domaine}/data/content.
But I think it always happens and should be fixed.
Actual results:
Expected results:","org.jboss.as.host.controller.mgmt.MasterDomainControllerOperationHandlerImpl
org.jboss.as.server.controller.resources.ServerRootResourceDefinition
org.jboss.as.host.controller.ManagedServerOperationsFactory
org.jboss.as.host.controller.DomainModelControllerService
org.jboss.as.host.controller.RemoteDomainConnectionService
org.jboss.as.test.shared.ModelParserUtils
org.jboss.as.server.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentAddHandler
org.jboss.as.server.logging.ServerLogger
org.jboss.as.server.deployment.DeploymentRemoveHandler
org.jboss.as.domain.controller.resources.ServerGroupResourceDefinition
org.jboss.as.repository.LocalDeploymentFileRepository
org.jboss.as.domain.controller.operations.ApplyRemoteMasterDomainModelHandler
org.jboss.as.server.deployment.DeploymentReplaceHandler
org.jboss.as.domain.controller.resources.DomainRootDefinition
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentDefinition
org.jboss.as.repository.logging.DeploymentRepositoryLogger
org.jboss.as.domain.controller.operations.deployment.DeploymentFullReplaceHandler
org.jboss.as.core.model.test.LegacyKernelServicesImpl
org.jboss.as.subsystem.test.TestModelControllerService
org.jboss.as.host.controller.HostControllerService
org.jboss.as.domain.controller.resources.DomainDeploymentResourceDefinition
org.jboss.as.core.model.test.TestModelControllerService
org.jboss.as.server.mgmt.domain.RemoteFileRepositoryService
org.jboss.as.server.deploymentoverlay.DeploymentOverlayContentAdd
org.jboss.as.server.ApplicationServerService
org.jboss.as.repository.LocalFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeAddHandler
org.jboss.as.server.test.InterfaceManagementUnitTestCase
org.jboss.as.host.controller.model.host.HostResourceDefinition
org.jboss.as.server.deployment.DeploymentAddHandlerTestCase
org.jboss.as.repository.DeploymentFileRepository
org.jboss.as.domain.controller.operations.deployment.ServerGroupDeploymentReplaceHandler
org.jboss.as.repository.ContentRepository
org.jboss.as.domain.controller.operations.deployment.DeploymentAddHandler
org.jboss.as.domain.controller.operations.deployment.DeploymentRemoveHandler
org.jboss.as.management.client.content.ManagedDMRContentTypeResource
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler
org.jboss.as.server.deployment.DeploymentFullReplaceHandler"
FILE,WFCORE,WFCORE-626,2015-04-06T15:53:19.000-05:00,Global list-get operation can inadvertently create list elements,"clear(name=attribute)
  get(name=attribute, index=0)
  add(name=attribute, value=test)
  get(name=attribute, index=0)
Consider the following sequence of operations:
:list-clear(name=attribute)
:list-get(name=attribute, index=0)
:list-add(name=attribute, value=test)
:list-get(name=attribute, index=0)
return <undefined> expected
return <undefined>
create missing element at index operate on index","org.jboss.as.controller.operations.global.ListOperations
org.jboss.as.controller.operations.global.MapOperations"
FILE,WFCORE,WFCORE-442,2014-12-02T19:15:13.000-06:00,AbstractMultiTargetHandler-based handlers do not propagate failures to the top level failure-description,"{read-only=1} 
 {




                ""name"" => ""jboss.server.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""user.home"",




                ""path"" => ""/Users/hbraun"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.base.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
  
 {




                ""name"" => ""user.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.data.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/data"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.home.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.server.log.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/log"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            } 
 {




                ""name"" => ""jboss.controller.temp.dir"",




                ""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.Alpha14-SNAPSHOT/standalone/tmp"",




                ""read-only"" => true,




                ""relative-to"" => undefined




            }
An example is worth a thousand words:
[standalone@localhost:9990 /] /path=*:query(where={read-only=1})
{
""outcome"" => ""failed"",
""result"" => [
{
""address"" => [(""path"" => ""jboss.server.temp.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.server.temp.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone/tmp"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""user.home"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""user.home"",
""path"" => ""/Users/hbraun"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.server.base.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.server.base.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""java.home"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""java.home"",
""path"" => ""/Library/Java/JavaVirtualMachines/jdk1.7.0_67.
jdk/Contents/Home/jre"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""user.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""user.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.server.data.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.server.data.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone/data"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.home.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.home.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.server.log.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.server.log.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone/log"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.server.config.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.server.config.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone/configuration"",
""read-only"" => true,
""relative-to"" => undefined
},
""rolled-back"" => true
},
{
""address"" => [(""path"" => ""jboss.controller.temp.dir"")],
""outcome"" => ""failed"",
""result"" => {
""name"" => ""jboss.controller.temp.dir"",
""path"" => ""/Users/hbraun/dev/prj/wildfly-core/core-build/target/wildfly-core-1.0.0.
Alpha14-SNAPSHOT/standalone/tmp"",
""read-only"" => true,
""relative-to"" => undefined
},
""failure-description"" => ""Illegal argument for attribute 'read-only'.
Expected type BOOLEAN"",
""rolled-back"" => true
}
],
""rolled-back"" => true
}
have failure description
ReadResourceDescriptionHandler handles similar things but has logic for creating an overall failure-description.",org.jboss.as.controller.operations.global.GlobalOperationHandlers
FILE,WFCORE,WFCORE-716,2015-05-27T10:21:36.000-05:00,Once server in reload-required state capabilities no longer checked at stage Model.,"attribute(name=security-realm)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }




 
 attribute(name=security-domain, value=MgMtDom)




 {




    ""outcome"" => ""success"",




    ""response-headers"" => {




        ""operation-requires-reload"" => true,




        ""process-state"" => ""reload-required""




    }
check e.g. check reload-required capabilities check requirements check state be in state
[standalone@localhost:9990 /] .
/core-service=management/management-interface=http-interface:undefine-attribute(name=security-realm)
{
""outcome"" => ""success"",
""response-headers"" => {
""operation-requires-reload"" => true,
""process-state"" => ""reload-required""
}
}
The following command is referencing a non-existent capability: -
[standalone@localhost:9990 /] .
/core-service=management/management-interface=http-interface:write-attribute(name=security-domain, value=MgMtDom)
{
""outcome"" => ""success"",
""response-headers"" => {
""operation-requires-reload"" => true,
""process-state"" => ""reload-required""
}
}
When I execute :reload it will fail: -
11:21:18,567 ERROR [org.jboss.as.controller.management-operation] (Controller Boot Thread) WFLYCTL0013: Operation (""add"") failed - address: ([
(""core-service"" => ""management""),
(""management-interface"" => ""http-interface"")
]): java.lang.IllegalStateException: WFLYCTL0364: Capability 'org.wildfly.security.security-domain.
MgMtDom' is unknown.
at org.jboss.as.controller.ModelControllerImpl$CapabilityRegistryImpl.getCapabilityRegistration(ModelControllerImpl.java:1388)",org.jboss.as.controller.CapabilityReferenceRecorder
FILE,WFCORE,WFCORE-815,2015-07-13T07:57:45.000-05:00,One profile can have more ancestors with same submodules,"add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)

 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'. Overriding subsystems is not supported""} 
 add(name=includes, value=mail-01)
  add(name=includes, value=mail-02)
Description of problem:
have more ancestors with same submodules
lead to wflyctl0212
Hierarchical composition of profiles was added to AS with EAP7-281 and WFCORE-382
How reproducible:
Always
Steps to Reproduce:
get fresh EAP
.
/domain.sh
.
/jboss-cli.sh -c
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)
Actual results:
Expected results:
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0401: Profile 'mail-01' defines subsystem 'mail' which is also defined in its ancestor profile 'mail-02'.
Overriding subsystems is not supported""},
""rolled-back"" => true
}
Workaround:
Add any subsystem to default-new profile:
/profile=mail-01:add
/profile=mail-02:add
/profile=mail-01/subsystem=mail:add
/profile=mail-02/subsystem=mail:add
/profile=default-new:add
/profile=default-new/subsystem=jdr:add
/profile=default-new:list-add(name=includes, value=mail-01)
/profile=default-new:list-add(name=includes, value=mail-02)","org.jboss.as.domain.controller.operations.ProfileIncludesHandlerTestCase
org.jboss.as.domain.controller.operations.SocketBindingGroupIncludesHandlerTestCase
org.jboss.as.host.controller.logging.HostControllerLogger"
FILE,WFCORE,WFCORE-955,2015-08-27T14:34:07.000-05:00,Server is not responding after attempt to set parent of profile to non-existent profile,"add()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""java.lang.NullPointerException:null""




}




 
 add()
Description of problem:
Server is not responding after attempt to set parent of profile to non-existent profile.
Server is not responding also after attempt to set parent of socket-binding-group to non-existent socket-binding-group.
This works correctly on wildfly-core (2.0.0.
Beta4).
But this occurs on wildfly (master branch) and on EAP 7.0.0.
DR9. It may be some integration issue.
Priority of this jira tends to be critical.
How reproducible:
Always
Steps to Reproduce (profile):
Get fresh EAP
.
/domain.sh
.
/jboss-cli.sh
/profile=new:add()
/profile=new:write-attribute(name=includes,value=[nonsence])
/profile=new:remove
[domain@localhost:9990 /] /profile=new:write-attribute(name=includes,value=[nonsence])
{
""outcome"" => ""failed"",
""failure-description"" => ""java.lang.NullPointerException:null""
}
[domain@localhost:9990 /] /profile=new:remove
server needs to be restarted
Expected results:
[domain@localhost:9990 /] /profile=new:write-attribute(name=includes,value=[nonsence])
{
""outcome"" => ""failed"",
""failure-description"" => {""domain-failure-description"" => ""WFLYCTL0369: Required capabilities are not available:
org.wildfly.domain.profile.nonsence in context 'profiles'""},
""rolled-back"" => true
}
[domain@localhost:9990 /] /profile=new:remove
{
""outcome"" => ""success"",
""result"" => undefined,
""server-groups"" => undefined
}
Steps to reproduce (socket-binding-group):
Get fresh EAP
.
/domain.sh
.
/jboss-cli.sh
/profile=new:add()
/socket-binding-group=testt:add(default-interface=public,includes=[nonsence])
/profile=new:remove","org.jboss.as.controller.OperationContextImpl
org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.controller.SocketCapabilityResolutionUnitTestCase
org.jboss.as.controller.capability.registry.IncludingResourceCapabilityScope
org.jboss.as.controller.AbstractCapabilityResolutionTestCase"
FILE,WFCORE,WFCORE-876,2015-08-12T07:52:43.000-05:00,Reload or Shutdown inside IF statement is performed before the if/else block batch is executed,"resource()
Executing a reload or shutdown inside an if/else block results in the reload or restart occurring before the other commands in the batch are executed.
if (outcome == success) of /subsystem=logging/logger=org.jboss.as.cli:read-resource()
/subsystem=logging:write-attribute(name=use-deployment-logging-config
shutdown --restart=true
end-if
leave server in reload-required state","org.jboss.as.cli.handlers.trycatch.TryCatchFinallyControlFlow
org.jboss.as.test.integration.management.cli.ifelse.NonExistingPathComparisonTestCase
org.jboss.as.test.integration.management.cli.TryCatchFinallyTestCase
org.jboss.as.cli.handlers.ifelse.IfElseControlFlow
org.jboss.as.test.integration.management.cli.ifelse.BasicIfElseTestCase"
FILE,WFCORE,WFCORE-1007,2015-09-24T06:45:11.000-05:00,Warnings about missing notification descriptions when an operation removes an extension,"migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}




 
 migrate()




 {




    ""outcome"" => ""success"",




    ""result"" => {""migration-warnings"" => []}
When I use migration operation the console log is filled with warning messages of type
WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
This is the same either for jacorb or web or messaging subsystem.
If I do the sequence of operation
[standalone@localhost:9999 /] /subsystem=jacorb:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=messaging:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
[standalone@localhost:9999 /] /subsystem=we
web  webservices  weld
[standalone@localhost:9999 /] /subsystem=web
web  webservices
[standalone@localhost:9999 /] /subsystem=web:migrate()
{
""outcome"" => ""success"",
""result"" => {""migration-warnings"" => []}
}
2015-09-24 08:41:09,729 WARN  [org.jboss.as.controller] (management-handler-thread - 1) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""jacorb"")]
2015-09-24 08:43:13,229 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""DLQ"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""jms-queue"" => ""ExpiryQueue"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""pooled-connection-factory"" => ""hornetq-ra"")
]
2015-09-24 08:43:13,230 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""RemoteConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""connection-factory"" => ""InVmConnectionFactory"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""address-setting"" => ""#"")
]
2015-09-24 08:43:13,231 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#""),
(""role"" => ""guest"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""security-setting"" => ""#"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-acceptor"" => ""in-vm"")
]
2015-09-24 08:43:13,232 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""direct-deliver"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty-throughput"")
]
2015-09-24 08:43:13,233 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-acceptor"" => ""netty"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""in-vm-connector"" => ""in-vm"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput""),
(""param"" => ""batch-delay"")
]
2015-09-24 08:43:13,234 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty-throughput"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default""),
(""remote-connector"" => ""netty"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""messaging""),
(""hornetq-server"" => ""default"")
]
2015-09-24 08:43:13,235 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""messaging"")]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""jsp-configuration"")
]
2015-09-24 08:43:20,957 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""static-resources"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""configuration"" => ""container"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""virtual-server"" => ""default-host"")
]
2015-09-24 08:43:20,958 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [
(""subsystem"" => ""web""),
(""connector"" => ""http"")
]
2015-09-24 08:43:20,959 WARN  [org.jboss.as.controller] (management-handler-thread - 7) WFLYCTL0357: Notification of type resource-removed is not described for the resource at the address [(""subsystem"" => ""web"")]","org.jboss.as.controller.AbstractOperationContext
org.jboss.as.controller.logging.ControllerLogger"
FILE,WFCORE,WFCORE-1027,2015-10-01T18:16:10.000-05:00,Inconsistent read-resource results with host scoped roles,"{roles=master-monitor}




 
 {




                ""directory-grouping"" => ""by-server"",




                ""domain-controller"" => {""local"" => {} 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                    ""management"" => undefined,




                    ""public"" => undefined,




                    ""unsecure"" => undefined




                } 
 {""default"" => undefined} 
 {""jmx"" => undefined} 
 {roles=slave-maintainer}




 
 {roles=slave-maintainer}




 
  
 {""org.jboss.as.jmx"" => undefined} 
 {




                ""management"" => undefined,




                ""public"" => undefined,




                ""unsecure"" => undefined




            } 
 {""default"" => undefined} 
 {""jmx"" => undefined}
Setting up host scoped roles as follows https://gist.github.com/heiko-braun/0dc810ed04db8739defd there are inconsistent results in the filtering.
use role select master select role show filtered resources appear in results
[domain@localhost:9990 /] /host=*:read-resource{roles=master-monitor}
{
""outcome"" => ""success"",
""result"" => [
{
""address"" => [(""host"" => ""master"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""local"" => {}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => true,
""name"" => ""master"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => ""WildFly Core"",
""product-version"" => ""2.0.0.CR6-SNAPSHOT"",
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined,
""server-three"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
},
{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}
]
}
use role select slave select role get proper access-control header
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""localhost"")],
""outcome"" => ""success"",
""result"" => undefined
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
The same output on master with WFCORE-994 applied:
[domain@localhost:9990 /] /host=*:read-resource{roles=slave-maintainer}
{
""outcome"" => ""success"",
""result"" => [{
""address"" => [(""host"" => ""slave"")],
""outcome"" => ""success"",
""result"" => {
""directory-grouping"" => ""by-server"",
""domain-controller"" => {""remote"" => {
""protocol"" => undefined,
""port"" => undefined,
""host"" => undefined,
""username"" => undefined,
""ignore-unused-configuration"" => undefined,
""admin-only-policy"" => undefined,
""security-realm"" => ""ManagementRealm""
}},
""management-major-version"" => 4,
""management-micro-version"" => 0,
""management-minor-version"" => 0,
""master"" => false,
""name"" => ""slave"",
""namespaces"" => [],
""organization"" => undefined,
""product-name"" => undefined,
""product-version"" => undefined,
""release-codename"" => ""Kenny"",
""release-version"" => ""2.0.0.CR6-SNAPSHOT"",
""schema-locations"" => [],
""core-service"" => {
""host-environment"" => undefined,
""platform-mbean"" => undefined,
""management"" => undefined,
""discovery-options"" => undefined,
""ignored-resources"" => undefined,
""patching"" => undefined,
""module-loading"" => undefined
},
""extension"" => {""org.jboss.as.jmx"" => undefined},
""interface"" => {
""management"" => undefined,
""public"" => undefined,
""unsecure"" => undefined
},
""jvm"" => {""default"" => undefined},
""path"" => undefined,
""server"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""server-config"" => {
""server-one"" => undefined,
""server-two"" => undefined
},
""socket-binding-group"" => undefined,
""subsystem"" => {""jmx"" => undefined},
""system-property"" => undefined
}
}],
""response-headers"" => {""access-control"" => [{
""absolute-address"" => [],
""relative-address"" => [],
""filtered-children-types"" => [""host""]
}]}
}","org.jboss.as.test.integration.domain.rbac.RBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.AbstractHostScopedRolesTestCase
org.jboss.as.controller.operations.global.GlobalOperationHandlers
org.jboss.as.test.integration.domain.rbac.JmxRBACProviderHostScopedRolesTestCase
org.jboss.as.test.integration.domain.rbac.ListRoleNamesTestCase
org.jboss.as.test.integration.domain.rbac.WildcardReadsTestCase"
FILE,WFCORE,WFCORE-989,2015-09-19T03:11:07.000-05:00,Premature registration of reconnecting server,"{""server-group"" => {




        ""main-server-group"" => {""host"" => {""slave"" => {""main-three"" => ""WFLYHC0153: Channel closed""}
register ProxyController for reconnecting server
be not in state request state to server
call back with DomainServerProtocol.SERVER_RECONNECT_REQUEST
come in during window
If that request is part of a domain operation rollout (because during the window the HC regarded the server as 'live' and instructed the DC to send rollout ops to it), then the domain rollout will be affected.
This happened in the testsuite failure I just saw.
2015-09-18 17:59:32,229 INFO  [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Core 2.0.0.CR2-SNAPSHOT ""Kenny"" (Host Controller) started in 93ms - Started 48 of 51 services (15 services are lazy, passive or on-demand)
2015-09-18 17:59:32,247 INFO  [org.jboss.as.controller.management-operation] (Remoting ""slave:MANAGEMENT"" task-16) Initialized for 1849055188
2015-09-18 17:59:32,247 INFO  [org.jboss.as.host.controller] (Remoting ""slave:MANAGEMENT"" task-14) WFLYHC0021: Server [Server:main-three] connected using connection [Channel ID 08aa200b (inbound) of Remoting connection 27e3d90d to /127.0.0.1:49272]
2015-09-18 17:59:32,247 INFO  [org.jboss.as.host.controller] (Remoting ""slave:MANAGEMENT"" task-15) WFLYHC0021: Server [Server:other-two] connected using connection [Channel ID 02bd2646 (inbound) of Remoting connection 64a8b7f2 to /127.0.0.1:49273]
2015-09-18 17:59:32,247 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 34) Initialized for 1849055188
2015-09-18 17:59:32,252 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 34) sending prepared response for 1849055188  --- interrupted: false
2015-09-18 17:59:32,254 INFO  [org.jboss.as.controller.management-operation] (Remoting ""slave:MANAGEMENT"" task-1) Initialized for 1933145457
2015-09-18 17:59:32,254 INFO  [org.jboss.as.controller.management-operation] (Remoting ""slave:MANAGEMENT"" task-1) Initialized for 481103151
2015-09-18 17:59:32,254 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 36) Initialized for 1933145457
2015-09-18 17:59:32,254 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 37) Initialized for 481103151
2015-09-18 17:59:32,255 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 36) sending pre-prepare failed response for 1933145457  --- interrupted: false
2015-09-18 17:59:32,256 INFO  [org.jboss.as.controller.management-operation] (Host Controller Service Threads - 37) sending pre-prepare failed response for 481103151  --- interrupted: false
complete boot
send prepared response to request
invoke on servers
send pre-prepare failed response to requests
ask requests to proxy ask requests to servers
{
""operation"" => ""add"",
""address"" => [(""extension"" => ""org.jboss.as.test.hc.extension"")]
}
{
""outcome"" => ""failed"",
""result"" => undefined,
""failure-description"" => {""WFLYDC0074: Operation failed or was rolled back on all servers.
Server failures:"" => {""server-group"" => {
""main-server-group"" => {""host"" => {""slave"" => {""main-three"" => ""WFLYHC0153: Channel closed""}}},
""other-server-group"" => {""host"" => {""slave"" => {""other-two"" => ""WFLYHC0153: Channel closed""}}}
}}},
""rolled-back"" => true,
""server-groups"" => {
""main-server-group"" => {""host"" => {
""master"" => {""main-one"" => {""response"" => {
""outcome"" => ""failed"",
""result"" => [(""HC"" => ""1.1.1"")],
""rolled-back"" => true
}}},
""slave"" => {""main-three"" => {""response"" => {
""outcome"" => ""failed"",
""result"" => undefined,
""failure-description"" => ""WFLYHC0153: Channel closed"",
""rolled-back"" => true
}}}
}},
""other-server-group"" => {""host"" => {""slave"" => {""other-two"" => {""response"" => {
""outcome"" => ""failed"",
""result"" => undefined,
""failure-description"" => ""WFLYHC0153: Channel closed"",
""rolled-back"" => true
}}}}}
}
}
The ""WFLYHC0153: Channel closed"" failure is what is produced when an attempt is made to invoke on a disconnected proxy controller.","org.jboss.as.host.controller.ServerInventoryImpl
org.jboss.as.host.controller.mgmt.ServerToHostProtocolHandler"
FILE,WFCORE,WFCORE-1214,2015-12-11T23:17:45.000-06:00,Operation headers not propagated to domain servers when 'composite' op is used,"{blocking-timeout=5;rollback-on-runtime-failure=false}  
 {

[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3)     ""blocking-timeout"" => ""5"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""rollback-on-runtime-failure"" => ""false"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""caller-type"" => ""user"",

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""access-mechanism"" => ""NATIVE""

[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3) }
add request headers to op not propagate to servers not propagate during domain rollout
For example, if I add some stdout printing of what the headers are on the various processes and invoke this:
[domain@localhost:9990 /] deploy ~/tmp/helloworld.
war --headers={blocking-timeout=5;rollback-on-runtime-failure=false} --all-server-groups
log on HC
[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3) ""composite"" headers:
{
[Host Controller] 10:53:40,697 INFO  [stdout] (management-handler-thread - 3)     ""blocking-timeout"" => ""5"",
[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""rollback-on-runtime-failure"" => ""false"",
[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""caller-type"" => ""user"",
[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3)     ""access-mechanism"" => ""NATIVE""
[Host Controller] 10:53:40,698 INFO  [stdout] (management-handler-thread - 3) }
[Host Controller] 10:53:40,727 INFO  [org.jboss.as.repository] (management-handler-thread - 3) WFLYDR0001: Content added at location /Users/bstansberry/dev/wildfly/wildfly-core/dist/target/wildfly-core-2.0.5.
Final-SNAPSHOT/domain/data/content/6f/cd9eae343ed6d5aa9fffa83012d155b1ef911c/content
[Server:server-one] 10:53:40,772 INFO  [stdout] (ServerService Thread Pool  11) ""composite"" headers: null
[Server:server-two] 10:53:40,772 INFO  [stdout] (ServerService Thread Pool  11) ""composite"" headers: null
The HC logs, then the servers report.
The user-specified headers are not included.
Invoke the same op without the batch and this is logged:
[Host Controller] 10:43:50,400 INFO  [stdout] (management-handler-thread - 4) ""composite"" headers: {
[Host Controller] 10:43:50,401 INFO  [stdout] (management-handler-thread - 4)     ""blocking-timeout"" => ""5"",
[Host Controller] 10:43:50,401 INFO  [stdout] (management-handler-thread - 4)     ""rollback-on-runtime-failure"" => ""false"",
[Host Controller] 10:43:50,401 INFO  [stdout] (management-handler-thread - 4)     ""caller-type"" => ""user"",
[Host Controller] 10:43:50,401 INFO  [stdout] (management-handler-thread - 4)     ""access-mechanism"" => ""NATIVE""
[Host Controller] 10:43:50,401 INFO  [stdout] (management-handler-thread - 4) }
[Host Controller] 10:43:50,425 INFO  [org.jboss.as.repository] (management-handler-thread - 4) WFLYDR0001: Content added at location /Users/bstansberry/dev/wildfly/wildfly-core/dist/target/wildfly-core-2.0.5.
Final-SNAPSHOT/domain/data/content/6f/cd9eae343ed6d5aa9fffa83012d155b1ef911c/content
[Server:server-two] 10:43:50,464 INFO  [stdout] (ServerService Thread Pool -- 11) ""composite"" headers: {
[Server:server-two] 10:43:50,464 INFO  [stdout] (ServerService Thread Pool -- 11)     ""blocking-timeout"" => ""5"",
[Server:server-two] 10:43:50,464 INFO  [stdout] (ServerService Thread Pool -- 11)     ""rollback-on-runtime-failure"" => ""false"",
[Server:server-one] 10:43:50,464 INFO  [stdout] (ServerService Thread Pool -- 11) ""composite"" headers: {
[Server:server-two] 10:43:50,464 INFO  [stdout] (ServerService Thread Pool -- 11)     ""access-mechanism"" => ""NATIVE"",
[Server:server-one] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11)     ""blocking-timeout"" => ""5"",
[Server:server-two] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11)     ""domain-uuid"" => ""216d2e99-dba5-4c89-8020-b0c16bd553c5""
[Server:server-one] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11)     ""rollback-on-runtime-failure"" => ""false"",
[Server:server-two] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11) }
[Server:server-one] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11)     ""access-mechanism"" => ""NATIVE"",
[Server:server-one] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11)     ""domain-uuid"" => ""216d2e99-dba5-4c89-8020-b0c16bd553c5""
[Server:server-one] 10:43:50,465 INFO  [stdout] (ServerService Thread Pool -- 11) }
Expected headers are present.
note deploy
involve use among other places involve use of composite","org.jboss.as.domain.controller.operations.coordination.DomainRolloutStepHandler
org.jboss.as.domain.controller.operations.coordination.OperationCoordinatorStepHandler"
FILE,WFCORE,WFCORE-1198,2015-12-09T09:30:00.000-06:00,CLI does not resolve multiple properties if one property is undefined,"{PROFILE-NAME}  {APP-VERSION}  {VAR}  add(auto-start=true, group=""${PROFILE-NAME}${APP-VERSION}-server-group"")
https://bugzilla.redhat.com/show_bug.cgi?id=1289316 description:
have value work with EAP 6.4.3 +
For example :
cat props.properties
-----------------------------------------
PROFILE-NAME=TestProfile
SERVER-INSTANCE-NUMBER=TestInstance
APP-VERSION=
VAR=test
-----------------------------------------
cat test.cli :
---------------------------------------
/host=master/server-config=${PROFILE-NAME}${APP-VERSION}${SERVER-INSTANCE-NUMBER}${VAR}:add(auto-start=true, group=""${PROFILE-NAME}${APP-VERSION}-server-group"")
---------------------------------------
and if I execute "".
/jboss-cli.sh --connect --file=test.cli --properties=props.properties"" then I have the following in the host.xml "":
----------
...
<server name=""TestProfile${SERVER-INSTANCE-NUMBER}test"" group=""TestProfile-server-group"" auto-start=""true""/>
...
-----------
have value","org.jboss.as.cli.parsing.test.PropertyReplacementTestCase
org.jboss.as.cli.parsing.ExpressionBaseState"
FILE,WFCORE,WFCORE-701,2015-05-19T15:06:17.000-05:00,Inconsistent domain server status reports between server-config resource and server resource,"attribute(name=status)




 {




    ""outcome"" => ""success"",




    ""result"" => ""FAILED""




}




  attribute(name=server-state)




 {




    ""outcome"" => ""success"",




    ""result"" => ""STOPPED""




}
fail in way
To reproduce, run domain.sh, find the pid of a server process, and kill -9 <thepid>.
Then with the CLI:
[domain@localhost:9990 /] /host=master/server-config=server-two:read-attribute(name=status)
{
""outcome"" => ""success"",
""result"" => ""FAILED""
}
[domain@localhost:9990 /] /host=master/server=server-two:read-attribute(name=server-state)
{
""outcome"" => ""success"",
""result"" => ""STOPPED""
}",org.jboss.as.host.controller.ManagedServer
FILE,WFCORE,WFCORE-1570,2016-05-27T12:51:56.000-05:00,Saved rollout-plan 'name' or 'id' attribute discrepancy,"group(rolling-to-servers=false,max-failed-servers=1)  group(rolling-to-servers=true,max-failure-percentage=20)  
 {rollout id=my-rollout-plan}
When using rollout plans for EAP deployment scenarios I can create my own named rollout-plan for ease of use.
I can then apply rollout command later on, referring with name of my own rollout plan that should be used.
There is minor discrepancy in the way I create and use such rollout plan though.
When I create rollout-plan, I use command like:
rollout-plan add --name=my-rollout-plan --content={rollout main-server-group(rolling-to-servers=false,max-failed-servers=1),other-server-group(rolling-to-servers=true,max-failure-percentage=20) rollback-across-groups=true}
see name attribute name rollout plan
When I then refer to it I use following command:
deploy /path/to/test-application.
war --all-server-groups --headers={rollout id=my-rollout-plan}
see id attribute
Note: examples are used from our documentation.
Note: I do not know whether I am missing something but I was not able to retrieve more info how to use rollout header operation in deploy command directly in CLI.","org.jboss.as.cli.parsing.operation.header.RolloutPlanState
org.jboss.as.cli.parsing.operation.header.RolloutPlanHeaderCallbackHandler
org.jboss.as.cli.operation.impl.RolloutPlanCompleter"
FILE,WFCORE,WFCORE-1578,2016-06-07T05:13:13.000-05:00,Better check of names of existing resources when adding '{local|remote-destination-outbound-socket-binding',"{remote|local} 
   add()




    add(host=localhost,port=8765)




 
   add(socket-binding-ref=http)




 
  
  
     
  
 
  
 {remote|local}
Lets have some /socket-binding-group=standard-sockets/socket-binding with particular name.
Then when I create some /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding or /socket-binding-group=standard-sockets/local-destination-outbound-socket-binding using same name as of already existing socket-binding resource, add operation is successful but when I perform server reload, it crashes as it is not able to parse configuration.
See:
Start EAP and log to CLI create your own socket-binding resource and {remote|local}-destination-outbound-socket-binding resource with same names and perform reload
/socket-binding-group=standard-sockets/socket-binding=myBinding:add()
/socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=myBinding:add(host=localhost,port=8765)
or
/socket-binding-group=standard-sockets/local-destination-outbound-socket-binding=myBinding:add(socket-binding-ref=http)
reload
crash with following stacktrace
17:31:40,447 INFO  [org.jboss.as.connector.deployers.jdbc] (MSC service thread 1-7) WFLYJCA0019: Stopped Driver service with driver-name = h2
17:31:40,453 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0008: Undertow HTTP listener default suspending
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-8) WFLYUT0007: Undertow HTTP listener default stopped, was bound to 127.0.0.1:8080
17:31:40,454 INFO  [org.wildfly.extension.undertow] (MSC service thread 1-3) WFLYUT0004: Undertow 1.3.21.Final-redhat-1 stopping
17:31:40,458 INFO  [org.jboss.as.mail.extension] (MSC service thread 1-7) WFLYMAIL0002: Unbound mail session [java:jboss/mail/Default]
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 22ms
17:31:40,461 INFO  [org.jboss.as] (MSC service thread 1-5) WFLYSRV0049: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) starting
17:31:40,489 ERROR [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0055: Caught exception during boot: org.jboss.as.controller.persistence.ConfigurationPersistenceException: WFLYCTL0085: Failed to parse configuration
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:131)
at org.jboss.as.server.ServerService.boot(ServerService.java:356)
at org.jboss.as.controller.AbstractControllerService$1.run(AbstractControllerService.java:299)
at java.lang.Thread.run(Thread.java:745)
Caused by: javax.xml.stream.XMLStreamException: ParseError at [row,col]:[410,9]
Message: WFLYCTL0042: A socket-binding or a outbound-socket-binding myBinding already declared has already been declared in socket-binding-group standard-sockets
at org.jboss.as.server.parsing.StandaloneXml_4.
parseSocketBindingGroup(StandaloneXml_4.java:518)
at org.jboss.as.server.parsing.StandaloneXml_4.
readServerElement(StandaloneXml_4.java:254)
at org.jboss.as.server.parsing.StandaloneXml_4.
readElement(StandaloneXml_4.java:141)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:103)
at org.jboss.as.server.parsing.StandaloneXml.readElement(StandaloneXml.java:49)
at org.jboss.staxmapper.XMLMapperImpl.processNested(XMLMapperImpl.java:110)
at org.jboss.staxmapper.XMLMapperImpl.parseDocument(XMLMapperImpl.java:69)
at org.jboss.as.controller.persistence.XmlConfigurationPersister.load(XmlConfigurationPersister.java:123)
... 3 more
17:31:40,490 FATAL [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0056: Server boot has failed in an unrecoverable manner; exiting.
See previous messages for details.
17:31:40,491 INFO  [org.jboss.as.server] (Thread-2) WFLYSRV0220: Server shutdown has been requested.
17:31:40,496 INFO  [org.jboss.as] (MSC service thread 1-2) WFLYSRV0050: JBoss EAP 7.0.0.
GA (WildFly Core 2.1.2.Final-redhat-1) stopped in 3ms
After this occurs, one needs to fix .
/standalone/configuration/standalone.xml manually by removing duplicate resources.
Note: not sure whether CLI component is appropriate, please change if there is better component for this.","org.jboss.as.controller.logging.ControllerLogger
org.jboss.as.server.services.net.LocalDestinationOutboundSocketBindingAddHandler
org.jboss.as.server.services.net.SocketBindingAddHandler
org.jboss.as.server.services.net.RemoteDestinationOutboundSocketBindingAddHandler"
FILE,WFCORE,WFCORE-1635,2016-07-05T07:04:51.000-05:00,Write attribute on a new deployment scanner fails in batch,"add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)




 
 
 add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)




  attribute(name=scan-interval, value=6000)
Creating a new deployment-scanner and altering it's attribute fails if done in single batch.
run commands without batch run batch on CLI embed-server
reproduce
batch
/subsystem=deployment-scanner/scanner=scan:add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)
/subsystem=deployment-scanner/scanner=scan:write-attribute(name=scan-interval, value=6000)
run-batch
fails with
08:09:19,076 ERROR [org.jboss.as.controller.management-operation] (management-handler-thread - 4) WFLYCTL0013: Operation (""write-attribute"") failed - address: ([
(""subsystem"" => ""deployment-scanner""),
(""scanner"" => ""scan"")
]): java.lang.IllegalStateException
at org.jboss.as.server.deployment.scanner.DeploymentScannerService.getValue(DeploymentScannerService.java:234)
at org.jboss.as.server.deployment.scanner.DeploymentScannerService.getValue(DeploymentScannerService.java:62)
at org.jboss.msc.service.ServiceControllerImpl.getValue(ServiceControllerImpl.java:1158)
at org.jboss.as.controller.OperationContextImpl$OperationContextServiceController.getValue(OperationContextImpl.java:2282)
at org.jboss.as.server.deployment.scanner.AbstractWriteAttributeHandler.applyUpdateToRuntime(AbstractWriteAttributeHandler.java:58)
at org.jboss.as.controller.AbstractWriteAttributeHandler$1.execute(AbstractWriteAttributeHandler.java:104)
at org.jboss.as.controller.AbstractOperationContext.executeStep(AbstractOperationContext.java:890)
at org.jboss.as.controller.AbstractOperationContext.processStages(AbstractOperationContext.java:659)
at org.jboss.as.controller.AbstractOperationContext.executeOperation(AbstractOperationContext.java:370)
at org.jboss.as.controller.OperationContextImpl.executeOperation(OperationContextImpl.java:1344)
at org.jboss.as.controller.ModelControllerImpl.internalExecute(ModelControllerImpl.java:392)
at org.jboss.as.controller.ModelControllerImpl.execute(ModelControllerImpl.java:217)
at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.doExecute(ModelControllerClientOperationHandler.java:208)
at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler.access$300(ModelControllerClientOperationHandler.java:130)
at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:152)
at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1$1.run(ModelControllerClientOperationHandler.java:148)
at java.security.AccessController.doPrivileged(AccessController.java:686)
at javax.security.auth.Subject.doAs(Subject.java:569)
at org.jboss.as.controller.AccessAuditContext.doAs(AccessAuditContext.java:92)
at org.jboss.as.controller.remote.ModelControllerClientOperationHandler$ExecuteRequestHandler$1.execute(ModelControllerClientOperationHandler.java:148)
at org.jboss.as.protocol.mgmt.AbstractMessageHandler$ManagementRequestContextImpl$1.doExecute(AbstractMessageHandler.java:363)
at org.jboss.as.protocol.mgmt.AbstractMessageHandler$AsyncTaskRunner.run(AbstractMessageHandler.java:472)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.lang.Thread.run(Thread.java:785)
at org.jboss.threads.JBossThread.run(JBossThread.java:320)
using the embed server works
embed-server
batch
/subsystem=deployment-scanner/scanner=scan:add(path=log, relative-to=""jboss.server.base.dir"", auto-deploy-exploded=false, scan-enabled=false)
/subsystem=deployment-scanner/scanner=scan:write-attribute(name=scan-interval, value=6000)
run-batch
Setting only as minor as there is no real use case behind this (scan-interval can be set while adding a new scanner) - run into it quite accidentally.
No regression against previous release.",org.jboss.as.server.deployment.scanner.AbstractWriteAttributeHandler
FILE,WFCORE,WFCORE-1590,2016-06-12T14:18:43.000-05:00,Default parameter length validating ignores setMinSize(0),"static final SimpleAttributeDefinition REPLACEMENT = new SimpleAttributeDefinitionBuilder(ElytronDescriptionConstants.REPLACEMENT, ModelType.STRING, false)




        .setAllowExpression(true)




        .setMinSize(0)




        .setFlags(AttributeAccess.Flag.RESTART_RESOURCE_SERVICES)




        .build();






 
 add(pattern=""@ELYTRON.ORG"", replacement="""", replace-all=true)
With the following attribute definition: -
static final SimpleAttributeDefinition REPLACEMENT = new SimpleAttributeDefinitionBuilder(ElytronDescriptionConstants.REPLACEMENT, ModelType.STRING, false)
.
setAllowExpression(true)
.
setMinSize(0)
.
setFlags(AttributeAccess.Flag.RESTART_RESOURCE_SERVICES)
.
build();
The following error is reported if an empty string is used as a parameter: -
[standalone@localhost:9990 /] .
/subsystem=elytron/regex-name-rewriter=strip-realm:add(pattern=""@ELYTRON.
ORG"", replacement="""", replace-all=true)
{
""outcome"" => ""failed"",
""failure-description"" => ""WFLYCTL0113: '' is an invalid value for parameter replacement.
Values must have a minimum length of 1 characters"",
""rolled-back"" => true
}","org.jboss.as.controller.operations.validation.BytesValidator
org.jboss.as.controller.SimpleAttributeDefinitionUnitTestCase
org.jboss.as.controller.test.WriteAttributeOperationTestCase
org.jboss.as.controller.AbstractAttributeDefinitionBuilder
org.jboss.as.controller.AttributeDefinition"
FILE,WFCORE,WFCORE-1718,2016-08-16T09:49:11.000-05:00,Handlers within Audit Logger are not removed properly when Audit Logger is removed,"remove()
  remove()
 
 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0158: Operation handler failed: java.lang.NullPointerException"",




    ""rolled-back"" => true




}






   
 attribute(name=level,value=DEBUG)
This leads to a couple of issues:
remove referenced File/syslog handlers
If user tries to remove them the NullPointerException is given as a result.
Try following commands:
/core-service=management/access=audit/file-handler=file:remove()
/core-service=management/access=audit/syslog-handler=my-syslog-handler:remove()
{
""outcome"" => ""failed"",
""failure-description"" => ""WFLYCTL0158: Operation handler failed: java.lang.NullPointerException"",
""rolled-back"" => true
}
send auditable events to referenced File/syslog handlers
Create auditable event (e.g. /subsystem=logging/logger=com.arjuna:write-attribute(name=level,value=DEBUG))
log in file
log in syslog","org.jboss.as.domain.management.audit.AuditLogLoggerResourceDefinition
org.jboss.as.domain.management.audit.AccessAuditResourceDefinition
org.jboss.as.domain.management.audit.AuditLogHandlerReferenceResourceDefinition"
FILE,WFCORE,WFCORE-1765,2016-09-05T16:22:02.000-05:00,unclear NullPointerException if the deployment-scanner element is removed from the configuration,"{xml}
         {xml}
If the deployment scanner element is removed from the configuration of the standalone server a NullPointerException is logged which is unclear and difficult to find as the stack does not show any hint.
Config:
{xml}
<subsystem xmlns=""urn:jboss:domain:deployment-scanner:2.0"">
<!-- deployment-scanner path=""deployments"" relative-to=""jboss.server.base.dir"" scan-interval=""5000"" runtime-failure-causes-rollback=""${jboss.deployment.scanner.rollback.on.failure:false}""/ -->
</subsystem>{xml}
ERROR [org.jboss.as.controller.management-operation] (ServerService Thread Pool  34) WFLYCTL0403: Unexpected failure during execution of the following operation(s): []: java.lang.NullPointerException
at org.jboss.as.controller.AbstractOperationContext$Step.access$300(AbstractOperationContext.java:1185)
at org.jboss.as.controller.AbstractOperationContext.executeResultHandlerPhase(AbstractOperationContext.java:767)
at org.jboss.as.controller.AbstractOperationContext.executeDoneStage(AbstractOperationContext.java:753)
at org.jboss.as.controller.AbstractOperationContext.processStages(AbstractOperationContext.java:680)
at org.jboss.as.controller.AbstractOperationContext.executeOperation(AbstractOperationContext.java:370)
at org.jboss.as.controller.ParallelBootOperationStepHandler$ParallelBootTask.run(ParallelBootOperationStepHandler.java:359)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
at org.jboss.threads.JBossThread.run(JBossThread.java:320)",org.jboss.as.controller.ParallelBootOperationStepHandler
FILE,WFCORE,WFCORE-1793,2016-09-14T08:08:21.000-05:00,add-content operation fails to overwrite existing content with overwrite=true set when passing content by file path,"{""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




 
 {""outcome"" => ""success""}




  {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt} 
  
 {path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}
Upon overwriting content in managed exploded deployments on wildfly-core, the following errors are produced:
CLI
[standalone@localhost:9990 /] deploy /home/mjurc/testing/eap7-204/jboss-kitchensink-ear.
ear
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:undeploy
{""outcome"" => ""success""}
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:explode
{""outcome"" => ""success""}
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:deploy
{""outcome"" => ""success""}
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:add-content(content=[{path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}], overwrite=true)
{
""outcome"" => ""failed"",
""failure-description"" => ""org.jboss.as.repository.ExplodedContentException: WFLYDR0021: Error updating content of exploded deployment"",
""rolled-back"" => true
}
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:add-content(content=[{path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}], overwrite=false)
{
""outcome"" => ""failed"",
""failure-description"" => ""org.jboss.as.repository.ExplodedContentException: WFLYDR0021: Error updating content of exploded deployment"",
""rolled-back"" => true
}
[standalone@localhost:9990 /] /deployment=jboss-kitchensink-ear.
ear:add-content(content=[{path=/home/mjurc/testing/eap7-204/test.txt, target-path=test.txt}])
{
""outcome"" => ""failed"",
""failure-description"" => ""org.jboss.as.repository.ExplodedContentException: WFLYDR0021: Error updating content of exploded deployment"",
""rolled-back"" => true
}
Server
09:41:36,029 WARN  [org.jboss.as.repository] (management-handler-thread - 5) java.nio.file.FileAlreadyExistsException: /home/mjurc/testing/wildfly-core/build/target/wildfly-core-3.0.0.
Alpha8-SNAPSHOT/standalone/data/content/content7797527203290314566/content/test.txt
09:45:27,505 WARN  [org.jboss.as.repository] (management-handler-thread - 12) java.nio.file.FileAlreadyExistsException: /home/mjurc/testing/wildfly-core/build/target/wildfly-core-3.0.0.
Alpha8-SNAPSHOT/standalone/data/content/content721393778736298367/content/test.txt
09:45:36,352 WARN  [org.jboss.as.repository] (management-handler-thread - 10) java.nio.file.FileAlreadyExistsException: /home/mjurc/testing/wildfly-core/build/target/wildfly-core-3.0.0.
Alpha8-SNAPSHOT/standalone/data/content/content344811471223714239/content/test.txt
This issue does not seem to arise when the content is passed to the server by input stream index.","org.jboss.as.server.controller.resources.DeploymentAttributes
org.jboss.as.server.deployment.ExplodedDeploymentAddContentHandler"
FILE,WFCORE,WFCORE-1864,2016-10-13T09:12:31.000-05:00,Whitespaces are not removed from dependencies in module add command,"{{
...
    <dependencies>
        <module name=""org.a""/>
        <module name="" org.b ""/>
    </dependencies>
...
}}
Running module add --name=foo.bar --resources=foo.jar --dependencies=[org.a, org.b ] will result in following dependencies in module.xml
{{
...
<dependencies>
<module name=""org.a""/>
<module name="" org.b ""/>
</dependencies>
...
}}","org.jboss.as.cli.handlers.module.ASModuleHandler
org.jboss.as.test.integration.management.cli.ModuleTestCase"
FILE,WFCORE,WFCORE-1908,2016-10-31T08:13:57.000-05:00,Tab completion suggest writing attribute which has access type metric and is not writable,"attribute(name=message-count, value=5)




 {




    ""outcome"" => ""failed"",




    ""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",




    ""rolled-back"" => true




}
suggest attributes
Example
/subsystem=messaging-activemq/server=default/jms-queue=DLQ:write-attribute(name=<TAB>
consumer-count  delivering-count  entries  legacy-entries  message-count  messages-added  scheduled-count
From executing :read-resource-description we can see, attributes consumer-count, delivering-count, message-count, messages-added, scheduled-count are of type metric.
On attempt to write metric attribute, for example message-count, non writable error is printed
[standalone@localhost:9990 jms-queue=q] :write-attribute(name=message-count, value=5)
{
""outcome"" => ""failed"",
""failure-description"" => ""WFLYCTL0048: Attribute message-count is not writable"",
""rolled-back"" => true
}","org.jboss.as.cli.impl.AttributeNamePathCompleter
org.jboss.as.cli.parsing.test.AttributeNamePathCompletionTestCase
org.jboss.as.cli.Util"
FILE,WFCORE,WFCORE-1936,2016-11-04T10:57:06.000-05:00,"Value of parameters ""restart-required"" for fixed-*port attributes does not match reality for socket-binding and *-destination-outbound-socket-binding in CLI","description(recursive=true)
But reality is different.
If you tries to change such attributes you are informed that reload is necessary.
The attributes are defined as ""restart-required"" => ""no-services"", see /socket-binding-group=standard-sockets:read-resource-description(recursive=true)","org.jboss.as.server.services.net.OutboundSocketBindingResourceDefinition
org.jboss.as.controller.resource.AbstractSocketBindingResourceDefinition"
FILE,WFCORE,WFCORE-1959,2016-11-08T16:28:30.000-06:00,Deploying an empty managed exploded deployment to server group in domain fails,"{empty=true} 
 add()
Deploying an empty exploded deployment created on domain controller fails with the following:
[domain@localhost:9990 /] /deployment=empty-deployment.jar:add(content=[{empty=true}])
{
""outcome"" => ""success"",
""result"" => undefined,
""server-groups"" => undefined
}
[domain@localhost:9990 /] /server-group=main-server-group/deployment=empty-deployment.jar:add()
{
""outcome"" => ""failed"",
""result"" => undefined,
""failure-description"" => {""WFLYDC0074: Operation failed or was rolled back on all servers.
Server failures:"" => {""server-group"" => {""main-server-group"" => {""host"" => {""master"" => {
""server-one"" => ""WFLYSRV0201: Cannot have more than one of [bytes, input-stream-index, hash, url, empty]"",
""server-two"" => ""WFLYSRV0201: Cannot have more than one of [bytes, input-stream-index, hash, url, empty]""
}}}}}},
""rolled-back"" => true,
""server-groups"" => {""main-server-group"" => {""host"" => {""master"" => {
""server-one"" => {""response"" => {
""outcome"" => ""failed"",
""failure-description"" => ""WFLYSRV0201: Cannot have more than one of [bytes, input-stream-index, hash, url, empty]"",
""rolled-back"" => true
}},
""server-two"" => {""response"" => {
""outcome"" => ""failed"",
""failure-description"" => ""WFLYSRV0201: Cannot have more than one of [bytes, input-stream-index, hash, url, empty]"",
""rolled-back"" => true
}}
}}}}
}",org.jboss.as.domain.controller.operations.coordination.ServerOperationResolver
METHOD,eclipse-2.0,31779,2003-02-13T09:55:00.000-06:00,[resources] UnifiedTree should ensure file/folder exists,"getStat()
Build: I20030211 using natives (Linux/Windows)
find new file from file system
not exist for different reasons
This problem appears to the user when executing refresh operations.
At the first moment, the file is found in the file system and assumed to be a folder, and a corresponding resource is created in the workspace.
At the second refresh, the folder corresponding to that resource is not found in the file system, and then it is removed from the workspace.
Bugs that revealed this problem: bug 21217 and bug 13463.","org.eclipse.core.internal.localstore.UnifiedTree:addChildrenFromFileSystem(UnifiedTreeNode, String, Object[], int)
org.eclipse.core.internal.localstore.UnifiedTree:createChildNodeFromFileSystem(UnifiedTreeNode, String, String)"
